{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  CSV erkennen by David Maksimovic 24.06.2019\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "#import keras\n",
    "\n",
    "#from keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,LeakyReLU, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot_ng as pydot\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "########### Normalisieren ###########\n",
    "\n",
    "#Ist schon normalisiert\n",
    "########### Normalisieren ###########\n",
    "\n",
    "#Ist schon normalisiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(15x40)_120k_Files.pickle\",\"rb\"))\n",
    "Y=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(15x40)_120k_Files.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much from one kind, how much from the other: \n",
      " [59977 60028]\n",
      "How do they look like? \n",
      " [[0 1]\n",
      " [1 0]]\n",
      "Percentage of one kind: \n",
      " 50.021249114620225\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(Y, return_counts=True, axis=0)\n",
    "print(\"How much from one kind, how much from the other: \\n\",counts)\n",
    "print(\"How do they look like? \\n\",unique)\n",
    "print(\"Percentage of one kind: \\n\", 100/(counts[0]+counts[1])*counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cbook as cbook\n",
    "from matplotlib.colors import DivergingNorm\n",
    "from matplotlib import ticker, cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0529 12:53:31.541464 23116 font_manager.py:1282] findfont: Font family ['normal'] not found. Falling back to DejaVu Sans.\n",
      "W0529 12:53:31.556986 23116 font_manager.py:1282] findfont: Font family ['normal'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1112.73x589.091 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1112.73x589.091 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1112.73x589.091 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1112.73x589.091 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1112.73x589.091 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1112.73x589.091 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 256)\n",
    "newcolors = viridis(np.linspace(0, 1, 256))\n",
    "black = np.array([0, 0, 0, 1])\n",
    "newcolors[0, :] = black\n",
    "\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "viridis_r = cm.get_cmap('viridis_r', 256)\n",
    "newcolors_r = viridis_r(np.linspace(0, 1, 256))\n",
    "black_r = np.array([0, 0, 0, 1])\n",
    "newcolors_r[0, :] = black_r\n",
    "\n",
    "newcmp_r = ListedColormap(newcolors_r)\n",
    "plt.rcParams['figure.figsize'] = [17/1.1, 9/1.1]\n",
    "\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 22}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "\n",
    "SMALL_SIZE = 13\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE) # fontsize of the figure title\n",
    "title_font = {'fontname':'Arial', 'size':'20', 'color':'black', 'weight':'bold',\n",
    "              #'verticalalignment':'bottom'\n",
    "             }\n",
    "\n",
    "\n",
    "b=range(0,6,1)#130 im Normalen set           und  im Testset 211 und 611\n",
    "for a in b:\n",
    "    if (YTraining[a] == (1,0)).all():\n",
    "        Title=\"Muon\"\n",
    "        \n",
    "        \n",
    "        #print(Title)\n",
    "    else:\n",
    "        Title=\"Electron\"\n",
    "        #print(Title)\n",
    "    #4, 15\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2) #ncols=2\n",
    "\n",
    "    img1 = ax1.imshow(XTraining[a,:,:,0], cmap=newcmp, interpolation='None',extent=[-180,180,-90,90])\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    ax1.set_title('Charge')\n",
    "    ax1.set_xlabel('Theta')\n",
    "    ax1.set_ylabel('Phi')\n",
    "    ax1.text(120, 75, '{}'.format(Title), #style='italic',\n",
    "            bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 5})\n",
    "    cbar = fig.colorbar(img1, cax=cax1)\n",
    "    cbar.set_label('Normed charge', rotation=270)\n",
    "    cbar.ax.get_yaxis().labelpad = 15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #img2 = ax2.imshow(X[a,:,:,1], cmap='twilight_shifted', interpolation='None',norm=DivergingNorm(0.18), extent=[-180,180,-90,90])\n",
    "    img2 = ax2.imshow(XTraining[a,:,:,1], cmap=newcmp_r, interpolation='None',\n",
    "                      #norm=DivergingNorm(0.105),\n",
    "                      extent=[-180,180,-90,90])\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    ax2.set_title('Time')\n",
    "    ax2.set_xlabel('Theta')\n",
    "    ax2.set_ylabel('Phi')\n",
    "    ax2.text(120, 75, '{}'.format(Title), #style='italic',\n",
    "            bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 5})\n",
    "\n",
    "    ax2.text(215, -75, 'Early') #style='italic')\n",
    "            #bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 5})\n",
    "    ax2.text(215, +70, 'Late')\n",
    "\n",
    "\n",
    "\n",
    "    cbar2 = fig.colorbar(img2, cax=cax2)\n",
    "    cbar2.set_label('Normed time', rotation=270)\n",
    "    cbar2.ax.get_yaxis().labelpad = 15\n",
    "\n",
    "\n",
    "    plt.tight_layout(h_pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 23k Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "(17000, 10, 16, 2) (2500, 10, 16, 2) (4052, 10, 16, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(X, Y))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:5]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:17000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[17000:19500]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[19500:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)\n",
    "del X,Y,X1,X2,X3,Y1,Y2,Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 120k Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "(85000, 15, 40, 2) (20000, 15, 40, 2) (15005, 15, 40, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(X, Y))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:5]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:85000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[85000:105000]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[105000:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)\n",
    "del X,Y,X1,X2,X3,Y1,Y2,Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do I want 50/50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTrainingnew=[]\n",
    "XTrainingnew=[]\n",
    "index=index2=index3=0\n",
    "for i in YTraining:\n",
    "    \n",
    "    if i[0]==1 and index2 <35000:\n",
    "        XTrainingnew.append(XTraining[index])\n",
    "        YTrainingnew.append(YTraining[index])\n",
    "        index2=index2+1\n",
    "        \n",
    "    if i[1]==1 and index3 <35000:\n",
    "        XTrainingnew.append(XTraining[index])\n",
    "        YTrainingnew.append(YTraining[index])\n",
    "        index3=index3+1\n",
    "        \n",
    "    index=index+1\n",
    "YTrainingnew=np.array(YTrainingnew)\n",
    "XTrainingnew=np.array(XTrainingnew)\n",
    "\n",
    "### Val:\n",
    "Yvalnew=[]\n",
    "XValnew=[]\n",
    "index=index2=index3=0\n",
    "for i in Yval:\n",
    "    \n",
    "    if i[0]==1 and index2 <7000:\n",
    "        XValnew.append(XVal[index])\n",
    "        Yvalnew.append(Yval[index])\n",
    "        index2=index2+1\n",
    "        \n",
    "    if i[1]==1 and index3 <7000:\n",
    "        XValnew.append(XVal[index])\n",
    "        Yvalnew.append(Yval[index])\n",
    "        index3=index3+1\n",
    "        \n",
    "    index=index+1\n",
    "Yvalnew=np.array(Yvalnew)\n",
    "XValnew=np.array(XValnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testen der besten Methode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_ints = [y.argmax() for y in YTrainingnew]\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_ints),\n",
    "                                                 y_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelle Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 12:56:29.027853 23116 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                60050     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 60,352\n",
      "Trainable params: 60,252\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 49us/sample - loss: 0.5907 - acc: 0.6830 - val_loss: 0.5718 - val_acc: 0.6981\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 4s 45us/sample - loss: 0.5709 - acc: 0.6948 - val_loss: 0.5667 - val_acc: 0.7004\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5691 - acc: 0.6984 - val_loss: 0.5693 - val_acc: 0.7002\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5681 - acc: 0.6975 - val_loss: 0.5649 - val_acc: 0.7006\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5673 - acc: 0.6992 - val_loss: 0.5676 - val_acc: 0.7009\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5664 - acc: 0.6987 - val_loss: 0.5653 - val_acc: 0.6999\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5668 - acc: 0.6988 - val_loss: 0.5644 - val_acc: 0.7010\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5659 - acc: 0.6994 - val_loss: 0.5644 - val_acc: 0.7019\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5652 - acc: 0.7009 - val_loss: 0.5645 - val_acc: 0.7023\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5646 - acc: 0.7014 - val_loss: 0.5622 - val_acc: 0.7025\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5639 - acc: 0.7015 - val_loss: 0.5621 - val_acc: 0.7010\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5628 - acc: 0.7012 - val_loss: 0.5614 - val_acc: 0.7028\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5618 - acc: 0.7024 - val_loss: 0.5585 - val_acc: 0.7032\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5583 - acc: 0.7047 - val_loss: 0.5551 - val_acc: 0.7042\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5530 - acc: 0.7078 - val_loss: 0.5433 - val_acc: 0.7139\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5415 - acc: 0.7146 - val_loss: 0.5277 - val_acc: 0.7289\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5334 - acc: 0.7228 - val_loss: 0.5223 - val_acc: 0.7334\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5291 - acc: 0.7254 - val_loss: 0.5193 - val_acc: 0.7343\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5255 - acc: 0.7283 - val_loss: 0.5172 - val_acc: 0.7348\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5225 - acc: 0.7312 - val_loss: 0.5141 - val_acc: 0.7397\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5197 - acc: 0.7309 - val_loss: 0.5101 - val_acc: 0.7426\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5158 - acc: 0.7361 - val_loss: 0.5071 - val_acc: 0.7423\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5131 - acc: 0.7370 - val_loss: 0.5045 - val_acc: 0.7459\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5106 - acc: 0.7377 - val_loss: 0.5025 - val_acc: 0.7488\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5087 - acc: 0.7402 - val_loss: 0.4995 - val_acc: 0.7495\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5052 - acc: 0.7413 - val_loss: 0.4980 - val_acc: 0.7518\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5022 - acc: 0.7444 - val_loss: 0.4966 - val_acc: 0.7538\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.4990 - acc: 0.7462 - val_loss: 0.4949 - val_acc: 0.7552\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4983 - acc: 0.7472 - val_loss: 0.4945 - val_acc: 0.7570\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4964 - acc: 0.7482 - val_loss: 0.4930 - val_acc: 0.7570\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               120100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 120,702\n",
      "Trainable params: 120,502\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 45us/sample - loss: 0.5908 - acc: 0.6830 - val_loss: 0.5713 - val_acc: 0.6969\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5713 - acc: 0.6952 - val_loss: 0.5680 - val_acc: 0.7003\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5695 - acc: 0.6967 - val_loss: 0.5677 - val_acc: 0.7007\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5685 - acc: 0.6990 - val_loss: 0.5698 - val_acc: 0.7012\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5678 - acc: 0.6990 - val_loss: 0.5656 - val_acc: 0.7027\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5673 - acc: 0.6996 - val_loss: 0.5691 - val_acc: 0.6982\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5665 - acc: 0.6993 - val_loss: 0.5655 - val_acc: 0.7019\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5658 - acc: 0.7000 - val_loss: 0.5669 - val_acc: 0.7017\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5664 - acc: 0.7004 - val_loss: 0.5634 - val_acc: 0.7010\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5643 - acc: 0.7002 - val_loss: 0.5622 - val_acc: 0.7006\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5627 - acc: 0.7025 - val_loss: 0.5567 - val_acc: 0.7049\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5547 - acc: 0.7076 - val_loss: 0.5417 - val_acc: 0.7167\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5441 - acc: 0.7148 - val_loss: 0.5292 - val_acc: 0.7273\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5351 - acc: 0.7228 - val_loss: 0.5238 - val_acc: 0.7311\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5320 - acc: 0.7237 - val_loss: 0.5231 - val_acc: 0.7335\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5288 - acc: 0.7267 - val_loss: 0.5185 - val_acc: 0.7337\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5231 - acc: 0.7294 - val_loss: 0.5144 - val_acc: 0.7383\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5192 - acc: 0.7343 - val_loss: 0.5104 - val_acc: 0.7424\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5136 - acc: 0.7394 - val_loss: 0.5020 - val_acc: 0.7488\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5061 - acc: 0.7426 - val_loss: 0.4968 - val_acc: 0.7534\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5023 - acc: 0.7478 - val_loss: 0.4942 - val_acc: 0.7558\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5001 - acc: 0.7477 - val_loss: 0.4928 - val_acc: 0.7553\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.4960 - acc: 0.7509 - val_loss: 0.4905 - val_acc: 0.7570\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4936 - acc: 0.7511 - val_loss: 0.4891 - val_acc: 0.7585\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4921 - acc: 0.7524 - val_loss: 0.4877 - val_acc: 0.7593\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4897 - acc: 0.7544 - val_loss: 0.4878 - val_acc: 0.7598\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4872 - acc: 0.7550 - val_loss: 0.4871 - val_acc: 0.7602\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4869 - acc: 0.7560 - val_loss: 0.4885 - val_acc: 0.7623\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4840 - acc: 0.7570 - val_loss: 0.4865 - val_acc: 0.7622\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4808 - acc: 0.7617 - val_loss: 0.4865 - val_acc: 0.7612\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 160)               192160    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 193,122\n",
      "Trainable params: 192,802\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5939 - acc: 0.6819 - val_loss: 0.5734 - val_acc: 0.6991\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5715 - acc: 0.6954 - val_loss: 0.5669 - val_acc: 0.6973\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5700 - acc: 0.6956 - val_loss: 0.5699 - val_acc: 0.6969\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5697 - acc: 0.6968 - val_loss: 0.5673 - val_acc: 0.7004\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5685 - acc: 0.6976 - val_loss: 0.5683 - val_acc: 0.6993\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5677 - acc: 0.6974 - val_loss: 0.5666 - val_acc: 0.7005\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5675 - acc: 0.6981 - val_loss: 0.5651 - val_acc: 0.7016\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5668 - acc: 0.6989 - val_loss: 0.5670 - val_acc: 0.7022\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5661 - acc: 0.7000 - val_loss: 0.5648 - val_acc: 0.7012\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 45us/sample - loss: 0.5651 - acc: 0.7004 - val_loss: 0.5628 - val_acc: 0.7010\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5614 - acc: 0.7025 - val_loss: 0.5551 - val_acc: 0.7057\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5529 - acc: 0.7090 - val_loss: 0.5407 - val_acc: 0.7199\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5454 - acc: 0.7147 - val_loss: 0.5327 - val_acc: 0.7261\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5393 - acc: 0.7182 - val_loss: 0.5277 - val_acc: 0.7293\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5344 - acc: 0.7224 - val_loss: 0.5237 - val_acc: 0.7314\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5293 - acc: 0.7250 - val_loss: 0.5187 - val_acc: 0.7345\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5239 - acc: 0.7304 - val_loss: 0.5118 - val_acc: 0.7401\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5175 - acc: 0.7336 - val_loss: 0.5082 - val_acc: 0.7424\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5135 - acc: 0.7371 - val_loss: 0.5029 - val_acc: 0.7467\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5121 - acc: 0.7374 - val_loss: 0.5006 - val_acc: 0.7503\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5078 - acc: 0.7403 - val_loss: 0.4980 - val_acc: 0.7516\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5036 - acc: 0.7439 - val_loss: 0.4958 - val_acc: 0.7528\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5020 - acc: 0.7454 - val_loss: 0.4932 - val_acc: 0.7571\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5009 - acc: 0.7464 - val_loss: 0.4933 - val_acc: 0.7546\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4980 - acc: 0.7488 - val_loss: 0.4916 - val_acc: 0.7593\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.4950 - acc: 0.7506 - val_loss: 0.4911 - val_acc: 0.7566\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4914 - acc: 0.7526 - val_loss: 0.4883 - val_acc: 0.7596\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4893 - acc: 0.7542 - val_loss: 0.4874 - val_acc: 0.7605\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4861 - acc: 0.7558 - val_loss: 0.4870 - val_acc: 0.7607\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.4836 - acc: 0.7579 - val_loss: 0.4862 - val_acc: 0.7619\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               360300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 362,102\n",
      "Trainable params: 361,502\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5964 - acc: 0.6810 - val_loss: 0.5753 - val_acc: 0.6988\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5732 - acc: 0.6946 - val_loss: 0.5735 - val_acc: 0.6956\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5717 - acc: 0.6946 - val_loss: 0.5680 - val_acc: 0.7009\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5701 - acc: 0.6961 - val_loss: 0.5755 - val_acc: 0.7000\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5700 - acc: 0.6969 - val_loss: 0.5701 - val_acc: 0.6993\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5687 - acc: 0.6974 - val_loss: 0.5696 - val_acc: 0.7005\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5684 - acc: 0.6976 - val_loss: 0.5659 - val_acc: 0.7017\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5673 - acc: 0.6987 - val_loss: 0.5655 - val_acc: 0.6992\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5668 - acc: 0.6991 - val_loss: 0.5652 - val_acc: 0.7006\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5634 - acc: 0.7011 - val_loss: 0.5519 - val_acc: 0.7088\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5524 - acc: 0.7089 - val_loss: 0.5411 - val_acc: 0.7222\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5469 - acc: 0.7129 - val_loss: 0.5362 - val_acc: 0.7224\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5432 - acc: 0.7157 - val_loss: 0.5325 - val_acc: 0.7248\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5406 - acc: 0.7181 - val_loss: 0.5313 - val_acc: 0.7264\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5382 - acc: 0.7182 - val_loss: 0.5302 - val_acc: 0.7244\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5314 - acc: 0.7251 - val_loss: 0.5202 - val_acc: 0.7345\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5230 - acc: 0.7309 - val_loss: 0.5099 - val_acc: 0.7409\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5126 - acc: 0.7384 - val_loss: 0.5015 - val_acc: 0.7497\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5069 - acc: 0.7413 - val_loss: 0.4957 - val_acc: 0.7520\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5004 - acc: 0.7480 - val_loss: 0.4911 - val_acc: 0.7578\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4972 - acc: 0.7492 - val_loss: 0.4907 - val_acc: 0.7569\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.4938 - acc: 0.7514 - val_loss: 0.4886 - val_acc: 0.7573\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4907 - acc: 0.7541 - val_loss: 0.4870 - val_acc: 0.7595\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4871 - acc: 0.7566 - val_loss: 0.4865 - val_acc: 0.7610\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4834 - acc: 0.7598 - val_loss: 0.4857 - val_acc: 0.7610\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4820 - acc: 0.7590 - val_loss: 0.4848 - val_acc: 0.7627\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4801 - acc: 0.7625 - val_loss: 0.4849 - val_acc: 0.7610\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4778 - acc: 0.7627 - val_loss: 0.4846 - val_acc: 0.7626\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4778 - acc: 0.7618 - val_loss: 0.4841 - val_acc: 0.7635\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4756 - acc: 0.7637 - val_loss: 0.4846 - val_acc: 0.7625\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 600)               720600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 724,202\n",
      "Trainable params: 723,002\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 46us/sample - loss: 0.5987 - acc: 0.6809 - val_loss: 0.5710 - val_acc: 0.6953\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5759 - acc: 0.6914 - val_loss: 0.5725 - val_acc: 0.6939\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5744 - acc: 0.6939 - val_loss: 0.5728 - val_acc: 0.6978\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5729 - acc: 0.6945 - val_loss: 0.5763 - val_acc: 0.6906\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5729 - acc: 0.6925 - val_loss: 0.5694 - val_acc: 0.6978\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5717 - acc: 0.6963 - val_loss: 0.5693 - val_acc: 0.6951\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5698 - acc: 0.6971 - val_loss: 0.5714 - val_acc: 0.6991\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5694 - acc: 0.6961 - val_loss: 0.5702 - val_acc: 0.6953\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5687 - acc: 0.6972 - val_loss: 0.5680 - val_acc: 0.6987\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.5668 - acc: 0.6989 - val_loss: 0.5571 - val_acc: 0.7043\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5535 - acc: 0.7083 - val_loss: 0.5403 - val_acc: 0.7191\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5462 - acc: 0.7141 - val_loss: 0.5347 - val_acc: 0.7229\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5403 - acc: 0.7176 - val_loss: 0.5271 - val_acc: 0.7308\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5324 - acc: 0.7244 - val_loss: 0.5179 - val_acc: 0.7344\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5242 - acc: 0.7307 - val_loss: 0.5102 - val_acc: 0.7396\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5150 - acc: 0.7365 - val_loss: 0.5047 - val_acc: 0.7432\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5111 - acc: 0.7392 - val_loss: 0.4989 - val_acc: 0.7499\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5047 - acc: 0.7450 - val_loss: 0.4937 - val_acc: 0.7544\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5007 - acc: 0.7457 - val_loss: 0.4918 - val_acc: 0.7552\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4959 - acc: 0.7509 - val_loss: 0.4910 - val_acc: 0.7585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4928 - acc: 0.7516 - val_loss: 0.4894 - val_acc: 0.7581\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.4919 - acc: 0.7543 - val_loss: 0.4875 - val_acc: 0.7602\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4876 - acc: 0.7558 - val_loss: 0.4864 - val_acc: 0.7609\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.4876 - acc: 0.7572 - val_loss: 0.4858 - val_acc: 0.7602\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4832 - acc: 0.7586 - val_loss: 0.4857 - val_acc: 0.7615\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4817 - acc: 0.7592 - val_loss: 0.4854 - val_acc: 0.7616\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.4792 - acc: 0.7624 - val_loss: 0.4842 - val_acc: 0.7627\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4781 - acc: 0.7621 - val_loss: 0.4844 - val_acc: 0.7615\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4752 - acc: 0.7643 - val_loss: 0.4853 - val_acc: 0.7618\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4726 - acc: 0.7668 - val_loss: 0.4843 - val_acc: 0.7627\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                60050     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 63,102\n",
      "Trainable params: 62,902\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.6022 - acc: 0.6774 - val_loss: 0.5653 - val_acc: 0.7003\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5724 - acc: 0.6959 - val_loss: 0.5648 - val_acc: 0.7013\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5658 - acc: 0.7001 - val_loss: 0.5556 - val_acc: 0.7032\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5511 - acc: 0.7100 - val_loss: 0.5331 - val_acc: 0.7243\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.5358 - acc: 0.7218 - val_loss: 0.5216 - val_acc: 0.7299\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5239 - acc: 0.7294 - val_loss: 0.5143 - val_acc: 0.7378\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5120 - acc: 0.7402 - val_loss: 0.5060 - val_acc: 0.7441\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5030 - acc: 0.7453 - val_loss: 0.4989 - val_acc: 0.7502\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4977 - acc: 0.7498 - val_loss: 0.4971 - val_acc: 0.7503\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4929 - acc: 0.7535 - val_loss: 0.4915 - val_acc: 0.7539\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4892 - acc: 0.7563 - val_loss: 0.4896 - val_acc: 0.7564\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4844 - acc: 0.7600 - val_loss: 0.4905 - val_acc: 0.7555\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4802 - acc: 0.7619 - val_loss: 0.4876 - val_acc: 0.7585\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4773 - acc: 0.7634 - val_loss: 0.4876 - val_acc: 0.7571\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4747 - acc: 0.7645 - val_loss: 0.4856 - val_acc: 0.7594\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.4726 - acc: 0.7674 - val_loss: 0.4853 - val_acc: 0.7623\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.4688 - acc: 0.7697 - val_loss: 0.4850 - val_acc: 0.7628\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4664 - acc: 0.7719 - val_loss: 0.4830 - val_acc: 0.7616\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4637 - acc: 0.7716 - val_loss: 0.4834 - val_acc: 0.7594\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4616 - acc: 0.7732 - val_loss: 0.4831 - val_acc: 0.7619\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4587 - acc: 0.7762 - val_loss: 0.4807 - val_acc: 0.7630\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4561 - acc: 0.7766 - val_loss: 0.4808 - val_acc: 0.7638\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4562 - acc: 0.7761 - val_loss: 0.4808 - val_acc: 0.7633\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4520 - acc: 0.7806 - val_loss: 0.4841 - val_acc: 0.7623\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4478 - acc: 0.7816 - val_loss: 0.4856 - val_acc: 0.7595\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4480 - acc: 0.7827 - val_loss: 0.4815 - val_acc: 0.7655\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4473 - acc: 0.7827 - val_loss: 0.4823 - val_acc: 0.7649\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.4437 - acc: 0.7837 - val_loss: 0.4805 - val_acc: 0.7634\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4425 - acc: 0.7856 - val_loss: 0.4818 - val_acc: 0.7652\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4388 - acc: 0.7891 - val_loss: 0.4799 - val_acc: 0.7664\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               120100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 131,202\n",
      "Trainable params: 130,802\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 5s 60us/sample - loss: 0.6101 - acc: 0.6727 - val_loss: 0.5662 - val_acc: 0.6981\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5725 - acc: 0.6949 - val_loss: 0.5618 - val_acc: 0.7017\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.5599 - acc: 0.7048 - val_loss: 0.5431 - val_acc: 0.7182\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5414 - acc: 0.7161 - val_loss: 0.5266 - val_acc: 0.7283\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.5259 - acc: 0.7302 - val_loss: 0.5159 - val_acc: 0.7383\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5141 - acc: 0.7387 - val_loss: 0.5237 - val_acc: 0.7340\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5007 - acc: 0.7483 - val_loss: 0.4942 - val_acc: 0.7527\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4920 - acc: 0.7548 - val_loss: 0.4951 - val_acc: 0.7547\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4853 - acc: 0.7585 - val_loss: 0.4845 - val_acc: 0.7595\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4785 - acc: 0.7622 - val_loss: 0.4826 - val_acc: 0.7588\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4737 - acc: 0.7654 - val_loss: 0.4836 - val_acc: 0.7588\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4696 - acc: 0.7698 - val_loss: 0.4818 - val_acc: 0.7606\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4657 - acc: 0.7717 - val_loss: 0.4805 - val_acc: 0.7605\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4621 - acc: 0.7734 - val_loss: 0.4781 - val_acc: 0.7646\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4582 - acc: 0.7761 - val_loss: 0.4806 - val_acc: 0.7628\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4544 - acc: 0.7784 - val_loss: 0.4824 - val_acc: 0.7614\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4515 - acc: 0.7800 - val_loss: 0.4822 - val_acc: 0.7617\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4480 - acc: 0.7812 - val_loss: 0.4793 - val_acc: 0.7637\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4421 - acc: 0.7851 - val_loss: 0.4792 - val_acc: 0.7671\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4418 - acc: 0.7864 - val_loss: 0.4808 - val_acc: 0.7650\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.4356 - acc: 0.7898 - val_loss: 0.4820 - val_acc: 0.7610\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4329 - acc: 0.7919 - val_loss: 0.4810 - val_acc: 0.7629\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4309 - acc: 0.7926 - val_loss: 0.4837 - val_acc: 0.7647\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4256 - acc: 0.7966 - val_loss: 0.4832 - val_acc: 0.7655\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4241 - acc: 0.7959 - val_loss: 0.4851 - val_acc: 0.7663\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4197 - acc: 0.7998 - val_loss: 0.4856 - val_acc: 0.7627\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4183 - acc: 0.8007 - val_loss: 0.4899 - val_acc: 0.7613\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.4163 - acc: 0.8007 - val_loss: 0.4871 - val_acc: 0.7631\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4126 - acc: 0.8036 - val_loss: 0.4866 - val_acc: 0.7645\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4104 - acc: 0.8066 - val_loss: 0.4894 - val_acc: 0.7627\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 160)               192160    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 219,522\n",
      "Trainable params: 218,882\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 5s 60us/sample - loss: 0.6137 - acc: 0.6725 - val_loss: 0.5715 - val_acc: 0.6988\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5669 - acc: 0.6997 - val_loss: 0.5439 - val_acc: 0.7160\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5436 - acc: 0.7178 - val_loss: 0.5367 - val_acc: 0.7207\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5261 - acc: 0.7296 - val_loss: 0.5151 - val_acc: 0.7380\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5067 - acc: 0.7428 - val_loss: 0.4950 - val_acc: 0.7507\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4937 - acc: 0.7526 - val_loss: 0.4906 - val_acc: 0.7541\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4855 - acc: 0.7570 - val_loss: 0.4908 - val_acc: 0.7542\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4785 - acc: 0.7617 - val_loss: 0.4894 - val_acc: 0.7520\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4717 - acc: 0.7690 - val_loss: 0.4903 - val_acc: 0.7527\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4662 - acc: 0.7708 - val_loss: 0.4832 - val_acc: 0.7596\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4623 - acc: 0.7735 - val_loss: 0.4835 - val_acc: 0.7595\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4561 - acc: 0.7774 - val_loss: 0.4822 - val_acc: 0.7614\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4503 - acc: 0.7815 - val_loss: 0.4784 - val_acc: 0.7652\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4467 - acc: 0.7826 - val_loss: 0.4805 - val_acc: 0.7609\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4431 - acc: 0.7855 - val_loss: 0.4787 - val_acc: 0.7624\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4370 - acc: 0.7888 - val_loss: 0.4769 - val_acc: 0.7664\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4321 - acc: 0.7918 - val_loss: 0.4815 - val_acc: 0.7657\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4278 - acc: 0.7953 - val_loss: 0.4808 - val_acc: 0.7655\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4210 - acc: 0.7988 - val_loss: 0.4856 - val_acc: 0.7653\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4177 - acc: 0.8002 - val_loss: 0.4879 - val_acc: 0.7651\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4149 - acc: 0.8017 - val_loss: 0.4848 - val_acc: 0.7637\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4096 - acc: 0.8052 - val_loss: 0.4865 - val_acc: 0.7659\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.4056 - acc: 0.8078 - val_loss: 0.4921 - val_acc: 0.7631\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4042 - acc: 0.8092 - val_loss: 0.4912 - val_acc: 0.7653\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.3974 - acc: 0.8135 - val_loss: 0.4944 - val_acc: 0.7631\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.3950 - acc: 0.8144 - val_loss: 0.4924 - val_acc: 0.7612\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.3908 - acc: 0.8179 - val_loss: 0.4992 - val_acc: 0.7606\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.3867 - acc: 0.8188 - val_loss: 0.4999 - val_acc: 0.7641\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.3820 - acc: 0.8211 - val_loss: 0.5054 - val_acc: 0.7627\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.3822 - acc: 0.8217 - val_loss: 0.5083 - val_acc: 0.7610\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 300)               360300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 453,602\n",
      "Trainable params: 452,402\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.6153 - acc: 0.6718 - val_loss: 0.5661 - val_acc: 0.7002\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5646 - acc: 0.7011 - val_loss: 0.5419 - val_acc: 0.7163\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5351 - acc: 0.7230 - val_loss: 0.5192 - val_acc: 0.7331\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5124 - acc: 0.7405 - val_loss: 0.5068 - val_acc: 0.7439\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.4955 - acc: 0.7501 - val_loss: 0.5026 - val_acc: 0.7430\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4852 - acc: 0.7585 - val_loss: 0.4910 - val_acc: 0.7528\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.4764 - acc: 0.7627 - val_loss: 0.4826 - val_acc: 0.7594\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4659 - acc: 0.7707 - val_loss: 0.4850 - val_acc: 0.7611\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4587 - acc: 0.7748 - val_loss: 0.4809 - val_acc: 0.7606\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.4510 - acc: 0.7800 - val_loss: 0.4831 - val_acc: 0.7617\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.4439 - acc: 0.7851 - val_loss: 0.4863 - val_acc: 0.7575\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.4357 - acc: 0.7906 - val_loss: 0.4833 - val_acc: 0.7627\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 61us/sample - loss: 0.4284 - acc: 0.7951 - val_loss: 0.4828 - val_acc: 0.7622\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.4192 - acc: 0.8010 - val_loss: 0.4870 - val_acc: 0.7609\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4142 - acc: 0.8043 - val_loss: 0.4880 - val_acc: 0.7588\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4068 - acc: 0.8074 - val_loss: 0.4963 - val_acc: 0.7556\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.3998 - acc: 0.8130 - val_loss: 0.4969 - val_acc: 0.7570\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.3926 - acc: 0.8167 - val_loss: 0.5042 - val_acc: 0.7577\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.3877 - acc: 0.8194 - val_loss: 0.5023 - val_acc: 0.7581\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.3795 - acc: 0.8234 - val_loss: 0.5137 - val_acc: 0.7577\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.3725 - acc: 0.8274 - val_loss: 0.5128 - val_acc: 0.7531\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.3670 - acc: 0.8308 - val_loss: 0.5224 - val_acc: 0.7538\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.3630 - acc: 0.8342 - val_loss: 0.5294 - val_acc: 0.7430\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 60us/sample - loss: 0.3544 - acc: 0.8367 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.3497 - acc: 0.8407 - val_loss: 0.5317 - val_acc: 0.7534\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.3439 - acc: 0.8437 - val_loss: 0.5422 - val_acc: 0.7462\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.3378 - acc: 0.8464 - val_loss: 0.5514 - val_acc: 0.7517\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.3320 - acc: 0.8494 - val_loss: 0.5547 - val_acc: 0.7494\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 60us/sample - loss: 0.3273 - acc: 0.8518 - val_loss: 0.5625 - val_acc: 0.7496\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.3211 - acc: 0.8556 - val_loss: 0.5642 - val_acc: 0.7538\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 600)               720600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 1,087,202\n",
      "Trainable params: 1,084,802\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.6300 - acc: 0.6722 - val_loss: 0.5819 - val_acc: 0.6958\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.5701 - acc: 0.6966 - val_loss: 0.5467 - val_acc: 0.7143\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.5390 - acc: 0.7202 - val_loss: 0.5173 - val_acc: 0.7328\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5086 - acc: 0.7403 - val_loss: 0.5010 - val_acc: 0.7482\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.4919 - acc: 0.7527 - val_loss: 0.4953 - val_acc: 0.7545\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.4763 - acc: 0.7632 - val_loss: 0.4894 - val_acc: 0.7558\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.4654 - acc: 0.7714 - val_loss: 0.4854 - val_acc: 0.7604\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4542 - acc: 0.7796 - val_loss: 0.4846 - val_acc: 0.7563\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.4426 - acc: 0.7866 - val_loss: 0.4932 - val_acc: 0.7563\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.4306 - acc: 0.7929 - val_loss: 0.4944 - val_acc: 0.7552\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.4229 - acc: 0.7980 - val_loss: 0.4942 - val_acc: 0.7539\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.4101 - acc: 0.8059 - val_loss: 0.5085 - val_acc: 0.7576\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.4003 - acc: 0.8120 - val_loss: 0.5046 - val_acc: 0.7578\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.3904 - acc: 0.8180 - val_loss: 0.5087 - val_acc: 0.7600\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.3780 - acc: 0.8242 - val_loss: 0.5263 - val_acc: 0.7487\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.3660 - acc: 0.8324 - val_loss: 0.5265 - val_acc: 0.7509\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.3549 - acc: 0.8376 - val_loss: 0.5342 - val_acc: 0.7488\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.3473 - acc: 0.8420 - val_loss: 0.5370 - val_acc: 0.7497\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.3377 - acc: 0.8471 - val_loss: 0.5568 - val_acc: 0.7498\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.3265 - acc: 0.8531 - val_loss: 0.5718 - val_acc: 0.7555\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.3169 - acc: 0.8585 - val_loss: 0.5721 - val_acc: 0.7431\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 60us/sample - loss: 0.3081 - acc: 0.8633 - val_loss: 0.5880 - val_acc: 0.7366\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.3027 - acc: 0.8645 - val_loss: 0.6011 - val_acc: 0.7473\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.2924 - acc: 0.8711 - val_loss: 0.6106 - val_acc: 0.7387\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.2837 - acc: 0.8755 - val_loss: 0.6052 - val_acc: 0.7405\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.2790 - acc: 0.8780 - val_loss: 0.6171 - val_acc: 0.7390\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.2679 - acc: 0.8823 - val_loss: 0.6307 - val_acc: 0.7368\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.2623 - acc: 0.8864 - val_loss: 0.6456 - val_acc: 0.7419\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.2572 - acc: 0.8883 - val_loss: 0.6641 - val_acc: 0.7362\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.2536 - acc: 0.8907 - val_loss: 0.6710 - val_acc: 0.7436\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 50)                60050     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 65,852\n",
      "Trainable params: 65,552\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.6148 - acc: 0.6720 - val_loss: 0.5678 - val_acc: 0.7017\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.5744 - acc: 0.6952 - val_loss: 0.5612 - val_acc: 0.7028\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.5631 - acc: 0.7017 - val_loss: 0.5461 - val_acc: 0.7157\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.5461 - acc: 0.7150 - val_loss: 0.5290 - val_acc: 0.7273\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.5356 - acc: 0.7223 - val_loss: 0.5251 - val_acc: 0.7301\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5285 - acc: 0.7280 - val_loss: 0.5180 - val_acc: 0.7346\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.5172 - acc: 0.7366 - val_loss: 0.5140 - val_acc: 0.7375\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5086 - acc: 0.7431 - val_loss: 0.5095 - val_acc: 0.7429\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5037 - acc: 0.7465 - val_loss: 0.5066 - val_acc: 0.7429\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4992 - acc: 0.7500 - val_loss: 0.5047 - val_acc: 0.7452\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4913 - acc: 0.7551 - val_loss: 0.5000 - val_acc: 0.7472\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4846 - acc: 0.7594 - val_loss: 0.4912 - val_acc: 0.7555\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4765 - acc: 0.7660 - val_loss: 0.4889 - val_acc: 0.7566\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4744 - acc: 0.7664 - val_loss: 0.4840 - val_acc: 0.7609\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4675 - acc: 0.7722 - val_loss: 0.4809 - val_acc: 0.7632\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4647 - acc: 0.7737 - val_loss: 0.4823 - val_acc: 0.7627\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4609 - acc: 0.7750 - val_loss: 0.4835 - val_acc: 0.7620\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4567 - acc: 0.7780 - val_loss: 0.4846 - val_acc: 0.7610\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4558 - acc: 0.7775 - val_loss: 0.4858 - val_acc: 0.7625\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4551 - acc: 0.7802 - val_loss: 0.4813 - val_acc: 0.7635\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4499 - acc: 0.7821 - val_loss: 0.4826 - val_acc: 0.7653\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4495 - acc: 0.7829 - val_loss: 0.4848 - val_acc: 0.7642\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4464 - acc: 0.7862 - val_loss: 0.4851 - val_acc: 0.7651\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4423 - acc: 0.7878 - val_loss: 0.4847 - val_acc: 0.7656\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4407 - acc: 0.7895 - val_loss: 0.4895 - val_acc: 0.7649\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4395 - acc: 0.7894 - val_loss: 0.4846 - val_acc: 0.7645\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4368 - acc: 0.7909 - val_loss: 0.4858 - val_acc: 0.7649\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4341 - acc: 0.7931 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4326 - acc: 0.7946 - val_loss: 0.4877 - val_acc: 0.7660\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4305 - acc: 0.7966 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               120100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 141,702\n",
      "Trainable params: 141,102\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.6222 - acc: 0.6693 - val_loss: 0.5631 - val_acc: 0.7010\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.5702 - acc: 0.6950 - val_loss: 0.5493 - val_acc: 0.7104\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.5484 - acc: 0.7131 - val_loss: 0.5312 - val_acc: 0.7265\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.5313 - acc: 0.7265 - val_loss: 0.5394 - val_acc: 0.7269\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5171 - acc: 0.7365 - val_loss: 0.5159 - val_acc: 0.7370\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.5067 - acc: 0.7442 - val_loss: 0.5051 - val_acc: 0.7447\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5020 - acc: 0.7471 - val_loss: 0.5080 - val_acc: 0.7427\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4953 - acc: 0.7517 - val_loss: 0.5058 - val_acc: 0.7489\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4896 - acc: 0.7559 - val_loss: 0.4971 - val_acc: 0.7506\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4816 - acc: 0.7618 - val_loss: 0.4868 - val_acc: 0.7590\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4719 - acc: 0.7671 - val_loss: 0.4837 - val_acc: 0.7599\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4666 - acc: 0.7704 - val_loss: 0.4903 - val_acc: 0.7599\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4617 - acc: 0.7742 - val_loss: 0.4849 - val_acc: 0.7578\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4570 - acc: 0.7766 - val_loss: 0.4821 - val_acc: 0.7625\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4549 - acc: 0.7787 - val_loss: 0.4808 - val_acc: 0.7631\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4492 - acc: 0.7824 - val_loss: 0.4825 - val_acc: 0.7622\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4453 - acc: 0.7839 - val_loss: 0.4834 - val_acc: 0.7631\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4423 - acc: 0.7861 - val_loss: 0.4836 - val_acc: 0.7645\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4389 - acc: 0.7876 - val_loss: 0.4812 - val_acc: 0.7635\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4359 - acc: 0.7891 - val_loss: 0.4882 - val_acc: 0.7639\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4321 - acc: 0.7904 - val_loss: 0.4849 - val_acc: 0.7653\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4292 - acc: 0.7942 - val_loss: 0.4908 - val_acc: 0.7643\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4262 - acc: 0.7952 - val_loss: 0.4861 - val_acc: 0.7681\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4217 - acc: 0.7986 - val_loss: 0.4966 - val_acc: 0.7660\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4212 - acc: 0.7988 - val_loss: 0.4834 - val_acc: 0.7663\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4157 - acc: 0.8022 - val_loss: 0.4877 - val_acc: 0.7667\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4130 - acc: 0.8029 - val_loss: 0.4912 - val_acc: 0.7672\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4122 - acc: 0.8049 - val_loss: 0.4936 - val_acc: 0.7675\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4088 - acc: 0.8062 - val_loss: 0.4905 - val_acc: 0.7674\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4051 - acc: 0.8086 - val_loss: 0.4971 - val_acc: 0.7646\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 160)               192160    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 245,922\n",
      "Trainable params: 244,962\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.6263 - acc: 0.6678 - val_loss: 0.5659 - val_acc: 0.6995\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5665 - acc: 0.6996 - val_loss: 0.5442 - val_acc: 0.7181\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.5415 - acc: 0.7183 - val_loss: 0.5218 - val_acc: 0.7341\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.5232 - acc: 0.7311 - val_loss: 0.5155 - val_acc: 0.7390\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.5102 - acc: 0.7422 - val_loss: 0.5083 - val_acc: 0.7426\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.5018 - acc: 0.7479 - val_loss: 0.5084 - val_acc: 0.7415\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4942 - acc: 0.7529 - val_loss: 0.4970 - val_acc: 0.7501\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4838 - acc: 0.7602 - val_loss: 0.4977 - val_acc: 0.7490\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4757 - acc: 0.7652 - val_loss: 0.4925 - val_acc: 0.7530\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4692 - acc: 0.7684 - val_loss: 0.4880 - val_acc: 0.7575\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4603 - acc: 0.7746 - val_loss: 0.4846 - val_acc: 0.7589\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4545 - acc: 0.7784 - val_loss: 0.4841 - val_acc: 0.7616\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4481 - acc: 0.7810 - val_loss: 0.4891 - val_acc: 0.7578\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4433 - acc: 0.7860 - val_loss: 0.4957 - val_acc: 0.7624\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4383 - acc: 0.7879 - val_loss: 0.4848 - val_acc: 0.7635\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4325 - acc: 0.7926 - val_loss: 0.4865 - val_acc: 0.7631\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4277 - acc: 0.7956 - val_loss: 0.4903 - val_acc: 0.7573\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4222 - acc: 0.7980 - val_loss: 0.4961 - val_acc: 0.7617\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4198 - acc: 0.7998 - val_loss: 0.4891 - val_acc: 0.7602\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4126 - acc: 0.8041 - val_loss: 0.4954 - val_acc: 0.7625\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4089 - acc: 0.8063 - val_loss: 0.4922 - val_acc: 0.7656\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4034 - acc: 0.8099 - val_loss: 0.5005 - val_acc: 0.7653\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.3996 - acc: 0.8114 - val_loss: 0.5003 - val_acc: 0.7623\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3972 - acc: 0.8126 - val_loss: 0.5024 - val_acc: 0.7621\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3931 - acc: 0.8156 - val_loss: 0.5139 - val_acc: 0.7621\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3877 - acc: 0.8182 - val_loss: 0.5083 - val_acc: 0.7631\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3837 - acc: 0.8204 - val_loss: 0.5120 - val_acc: 0.7634\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3790 - acc: 0.8228 - val_loss: 0.5054 - val_acc: 0.7604\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3769 - acc: 0.8240 - val_loss: 0.5116 - val_acc: 0.7600\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.3721 - acc: 0.8264 - val_loss: 0.5173 - val_acc: 0.7648\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 300)               360300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 545,102\n",
      "Trainable params: 543,302\n",
      "Non-trainable params: 1,800\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.6209 - acc: 0.6723 - val_loss: 0.5519 - val_acc: 0.7084\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.5514 - acc: 0.7111 - val_loss: 0.5357 - val_acc: 0.7261\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.5232 - acc: 0.7322 - val_loss: 0.5101 - val_acc: 0.7412\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.5027 - acc: 0.7470 - val_loss: 0.5015 - val_acc: 0.7465\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4885 - acc: 0.7571 - val_loss: 0.5071 - val_acc: 0.7375\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4780 - acc: 0.7631 - val_loss: 0.4895 - val_acc: 0.7550\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4661 - acc: 0.7714 - val_loss: 0.4841 - val_acc: 0.7588\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4592 - acc: 0.7745 - val_loss: 0.4822 - val_acc: 0.7620\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4506 - acc: 0.7817 - val_loss: 0.5004 - val_acc: 0.7404\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4433 - acc: 0.7868 - val_loss: 0.4881 - val_acc: 0.7602\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4346 - acc: 0.7909 - val_loss: 0.4853 - val_acc: 0.7617\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4259 - acc: 0.7962 - val_loss: 0.4877 - val_acc: 0.7599\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4201 - acc: 0.8009 - val_loss: 0.4926 - val_acc: 0.7594\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4116 - acc: 0.8046 - val_loss: 0.4894 - val_acc: 0.7595\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4051 - acc: 0.8082 - val_loss: 0.4963 - val_acc: 0.7588\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.3984 - acc: 0.8127 - val_loss: 0.5075 - val_acc: 0.7610\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3913 - acc: 0.8169 - val_loss: 0.4993 - val_acc: 0.7596\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3834 - acc: 0.8207 - val_loss: 0.5149 - val_acc: 0.7588\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.3779 - acc: 0.8238 - val_loss: 0.5192 - val_acc: 0.7574\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3714 - acc: 0.8276 - val_loss: 0.5283 - val_acc: 0.7596\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3643 - acc: 0.8296 - val_loss: 0.5289 - val_acc: 0.7514\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3558 - acc: 0.8356 - val_loss: 0.5303 - val_acc: 0.7598\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3523 - acc: 0.8382 - val_loss: 0.5410 - val_acc: 0.7567\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.3461 - acc: 0.8390 - val_loss: 0.5435 - val_acc: 0.7561\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.3425 - acc: 0.8438 - val_loss: 0.5482 - val_acc: 0.7568\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3358 - acc: 0.8474 - val_loss: 0.5490 - val_acc: 0.7552\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.3322 - acc: 0.8492 - val_loss: 0.5636 - val_acc: 0.7531\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3275 - acc: 0.8516 - val_loss: 0.5683 - val_acc: 0.7550\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3217 - acc: 0.8536 - val_loss: 0.5634 - val_acc: 0.7546\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.3166 - acc: 0.8574 - val_loss: 0.5750 - val_acc: 0.7506\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 600)               720600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 1,450,202\n",
      "Trainable params: 1,446,602\n",
      "Non-trainable params: 3,600\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.6357 - acc: 0.6696 - val_loss: 0.5635 - val_acc: 0.6985\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5492 - acc: 0.7124 - val_loss: 0.5342 - val_acc: 0.7299\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5179 - acc: 0.7360 - val_loss: 0.5111 - val_acc: 0.7416\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4991 - acc: 0.7492 - val_loss: 0.5052 - val_acc: 0.7400\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4845 - acc: 0.7601 - val_loss: 0.4984 - val_acc: 0.7488\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4722 - acc: 0.7677 - val_loss: 0.4956 - val_acc: 0.7502\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4633 - acc: 0.7745 - val_loss: 0.5019 - val_acc: 0.7392\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4505 - acc: 0.7815 - val_loss: 0.4950 - val_acc: 0.7569\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4398 - acc: 0.7887 - val_loss: 0.4905 - val_acc: 0.7566\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4288 - acc: 0.7945 - val_loss: 0.4959 - val_acc: 0.7607\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4200 - acc: 0.8012 - val_loss: 0.4930 - val_acc: 0.7584\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4097 - acc: 0.8053 - val_loss: 0.5071 - val_acc: 0.7577\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.3975 - acc: 0.8130 - val_loss: 0.5010 - val_acc: 0.7586\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3886 - acc: 0.8181 - val_loss: 0.5145 - val_acc: 0.7596\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.3752 - acc: 0.8263 - val_loss: 0.5214 - val_acc: 0.7519\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.3645 - acc: 0.8311 - val_loss: 0.5254 - val_acc: 0.7585\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.3537 - acc: 0.8363 - val_loss: 0.5571 - val_acc: 0.7555\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.3414 - acc: 0.8434 - val_loss: 0.5473 - val_acc: 0.7555\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.3287 - acc: 0.8527 - val_loss: 0.5585 - val_acc: 0.7503\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3209 - acc: 0.8545 - val_loss: 0.5491 - val_acc: 0.7539\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.3082 - acc: 0.8619 - val_loss: 0.5831 - val_acc: 0.7508\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.2970 - acc: 0.8680 - val_loss: 0.5996 - val_acc: 0.7538\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.2899 - acc: 0.8716 - val_loss: 0.5989 - val_acc: 0.7485\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.2820 - acc: 0.8746 - val_loss: 0.6046 - val_acc: 0.7496\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.2751 - acc: 0.8794 - val_loss: 0.6275 - val_acc: 0.7398\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.2660 - acc: 0.8832 - val_loss: 0.6458 - val_acc: 0.7466\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.2602 - acc: 0.8862 - val_loss: 0.6546 - val_acc: 0.7423\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.2541 - acc: 0.8890 - val_loss: 0.6502 - val_acc: 0.7492\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.2499 - acc: 0.8899 - val_loss: 0.6616 - val_acc: 0.7396\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.2427 - acc: 0.8938 - val_loss: 0.6900 - val_acc: 0.7466\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 50)                60050     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 68,602\n",
      "Trainable params: 68,202\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.6242 - acc: 0.6670 - val_loss: 0.5690 - val_acc: 0.7002\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.5776 - acc: 0.6951 - val_loss: 0.5662 - val_acc: 0.7014\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.5707 - acc: 0.6975 - val_loss: 0.5591 - val_acc: 0.7051\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.5553 - acc: 0.7066 - val_loss: 0.5355 - val_acc: 0.7222\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.5412 - acc: 0.7191 - val_loss: 0.5352 - val_acc: 0.7265\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.5328 - acc: 0.7253 - val_loss: 0.5272 - val_acc: 0.7300\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.5231 - acc: 0.7319 - val_loss: 0.5187 - val_acc: 0.7386\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.5143 - acc: 0.7399 - val_loss: 0.5155 - val_acc: 0.7390\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.5056 - acc: 0.7452 - val_loss: 0.5124 - val_acc: 0.7441\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.5023 - acc: 0.7482 - val_loss: 0.5103 - val_acc: 0.7437\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4952 - acc: 0.7535 - val_loss: 0.4989 - val_acc: 0.7521\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4878 - acc: 0.7582 - val_loss: 0.4958 - val_acc: 0.7559\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4815 - acc: 0.7622 - val_loss: 0.4881 - val_acc: 0.7586\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4778 - acc: 0.7648 - val_loss: 0.4873 - val_acc: 0.7601\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4719 - acc: 0.7690 - val_loss: 0.4868 - val_acc: 0.7616\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4698 - acc: 0.7708 - val_loss: 0.4846 - val_acc: 0.7628\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4653 - acc: 0.7735 - val_loss: 0.4835 - val_acc: 0.7638\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4628 - acc: 0.7757 - val_loss: 0.4828 - val_acc: 0.7648\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4589 - acc: 0.7784 - val_loss: 0.4817 - val_acc: 0.7632\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4578 - acc: 0.7796 - val_loss: 0.4859 - val_acc: 0.7643\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4548 - acc: 0.7817 - val_loss: 0.4832 - val_acc: 0.7644\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4516 - acc: 0.7825 - val_loss: 0.4839 - val_acc: 0.7656\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4500 - acc: 0.7832 - val_loss: 0.4819 - val_acc: 0.7648\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4465 - acc: 0.7863 - val_loss: 0.4823 - val_acc: 0.7641\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4428 - acc: 0.7905 - val_loss: 0.4861 - val_acc: 0.7674\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4419 - acc: 0.7897 - val_loss: 0.4841 - val_acc: 0.7656\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4404 - acc: 0.7916 - val_loss: 0.4913 - val_acc: 0.7644\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4357 - acc: 0.7936 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4344 - acc: 0.7956 - val_loss: 0.4983 - val_acc: 0.7618\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4331 - acc: 0.7960 - val_loss: 0.4885 - val_acc: 0.7653\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               120100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 152,202\n",
      "Trainable params: 151,402\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.6337 - acc: 0.6618 - val_loss: 0.5648 - val_acc: 0.7027\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.5736 - acc: 0.6954 - val_loss: 0.5503 - val_acc: 0.7103\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.5525 - acc: 0.7103 - val_loss: 0.5325 - val_acc: 0.7303\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.5357 - acc: 0.7232 - val_loss: 0.5229 - val_acc: 0.7323\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.5222 - acc: 0.7326 - val_loss: 0.5167 - val_acc: 0.7375\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.5109 - acc: 0.7420 - val_loss: 0.5090 - val_acc: 0.7426\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.5020 - acc: 0.7485 - val_loss: 0.5029 - val_acc: 0.7484\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4925 - acc: 0.7542 - val_loss: 0.4975 - val_acc: 0.7543\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4826 - acc: 0.7608 - val_loss: 0.4959 - val_acc: 0.7578\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4751 - acc: 0.7656 - val_loss: 0.4879 - val_acc: 0.7584\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4702 - acc: 0.7689 - val_loss: 0.4898 - val_acc: 0.7611\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4653 - acc: 0.7715 - val_loss: 0.4879 - val_acc: 0.7586\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4592 - acc: 0.7768 - val_loss: 0.4876 - val_acc: 0.7632\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4560 - acc: 0.7793 - val_loss: 0.4872 - val_acc: 0.7588\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4520 - acc: 0.7817 - val_loss: 0.4886 - val_acc: 0.7613\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4479 - acc: 0.7837 - val_loss: 0.4834 - val_acc: 0.7636\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4431 - acc: 0.7866 - val_loss: 0.4856 - val_acc: 0.7650\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4380 - acc: 0.7894 - val_loss: 0.4859 - val_acc: 0.7646\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4345 - acc: 0.7930 - val_loss: 0.4811 - val_acc: 0.7645\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4318 - acc: 0.7936 - val_loss: 0.4807 - val_acc: 0.7656\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4281 - acc: 0.7944 - val_loss: 0.4960 - val_acc: 0.7670\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4253 - acc: 0.7961 - val_loss: 0.5014 - val_acc: 0.7667\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4208 - acc: 0.7992 - val_loss: 0.4935 - val_acc: 0.7684\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4192 - acc: 0.8010 - val_loss: 0.4940 - val_acc: 0.7660\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4162 - acc: 0.8024 - val_loss: 0.4954 - val_acc: 0.7663\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4129 - acc: 0.8056 - val_loss: 0.4945 - val_acc: 0.7652\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4105 - acc: 0.8061 - val_loss: 0.4986 - val_acc: 0.7666\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4072 - acc: 0.8071 - val_loss: 0.5009 - val_acc: 0.7625\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4048 - acc: 0.8106 - val_loss: 0.5054 - val_acc: 0.7666\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4006 - acc: 0.8112 - val_loss: 0.5085 - val_acc: 0.7602\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 160)               192160    \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 272,322\n",
      "Trainable params: 271,042\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.6217 - acc: 0.6685 - val_loss: 0.5659 - val_acc: 0.6988\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.5676 - acc: 0.6996 - val_loss: 0.5506 - val_acc: 0.7117\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.5461 - acc: 0.7157 - val_loss: 0.5322 - val_acc: 0.7272\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.5279 - acc: 0.7305 - val_loss: 0.5171 - val_acc: 0.7372\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.5133 - acc: 0.7419 - val_loss: 0.5153 - val_acc: 0.7385\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5061 - acc: 0.7450 - val_loss: 0.5170 - val_acc: 0.7407\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4968 - acc: 0.7508 - val_loss: 0.5070 - val_acc: 0.7477\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4887 - acc: 0.7576 - val_loss: 0.5047 - val_acc: 0.7477\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4798 - acc: 0.7635 - val_loss: 0.4913 - val_acc: 0.7549\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4706 - acc: 0.7694 - val_loss: 0.4953 - val_acc: 0.7517\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4612 - acc: 0.7760 - val_loss: 0.4924 - val_acc: 0.7556\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4562 - acc: 0.7782 - val_loss: 0.4863 - val_acc: 0.7610\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4491 - acc: 0.7813 - val_loss: 0.4891 - val_acc: 0.7614\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4448 - acc: 0.7844 - val_loss: 0.4890 - val_acc: 0.7638\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4398 - acc: 0.7880 - val_loss: 0.4859 - val_acc: 0.7659\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4363 - acc: 0.7890 - val_loss: 0.4878 - val_acc: 0.7621\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4303 - acc: 0.7943 - val_loss: 0.4914 - val_acc: 0.7655\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4238 - acc: 0.7982 - val_loss: 0.4980 - val_acc: 0.7632\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4212 - acc: 0.7987 - val_loss: 0.4912 - val_acc: 0.7663\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4155 - acc: 0.8019 - val_loss: 0.4897 - val_acc: 0.7623\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4116 - acc: 0.8031 - val_loss: 0.5033 - val_acc: 0.7668\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4072 - acc: 0.8073 - val_loss: 0.5054 - val_acc: 0.7667\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4004 - acc: 0.8108 - val_loss: 0.5069 - val_acc: 0.7634\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3978 - acc: 0.8127 - val_loss: 0.5126 - val_acc: 0.7670\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3951 - acc: 0.8138 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3902 - acc: 0.8166 - val_loss: 0.5186 - val_acc: 0.7638\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3878 - acc: 0.8194 - val_loss: 0.5126 - val_acc: 0.7656\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.3835 - acc: 0.8218 - val_loss: 0.5100 - val_acc: 0.7654\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.3799 - acc: 0.8239 - val_loss: 0.5091 - val_acc: 0.7631\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3780 - acc: 0.8248 - val_loss: 0.5206 - val_acc: 0.7628\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 300)               360300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 636,602\n",
      "Trainable params: 634,202\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.6213 - acc: 0.6705 - val_loss: 0.5505 - val_acc: 0.7110\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.5561 - acc: 0.7071 - val_loss: 0.5419 - val_acc: 0.7255\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.5349 - acc: 0.7232 - val_loss: 0.5229 - val_acc: 0.7327\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.5204 - acc: 0.7348 - val_loss: 0.5165 - val_acc: 0.7376\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.5057 - acc: 0.7444 - val_loss: 0.5126 - val_acc: 0.7413\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4911 - acc: 0.7565 - val_loss: 0.5001 - val_acc: 0.7490\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4774 - acc: 0.7652 - val_loss: 0.4999 - val_acc: 0.7456\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4681 - acc: 0.7707 - val_loss: 0.4952 - val_acc: 0.7574\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4577 - acc: 0.7781 - val_loss: 0.4849 - val_acc: 0.7583\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4504 - acc: 0.7821 - val_loss: 0.4970 - val_acc: 0.7577\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4439 - acc: 0.7878 - val_loss: 0.4887 - val_acc: 0.7599\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4350 - acc: 0.7918 - val_loss: 0.4927 - val_acc: 0.7653\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4273 - acc: 0.7969 - val_loss: 0.4903 - val_acc: 0.7599\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4193 - acc: 0.8009 - val_loss: 0.4884 - val_acc: 0.7624\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4137 - acc: 0.8046 - val_loss: 0.4926 - val_acc: 0.7644\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4061 - acc: 0.8098 - val_loss: 0.4981 - val_acc: 0.7607\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.3998 - acc: 0.8137 - val_loss: 0.5105 - val_acc: 0.7612\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.3931 - acc: 0.8167 - val_loss: 0.5026 - val_acc: 0.7641\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3895 - acc: 0.8179 - val_loss: 0.5094 - val_acc: 0.7643\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.3784 - acc: 0.8245 - val_loss: 0.5234 - val_acc: 0.7630\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3734 - acc: 0.8278 - val_loss: 0.5217 - val_acc: 0.7624\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3663 - acc: 0.8313 - val_loss: 0.5259 - val_acc: 0.7595\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3615 - acc: 0.8333 - val_loss: 0.5231 - val_acc: 0.7609\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.3564 - acc: 0.8377 - val_loss: 0.5400 - val_acc: 0.7594\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3476 - acc: 0.8417 - val_loss: 0.5521 - val_acc: 0.7602\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3456 - acc: 0.8428 - val_loss: 0.5444 - val_acc: 0.7593\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3430 - acc: 0.8448 - val_loss: 0.5510 - val_acc: 0.7606\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.3367 - acc: 0.8474 - val_loss: 0.5534 - val_acc: 0.7599\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3308 - acc: 0.8511 - val_loss: 0.5499 - val_acc: 0.7586\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.3262 - acc: 0.8529 - val_loss: 0.5445 - val_acc: 0.7573\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 600)               720600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 1,813,202\n",
      "Trainable params: 1,808,402\n",
      "Non-trainable params: 4,800\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.6411 - acc: 0.6664 - val_loss: 0.5558 - val_acc: 0.7011\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.5534 - acc: 0.7095 - val_loss: 0.5460 - val_acc: 0.7235\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.5302 - acc: 0.7280 - val_loss: 0.5109 - val_acc: 0.7413\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.5091 - acc: 0.7442 - val_loss: 0.5058 - val_acc: 0.7465\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4921 - acc: 0.7551 - val_loss: 0.5067 - val_acc: 0.7454\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4783 - acc: 0.7640 - val_loss: 0.4992 - val_acc: 0.7480\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4677 - acc: 0.7715 - val_loss: 0.4965 - val_acc: 0.7505\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4573 - acc: 0.7772 - val_loss: 0.4960 - val_acc: 0.7541\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4485 - acc: 0.7828 - val_loss: 0.4935 - val_acc: 0.7569\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4390 - acc: 0.7903 - val_loss: 0.4939 - val_acc: 0.7598\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4301 - acc: 0.7952 - val_loss: 0.4991 - val_acc: 0.7525\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 88us/sample - loss: 0.4197 - acc: 0.8007 - val_loss: 0.5013 - val_acc: 0.7589\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4109 - acc: 0.8062 - val_loss: 0.5058 - val_acc: 0.7581\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.3987 - acc: 0.8137 - val_loss: 0.5084 - val_acc: 0.7584\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3902 - acc: 0.8181 - val_loss: 0.4956 - val_acc: 0.7605\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3780 - acc: 0.8261 - val_loss: 0.5267 - val_acc: 0.7564\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.3675 - acc: 0.8314 - val_loss: 0.5061 - val_acc: 0.7583\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.3567 - acc: 0.8376 - val_loss: 0.5297 - val_acc: 0.7569\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3460 - acc: 0.8436 - val_loss: 0.5583 - val_acc: 0.7569\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3366 - acc: 0.8489 - val_loss: 0.5490 - val_acc: 0.7556\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.3275 - acc: 0.8536 - val_loss: 0.5688 - val_acc: 0.7580\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.3188 - acc: 0.8575 - val_loss: 0.5678 - val_acc: 0.7564\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.3082 - acc: 0.8634 - val_loss: 0.5677 - val_acc: 0.7526\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.2979 - acc: 0.8702 - val_loss: 0.5890 - val_acc: 0.7509\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.2920 - acc: 0.8728 - val_loss: 0.6082 - val_acc: 0.7549\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.2839 - acc: 0.8752 - val_loss: 0.6034 - val_acc: 0.7513\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.2768 - acc: 0.8787 - val_loss: 0.6087 - val_acc: 0.7516\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.2701 - acc: 0.8836 - val_loss: 0.6259 - val_acc: 0.7502\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.2621 - acc: 0.8862 - val_loss: 0.6442 - val_acc: 0.7505\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.2561 - acc: 0.8886 - val_loss: 0.6342 - val_acc: 0.7456\n"
     ]
    }
   ],
   "source": [
    "Tiefe = [1,2,3,4]\n",
    "Batchgrose = [128]\n",
    "Breite = [50,100,160,300,600]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-Charge-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\LAPPD5x5Perceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTraining.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                \n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "            history=model.fit(XTraining,YTraining,\n",
    "                              validation_data=(XVal,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 15, 40, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 50)                60050     \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 65,852\n",
      "Trainable params: 65,552\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.6289 - acc: 0.6662\n",
      "Epoch 00001: val_acc improved from -inf to 0.69920, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.70.model\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.6288 - acc: 0.6662 - val_loss: 0.5680 - val_acc: 0.6992\n",
      "Epoch 2/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.6959\n",
      "Epoch 00002: val_acc improved from 0.69920 to 0.69940, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.70.model\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.5770 - acc: 0.6959 - val_loss: 0.5668 - val_acc: 0.6994\n",
      "Epoch 3/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.6976\n",
      "Epoch 00003: val_acc improved from 0.69940 to 0.70195, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.70.model\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.5710 - acc: 0.6975 - val_loss: 0.5622 - val_acc: 0.7020\n",
      "Epoch 4/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.5657 - acc: 0.7006\n",
      "Epoch 00004: val_acc improved from 0.70195 to 0.70825, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.71.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.5657 - acc: 0.7006 - val_loss: 0.5519 - val_acc: 0.7082\n",
      "Epoch 5/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.7115\n",
      "Epoch 00005: val_acc improved from 0.70825 to 0.72360, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.72.model\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.5488 - acc: 0.7115 - val_loss: 0.5350 - val_acc: 0.7236\n",
      "Epoch 6/80\n",
      "84096/85000 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.7202\n",
      "Epoch 00006: val_acc improved from 0.72360 to 0.72975, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.73.model\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.5369 - acc: 0.7202 - val_loss: 0.5259 - val_acc: 0.7297\n",
      "Epoch 7/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.7294\n",
      "Epoch 00007: val_acc improved from 0.72975 to 0.73365, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.73.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.5271 - acc: 0.7295 - val_loss: 0.5198 - val_acc: 0.7337\n",
      "Epoch 8/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.7369\n",
      "Epoch 00008: val_acc improved from 0.73365 to 0.73675, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.74.model\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.5161 - acc: 0.7368 - val_loss: 0.5137 - val_acc: 0.7368\n",
      "Epoch 9/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7428\n",
      "Epoch 00009: val_acc improved from 0.73675 to 0.74120, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.74.model\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.5079 - acc: 0.7428 - val_loss: 0.5075 - val_acc: 0.7412\n",
      "Epoch 10/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.7470\n",
      "Epoch 00010: val_acc improved from 0.74120 to 0.74415, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.74.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.5025 - acc: 0.7469 - val_loss: 0.5067 - val_acc: 0.7441\n",
      "Epoch 11/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.7489\n",
      "Epoch 00011: val_acc improved from 0.74415 to 0.74490, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.74.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4981 - acc: 0.7488 - val_loss: 0.5045 - val_acc: 0.7449\n",
      "Epoch 12/80\n",
      "84864/85000 [============================>.] - ETA: 0s - loss: 0.4944 - acc: 0.7515\n",
      "Epoch 00012: val_acc improved from 0.74490 to 0.74710, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.75.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4944 - acc: 0.7514 - val_loss: 0.5042 - val_acc: 0.7471\n",
      "Epoch 13/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.7564\n",
      "Epoch 00013: val_acc improved from 0.74710 to 0.74800, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.75.model\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4895 - acc: 0.7563 - val_loss: 0.5003 - val_acc: 0.7480\n",
      "Epoch 14/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.7600\n",
      "Epoch 00014: val_acc improved from 0.74800 to 0.75425, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.75.model\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4851 - acc: 0.7603 - val_loss: 0.4969 - val_acc: 0.7542\n",
      "Epoch 15/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.7633\n",
      "Epoch 00015: val_acc improved from 0.75425 to 0.75490, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.75.model\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4795 - acc: 0.7633 - val_loss: 0.4930 - val_acc: 0.7549\n",
      "Epoch 16/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.4748 - acc: 0.7665\n",
      "Epoch 00016: val_acc improved from 0.75490 to 0.75785, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.76.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4748 - acc: 0.7665 - val_loss: 0.4886 - val_acc: 0.7578\n",
      "Epoch 17/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.4697 - acc: 0.7694\n",
      "Epoch 00017: val_acc improved from 0.75785 to 0.75940, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.76.model\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4697 - acc: 0.7694 - val_loss: 0.4837 - val_acc: 0.7594\n",
      "Epoch 18/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4654 - acc: 0.7711\n",
      "Epoch 00018: val_acc improved from 0.75940 to 0.76070, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.76.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4654 - acc: 0.7711 - val_loss: 0.4842 - val_acc: 0.7607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.7749\n",
      "Epoch 00019: val_acc improved from 0.76070 to 0.76200, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.76.model\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4623 - acc: 0.7746 - val_loss: 0.4813 - val_acc: 0.7620\n",
      "Epoch 20/80\n",
      "84864/85000 [============================>.] - ETA: 0s - loss: 0.4603 - acc: 0.7759\n",
      "Epoch 00020: val_acc did not improve from 0.76200\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4602 - acc: 0.7760 - val_loss: 0.4854 - val_acc: 0.7594\n",
      "Epoch 21/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4579 - acc: 0.7776\n",
      "Epoch 00021: val_acc did not improve from 0.76200\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4579 - acc: 0.7776 - val_loss: 0.4844 - val_acc: 0.7608\n",
      "Epoch 22/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.7800\n",
      "Epoch 00022: val_acc improved from 0.76200 to 0.76310, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.76.model\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4531 - acc: 0.7801 - val_loss: 0.4852 - val_acc: 0.7631\n",
      "Epoch 23/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.7830\n",
      "Epoch 00023: val_acc improved from 0.76310 to 0.76405, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.76.model\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4521 - acc: 0.7828 - val_loss: 0.4817 - val_acc: 0.7641\n",
      "Epoch 24/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.7839\n",
      "Epoch 00024: val_acc did not improve from 0.76405\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4505 - acc: 0.7838 - val_loss: 0.4823 - val_acc: 0.7623\n",
      "Epoch 25/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.7833\n",
      "Epoch 00025: val_acc did not improve from 0.76405\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4478 - acc: 0.7835 - val_loss: 0.4852 - val_acc: 0.7620\n",
      "Epoch 26/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.4440 - acc: 0.7856\n",
      "Epoch 00026: val_acc did not improve from 0.76405\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4441 - acc: 0.7855 - val_loss: 0.4850 - val_acc: 0.7635\n",
      "Epoch 27/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.7864\n",
      "Epoch 00027: val_acc did not improve from 0.76405\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4446 - acc: 0.7864 - val_loss: 0.4839 - val_acc: 0.7637\n",
      "Epoch 28/80\n",
      "84224/85000 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.7893\n",
      "Epoch 00028: val_acc improved from 0.76405 to 0.76440, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.76.model\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4402 - acc: 0.7893 - val_loss: 0.4861 - val_acc: 0.7644\n",
      "Epoch 29/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.4377 - acc: 0.7909\n",
      "Epoch 00029: val_acc improved from 0.76440 to 0.76535, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.77.model\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4377 - acc: 0.7909 - val_loss: 0.4867 - val_acc: 0.7653\n",
      "Epoch 30/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.7914\n",
      "Epoch 00030: val_acc improved from 0.76535 to 0.76655, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.77.model\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4373 - acc: 0.7913 - val_loss: 0.4819 - val_acc: 0.7666\n",
      "Epoch 31/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.4341 - acc: 0.7932\n",
      "Epoch 00031: val_acc did not improve from 0.76655\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4341 - acc: 0.7932 - val_loss: 0.4815 - val_acc: 0.7657\n",
      "Epoch 32/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.4327 - acc: 0.7939\n",
      "Epoch 00032: val_acc did not improve from 0.76655\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4327 - acc: 0.7940 - val_loss: 0.4891 - val_acc: 0.7645\n",
      "Epoch 33/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.7941\n",
      "Epoch 00033: val_acc did not improve from 0.76655\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4313 - acc: 0.7943 - val_loss: 0.4829 - val_acc: 0.7664\n",
      "Epoch 34/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4295 - acc: 0.7966\n",
      "Epoch 00034: val_acc improved from 0.76655 to 0.76700, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.77.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4297 - acc: 0.7965 - val_loss: 0.4889 - val_acc: 0.7670\n",
      "Epoch 35/80\n",
      "84224/85000 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.7978\n",
      "Epoch 00035: val_acc improved from 0.76700 to 0.76825, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.77.model\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4264 - acc: 0.7977 - val_loss: 0.4873 - val_acc: 0.7682\n",
      "Epoch 36/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4250 - acc: 0.7991\n",
      "Epoch 00036: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4251 - acc: 0.7990 - val_loss: 0.4927 - val_acc: 0.7678\n",
      "Epoch 37/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.7987\n",
      "Epoch 00037: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4243 - acc: 0.7986 - val_loss: 0.4891 - val_acc: 0.7650\n",
      "Epoch 38/80\n",
      "84224/85000 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8006\n",
      "Epoch 00038: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4224 - acc: 0.8005 - val_loss: 0.4875 - val_acc: 0.7671\n",
      "Epoch 39/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8035\n",
      "Epoch 00039: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4199 - acc: 0.8034 - val_loss: 0.4909 - val_acc: 0.7666\n",
      "Epoch 40/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8054\n",
      "Epoch 00040: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4172 - acc: 0.8053 - val_loss: 0.4921 - val_acc: 0.7661\n",
      "Epoch 41/80\n",
      "84864/85000 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.8053\n",
      "Epoch 00041: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4145 - acc: 0.8053 - val_loss: 0.4920 - val_acc: 0.7663\n",
      "Epoch 42/80\n",
      "84864/85000 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.8065\n",
      "Epoch 00042: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4146 - acc: 0.8065 - val_loss: 0.4919 - val_acc: 0.7663\n",
      "Epoch 43/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8060\n",
      "Epoch 00043: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4139 - acc: 0.8060 - val_loss: 0.4924 - val_acc: 0.7664\n",
      "Epoch 44/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8078\n",
      "Epoch 00044: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4115 - acc: 0.8078 - val_loss: 0.4914 - val_acc: 0.7665\n",
      "Epoch 45/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8091\n",
      "Epoch 00045: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4097 - acc: 0.8091 - val_loss: 0.4946 - val_acc: 0.7678\n",
      "Epoch 46/80\n",
      "84864/85000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8104\n",
      "Epoch 00046: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4074 - acc: 0.8104 - val_loss: 0.4944 - val_acc: 0.7662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8115\n",
      "Epoch 00047: val_acc did not improve from 0.76825\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4073 - acc: 0.8116 - val_loss: 0.4961 - val_acc: 0.7668\n",
      "Epoch 48/80\n",
      "84224/85000 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8118\n",
      "Epoch 00048: val_acc improved from 0.76825 to 0.76885, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.77.model\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4042 - acc: 0.8118 - val_loss: 0.4912 - val_acc: 0.7689\n",
      "Epoch 49/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8127\n",
      "Epoch 00049: val_acc improved from 0.76885 to 0.76970, saving model to Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.77.model\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4038 - acc: 0.8127 - val_loss: 0.4989 - val_acc: 0.7697\n",
      "Epoch 50/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8146\n",
      "Epoch 00050: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4025 - acc: 0.8143 - val_loss: 0.5001 - val_acc: 0.7656\n",
      "Epoch 51/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.4034 - acc: 0.8136\n",
      "Epoch 00051: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4032 - acc: 0.8136 - val_loss: 0.4993 - val_acc: 0.7658\n",
      "Epoch 52/80\n",
      "84224/85000 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8148\n",
      "Epoch 00052: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4006 - acc: 0.8148 - val_loss: 0.5001 - val_acc: 0.7657\n",
      "Epoch 53/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8158\n",
      "Epoch 00053: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.3982 - acc: 0.8158 - val_loss: 0.4961 - val_acc: 0.7668\n",
      "Epoch 54/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8157\n",
      "Epoch 00054: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.3971 - acc: 0.8158 - val_loss: 0.5039 - val_acc: 0.7648\n",
      "Epoch 55/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.8183\n",
      "Epoch 00055: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.3959 - acc: 0.8184 - val_loss: 0.5044 - val_acc: 0.7656\n",
      "Epoch 56/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8200\n",
      "Epoch 00056: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3948 - acc: 0.8200 - val_loss: 0.5112 - val_acc: 0.7659\n",
      "Epoch 57/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8205\n",
      "Epoch 00057: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.3934 - acc: 0.8204 - val_loss: 0.5072 - val_acc: 0.7667\n",
      "Epoch 58/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8208\n",
      "Epoch 00058: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.3894 - acc: 0.8207 - val_loss: 0.5095 - val_acc: 0.7663\n",
      "Epoch 59/80\n",
      "84736/85000 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8218\n",
      "Epoch 00059: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.3907 - acc: 0.8218 - val_loss: 0.5022 - val_acc: 0.7646\n",
      "Epoch 60/80\n",
      "84224/85000 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8213\n",
      "Epoch 00060: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.3902 - acc: 0.8213 - val_loss: 0.5059 - val_acc: 0.7661\n",
      "Epoch 61/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8248\n",
      "Epoch 00061: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.3850 - acc: 0.8247 - val_loss: 0.5121 - val_acc: 0.7649\n",
      "Epoch 62/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8243\n",
      "Epoch 00062: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3842 - acc: 0.8243 - val_loss: 0.5081 - val_acc: 0.7656\n",
      "Epoch 63/80\n",
      "84096/85000 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8257\n",
      "Epoch 00063: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3831 - acc: 0.8255 - val_loss: 0.5100 - val_acc: 0.7663\n",
      "Epoch 64/80\n",
      "84224/85000 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8255\n",
      "Epoch 00064: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.3837 - acc: 0.8255 - val_loss: 0.5112 - val_acc: 0.7643\n",
      "Epoch 65/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8271\n",
      "Epoch 00065: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3805 - acc: 0.8271 - val_loss: 0.5075 - val_acc: 0.7664\n",
      "Epoch 66/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.8267\n",
      "Epoch 00066: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.3812 - acc: 0.8268 - val_loss: 0.5149 - val_acc: 0.7647\n",
      "Epoch 67/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8289\n",
      "Epoch 00067: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.3805 - acc: 0.8289 - val_loss: 0.5065 - val_acc: 0.7641\n",
      "Epoch 68/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8285\n",
      "Epoch 00068: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.3786 - acc: 0.8285 - val_loss: 0.5197 - val_acc: 0.7615\n",
      "Epoch 69/80\n",
      "84224/85000 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8288\n",
      "Epoch 00069: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.3782 - acc: 0.8286 - val_loss: 0.5165 - val_acc: 0.7615\n",
      "Epoch 70/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.3750 - acc: 0.8311- ETA\n",
      "Epoch 00070: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3750 - acc: 0.8311 - val_loss: 0.5265 - val_acc: 0.7624\n",
      "Epoch 71/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8317\n",
      "Epoch 00071: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.3742 - acc: 0.8317 - val_loss: 0.5213 - val_acc: 0.7619\n",
      "Epoch 72/80\n",
      "84096/85000 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8325\n",
      "Epoch 00072: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.3735 - acc: 0.8326 - val_loss: 0.5169 - val_acc: 0.7620\n",
      "Epoch 73/80\n",
      "84864/85000 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8322\n",
      "Epoch 00073: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.3720 - acc: 0.8323 - val_loss: 0.5194 - val_acc: 0.7623\n",
      "Epoch 74/80\n",
      "84992/85000 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8319\n",
      "Epoch 00074: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.3731 - acc: 0.8319 - val_loss: 0.5107 - val_acc: 0.7634\n",
      "Epoch 75/80\n",
      "84480/85000 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8350\n",
      "Epoch 00075: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.3676 - acc: 0.8351 - val_loss: 0.5219 - val_acc: 0.7621\n",
      "Epoch 76/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.3699 - acc: 0.8338\n",
      "Epoch 00076: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.3697 - acc: 0.8339 - val_loss: 0.5274 - val_acc: 0.7627\n",
      "Epoch 77/80\n",
      "84608/85000 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8358\n",
      "Epoch 00077: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.3679 - acc: 0.8357 - val_loss: 0.5346 - val_acc: 0.7608\n",
      "Epoch 78/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8354\n",
      "Epoch 00078: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.3679 - acc: 0.8355 - val_loss: 0.5227 - val_acc: 0.7610\n",
      "Epoch 79/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8352\n",
      "Epoch 00079: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.3678 - acc: 0.8352 - val_loss: 0.5319 - val_acc: 0.7603\n",
      "Epoch 80/80\n",
      "84352/85000 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8370\n",
      "Epoch 00080: val_acc did not improve from 0.76970\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.3648 - acc: 0.8368 - val_loss: 0.5253 - val_acc: 0.7588\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1112.73x589.091 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1112.73x589.091 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "Tiefe = [3]\n",
    "Batchgrose = [128]\n",
    "Breite = [50]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-Charge-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\LAPPD5x5Perceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTraining.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                \n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            model.summary()\n",
    "                          \n",
    "            filepath=\"Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "            history=model.fit(XTraining,YTraining,\n",
    "                              validation_data=(XVal,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint\n",
    "                        #tensorboard \n",
    "            ],epochs=80)\n",
    "          \n",
    "        \n",
    "            print(history.history.keys())\n",
    "            # summarize history for accuracy\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Ende des Versuchs: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcsAAAFVCAYAAACAbsR0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd3hUVf7H8feZTHpvEAikQCD0jhRBEQHBgisWVLBR7PpbXUVdy+paVnTtHbGgoGJb7FKkF5GA9B4gEEpIT0hIP78/zgRDSEibZCbJ9/U8eWLuvXPnO+zCJ+fcU5TWGiGEEEJUzuLoAoQQQghnJ2EphBBCVEHCUgghhKiChKUQQghRBQlLIYQQogoSlkIIIUQVJCyFaMKUUhOVUgdqcP3HSqmZZzm/VCn1mF2KE6IRkbAUQgghqiBhKYQQQlRBwlIIB1FKHVBKPaaUWqKUOqGU2qKU6qGUuk4ptVcplamUmqmUspZ5TQ+l1GKlVLpSap/t9S5lzp+jlIqz3W8l0K7ce3oppf6rlNqvlEpTSv2qlIqpw2eotB6llJtSaoZS6rhSKksptVspdZXtXJRSar5SKsP22vVKqdja1iFEfZOwFMKxbgLuBAKBTcD/gAuAnkB3YCxwDYBSyh9YCCwBwoBLgEnA/WXO/wJ8DQQB99nuXdZMoBMw0HaPtcCPSinXmhZeVT3AzUB/oLPW2g+4ENhuO/cccBBoCYQAtwAZNa1BiIYiYSmEY83QWu/QWhcCn2Fago9qrXO01geBpZjAARNGBcAzWut8rfUOYDowxXb+UiAHmK61LtBarwM+KH0jpVQIcB1wp9Y6SWtdADwFtAIG1KL2quopAHyALkopq9b6kNZ6e5lzYUA7rXWx1nqz1jqpFjUI0SAkLIVwrKNl/jsXKNZaJ5c75mv777bAAX367gfxtuMAbYCEcuf3l/nvaNv3zbbuzwwgDXAtc4+aqKqe2ZiW7CtAqlLq2zJdvg/aavtBKXVUKfWGUsqnFjUI0SAkLIVoPA4BkUopVeZYO9txgMMVnI8u898Jtu8dtNYBZb68tNaf27serXWR1nq61rofEIkJ/g9t55K11vdqrWOAc4FhwLRa1CBEg5CwFKLx+AnwAP5pGzwTCzzEX12tP2K6PR9USrkqpfpgniECoLU+junqfVspFQ6glApQSl1Ry1bdWetRSg1XSvW1PQ89iekiLrKdG6+UirYFbSamW7aoFjUI0SAkLIVoJLTWmcAoYASQBMwHPgFetp3PwDxHHA+kA68D75S7zVRgF7BUKZUNbAGuBmq8sW1V9WAG73xqq+UopnV5m+1cb2AZcALYBmwA/lvTGoRoKEo2fxZCCCHOTlqWQgghRBUkLIUQQogqSFgKIYQQVZCwFEIIIaogYSmEEEJUwVr1JU1TSEiIjoqKcnQZQgghnMj69etTtNah5Y8327CMiooiLi7O0WUIIYRwIkqphIqOSzesEEIIUQUJSyGEEKIKEpZCCCFEFSQshRBCiCpIWAohhBBVaLajYc+msLCQxMRE8vLyHF2K03NxcSEgIICQkBAsFvndSwjRNElYViAxMRFfX1+ioqI4fV9bUZbWmsLCQpKSkkhMTCQiIsLRJQkhRL2QpkAF8vLyCA4OlqCsglIKNzc3wsPDycnJcXQ5QghRbyQsKyFBWX3S/SqEaOrkX7k6SM8pICe/yNFlCCGEqGcSlnVwNDOP9NwCR5dRZytWrCAgIMDRZQghhNOSsKwDq4uiqFg7tIZhw4bxzDPP1OkeQ4cOJSMjw04VCSFE0yNhWQdWi6KoxLFhWZXCwkJHlyCEEI2ehGUduLpYKCoucdj733333axYsYKnn34aHx8fYmNjufnmm5kwYQK33HILQUFB3HvvveTm5jJu3DjCwsLw8/OjT58+LFy48NR9li5ditX61yyim2++mRtuuIGpU6cSEBBAeHg47733niM+ohBCOAWZZ1kNT/2wje1Hss44XlBcQmFxCd5u9vtj7NLaj39d1rVa17755pts3bqVESNG8NhjjwEm6L766is+/fRTZs6cSX5+PiUlJYwbN45Zs2bh4eHBq6++ypVXXkl8fDyhoWds2wbA119/zdy5c3nvvfeYN28e48ePZ/To0URGRtrtswohRGMhLcs6UABO2As7ZMgQxo8fj4uLC15eXvj4+DBx4kR8fX1xdXXlwQcfxM3NjXXr1lV6j+HDhzN27FgsFgvjxo0jICCAjRs3NuCnEEII5yEty2qorKWXnlvAobRcOrb0xcPVpYGrqlxUVNRpP588eZJp06bx008/kZKSgsViITs7m+Tk5Erv0apVq9N+9vb2Jjs7uz7KFUIIpyctyzqwWszCBY4c5FPRggDlj7388sssW7aM3377jczMTDIyMggMDERrJ2wWCyGEE5KWZR24uphQcuQgn7CwMPbu3XvWa7KysnB3dyc4OJiCggKmT58uU0WEEKIGpGVZB87QsrzvvvuIi4sjICCArl0r7i6+//77CQgIoHXr1rRv3x4vL68zumqFEEJUTjXXrrh+/frpuLi4Cs/t2LGDzp07V3kPrTVbD2cR6utGmL+nvUtsVKr7ZyaEEM5MKbVea92v/HFpWdaBUsopVvERQghRvyQs68hqURQ6+So+Qggh6kbCso6sDl7FRwghRP2TsKyjxrA+rBBCiLqRsKwjV9szy+Y6UEoIIZoDCcs6srpY0GiKpXUphBBNloRlHTnDXEshhBD1S8KyjqxOsIqPEEKI+iVhWUelLUuZPiKEEE2XhGUdubrYumEdtDDBsGHDeOaZZ+xyr5tvvpkpU6bY5V5CCNGUSFjWkUUplFIUlUg3rBBCNFUSlnWklMLV4pgl7+6++25WrFjB008/jY+PD7GxsQC8//77dOvWDX9/f3r37s2CBQtOvebPP/9kyJAh+Pv7ExQUxODBg0lPT+eFF15gzpw5zJo1Cx8fH3x8fCguLm7wzySEEM5ItuiyA6uLhUIHDPB588032bp1KyNGjOCxxx4DYMaMGbzwwgt88803dO/enV9//ZVx48axceNGYmJiuOuuuxg9ejTLli2jpKSE9evX4+bmxrRp09i+fTtWq5WZM2c2+GcRQghn1uBhqZRyAZ4HbgY8gAXAbVrrlEqubwG8CFwKuAL7gIu11kds52OAd4FBQDrwitb6JbsW/cvDcGxLpafbFBZTggZXO/xxhnWHMc/X+uWvv/46TzzxBD179gTg4osv5oILLuCLL77gsccew83NjYMHD3Lo0CGioqIYOHBg3WsWQogmzhHdsA8DlwMDgDa2Y59WdKFSygP4DSgAYoEAYAJwwnbeBfgB2AGEAmOBh5RS4+ux/grqBGdZwGf//v3cddddBAQEnPpasmQJhw8fBuCjjz6ipKSEIUOGEB0dzeOPP05RUZGDqxZCCOfmiG7YW4F/a633ASilpgF7lVJRWusD5a69CROQd2qtC23HtpU5fx4QCTyitc4FNiil3gNuB+bareIqWnrpWXkcz8qje7g/Sim7vW11WCyn/74TGRnJU089xdVXX13h9dHR0Xz44YcAbNmyhVGjRhEdHc2kSZPOuJcQQgijQf91VEr5AxHA+tJjWut4IAvoUcFLLgC2A+8ppVKVUjuVUveXOd8T2K21PlHm2Abb8Yre/1alVJxSKi45ObmOn+Yvrg5cxScsLIy9e/ee+vm+++7jySefZOPGjWitOXnyJCtXrmTnzp0AzJo1iyNHjgAQEBCA1WrFarWeute+ffsokZG9QghxmoZuSvjZvmeWO55R5lxZIcAoYBPQCpgI/FMpNcF23rcG90JrPUNr3U9r3S80NLQW5VfMemquZcOHzH333UdcXBwBAQF07dqVqVOnMm3aNG655RYCAwOJiIjg6aefprDQNMwXL15M37598fHxYdCgQVx//fVMmGD+OKdMmUJOTg7BwcEEBATIaFghhLBp6G7YbNt3/3LHAzCty4quP6y1fs32c5xSajbmmecc2/nq3qveWG3dl45oWfbv35+tW7eeduymm27ipptuqvD6WbNmVXqvdu3asXbtWrvWJ4QQTUGDtiy11hnAQaBP6TGlVDtMS3BzBS/ZCFSUQKXHNgEdlVLeZc71th1vMKUty0IHreIjhBCifjliRMcMzIjVaKWUHzAdmF/B4B6Aj4FgpdRdSikXpVRPzGjYb23nlwMJwHNKKU+lVC/gNuC9+v4QZf3VspRnfUII0RQ5Iiyfx0z3WAccBlwwzyJRSk1QSp0arKO1TgAuBqZgula/Bp7UWs+1nS8GLgO6AanAz8CLWusvGuzTAC4WhUU5ZhUfIYQQ9a/Bp47YAu4B21f5c3MwzyLLHluK6Vqt7H57gQvtW2XNubpIWAohRFMlE+vsxGqxUCjdsEII0SRJWFZC13BJHmszblnKvEwhRFMnYVkBDw8PUlNTaxSYVhdLsxvgo7WmoKCAw4cP4+3tXfULhBCikZJdRyrQpk0bEhMTqckqP1l5hWSdLEJleDT4kneOZLVa8ff3JyQkxNGlCCFEvZGwrICrqyvR0dE1es0Xfxzk4e+3sOrh4YQHeNZTZUIIIRxBumHtJMTHHYCU7HwHVyKEEMLeJCztJNTXhGWyhKUQQjQ5EpZ2ciosT0hYCiFEUyNhaSfBPm6AtCyFEKIpkrC0E3erC/6erqRIy1IIIZocCUs7CvV1l5alEEI0QRKWdhTqI2EphBBNkYSlHYX6uks3rBBCNEESlnYUIi1LIYRokiQs7SjU152cgmKOZp50dClCCCHsSMLSjkZ2aYmnqwv3fPYnBUXNa1F1IYRoyiQs7SimhQ/Tr+pBXEI6z/y03dHlCCGEsBNZSN3OxvZszdbDmcxYvo/u4f5c3a+to0sSQghRR9KyrAfTLorl3JhgHp23lc2JGY4uRwghRB1JWNYDq4uFN67rQ6iPO7d/up5UmU4ihBCNmoRlPQnyduPdiX1JySlgyidx5BYUObokIYQQtSRhWY+6t/Hn9Wt7selQBnfM3iAjZIUQopGSsKyLla/A5i8h62ill4zu1opnr+jOst3JPPDVJkpKdAMWKIQQwh5kNGxtFRfBmrch57j5Oag9RA+FjqMhdsxpl153TgRpOQW8OH8XQd5u/OuyLiilHFC0EEKI2pCwrC0XK/xjJxzbAgdWwIGVsPVbWP8xDP0HDH8cygTincPak5ZTwAcr9xPk7ca9F3ZwXO1CCCFqRMKyLiwu0LqX+Rp8j2lt/nQ/rHgJCvPgomdPBaZSikcv7kzKiXxeXribK/u2ITzA08EfQAghRHXIM0t7crHCZa/BObfB72/BT/+Akr8G9VgsirsviAFg2a5kR1UphBCihiQs7U0pGDMdzv0/iPsAvr8bSopPnY5p4UN4gCdLdx13YJFCCCFqQrph64NSMOIpcPWCpf8B31Zw4eO2U4rzOobyw6YjFBSV4GaV31eEEMLZyb/U9UUpGPYwdB0Ha9+Fk+mnTg2LDeVEfhHrE9LPcgMhhBDOQsKyvg29HwpOwLoPTh06NyYEq0WxdLd0xQohRGMgYVnfwrpDzEj4/R0oNJtC+7hb6RcVKIN8hBCikZCwbAhD/g65KfDn7FOHhsW2YOexbI5l5jmwMCGEENUhYdkQIs+FNv1h9RtmLibmuSXA8t3SuhRCCGcnYdkQlIIh90FGAmyfB0BsS1/C/DzkuaUQQjQCEpYNpeMYCIk1i69rjVKK8zuGsmJPCkXFshuJEEI4swYPS6WUi1LqRaVUslIqWyn1jVIqpJJrhymltFLqRJmv1eWu0Uqp3HLX+DfMp6kBi8U8u0zaCnsXAXB+bCjZeUX8eSjDwcUJIYQ4G0e0LB8GLgcGAG1sxz49y/XFWmufMl+DK7hmVLlrMu1dtF10uwr82pjWJWYKiYtFyWo+Qgjh5BwRlrcC07XW+2yhNg0YrZSKckAtDcvqBgNvh4RVkBqPv6crfSMCWSpTSIQQwqk1aFjaukcjgPWlx7TW8UAW0KOSl7kopQ4ppY4ppX5SSvWs4JqvlFIpSqm1Sqlx9q/cjjpdYr6X6YrddiSL49kyhUQIIZxVQ7cs/Wzfy3eTZpQ5V9ZOoBcQDXQCNgOLlVKty1wzwna+DfAyMEcpNbqiN1dK3aqUilNKxSUnO6g1F9TObBS9ZyEA53csnUKS4ph6hBBCVKmhwzLb9r38AJwATOvyNFrrY1rrTVrrIq11htb6ESANGFPmmt+01nm2r7nAbGBCRW+utZ6hte6nte4XGhpqlw9UKx1Gmg2jC0/SpZUfYX4e/Lj5iOPqEUIIcVYNGpZa6wzgINCn9JhSqh2mVbm5mrcpAVQdzjtezEgoyoOEVVgsimv6t2XZ7mQOpeU6ujIhhBAVcMQAnxnAQ0qpaKWUHzAdmK+1PlD+QqXUcKVUjFLKopTyUUo9CbQE5tvOd1NKnaOUclNKuSql/gbcAHzZYJ+mNqLOBasH7DHPLa/t3xYFfLHuoGPrEkIIUSFHhOXzwA/AOuAw4AJMBFBKTVBKnShzbU/gN0z37T5gIDBSa33Idj4U+AhIB44DjwGTtNbfN8DnqD1XT4gaAnvNc8vWAZ4M79SCuesSKZQFCoQQwuk0eFhqrYu11g9orUO01r5a63Fa6xTbuTlaa58y176itY7UWntrrVtorUdrrdeVOb9Ea93Vdj7Q9jzyi4b+TLUSMxJS90LafgCuHxBByol8Fm5PcnBhQgghypPl7hwlZoT5XjqFpGMLwgM8+WytdMUKIYSzkbB0lOD2EBh1KixdLIpr+7dl5d4UDqTkOLY2IYQQp5GwdBSlTFfs/uVQaBYkuKZ/W1wsis//kNalEEI4EwlLR+owEgpz4aBZG76lnwcjOrfgq/WJ5BcVO7g4IYQQpSQsHSlqCLi4wd7fTh2aMCCStJwC5m+TgT5CCOEsJCwdyc0bIs89tfQdwJCYECKCvJjze4IDCxNCCFGWhKWjdRgJKbsgwzyntFgU1w+IYO3+NLYeds6dxoQQormRsHS00ikkexacOnT9gAh8Pay8uXivg4oSQggHyXbOR1ASlo4W0hFCO8Ef70OJGdTj5+HKLYOj+HXbMXYeO2N9eSGEaJr+eB9eioXE9VVf28AkLB1NKRj2CCTvhM1zTx2eNCQabzcXaV0KIZqH1HhY8DigYd1MR1dzBglLZ9DlcmjVC5b8B4ryAQjwcuOGQVH8tOUoe4+fqOIGQgjRiJUUw7w7zeyATpfCtm/hZHrF12YchJ8fhNy0Bi1RwtIZKAUXPg6ZB2H9rFOHpwyNxt1q4e0l0roUQjRha9+FQ7/DmOlw/jSzheGmSpb5XvA4/DEDvr8HtG6wEiUsnUX7CyFyCCx/EQrMcnchPu5MHBDJd5uOkJAqS+AJIZqglL3w27+h4xjoeS206gnhfSHuwzPD8Ogm2D4PWnSFnT9C3AcNVqaEpbNQCi58AnKOm9+ybG49rx0uFsXbS+IdWJwQQtSDkmKYd4fZ3/eyV82/gwD9JkHKbkhYffr1i58BjwC45SezXOiv/4SkbQ1SqoSlM4kYAB1Hw6rXTvXXt/Dz4Lr+bflmQyKH0nIdXKAQQtjRmjch8Q+4+L/gG/bX8a7jwN3ftC5LHfzdTLE79//AMxD+9g54+MPXk6Gg/v9tlLB0NsMfh7xMWPX6qUO3nd8epeD9FfscWJgQolnY8rX5qm97FsGip6DzZdD9qtPPuXlBr+tg+3eQk2K6Y3/7N3i3gAG3mWt8QuGKdyF5Byx4tN7LlbB0NmHdoNtV8Ps7kLwLgNYBnoztGc7X6xPJPFno4AKFEE3WiePw3V3wzeTKB9jYw7Gt8NXN0LKLaSGWdr+W1fcWKCmEjXMgfjEkrILzHjDLhJaKuRAG32taoNu/r796kbB0TqOeBncfmDsR8rMBuOXcKHILivly3SEHFyeEaLLWvAnFBRDez0zl2D3f/u+RdRQ+uwbcfeH6L833irToBBGDIe4jWPw0+LeFvjefed3wx6F1bzPdpB5JWDojv9Zw1Udmku68O0FruoX7MyA6iI9XH6CouMTRFQohmpqT6bDuA+h6Bdw4D8K6w5c3mWeF9pJ/wgRlXiZcP9f8W3c2/SZB+n448iec/xBY3c+8xuoGE7+FKz8885wdSVg6q+ihMPIp2PG9GfCDWdXncMZJFm53zrUThRCN2NoZUHAChv7DtPYmfA3+4Sbc7DHitLgIvp4ESVtNY6BVj6pf02UseAVDcAz0vK7y67yCwFK/cSZh6cwG3W1+y/vtKdi3lBGdW9I2yJMPV+13dGVCiKYk/wSsfQdiL4aWXc0xn1DTYnP1gk/HQcqeur3Hps9hz3wY8wJ0HFW911jd4Yb/me5aF2vd3r+OJCydmVIw9k2z2PrXk3DJSuTmwdGsO5DO5sQMR1cnhGhsso5C1pEzj6//yHTDDv3H6ccDI01glhTBhxeZ7tDa0NrMH2/ZDfpPqdlrW/WE4Pa1e187krB0du4+MH4OFBfClzdwTa8QfNytfLTqgKMrE0I0JlrD7HHweh8z2r7ENvahMA9WvwHR50Obfme+rmUXmDQfXL3h40th//LTz2cehu/uhunRcHRzxe+dsMp0vw64reKRr42AhGVjEBJj5hMd+RPf3x7m6r7h/Lj5CElZeY6uTAjRWBzdCMe3m8n/vz4Msy6DtP2wcTacSDqzVVlWSAxMng/+bWD2lbDjB7OQ+YLH4Y0+Zsek4gIz37Gi9VrXvmsWEuh+df19vnomYdlYdLoEznsQ/pzN3f6rKCrRzP49wdFVCSEai01zza4ety6By9+CY5vhnXPNbkdt+kP0eWd/vV9ruOUX0y365Y3wWi/TIu16BdwdZ6Zw7F8Oexed/rqMg7DzJzPtw9Wz3j5efZOwbEyGPQIxIwhe9ii3RqcyZ+1BTuQXOboqIYSzKy6CrV+b5TQ9A6H3RLhjNbQ9B3JT4Lxp1ese9QqCG78zq+5EDYHbV5per8BIM80jMBoWPnFqI3vAtjelgn6T6+3jNQQJy8bE4gLj3ge/1vwj41ksOcmyObQQwlj9Jqx9r+Jz8YshJ9ns6lEqoK0Zafp/m6o/OhXMCjrXfALXfWZWHCtldYMR/zJdvRs/M8cKcs22g50vNe/XiElYNjZeQXDtHNwKMpkb9C6zVu5mf4ps3yVEs7Z/uXle+OvDkLT9zPObvwDPILNTR1lKQWCU/ero8jez+s+SZ81Wg5vnQl4GDLjDfu/hIBKWjVFYdxj7Ou1zN/GQdS7P/FjBXw4hhHPKSbXvpsUFufD9vSb03P1g/iOn3z8vyzwz7DbOtP7qk1Jmuc7so7DmbdPSDesBEQPr930bgIRlY9XjGug/lZvVj7jv/p4lO487uiIhmo6T6WZRcXvb+Bm82A5+fsA8R6yOonxIWGNGrlZk6XNmSbixb5hxDfuWwu5f/zq/43soyoMe11b8enuLHAyxl8Cy582OIANub7TTRcqSsGzMLnqOkvD+/NdtBrO+n09BkawZK0SdFeTAzJFmPqI9FxI/tgV+vA/8ws2gl8/Hm1ZfeSUlZj3WZS+YeY3PR8BHo+GtAfDn7NOvPbwe1rxlRppGnwf9J5tFTOY/CkUF5ppNX0BQu4rnUNaXkU+Z1q1XMHS7suHetx5JWDZmVjcs18zC6u7Foyf+w5zlWx1dkRCN3/x/Qupe8GsFn42Hla+e2W2anWT2V6xsQE15eZlmuoVnINy6DC57zbQAPxgF6bYpYLlpZirGm33NajlLnjOv6zcZrvkUIgeZ7bO+v9csJFBUAN/dAz4tYeS/zT1cXOGi5yAtHv6YAZmJcGAl9BjfsK27kA5w6Svmy9Wj4d63Hjl2sT1Rd/7huI3/mPaf/I34ZQ9yvO88Wvg33rlMQjjUjh9h/cdw7t/NLhff3QmL/gXHd5iAyzluNmb/81PTtQlmLdWoIZXfU2sTcukJcMvPZs3VvjebZ4xzb4SZF0L74Waj46I8iBh0apoYXkF/3afTJWbgzIqX4OgmM+3j+Da47gvw8P/rug4jzWuXvWCeHaLNY5uG1vemhn/PeqR0DR40K6WsgIvWOr/MsVFAF2C51nqD/UusH/369dNxcXGOLsNu0ha8QNDqZ4l3iyW660AsLTpBaKx5uO4T6ujyhHB+WUfhncFmisPkRWYwjNaw/L+w5BkTbpmJgIJe10H/qfDlDebnO1advilxWavfNCNVRz0Lg+8+/VzyLrOrR06KCbR+k0+fjlGRnT/D/26H/EzTxXlVBVtTJe+CtweBLoa2A2Dyglr8gTRPSqn1Wusz+qxrGpbfAJla60m2n+8FXgXyARdgnNb6R/uUXL+aWliiNVu+eJzcHQvp6noUn+JMc1y5QPerYMh90KKzY2sUwlmVlMDsK+DQH3DbctONWNaOH8xzwNiLTeD5tzHH96+AWZeaqRFjnj/zvgmrzXPHTpeYuYkVdYUWngRdUnnYViQ1HuI+hCH3g3dwxdf8PA3+eA8uedk8yxTVYq+wPAz8n9b6a9vPh4AvtNYPKqXeBnprrQfZq+j61OTC0ub13/bw8sLdPDAkiLu7FcOuX8xO44U5ZoTa0Psb9kG/EI1BaevvstdMF2lN/PSAGbBzyy/muWKpjZ/DT/eDbyu4dSl4+Nmx4GrIyzR7VA66s2ZB3MzZKyzzgBFa65VKqe7ARqCj1jpeKXUBME9r7X/2uziHphqWWmv++b+tfP7HQZ6+vCs3DIoyAwfWvmcWM87LAKun+Yvr4W/mZYV0MF1Elf2GKkRTkrwbtv0PMg9B1mGza0bqXogdA+Nn13wgTP4JeGcQWFzN8m9Kwc8PmueakUPgqg/M4uWiUagsLGs6wCcJiAJWAqOBBK11vO2cJ1Dl3AWllAvwPHAz4AEsAG7TWqdUcO0wYAlQdomazVrrwWWuiQHeBQYB6cArWuuXavi5mgylFE9f3pXk7Dye+H4bob4ejO4WBhc8YrqPNn0B6QfMb535WWbo+tZvzRY6135e9fMSIRqz/Gz45HLIPgLeLUx3amhHE5RD/l67EaPuPmbf2U/Gwo9/h6RtZjuqoQ+YgToO3rRY2EdN/1f8CpiulOoJ3AK8WeZcb6A6W2k/DFwODABSgQ+BT4ExlVxfrLX2qeiELXh/ABYBY4FOwK9KqUSt9dxq1EGhfXwAACAASURBVNIkWV0svHFdH657/3fu/eJPXr+2twlMd184Z+qZL0hcD3MnmGHsV7wLXcY2fNFCNIQl/zEjRCcvgrb97XffdudD31vMJsqeQTDhazMqVTQZNZ1n+TDwHiaU3gGeK3OuL1CdgLoVmK613qe1zgSmAaOVUlE1rAXgPCASeERrnWsbjfsecHst7tWkeLq58PEt/ena2o8756zn8z8OVn5xm77mmUrLLmZ035L//LUxrBBNxdFNsPYd6HeLfYOy1Khn4MIn4PYVEpRNUI1allrrIuDflZwbV9XrlVL+QASwvszr4pVSWUAP4EAFL3OxDSRytb3un1rrTbZzPYHdWusTZa7fANxV9adp+gK83JgzZQB3ztnAI99uISU7n7uHx6Aq6mryDYObfjQDEpY9D4fj4PK3wbdlwxcuRKm8LDi01jwmSFhtulFDYyG0k/kKjDL7JSZt/av7MyDS7M5T9v+7JSXw4/1mRZkLn6ifWt19zr6BsmjUahSWSqkWgLfWer/tZwVMxcyz/E1r/UMVtygdDpZZ7nhGmXNl7QR6AdsAH+AhYLFSqrvW+gjgW4N7oZS6FdOyJSIioopSmwYvNyvv39iPh77ezEsLd5NyIp9/XdYVi6WCwHT1MJvChveB+Y/B2wNh7Otm7zohGtKJ42bFm0NrzbQKixVa94GACDjyJ2ybB5QZnKgsENTezCveu8hM9L9+rlkwAGDDx+YXwCtmmFV0hKihmj6z/BjYC9xr+/kp4J+2Y3crpaZorT8+y+uzbd/Lj5gNAM5YJFFrfQw4ZvsxA3hEKXUV5vnmB7b7VetetvvNAGaAGQ17ljqbFFcXC/+9uifBPm68v2I/hzNO8tLVvfD3cj3zYqWg/xSIOg++nQpzJ0KviWYOmbtvwxcvmqf5/zTrng59AKLOhTb9T5/+UJALKbvNAuIBERDaGdy8zLkjG+Hza+GDi+Dqj6FVD1j0JEQNdcxKNqJJqOkzyz7AYgCllAW4A9Mt2gl4Fvj72V6stc4ADtrug+0+7TAtwc3VrKEEKG0WbQI6KqXKTiLqbTsuyrBYFI9e0oV/X96VZbuTueSNFWxJLN8oLyO0I0xeaP6x2vQZvDsEjsnas8IOtDYr4VQ2bS1+MWz5yiykMfxRaDfszHmCbl7Quhd0vQLC+/4VlGCOT10MQdHw2dXw6TgTrpe83CR2vxCOUdOw9MeMYAUzoCcImGP7eTEQU417zAAeUkpFK6X8gOnAfK31gfIXKqWGK6VilFIWpZSPUupJoCVQuhXAciABeE4p5amU6gXchhnkIypw46Ao5t42iJISzZXvrGbO2gQqnWtrdYMLHzeTrYvy4YORZpqJELWRmQgrXja7Z7zSFX556MzALMyDn/5hulSH3F/79/Jrbf5/23E0JG0x00JCO9atftGs1bQbNhHzfHIFcAmwU2t92HbOH8irxj2eBwKBdYA7sBCYCKCUmgC8V2aqSE/gIyAEM9dyAzBSa30IQGtdrJS6DBOOqZiu2he11l/U8HM1K30iAvnx3qH8fe5GHv3fVtYnpDP9yh64ulTyu1PEQLNTwpc3wNe3mFGFFz4BFpeGLVw4v2NbYfkLZkcMNy9w9TKtwuM7YP9yQJuFwruOM0uxWd1g5NN/tfhWvARp++CGeXXfrcLdxywycGAlRJ5b548mmrearuDzCGb6yAJMWD6itX7Ndu45YKjWemh9FGpvTXUFn5ooKdG8vngPry7aw6U9WvHatb1xqWjgT6miAvj1IbMmZfsLzcokMlii4WntfN2JxYWw8hWz04W7j5nsX5ALhbnmu3eIeV7Y4xqzt6LWZpWbde+brv4LHzcr67wz2HStXvm+oz+RaKbssoKP1vo/tvVh+wP3YBYUKBUEzKxTlaJBWSyKv4/oiJebC8/9vBM3q4X/XtWz4pGyYFoBl74CrXqa9TA/HAM3fGu6vET9KymBeXfAwdUwbiZEDKjdfXLTTGsrIwFc3MyX1d18t1hNj4FyMd+L8szI1BPH4UQSnEwzXaThfc0aw36t4ehms5XVsS1mF4wxL1a9dKJSMOYFKM6HFf81731ghWmNXvRs7T6XEPWoRi3LpkRalqcrXYD9+gERPPu3bhXPxSxr/3L4/HrwDDBdZiHVeVwt6mTBY2ZzYM9AM/9wxL9g0D1gqWLogdZmnuKeBbBvmelGp6Z/7xV4h5r1hNMPQEmhOezbCnKSzao1l75c82lGJSUmaDd9bn6+9FWzaIAQDmKvtWFL97S8EhiCaU2mYZ5hfmtbtEA0QvcMjyGvsJi3l8bjbrXwxKVdzh6Y0efBzT/C7Cvhw1Fmea/wPpVfL+pm7QwTlP2nmi7L7++BhU/AgVVmicKymwSXdTLdTMbf9q1Z6LtNf7NeabvzzZZtxUWmdVeUD8UFUFIEJcVmH8SSYtPi82lpJvOXrnFamGdakYfXm7mLHv5wwaOV13A2FouZ22t1N3s69mlaGwaLpqOmzyxbYJ5Xlq62k4QZnRqFma4xSmudbPcq64G0LM+ktebpH3fw4ar93HJuFI9f0qXyLtlSqfHw6d9M1961c8wwf3Gmo5tMd2WfGyq/Jv2Aua798NPntO740cx3jb0Yxn9quke1NttCzf+nafENuQ+6XA4+Lf56XfwSmHcn5ByHYQ/DQNmqSYiq2GuLrtnA+ZhNnteVOd4f+AZYprU+y78GzkPCsmJlA3Nsz9b89+qeuFmr6ObLOgqzx5ltjq77AmIubJhiG4v8bHhrIGQlmlZU74lnXpObBjOG2Z4jupu1RbteYQbGfDYeWnaDm344fT4hmNVsvrvHTI9QFogaYkaapuyG39+GkI4wbga07t0gH1WIxs5eYZkG3K21/qyCcxOAN7TWteiLaXgSlpXTWvPusn1M/3UnQ2JCePeGvvi4V9Fjn5sGs8ZC6h7TJRvdKAZFV19xkdmtIjMRclMg+vzqb+b7y8NmL9GWXc0vFJMXmlVlSpUUm+7shFXmmd2xzbD9O/N+AIHRMGWRCc7KJG03Xa1bv4U0265559wKI546M2CFEJWyV1jmANdWtAasUmos8Fll22k5GwnLqn0Vd4iHv91C51a+fHTzOYT6up/9BTkp8PElkHHIjJKNGFi7Ny7IcY7uwrR95jnhnkVmk2Bd/Ne5Vr3gxnlVT51JXG/WKe0/Bc5/CN47z4wqvnXpX69d+ASses3siVjaTVtSYtZFjV8Mva43q9FUh9bmeSLajFoWQtSIvcLyN8xCAhdprXPKHPfGPMs8qbUeYYd6652EZfUs3pnEnXM2EObnwTd3DCbYp4rAzE6Cjy8232/8zmz/VdbZ5gjmZcLXk2HvQvCPgNY9TfdhWA8zUOX4DvOVvMMMPLnsNYgcXPG9yr/n1m9gybPmPi7uJrBc3E1rre0AM2k9YoAZrHJ0E6x8FbbPM1MpYi+G4BgIaGvmD+ammxGcLbqcPTCLC2HGBZCbCnetNS3RQ3/AR2MgZoTZbHv7PLPQQ7/JZjSpEMKh7BWWvYAlmHHnCzADfFoAF2HWax1WZvsspyZhWX1xB9KYMHMt3cL9mTNlAB6uVazck3nYBObJdOgwCrKP/fXl6mlGc/aaePqUh7T95tlcWrzpPsw+Zp7Hpe//6xqLFYI7QItOZrHsjAQ4bxqc92Dlu9Ef32Emvx9YYVpabc6xjf4sMN8zD5v3KSk0z/wCo0yL0s0X+k8yg2J8w8687+75ZtDN2QJz5auw6F9mFZmyUyrWvge/TIM+N8KWryGsu9kezep29j9XIUS9s0tY2m4UCvwDszBBK+AosBZ4WWudYodaG4SEZc38vOUod87ZwNierXnt2l5Vz8PMOAhzbzCB6dvK7C3o28qE3KHfzXZLl/zXTG4/sMoEjy4xoz2jz/vrPiczzD6FXkFmMnxpoORnm4URNn8BbQeaFV8CbNuu5WWZZ4sb58Dv75iRpSP+ZaYlVLREX0GumQKRsBoObzAtzH6TzRzSsykNzJZd4Yb/nR6Yafvh7UFmsNO1c05/ndbwzRTY+rX5M7l1acWBLIRocHYLy6ZCwrLm3l66lxd+3cW9F3bg/pG1XJRaa9j8JSx83KwKE3uxmSwfGAnXfwnB7Wt2v81fmnmEymK6STMPme7cUn1uhAufrHpFmdra9atZM9cz0IS5T6iZynF4PaTsNd2v/uFnvq4gx2wb1WuC2SVDCOEUah2WSql11GC5D631OTUvr+FJWNac1pppX2/mq/WJvDK+J1f0blP7m+VlwbLpZpRo5LlwzazarzObts8MkikuMs8U/duY4GzZDUJja19jde1bCnEfmZVsSr/yMs2WULIajRCNSl3C8mNqFpaN4l8HCcvaKSgq4aYP/2B9QjqzJp3DoPZ1bLHlpoFHQNVLtjU2JSVN7zMJ0QxIN2w5Epa1l5lbyJXvruZw+kk+uKkfg2POMv9PCCEakcrCUn71FTXm7+XK51MH0jbIk1s+Xsfy3Y1ihUMhhKg1CUtRK6G+7nw+dSDtQn2Y8kkcS3Yed3RJQghRbyQsRa0F+7jz+dQBxLb05dZP41iw7ZijSxJCiHohYSnqJMDLjdlTBtC1tT93fbaBJbukhSmEaHokLEWd+Xu68snkc4gN8+X2T9fz+75UR5ckhBB2JWEp7MLPw5VPJg2gbZAXU2bFselQhqNLEkIIu5GwFHYT5O3G7MkDCPR25aaP/mDXsWxHlySEEHYhYSnsKszfgzmTB+JutTDxg7XsSz7h6JKEEKLOJCyF3UUEezF78gCKSzRXv7uGjdIlK4Ro5CQsRb3o0NKXr24fhKebC9fN+F3mYQohGjUJS1Fv2of68O2dg2kX6s2UT+L4ct0hR5ckhBC1ImEp6lULXw/m3jaIwe2DmfbNZl5ZuJvikua5HrEQovGSsBT1zsfdygc39Wdcn3Be+20PV7+7mr3HZeCPEKLxkLAUDcLNauGlq3vy6vhe7EvJ4eLXV/DO0niKikscXZoQQlRJwlI0GKUUf+sdzoL7zmN4bAum/7qTK95ezZr4VJrrVnFCiMZBwlI0uBa+HrwzsQ9vXd+Ho5knue7937no1eXM/j2BnPwiR5cnhBBnkM2fhUOdLCjmh01HmLXmANuOZOHrbuWa/m2ZOrQdYf4eji5PCNHMVLb5s4SlcApaazYczGDW6gP8vOUoFqW4ul8b7hjWnjaBXo4uTwjRTEhYliNh6bwOpeXyzrJ4voo7hNYwrk84D1wUSwtfaWkKIepXZWEpzyyF02kb5MVzV3Rn+bQLmDgwknkbjzD+vd85lpnn6NKEEM2UhKVwWq38PXlybFc+nzqA5Ox8rntfAlMI4RgSlsLp9Y0MYtakc0jOzufaGWskMIUQDU7CUjQKfSMDmTXpHFJOFHDtjDUczTzp6JKEEM1Ig4elUspFKfWiUipZKZWtlPpGKRVSjdfdoZTSSqnHyh3XSqlcpdSJMl/+9fcJhKP0jQzkk8kmMMe8toIps+J4/bc9LN11nLScAkeXJ4RowqwOeM+HgcuBAUAq8CHwKTCmshcopSKBfwBbKrlklNZ6pZ3rFE6oT0QgX9w6kA9W7mdTYgaLdiSdOtc93J9Le7Tikh6tZLqJEMKuGnzqiFIqAfi31voD28/tgb1AtNb6QCWvWQS8D9wBLNJaP1PmnAaG1jQsZepI05CdV8jWw1lsOJjO/G3H2JyYCUDviADG9Q7nmv5tcbe6OLhKIURj4RRTR2zdoxHA+tJjWut4IAvoUclrbgNytdZzz3Lrr5RSKUqptUqpcfasWTg3Xw9XBrUP5q4LYvj+7iEse3AY00bHkl9YwuPfbWPky8v5ectRWXtWCFEnDf3M0s/2PbPc8Ywy505RSkUAj2FalJUZAUQDbYCXgTlKqdEVXaiUulUpFaeUiktOTq5p7aIRiAz25s5hMfz8f0OZNekcPF1duHPOBq56dw0bDqY7ujwhRCPV0GGZbftefgBOAKZ1Wd5M4Bmt9eHKbqi1/k1rnWf7mgvMBiZUcu0MrXU/rXW/0NDQWpQvGpPzO4by071D+M+47iSk5jLu7dX867utFBTJtmBCiJpp0LDUWmcAB4E+pceUUu0wrcrNFbxkJPCcrYs1BTgXeEQpteIsb1MCKPtVLRozq4uF686JYOmDw7h5cBSz1iQwXqaeCCFqyBHzLGcADymlopVSfsB0YH4lg3vaAj2BXravOOAt4CoApVQ3pdQ5Sik3pZSrUupvwA3Alw3wOUQj4uNu5cmxXXnr+j7sPpbNJa+vZNXelDOuk2ebQoiKOGLqyPNAILAOcAcWAhMBlFITgPe01j4AWuvEsi9USuUDWVrr0vkCocCbQBRQAMQDk7TW39f/xxCN0SU9WtGplS+3f7qeGz5Yy996h5NfWMLRzJMczcwjLaeASUOimXZRLEpJB4UQwpBdR0SzlJNfxOPfbWXBtiRa+LoT5u9BK39PsvMKWbA9ifH92vLcuO64WCQwhWhOKps64oiWpRAO5+1u5eVrep1xXGvNKwt38/rivWTlFfLqtb1knqYQQtaGFaIspRT3j4rl8Uu78MvWY0z+OI6c/CJHlyWEcDAJSyEqMHlINC9d3ZM1+1K56t01rK5gMJAQovmQsBSiElf2bcP7N/YlI7eA62euZcLM3/lTFjYQolmSAT5CVCGvsJjZvyfw9tJ40nIKGNmlJYPaBRPk7UaAlytB3m5EBHkR4OXm6FKFEHVU2QAfCUshqulEfhEfrtzP+yv2kZ13+nNMT1cXpo2O5aZBUVhkBK0QjZaEZTkSlqK2iks0WScLSc8tID23gLScQuasTWDprmT6RwXywlU9iQ7xdnSZQohakLAsR8JS2JPWmm82HObfP2wjv6iEB0bFctPgKNysMixAiMZEwrIcCUtRH5Ky8nj0f1tZtCMJH3cr58eGMqpLS4bFtsDf09XR5QkhqiCLEgjRAFr6efD+jX1ZsSeFn7ccZdGO4/y0+ShWi2JAuyAuiG3B8E4taBfq4+hShRA1IC1LIepRSYlmY2IGC7cn8duOJHYnnQAgKtiLCzq14KKuYfSPCpJl9YRwEtINW46EpXCEQ2m5LNl1nMU7j7M6PpWCohJCfNwY1TWMMd3CGNguGFcXec4phKNIWJYjYSkcLSe/iKW7kvl561GW7DxObkEx4QGeTL+yB0M6hDi6PCGaJQnLciQshTPJKyxm6a7jvDB/F/uSc7hhYCQPj+mEt7sMKxCiIVUWltLfI4QT8HB1YXS3Vvx871CmDIlm9toExry2gj/2pzm6NCEE0rJ0dBlCVOiP/Wk88NUmDqXn0jcikKEdQhnSIYSebfyxyjNNIeqNdMOWI2EpnF1OfhEzV+xn8c4kNh/ORGvw9bByTlQQnVr5EhvmR+cwX6JDvCVAhbATCctyJCxFY5KeU8Dq+FRW7Elmw8F09iXnUFRi/u66WS1c2KkF4/u3ZWiHUJmGIkQdSFiWI2EpGrP8omLij+ewKymLTYcy+X7TEdJyCmjt78FV/doyvn9bwgM8HV2mEI2OhGU5EpaiKSkoKmHRjiTmrjvE8j3JuCjF1f3acs/wGFpLaApRbRKW5UhYiqYqMT2XGcv38fkfB1EorjunLXddEEMLPw9HlyaE05OwLEfCUjR1hzNO8ubiPXwVl4iLRTFlaDR3DIvBR+ZuClEpCctyJCxFc3EwNZeXF+5i3sYjhPq68+CoWK7s20YGAglRAVmUQIhmKiLYi1ev7c3/7hxMm0BPpn2zmcveWMma+FRHlyZEoyFhKUQz0TsikG/vGMzr1/UmI7eA697/namfxLEv+YSjSxPC6UlYCtGMKKUY27M1ix8YxoMXxbJ6bwqjXlnOk99vIy2nwNHlCeG05JmlEM1YcnY+ryzazRd/HMTLzUpLP3e0hhKt0YCnqwttg7yIDPIiItiLyGBvBrYLwt3q4ujShagXMsCnHAlLIf6yOymbD1bs50RBERalsChQwIn8Ig6m5XIwLZe8whIAwgM8ueuCGK7q2wY3q3ROiaZFwrIcCUshqk9rTXJ2PpsSM3lryV42HsqgTaAn9wyPYVyfNrJhtWgyJCzLkbAUona01izdncyrC3ezKTGTVv4ejOsTzpV92tAu1MfR5QlRJxKW5UhYClE3WmuW7DrOJ2sSWL47mRIN/SIDuapvG8b2ao2Xmyx+IBofCctyJCyFsJ+krDzm/XmYr9Ynsvf4CYK83Zh0bhQ3DIrC39PV0eUJUW0SluVIWAphf1pr4hLSeWdpPIt3HsfH3crEgZFc3qs1blYLVovCohSebi6E+Lg7ulwhziBhWY6EpRD1a9uRTN5ZGs/PW45SUsE/M5f0aMVTY7tKaAqnUllYykMFIUS96Nranzev78OBlBy2HM6kRGuKijXFWnMgJYeZK/azJj6VJ8d25bIerVBK1qoVzkvCUghRr6JCvIkK8T7j+N96h/Pg15u59/M/+X7jER4eE0vrAE8ZGCScknTDCiEcprhE89Gq/bw4fxf5RWbRA09XF4J93AjxcadX2wAGtw9mQLvg0wYKlc77PJ6dT+dWfrKDirAbp3lmqZRyAZ4HbgY8gAXAbVrrlCpedwfwNvC41vqZMsdjgHeBQUA68IrW+qWq6pCwFMJ5HErLZU18Kqk5BaTl5JN6ooAjmSfZeCiDvMISLAq6h/vT0s+Dg2m5JKTmcrKwGIBOYb48cnFnzu8Y6uBPIZoCZ3pm+TBwOTAASAU+BD4FxlT2AqVUJPAPYEu54y7AD8AiYCzQCfhVKZWotZ5bL9ULIeyubZAXbYO8zjieX1TMnwczWB2fypr4FPan5BAZ7MW5MSFEBnvh6mLhnaXx3PThHwztEMIjYzrTpbWfAz6BaOoc0bJMAP6ttf7A9nN7YC8QrbU+UMlrFgHvA3cAi0pblkqpC4CfgBZa6xO2Y08DQ7TWF5ytDmlZCtE0FBSVMPv3BF5fvIfMk4Vc0Sucu4bH0F5WExK14BSbPyul/IEIYH3pMa11PJAF9KjkNbcBuZW0FHsCu0uD0maD7bgQohlws1qYNCSaZQ9ewK1D2/Hz1qOMfHkZ93z+J7uOZTu6PNFENHQ3bGn/SGa54xllzp2ilIoAHgMGVnI/3+rey3a/W4FbASIiIqpXsRCiUfD3dOWRizsz9bx2zFyxn0/XHOCHTUe4qGtLRnYJo09EANEh3jJFRdRKQ4dl6a95/uWOB2Bal+XNBJ7RWh8+y/2qey+01jOAGWC6YatTsBCicQnxcefhMZ247bx2fLRqP5/8nsD8bUkABHq50jsikH5RgQxqF0z3cH+ssmOKqAZHPbN8Smv9oe3ndkA8FTyzVEppIA0oLdIfKAA2aK2HlnlmGaq1zrG95t/AUHlmKYQAKCnR7E0+wYaEdNYnpLP+YDr7knMA8HG30j8qkEHtgzknOpiurf1ku7FmzpmmjjwK3AiMxoyG/QDw1VqPruDaNuUOfQWsAF7SWifZRsNuxUw/eRiIBeYD/6e1/uJsdUhYCtF8JWfns3Z/KmviU1mzL/VUeHq4WujVNoD+UUEMbBdM/6gg2eC6mXGmqSPPA4HAOsAdWAhMBFBKTQDe01r7AGitE8u+UCmVD2RprZNs54uVUpcB72GCNwN4saqgFEI0b6G+7lzaozWX9mgNwPGsPOIS0ll3II24A+m8tWQvbyzei4+7lfM6hnBBbAsu6NRC1rFtxmQFHyGEKOdEfhFr4lNZvDOJxTuPk5SVj4tF8eBFsdx2XjsZJNSEOVPLUgghnJqPu5WRXVoysktLtNZsO5LF20v38vwvO9l9LJvnxnXHw9XF0WWKBiRhKYQQZ6GUolu4P29d34c3F+/lpYW72ZeSw4wb+tLCzwOA3IIi/tifxsZDGfh6uBLm50GYv+3Lz0PWrm0CJCyFEKIalFLcc2EHOrT05f4vNzL2zVWM79+WtftT2ZCQQUFxSYWv69nGn0+nDMDPw7XC86JxkGeWQghRQ9uPZDH1kzgOZ5ykcys/hnYIYUhMCP2jgjhZWMyxzDySsvLYczybF37dRd/IQGZNOke6bhsBp5k64iwkLIUQdZFfVExufjGB3m5nve67jYf5+9yNjOjckncm9JFFEJycU6wNK4QQTYW71aXKoAS4vFc4T17WlYXbk3j42y001wZKYyfPLIUQop7dNDiK1JwCXv9tDwGerjxwUax0yTYyEpZCCNEA7hvRgfScAmau3M8Hq/bTJtCTmFAfYlr40LmVH/2jgmgT6ClzOJ2UhKUQQjQApRRPje3KkA4hbD+SRXzyCfYeP8Gq+FQKisxI2pZ+7vSLCqJba38ycgs4lJ7LwbRcEtNP0j7Uh+eu6E5smK+DP0nzJAN8hBDCgYpLNLuTsok7kMa6A+nEHUjjSGYeblYLbQI9aRPoRXiABwu2JZGVV8g9wztwx7D2suB7PZHRsOVIWAohnFXmyUJ83a1YyixmkHoin6d+2M73m47QuZUfz17RDS83FxJSc0lIzSEhNRcXi6JdiDftW/jQLtSHVn4ep91DVE3CshwJSyFEY7Rg2zEem7eV49n5px3393SlpESTnV906piPu5WLuoZxdb82DIgOkueh1SBrwwohRBMwqmsYA6KD+X7TYQK83IgM9iIyyBt/L1e01iSfyGdfcg7xySfYdCiDn7cc45sNiUQEeXF13zYM7RhKiI8bwd7ueLrJiNzqkpalEEI0YScLivll61G+iktkzb7U0855ubnQyt+D+0Z2PLVdWXMn3bDlSFgKIZqbQ2m57DqWTWpOPqk5BaSdKGDNvlS2Hcnikh6tePrybgRVY6GFpky6YYUQoplrG+RF2yCv044VFZfw7rJ4XvttD2v3pfLcFd0Z1TXMQRU6L2lZCiGEYPuRLP7x1SZ2HM2id0QAnVv50bGFDx1a+tKhhQ+hvu7NYoCQtCyFEEJUqktrP76761zeX7GPJTuP8+OmI2Tl/TWy1tvNhchgb6JCvIgK9qZHG3/O6xiK1/+3d/dBdtX1sP3sAQAAC3RJREFUHcffn908SR4WwpKQEHlIQB4CRIgyFCOiaHTCBItGTWqriA20HYs6KmPNTInt4EPV/sOUKYYKrVYnjRgRBjqg1RoKaAgCSRgBgYRAnsjDJhvyuNlv//j9Lh4umx673N17bvJ5zZy595zf2Xs/e3JzvnvO+d3zG9Z3Gek52HtY3TTeR5ZmZvYaEcFL3ft4atMufre5mzX5+5xrtu5m3bbd9PQGw4e08fbTOnnPWeO54JRjeXpTNyvWbufhtdtZ+cIOLjhlLLd8/C0tdR9cd/Cp42JpZtY/Bw72snzNNu5dvYn7ntjEi117Xmkb1t7G2SeMYcpxo1iy4gUuO2cCN847r2VujuDTsGZm1hBD29u4aEonF03p5PrZZ/HEhp08uq6L08eP5uwTOl45kjx13Ci+es9vmXTMG/ibWWc2OfXr42JpZmb9JompEzuYOrHjNW1XXzyZddt3c/Mvn2XS2KP4swtPAtKR6d0rN/C9h9bS3ibmTH8js845/pDXP6ugusnMzKylSWLh7Kms79rL9XesYsyIIWzcsZfbHljDhh17mdw5kgA+v+QxFv5kNbOnTWD2tImMGTGUCOiNIIB2ieFD2xg+pI1hQ9oYPWIoo4YPbvnyNUszMxtQL+/r4SPffpBVL+4E4KIpx/LJGafwztPHIcHyNdtZvHwdd6/cwJ4DB0tfr71NXPW2k/nMu9/EyAYXTXfwqeNiaWY2eDZ37+V7D67lvWcf3+cpW4DuvQdYvmYbB3tBgJSmg72wr+cg+3t62dfTy6PPd7H44XVM6BjBwsunMvOs8Q37DqiLZR0XSzOz1rVi7TYWLF3Fbzd2c+kZ41h4+dTX3J2oPw5VLA+fb4yamdkRY/pJY7nzr2ewYNaZPPjsVr60dOWAvp87+JiZWUsa2t7G/IsnM+vcCRzo6R3Q93KxNDOzlnbC0W8Y8PfwaVgzM7MSLpZmZmYlXCzNzMxKuFiamZmVcLE0MzMr4WJpZmZWwsXSzMyshIulmZlZCRdLMzOzEi6WZmZmJY7YUUckvQSsbcBLdQJbGvA6g8mZB4czDw5nHnitlhf6n/mkiDiufuERWywbRdLDfQ3nUmXOPDiceXA488BrtbzQ+Mw+DWtmZlbCxdLMzKyEi+Xr9+1mB+gHZx4czjw4nHngtVpeaHBmX7M0MzMr4SNLMzOzEi6WZmZmJVws+0lSu6RvSHpJUrek2yV1NjtXjaS5kpZJ2impp4/290laLWmPpFWSZjYjZyHP13OenZLWS1okaWzdOh+T9Iyk3ZJ+JWl6s/IWMt0g6bmce7OkH0o6sdBeucw1ktokPSApJE0qLK9UZkm3STogaVdh+qu6dSqVuUbSuyU9lDNvkXRToa1SmfP/v+I23pM/G+fn9krtM2okHS9pcd4Xb5f0X5KmFdobs50jwlM/JmAB8BQwGegAbgfuaXauQr73AvOAq4CeurbJwG7gT4FhwEeBl4GTm5j3K8B5wFDgOOAe4I5C+4yccSYwHLgO2ASMafJ2PgPoyM+PAv4ReKDKmQvZPwf8FAhgUlUzA7cBt/wf7ZXLnHNdAnQBc3KuEcD5Vc5cl/8GYHV+Xrl9RiHnj4D7gLE52z8A6wA1cjs3/R+kVSfS3X8+WZifknc6Tf/w1OW8pI9i+WVgWd2yZcD1zc5byHMZsKMw/6/AdwvzAp4HPt7srIVMI4FvAlurnhl4E/AM8Oa6Ylm5zH9Asaxc5pzjQeBrrZS5kGcIsAG4Ns9Xdp8BPA5cXZg/PX+mOxu5nX0ath8kdQAnAitqyyLiGWAncG6zcv0/TKOQPXskL6+KS0n/CWpelTnSJ/83VCCzpD+RtAPYBXwaWJibKplZUhvwHeALpCOfokpmBj4oaZukp/Llj1GFtsplljQSuADYK+mRfAr2F5Jqd5SpXOY6f0w6Y/Zveb7K+4xvkD4fnZJGAFcD90fEFhq4nV0s+2dMftxRt7yr0FZlo6lwdkkfBOaTCk9NZTNHxPcjogOYQCqUK3NTVTN/GtgYET/qo62KmW8kne7uBK4A3gEsKrRXMfMxpP3rfOBKYCJwL3C3pKOpZuaia4DFEVH7Y6rKef8HaAdeIv3B+gHSdocG5nax7J/u/NhRt/xo0tFl1XVT0eySPkTaEV4eEY8UmiqbuSYiNpKy35U7J1Uus6RTSdcqP3WIVSqXOSJWRMSmiOiNiNXAZ4E5kobnVSqXmd/vI26NiMcjYj/wVdI1+YuoZmYAJE0hndn558LiSubNZ0l+Suo/0kHqN3ADsEzSeBqY28WyH/JfW88D59eWSZpM+mvl8UP9XIU8RiF7dl5e3jSSPgHcDMyOiJ/XNb8qsySRrrc1NXMfhpCuXU6kmplnkDpQrZK0hXQqDeDx3MO0ipnr9eZH5cfKZY6IHcAa0rWz1zRTwcwF1wCPRcSvCssquc8gdeo5BbgxInZGxP6IuIVU2y6kkdu52RdnW3Ui9YZ9Mv9DjQGWAP/Z7FyFfO2k3nczgZ78fARpBzOF1LNtHukv3Xk0vzfstcBW4K2HaJ9BOsVyKanH2+dpfi/NNtIR2rg8PwlYCjxHKppVzHxUzlmbLiTtvN8CjKpo5rnA0fn5acADwO1V/mzkXF8AXgDOyp+H60idZjoqnHkYsBm4pm555fYZhWxPkk7Vj8zb+SpgP6kHb8O2c1N/yVaecjH6Jmm8tG5S9+XOZucq5Lsy7wTrp5Nz+/uA1cCe/DizyXkDOJA/2K9Mdet8DHg2Z/41ML3JmduAu/PO5WXgReDfgSlVzdzH73Ayhd6wVcwM/ALYlrfxc6Sv54ypW6dSmXMmAX8HbCRdJ/s58OaKZ56b92ej+mir1D6jkOtM4K68L95B6tDz/kZvZ98b1szMrISvWZqZmZVwsTQzMyvhYmlmZlbCxdLMzKyEi6WZmVkJF0szM7MSLpZm1i+SLsnjHZ7d7CxmA83F0szMrISLpZmZWQkXS7MWI2mGpP+WtFvSVkmLJI3ObVfmU6NvlbRM0p48BuQVfbzOpyQ9LWmfpN9J+mwf65wr6U5JXZJ2Sfq1pPfUrdYpaUlufzbfkN3ssOJiadZCJL0N+BnpfqNzgM8As4Bb61ZdDNxBGttvJbBE0rTC68wn3Xz6J8Bs0kAA35L0xcI6Z5DGCpwA/AVpLMmlwBvr3msRaRSHK0j3cf0nSRe8/t/WrDp8b1izFiJpGdATEe8sLHsXqYCeQxo95FZgQUR8Jbe3AU8Aj0bE3Dy/Drg3Ij5ReJ2bgI8C4yNir6QfAG8HTouIPX1kuYR0c/C/j4i/zcuGAuuBf4mIL9b/jFmr8pGlWYuQdBTwR8B/SBpSm4D7SSO2TC+svrT2JCJ6SUeZtaO9SaTxNpfUvcVi0nBz5+T5dwGL+yqUde4tvNcB4On8HmaHDRdLs9ZxDGlouJtIxbE27SONMVg8Pbq57mc3k06nUnjcVLdObX5sfjyWNP5ima66+f2ksVPNDhtDmh3AzP5gXaSxJxeSxtGst5402DfAONJg2hTma4VvQ2FZ0fj8uC0/buX3hdXsiOYjS7MWEREvAw8Bp0fEw31M6wurv9L7NV+jfD9p4FuAF0iF9UN1b/FhYCepQxCk66AfluSjRDvi+cjSrLVcB/xMUi/wQ9Ko9icClwELCuv9uaT9wCpgPnAqMA/SNUxJC4GbJW0F7gPeAfwl8KWI2Jtf48vAcuCXkr5FOtI8D9gaEd8Z0N/SrGJ8ZGnWQiLifuBi4Djgu8CdpAK6jldfg5xLOrr8MTAN+EhE/KbwOouAa/M6d5EK6eci4muFdZ4EZgBbgFtInYbmAGsH6Nczqyx/dcTsMCLpStJXR0ZHxK4mxzE7bPjI0szMrISLpZmZWQmfhjUzMyvhI0szM7MSLpZmZmYlXCzNzMxKuFiamZmVcLE0MzMr4WJpZmZW4n8B4B9lerBZQCQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [7, 5]\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 14:23:55.942609 23116 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0529 14:23:55.942609 23116 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0529 14:23:55.942609 23116 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"Perceptron_LAPPD_5x5_PID_120k-improvement-val-acc_0.77.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.4868744076112635\n",
      "Test accuracy:  0.7707431\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    " \n",
    "    print(cm)\n",
    " \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5067 2429]\n",
      " [1011 6498]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rounded_labels =np.argmax(YTest, axis=1)\n",
    "y_prob = np.array(model.predict(XTest, batch_size=128, verbose=0))\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "cm = confusion_matrix(rounded_labels, y_classes)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67596051 0.32403949]\n",
      " [0.13463843 0.86536157]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "cm = cm.reshape((2,2))\n",
    " \n",
    "class_names = [r\"$e^{-}$\", \"muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n Perceptron-model with 77% accuracy \\n Pure LAPPD (5x5 res)')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charge and Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,9,24,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,9,24,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,9,24,1)\n",
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,9,24,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,9,24,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,9,24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.6991 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6944 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6942 - acc: 0.5001 - val_loss: 0.6970 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6945 - acc: 0.4915 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6939 - acc: 0.4966 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.7000 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6943 - acc: 0.4948 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6939 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6932 - acc: 0.5049 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6935 - acc: 0.5006 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6934 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6933 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6934 - acc: 0.5009 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6935 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6933 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6933 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6932 - acc: 0.5034 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6934 - acc: 0.4916 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6933 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6931 - acc: 0.5075 - val_loss: 0.6954 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6938 - acc: 0.5022 - val_loss: 0.7340 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6938 - acc: 0.4942 - val_loss: 0.7051 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6934 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 215us/sample - loss: 0.6968 - acc: 0.4954 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6945 - acc: 0.5049 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6940 - acc: 0.5007 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6946 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.6938 - acc: 0.5079 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6941 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6938 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6934 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6941 - acc: 0.4912 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6935 - acc: 0.4961 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6949 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6934 - acc: 0.5036 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6933 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.5001 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6933 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6934 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6933 - acc: 0.4960 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.5041 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6934 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.5019 - val_loss: 0.6942 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.4916 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4994 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 239us/sample - loss: 0.7066 - acc: 0.5031 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6946 - acc: 0.5100 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6937 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6933 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.4952 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6939 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6936 - acc: 0.4997 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6933 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6933 - acc: 0.4886 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6932 - acc: 0.5012 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.4923 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6933 - acc: 0.5025 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.5007 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.5008 - val_loss: 0.8325 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.5040 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6931 - acc: 0.5111 - val_loss: 0.7057 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6934 - acc: 0.4940 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6934 - acc: 0.4941 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6932 - acc: 0.5057 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6933 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6931 - acc: 0.5036 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 309us/sample - loss: 0.8167 - acc: 0.4974 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.7259 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.7049 - acc: 0.5016 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6984 - acc: 0.5049 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6970 - acc: 0.4916 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6958 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6946 - acc: 0.5017 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6940 - acc: 0.5016 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.5031 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6941 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.5049 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6938 - acc: 0.4992 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6934 - acc: 0.5067 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6936 - acc: 0.5085 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.5022 - val_loss: 0.6954 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6940 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6935 - acc: 0.5061 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6939 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.5048 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6939 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.5005 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6938 - acc: 0.5005 - val_loss: 0.6952 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.5108 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6938 - acc: 0.4954 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 243us/sample - loss: 0.7052 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6958 - acc: 0.5006 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6950 - acc: 0.4998 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6945 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6937 - acc: 0.4972 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6936 - acc: 0.4917 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.4994 - val_loss: 0.7098 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6947 - acc: 0.5025 - val_loss: 0.8923 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6935 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6933 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6934 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.4996 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4999 - val_loss: 0.6994 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.5051 - val_loss: 0.7077 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6937 - acc: 0.5025 - val_loss: 0.7681 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.4986 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.4957 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6932 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6936 - acc: 0.4928 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6933 - acc: 0.5026 - val_loss: 0.6960 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6936 - acc: 0.4939 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5031 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 229us/sample - loss: 0.6980 - acc: 0.4980 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6946 - acc: 0.5015 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6946 - acc: 0.5040 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6940 - acc: 0.4974 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6935 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6932 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6933 - acc: 0.4976 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6934 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6935 - acc: 0.4925 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6933 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6933 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6934 - acc: 0.4922 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6933 - acc: 0.5031 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6934 - acc: 0.4931 - val_loss: 0.6996 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6934 - acc: 0.5026 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6936 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6937 - acc: 0.4992 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6933 - acc: 0.5024 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6934 - acc: 0.4961 - val_loss: 0.6938 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6937 - acc: 0.4974 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6936 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6936 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6938 - acc: 0.5014 - val_loss: 0.7525 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 245us/sample - loss: 0.7076 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6938 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.5035 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6935 - acc: 0.4869 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6935 - acc: 0.4952 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.5074 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4930 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6937 - acc: 0.4965 - val_loss: 0.7009 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.5051 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5048 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6935 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6938 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4908 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.4951 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.4975 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.4931 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4934 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 264us/sample - loss: 0.8525 - acc: 0.4995 - val_loss: 0.7174 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.7305 - acc: 0.4899 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.7069 - acc: 0.4988 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.7017 - acc: 0.5003 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6968 - acc: 0.5011 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6953 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6948 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6943 - acc: 0.5016 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6941 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6941 - acc: 0.4963 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.4992 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6942 - acc: 0.4956 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6938 - acc: 0.5029 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6935 - acc: 0.5056 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6939 - acc: 0.5019 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6937 - acc: 0.5005 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5065 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6939 - acc: 0.4972 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6937 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6939 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6938 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6938 - acc: 0.5021 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6935 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6937 - acc: 0.5045 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6935 - acc: 0.5065 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6940 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6937 - acc: 0.4984 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.5011 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 258us/sample - loss: 0.7049 - acc: 0.5006 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6977 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6967 - acc: 0.4962 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6951 - acc: 0.4911 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6950 - acc: 0.5016 - val_loss: 0.6974 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6961 - acc: 0.4940 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4937 - val_loss: 0.6979 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4965 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.4957 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.4986 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6932 - acc: 0.5029 - val_loss: 0.8747 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6937 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6941 - acc: 0.5004 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6949 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6934 - acc: 0.5036 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6938 - acc: 0.5012 - val_loss: 0.7536 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.5107 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.5033 - val_loss: 0.7008 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 245us/sample - loss: 0.7151 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6955 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6973 - acc: 0.4926 - val_loss: 0.6962 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6957 - acc: 0.4966 - val_loss: 0.6973 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6936 - acc: 0.5040 - val_loss: 0.6968 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6942 - acc: 0.4942 - val_loss: 0.7072 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6942 - acc: 0.4944 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6953 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.4941 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6934 - acc: 0.4967 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4966 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6932 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6935 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6934 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.5002 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6945 - acc: 0.4974 - val_loss: 0.7704 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6940 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5026 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6934 - acc: 0.4968 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 288us/sample - loss: 0.7108 - acc: 0.4930 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4945 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6936 - acc: 0.4972 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6932 - acc: 0.5075 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6933 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6988 - acc: 0.5001 - val_loss: 0.6993 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6936 - acc: 0.5033 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6932 - acc: 0.5035 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6935 - acc: 0.4929 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6933 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6935 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6935 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6933 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6953 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 300us/sample - loss: 0.8582 - acc: 0.4992 - val_loss: 0.7036 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.7314 - acc: 0.4963 - val_loss: 0.7209 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.7238 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.7013 - acc: 0.5056 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6990 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6961 - acc: 0.4991 - val_loss: 0.6949 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6950 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6940 - acc: 0.5013 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6937 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6940 - acc: 0.4938 - val_loss: 0.6946 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6936 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6939 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6936 - acc: 0.5049 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6938 - acc: 0.5010 - val_loss: 0.6953 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.5055 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5033 - val_loss: 0.6937 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6937 - acc: 0.5023 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6939 - acc: 0.4929 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6937 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6940 - acc: 0.5009 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6937 - acc: 0.4940 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5064 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6936 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6938 - acc: 0.4956 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6935 - acc: 0.5074 - val_loss: 0.7945 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6939 - acc: 0.4968 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 272us/sample - loss: 0.7031 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6981 - acc: 0.5012 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6945 - acc: 0.5015 - val_loss: 0.6978 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6936 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.7177 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6935 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6936 - acc: 0.4907 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6931 - acc: 0.5066 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6936 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6935 - acc: 0.4996 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4917 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6952 - acc: 0.4961 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6935 - acc: 0.5007 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6932 - acc: 0.5052 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6935 - acc: 0.5034 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6934 - acc: 0.5021 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.5057 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6941 - acc: 0.4966 - val_loss: 0.7059 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6936 - acc: 0.4998 - val_loss: 0.7030 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 275us/sample - loss: 0.7107 - acc: 0.4950 - val_loss: 0.6964 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6978 - acc: 0.4974 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6965 - acc: 0.4987 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6961 - acc: 0.4965 - val_loss: 0.8081 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6974 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6944 - acc: 0.4989 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.5025 - val_loss: 0.6958 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6935 - acc: 0.4984 - val_loss: 0.6971 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6933 - acc: 0.5006 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5024 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5064 - val_loss: 0.7020 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6934 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.5032 - val_loss: 0.6933 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6935 - acc: 0.4931 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6933 - acc: 0.5038 - val_loss: 0.7379 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6954 - acc: 0.4933 - val_loss: 0.7100 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6938 - acc: 0.4959 - val_loss: 0.6967 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.5035 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6937 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6932 - acc: 0.5042 - val_loss: 0.6968 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6937 - acc: 0.4975 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.5026 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6935 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6937 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4972 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 305us/sample - loss: 0.7133 - acc: 0.5046 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6947 - acc: 0.4994 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6939 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6936 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.4970 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6934 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6933 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6952 - acc: 0.5010 - val_loss: 0.9545 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6939 - acc: 0.5032 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6934 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6935 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6954 - acc: 0.4970 - val_loss: 0.6958 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.4992 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6935 - acc: 0.5057 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5029 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6936 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6935 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6935 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6934 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6934 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6933 - acc: 0.5042 - val_loss: 0.6978 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6934 - acc: 0.5024 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6934 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6933 - acc: 0.5044 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 399us/sample - loss: 0.8393 - acc: 0.5012 - val_loss: 1.0158 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.7334 - acc: 0.5003 - val_loss: 0.7640 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.7102 - acc: 0.4966 - val_loss: 0.7005 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.7004 - acc: 0.5053 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6972 - acc: 0.5025 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6958 - acc: 0.4962 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6940 - acc: 0.5045 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6940 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6941 - acc: 0.4945 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6937 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6937 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6937 - acc: 0.4993 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6934 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6937 - acc: 0.5017 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6936 - acc: 0.5032 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6940 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6982 - acc: 0.4969 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6934 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6939 - acc: 0.4989 - val_loss: 0.7054 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6937 - acc: 0.5008 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6937 - acc: 0.4992 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6936 - acc: 0.5038 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6938 - acc: 0.5004 - val_loss: 0.7227 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6945 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 290us/sample - loss: 0.6981 - acc: 0.5023 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6953 - acc: 0.4936 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6939 - acc: 0.4984 - val_loss: 0.6958 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6943 - acc: 0.4964 - val_loss: 0.6945 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6940 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6934 - acc: 0.4998 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6935 - acc: 0.5038 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6936 - acc: 0.5054 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6932 - acc: 0.5070 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6932 - acc: 0.5055 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6931 - acc: 0.5019 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6935 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6933 - acc: 0.4997 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6935 - acc: 0.4878 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.5081 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6931 - acc: 0.5063 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.4945 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6936 - acc: 0.4999 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4935 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6932 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6932 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6932 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 280us/sample - loss: 0.6969 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6945 - acc: 0.5017 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6949 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6945 - acc: 0.4864 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6941 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6938 - acc: 0.5022 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6937 - acc: 0.5039 - val_loss: 0.7158 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6942 - acc: 0.4959 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6935 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.5088 - val_loss: 0.6965 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6937 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6936 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6937 - acc: 0.4912 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.4969 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6931 - acc: 0.5067 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.5003 - val_loss: 0.7018 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6937 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6930 - acc: 0.5072 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.4997 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 302us/sample - loss: 0.7135 - acc: 0.4960 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6942 - acc: 0.5014 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6942 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6935 - acc: 0.5026 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6935 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6936 - acc: 0.4914 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.4963 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4922 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6932 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5034 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6933 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6933 - acc: 0.4974 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5010 - val_loss: 0.7667 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6936 - acc: 0.4960 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6932 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.5020 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6935 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6933 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6934 - acc: 0.5017 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6934 - acc: 0.5052 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6935 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6933 - acc: 0.5013 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6933 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6932 - acc: 0.5079 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6946 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 333us/sample - loss: 0.9228 - acc: 0.4969 - val_loss: 0.7006 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.7498 - acc: 0.4979 - val_loss: 0.7059 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.7165 - acc: 0.4983 - val_loss: 0.7087 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.7044 - acc: 0.5033 - val_loss: 0.7103 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6977 - acc: 0.5059 - val_loss: 0.6972 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6968 - acc: 0.5009 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6952 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6946 - acc: 0.5029 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6943 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.5058 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6941 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6937 - acc: 0.5033 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6938 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6943 - acc: 0.4955 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6941 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6936 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6939 - acc: 0.5016 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6937 - acc: 0.5066 - val_loss: 0.6954 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6937 - acc: 0.5048 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6937 - acc: 0.5021 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6937 - acc: 0.5049 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6940 - acc: 0.4984 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6939 - acc: 0.4981 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6935 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6940 - acc: 0.4972 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.5072 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6939 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6959 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 297us/sample - loss: 0.6997 - acc: 0.4977 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6967 - acc: 0.5005 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6952 - acc: 0.4984 - val_loss: 0.7037 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6949 - acc: 0.5058 - val_loss: 0.6962 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6944 - acc: 0.4950 - val_loss: 0.7055 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6940 - acc: 0.4969 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6942 - acc: 0.5012 - val_loss: 0.6952 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6937 - acc: 0.4925 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6933 - acc: 0.4966 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6937 - acc: 0.4931 - val_loss: 0.6977 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6934 - acc: 0.4925 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6934 - acc: 0.4977 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6932 - acc: 0.5087 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6942 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.5017 - val_loss: 0.7440 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4981 - val_loss: 0.6968 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.5068 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 297us/sample - loss: 0.7018 - acc: 0.5048 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6959 - acc: 0.5045 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6954 - acc: 0.5060 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6953 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6944 - acc: 0.4964 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6939 - acc: 0.4982 - val_loss: 0.6954 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6938 - acc: 0.4932 - val_loss: 0.6936 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6932 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.4941 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6934 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6932 - acc: 0.5036 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.4981 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6936 - acc: 0.5030 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6937 - acc: 0.4940 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6935 - acc: 0.4952 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6932 - acc: 0.5034 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.5023 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 328us/sample - loss: 0.6992 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6934 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6933 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6933 - acc: 0.5004 - val_loss: 0.6992 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6935 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.4937 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6933 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.5008 - val_loss: 2.0101 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6936 - acc: 0.5015 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6937 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4957 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6936 - acc: 0.5035 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6935 - acc: 0.4967 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6934 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6931 - acc: 0.5064 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.4886 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6933 - acc: 0.5029 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.5036 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 345us/sample - loss: 0.8444 - acc: 0.5026 - val_loss: 0.6970 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.7316 - acc: 0.5069 - val_loss: 0.7082 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.7151 - acc: 0.5006 - val_loss: 0.7015 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.7048 - acc: 0.5024 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6996 - acc: 0.5005 - val_loss: 0.6975 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6969 - acc: 0.4962 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6953 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5136 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6944 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6942 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6941 - acc: 0.4982 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6940 - acc: 0.5012 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6962 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6940 - acc: 0.4943 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6939 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.5030 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6938 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6938 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6937 - acc: 0.5042 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6936 - acc: 0.4978 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6936 - acc: 0.5064 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.5032 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6937 - acc: 0.5026 - val_loss: 0.6995 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6953 - acc: 0.5032 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6942 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6936 - acc: 0.5015 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6938 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 323us/sample - loss: 0.7034 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6970 - acc: 0.4993 - val_loss: 0.6976 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6970 - acc: 0.4939 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6947 - acc: 0.5008 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6940 - acc: 0.5011 - val_loss: 0.7028 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6949 - acc: 0.5022 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4993 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4898 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6931 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5002 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4937 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6934 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.4941 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6937 - acc: 0.4953 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6933 - acc: 0.5051 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6933 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4957 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.5001 - val_loss: 1.1859 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6936 - acc: 0.4974 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6933 - acc: 0.5017 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6933 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 327us/sample - loss: 0.6999 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6959 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6935 - acc: 0.4989 - val_loss: 0.6989 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6936 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6932 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6935 - acc: 0.4954 - val_loss: 0.7013 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6935 - acc: 0.4929 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6943 - acc: 0.4967 - val_loss: 0.7290 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6936 - acc: 0.5043 - val_loss: 0.7004 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6936 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4999 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6936 - acc: 0.4953 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.5022 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6934 - acc: 0.4998 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6932 - acc: 0.5063 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6936 - acc: 0.5048 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6942 - acc: 0.4951 - val_loss: 0.7645 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.5055 - val_loss: 0.6948 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.5024 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6936 - acc: 0.5000 - val_loss: 0.6956 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6936 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 340us/sample - loss: 0.7083 - acc: 0.4972 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6945 - acc: 0.4966 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5002 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6935 - acc: 0.5010 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6937 - acc: 0.4972 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6932 - acc: 0.5081 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.5053 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6937 - acc: 0.4979 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6936 - acc: 0.4976 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6973 - acc: 0.5018 - val_loss: 0.6952 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.4985 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6934 - acc: 0.4984 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6936 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6935 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6935 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6936 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6934 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6936 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6933 - acc: 0.5058 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5000 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6950 - acc: 0.5005 - val_loss: 7.2387 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6979 - acc: 0.4968 - val_loss: 0.6956 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6937 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6935 - acc: 0.4996 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.8238 - acc: 0.4989 - val_loss: 0.7324 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.7223 - acc: 0.4937 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.7073 - acc: 0.4971 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6978 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6965 - acc: 0.4951 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6948 - acc: 0.5044 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6935 - acc: 0.5079 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6940 - acc: 0.4966 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.5023 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6935 - acc: 0.5016 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6936 - acc: 0.5058 - val_loss: 0.6952 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6941 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.5068 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5058 - val_loss: 0.6956 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6933 - acc: 0.5029 - val_loss: 0.7026 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6938 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6976 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6939 - acc: 0.4932 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6936 - acc: 0.4987 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6935 - acc: 0.5011 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6939 - acc: 0.4935 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.5015 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4995 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4907 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6935 - acc: 0.4974 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4994 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 344us/sample - loss: 0.7160 - acc: 0.4992 - val_loss: 0.6959 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6980 - acc: 0.4998 - val_loss: 0.6989 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6941 - acc: 0.5081 - val_loss: 0.7035 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6938 - acc: 0.5014 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6937 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6933 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6937 - acc: 0.4952 - val_loss: 0.7427 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.7028 - acc: 0.5042 - val_loss: 0.7252 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6945 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.5018 - val_loss: 0.6982 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.4924 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6935 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5001 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.5045 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.5026 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6934 - acc: 0.5064 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6953 - acc: 0.5004 - val_loss: 0.7324 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6936 - acc: 0.5029 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6932 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5021 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.5000 - val_loss: 0.7850 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6935 - acc: 0.5014 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 345us/sample - loss: 0.7011 - acc: 0.5038 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6974 - acc: 0.4971 - val_loss: 0.7169 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6958 - acc: 0.5023 - val_loss: 0.7066 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.5038 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6937 - acc: 0.4941 - val_loss: 1.3461 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6939 - acc: 0.4988 - val_loss: 0.6951 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.4961 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6934 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6933 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.5080 - val_loss: 0.7514 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6934 - acc: 0.4954 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.4901 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4946 - val_loss: 0.7061 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6936 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6933 - acc: 0.5031 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.4929 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6931 - acc: 0.5013 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6944 - acc: 0.4956 - val_loss: 0.7013 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6935 - acc: 0.5022 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.5062 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6936 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6933 - acc: 0.5063 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6936 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 386us/sample - loss: 0.7197 - acc: 0.5029 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6950 - acc: 0.5000 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6934 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.5022 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6934 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6950 - acc: 0.4951 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6936 - acc: 0.5027 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6936 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6936 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6935 - acc: 0.4986 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6935 - acc: 0.4898 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6934 - acc: 0.4983 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.5042 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6936 - acc: 0.4927 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6935 - acc: 0.4926 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6935 - acc: 0.4943 - val_loss: 0.7327 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6935 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6935 - acc: 0.4992 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6936 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5083 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6936 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6934 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.4995 - val_loss: 0.7104 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6938 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6938 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6934 - acc: 0.4954 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 397us/sample - loss: 0.8456 - acc: 0.5011 - val_loss: 0.7070 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.7230 - acc: 0.5063 - val_loss: 0.6995 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.7064 - acc: 0.4990 - val_loss: 0.7119 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6988 - acc: 0.5005 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6960 - acc: 0.4974 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6943 - acc: 0.5038 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6943 - acc: 0.5013 - val_loss: 0.6965 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6939 - acc: 0.5036 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6938 - acc: 0.5030 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6935 - acc: 0.5026 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6935 - acc: 0.5046 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6939 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6935 - acc: 0.4989 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6958 - acc: 0.4993 - val_loss: 0.7042 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6938 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6957 - acc: 0.5021 - val_loss: 0.7313 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6940 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6937 - acc: 0.4938 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6939 - acc: 0.4945 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6937 - acc: 0.4936 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6935 - acc: 0.5003 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6938 - acc: 0.4924 - val_loss: 0.6943 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6936 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6938 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6944 - acc: 0.5006 - val_loss: 0.6976 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6940 - acc: 0.4918 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 410us/sample - loss: 0.7017 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.5043 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6932 - acc: 0.5045 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6935 - acc: 0.4942 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6932 - acc: 0.5063 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6935 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6932 - acc: 0.5044 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6932 - acc: 0.5036 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6933 - acc: 0.4928 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5029 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6933 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6932 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.4917 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6933 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6933 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6933 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6932 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.4954 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6933 - acc: 0.4932 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.4941 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6932 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6933 - acc: 0.4954 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 388us/sample - loss: 0.7023 - acc: 0.4989 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6938 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6959 - acc: 0.5034 - val_loss: 1.1817 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6934 - acc: 0.4875 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6934 - acc: 0.5012 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6946 - acc: 0.5034 - val_loss: 1.1723 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6938 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6934 - acc: 0.4891 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6933 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6933 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.4979 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6933 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6933 - acc: 0.4956 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6934 - acc: 0.4951 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6929 - acc: 0.5095 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6935 - acc: 0.4969 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6934 - acc: 0.4958 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6933 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6965 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 415us/sample - loss: 0.8681 - acc: 0.4996 - val_loss: 0.7042 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.7421 - acc: 0.4956 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.7127 - acc: 0.4906 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.7010 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6962 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6949 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6947 - acc: 0.4942 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6937 - acc: 0.5079 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6939 - acc: 0.5010 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6941 - acc: 0.4932 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6941 - acc: 0.4978 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6941 - acc: 0.5027 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6936 - acc: 0.5053 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6942 - acc: 0.4949 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6940 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6940 - acc: 0.4993 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6939 - acc: 0.4978 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6940 - acc: 0.4949 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6941 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6940 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.5078 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6938 - acc: 0.4985 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6938 - acc: 0.5045 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6938 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6938 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6937 - acc: 0.5040 - val_loss: 0.6948 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6938 - acc: 0.4970 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6938 - acc: 0.5014 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6942 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 427us/sample - loss: 0.8536 - acc: 0.4939 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.7370 - acc: 0.5001 - val_loss: 0.7000 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.7102 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.7007 - acc: 0.4995 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.6964 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6955 - acc: 0.4952 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6943 - acc: 0.4996 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6944 - acc: 0.5027 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6942 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6943 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6941 - acc: 0.4966 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6940 - acc: 0.4973 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6939 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6942 - acc: 0.4900 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.4960 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6940 - acc: 0.4972 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6941 - acc: 0.4965 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6937 - acc: 0.5032 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6938 - acc: 0.5046 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6942 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6935 - acc: 0.5057 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6937 - acc: 0.5032 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.5039 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6940 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6940 - acc: 0.4952 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6937 - acc: 0.5049 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 404us/sample - loss: 0.7181 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6941 - acc: 0.4962 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6934 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6936 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6934 - acc: 0.4954 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.4876 - val_loss: 0.7368 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6986 - acc: 0.4936 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6933 - acc: 0.4989 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6934 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6933 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6933 - acc: 0.4969 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6933 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6933 - acc: 0.4932 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6933 - acc: 0.4943 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6932 - acc: 0.5014 - val_loss: 0.7333 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.7134 - acc: 0.5060 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6961 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6945 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6942 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6944 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6941 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6939 - acc: 0.5064 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6942 - acc: 0.4994 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6939 - acc: 0.4976 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6940 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6939 - acc: 0.4957 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 408us/sample - loss: 0.7219 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6936 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6934 - acc: 0.4992 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6935 - acc: 0.5047 - val_loss: 0.7061 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6964 - acc: 0.4983 - val_loss: 1.0882 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6937 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6936 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.495 - 2s 120us/sample - loss: 0.6934 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6935 - acc: 0.5014 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6934 - acc: 0.5087 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6935 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6933 - acc: 0.5049 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6935 - acc: 0.5015 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6934 - acc: 0.5026 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6937 - acc: 0.4974 - val_loss: 0.7047 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6948 - acc: 0.4932 - val_loss: 7.7572 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.7104 - acc: 0.5038 - val_loss: 0.6984 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6947 - acc: 0.5016 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6940 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6946 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6942 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6942 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6939 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6937 - acc: 0.5033 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 440us/sample - loss: 0.8183 - acc: 0.5040 - val_loss: 0.7160 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.7236 - acc: 0.5024 - val_loss: 0.6952 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.7033 - acc: 0.4999 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6970 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6949 - acc: 0.5007 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6943 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6941 - acc: 0.5001 - val_loss: 0.6952 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6940 - acc: 0.4988 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6934 - acc: 0.5045 - val_loss: 0.6958 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6938 - acc: 0.5071 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6940 - acc: 0.5006 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6938 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6940 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6942 - acc: 0.5028 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6939 - acc: 0.5024 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6938 - acc: 0.5021 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6937 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6938 - acc: 0.5034 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6941 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6941 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6938 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6940 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.6940 - acc: 0.5009 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6940 - acc: 0.4984 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.6935 - acc: 0.4995 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6937 - acc: 0.5045 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6940 - acc: 0.5041 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6939 - acc: 0.4971 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 479us/sample - loss: 0.8265 - acc: 0.5041 - val_loss: 0.6974 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.7285 - acc: 0.5018 - val_loss: 0.7079 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.7054 - acc: 0.4941 - val_loss: 0.6968 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.6968 - acc: 0.5045 - val_loss: 0.7012 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6957 - acc: 0.4959 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6945 - acc: 0.4962 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6941 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6937 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6940 - acc: 0.4995 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6937 - acc: 0.5018 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6941 - acc: 0.5001 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6941 - acc: 0.5025 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6939 - acc: 0.4990 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6940 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6937 - acc: 0.5028 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.6939 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6940 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6937 - acc: 0.5021 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6939 - acc: 0.5024 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6940 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6937 - acc: 0.4986 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6938 - acc: 0.5036 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6941 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.4962 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6936 - acc: 0.5045 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 446us/sample - loss: 0.7321 - acc: 0.4946 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6943 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6937 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6963 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6934 - acc: 0.4979 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6933 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6933 - acc: 0.4971 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6934 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.7109 - acc: 0.4934 - val_loss: 1.2930 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.7062 - acc: 0.4980 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6948 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6944 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6940 - acc: 0.5066 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6939 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6939 - acc: 0.5031 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6934 - acc: 0.4993 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6941 - acc: 0.5059 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6939 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6939 - acc: 0.4891 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6941 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6938 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6937 - acc: 0.5013 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6941 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6932 - acc: 0.5053 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 461us/sample - loss: 0.7386 - acc: 0.4952 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6939 - acc: 0.4962 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6951 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6952 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6936 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6935 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6938 - acc: 0.4982 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6936 - acc: 0.4999 - val_loss: 1.1605 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6953 - acc: 0.5002 - val_loss: 2.5172 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6941 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6935 - acc: 0.4999 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6935 - acc: 0.4902 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6935 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6934 - acc: 0.5049 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6934 - acc: 0.5020 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6933 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6934 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6936 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6938 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6936 - acc: 0.4978 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.7073 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6951 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6941 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6943 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6938 - acc: 0.5037 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6941 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6941 - acc: 0.4993 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 483us/sample - loss: 0.8926 - acc: 0.4953 - val_loss: 0.7250 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.7538 - acc: 0.4996 - val_loss: 0.7268 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.7153 - acc: 0.5009 - val_loss: 0.7212 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.7044 - acc: 0.4971 - val_loss: 0.6996 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6980 - acc: 0.5019 - val_loss: 0.6954 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6958 - acc: 0.4969 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6938 - acc: 0.5071 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6944 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6943 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6940 - acc: 0.4979 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6940 - acc: 0.4959 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6939 - acc: 0.5015 - val_loss: 0.6963 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6940 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6935 - acc: 0.5045 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6939 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6936 - acc: 0.5060 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6942 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6937 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6940 - acc: 0.4957 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6938 - acc: 0.4995 - val_loss: 0.6989 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6938 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6939 - acc: 0.4969 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6936 - acc: 0.4991 - val_loss: 0.6959 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6938 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6938 - acc: 0.4940 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 502us/sample - loss: 0.9130 - acc: 0.5008 - val_loss: 0.8937 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.7589 - acc: 0.4972 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.7173 - acc: 0.5009 - val_loss: 0.7033 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.7028 - acc: 0.5052 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6968 - acc: 0.5052 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.6955 - acc: 0.5051 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6949 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.5027 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6941 - acc: 0.4982 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6941 - acc: 0.5011 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6938 - acc: 0.5007 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6940 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6939 - acc: 0.4997 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6935 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6942 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6940 - acc: 0.5021 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6938 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6937 - acc: 0.5068 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6940 - acc: 0.5023 - val_loss: 0.6953 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6938 - acc: 0.4958 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6940 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6938 - acc: 0.4996 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6941 - acc: 0.4976 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6936 - acc: 0.5006 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6940 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6941 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.6934 - acc: 0.5074 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 495us/sample - loss: 0.7321 - acc: 0.4935 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6942 - acc: 0.4967 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6935 - acc: 0.5032 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6936 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6988 - acc: 0.5028 - val_loss: 3.7493 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.7207 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6970 - acc: 0.5064 - val_loss: 0.6977 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6945 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6939 - acc: 0.5035 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6943 - acc: 0.4966 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.6938 - acc: 0.5005 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6943 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.6940 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6943 - acc: 0.4941 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6942 - acc: 0.4922 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6941 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6939 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6939 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6936 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6942 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6937 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.6940 - acc: 0.4913 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6936 - acc: 0.4993 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6935 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6940 - acc: 0.4970 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6940 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 493us/sample - loss: 0.7526 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6942 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6971 - acc: 0.5026 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6950 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6935 - acc: 0.4959 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6933 - acc: 0.5080 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6934 - acc: 0.5046 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6934 - acc: 0.5038 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6936 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6933 - acc: 0.5031 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6935 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.7183 - acc: 0.4938 - val_loss: 0.7678 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6980 - acc: 0.5017 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6946 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6944 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6942 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6937 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6938 - acc: 0.5050 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.6940 - acc: 0.4981 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6937 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.5031 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6938 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6943 - acc: 0.4985 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6939 - acc: 0.4997 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6940 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6938 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 535us/sample - loss: 0.8434 - acc: 0.5007 - val_loss: 0.6966 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.7342 - acc: 0.4998 - val_loss: 0.7194 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.7085 - acc: 0.5065 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.7001 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.6972 - acc: 0.4990 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6945 - acc: 0.5058 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6945 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6943 - acc: 0.4974 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6937 - acc: 0.5057 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6941 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6940 - acc: 0.4992 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6940 - acc: 0.5031 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6939 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6939 - acc: 0.4990 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6934 - acc: 0.5074 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6943 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6937 - acc: 0.5101 - val_loss: 0.6951 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6933 - acc: 0.5040 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.6939 - acc: 0.5045 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6937 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6935 - acc: 0.5003 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6943 - acc: 0.4941 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6938 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.6940 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6937 - acc: 0.5031 - val_loss: 0.6980 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6940 - acc: 0.4997 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6972 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6938 - acc: 0.4979 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 556us/sample - loss: 0.8280 - acc: 0.4989 - val_loss: 0.7281 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.7232 - acc: 0.5070 - val_loss: 0.6959 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.7051 - acc: 0.5030 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6970 - acc: 0.5109 - val_loss: 0.7052 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6963 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6949 - acc: 0.4993 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6944 - acc: 0.5004 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6945 - acc: 0.4939 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6938 - acc: 0.5058 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6962 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4979 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6938 - acc: 0.5064 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6941 - acc: 0.4977 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6937 - acc: 0.5047 - val_loss: 0.6957 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6944 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6939 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6943 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6938 - acc: 0.4989 - val_loss: 0.6943 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6939 - acc: 0.4967 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6935 - acc: 0.4989 - val_loss: 0.6951 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4945 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 538us/sample - loss: 0.8706 - acc: 0.4974 - val_loss: 0.7203 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.7454 - acc: 0.4878 - val_loss: 0.6992 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.7122 - acc: 0.5046 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.7017 - acc: 0.5055 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6992 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6959 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6954 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6951 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6943 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6942 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6943 - acc: 0.4934 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6940 - acc: 0.5008 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6939 - acc: 0.4961 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6938 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6940 - acc: 0.5011 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.6939 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.6937 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6940 - acc: 0.4960 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6938 - acc: 0.4972 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6937 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6942 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6940 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6941 - acc: 0.4884 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6936 - acc: 0.4984 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 555us/sample - loss: 0.7888 - acc: 0.5018 - val_loss: 0.7090 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.7171 - acc: 0.4956 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.7016 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6966 - acc: 0.4963 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6949 - acc: 0.4973 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6947 - acc: 0.4939 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6944 - acc: 0.4878 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4987 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6941 - acc: 0.4949 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6940 - acc: 0.4910 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6941 - acc: 0.5025 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.6936 - acc: 0.5011 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6934 - acc: 0.5064 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6944 - acc: 0.4943 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4989 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6936 - acc: 0.5064 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6939 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6942 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6938 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6937 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6940 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6941 - acc: 0.4975 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6938 - acc: 0.5042 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6935 - acc: 0.5039 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 577us/sample - loss: 0.8370 - acc: 0.5039 - val_loss: 0.6956 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.7281 - acc: 0.5058 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.7074 - acc: 0.5047 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6991 - acc: 0.5074 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6966 - acc: 0.5045 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6939 - acc: 0.5080 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6947 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6940 - acc: 0.4990 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6940 - acc: 0.5013 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6940 - acc: 0.4975 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6937 - acc: 0.5020 - val_loss: 0.6961 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6938 - acc: 0.5011 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6942 - acc: 0.4973 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.5006 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6939 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6938 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6940 - acc: 0.5002 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6941 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6935 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6935 - acc: 0.5041 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6938 - acc: 0.5026 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.4988 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 611us/sample - loss: 0.8295 - acc: 0.4977 - val_loss: 0.6969 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.7287 - acc: 0.4942 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.7076 - acc: 0.4966 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6998 - acc: 0.4964 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6973 - acc: 0.5017 - val_loss: 0.6953 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6945 - acc: 0.5017 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6942 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6944 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6942 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6938 - acc: 0.5031 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6935 - acc: 0.5079 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6939 - acc: 0.5069 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6939 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6942 - acc: 0.4936 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6940 - acc: 0.5028 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6938 - acc: 0.5008 - val_loss: 0.6958 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6939 - acc: 0.4978 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6942 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6940 - acc: 0.5002 - val_loss: 0.6949 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6936 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6938 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6937 - acc: 0.5052 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6939 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6938 - acc: 0.5058 - val_loss: 0.6963 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6935 - acc: 0.5081 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6941 - acc: 0.4983 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6940 - acc: 0.4928 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6940 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 593us/sample - loss: 0.8503 - acc: 0.4956 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.7347 - acc: 0.4961 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.7073 - acc: 0.5005 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.7002 - acc: 0.4962 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6954 - acc: 0.5032 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6948 - acc: 0.5032 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6943 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6943 - acc: 0.4932 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6938 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6943 - acc: 0.4942 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6941 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6937 - acc: 0.5044 - val_loss: 0.6962 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6937 - acc: 0.4953 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6938 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6940 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6941 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6937 - acc: 0.5037 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6977 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6942 - acc: 0.4942 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6940 - acc: 0.4909 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6941 - acc: 0.4922 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6936 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6939 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6939 - acc: 0.5022 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6937 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6942 - acc: 0.4946 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6937 - acc: 0.5002 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6937 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6938 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 600us/sample - loss: 0.9122 - acc: 0.4995 - val_loss: 0.7239 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.7563 - acc: 0.4969 - val_loss: 0.7097 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.7180 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.7054 - acc: 0.4960 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6988 - acc: 0.5007 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6957 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6951 - acc: 0.5000 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6946 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6941 - acc: 0.5026 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6937 - acc: 0.5027 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6942 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6938 - acc: 0.5051 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6938 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6943 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6939 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6937 - acc: 0.4988 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6938 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6939 - acc: 0.5018 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6938 - acc: 0.5021 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6941 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6935 - acc: 0.5070 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6941 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6936 - acc: 0.5042 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6938 - acc: 0.5069 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6937 - acc: 0.5027 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6935 - acc: 0.5014 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6939 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 629us/sample - loss: 0.8637 - acc: 0.5039 - val_loss: 0.7010 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.7387 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.7112 - acc: 0.5044 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.7011 - acc: 0.5056 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6979 - acc: 0.4982 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6962 - acc: 0.4931 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6950 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6944 - acc: 0.5038 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6943 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6940 - acc: 0.5025 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6941 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6940 - acc: 0.5026 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6941 - acc: 0.4954 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6940 - acc: 0.4990 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6936 - acc: 0.5028 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6938 - acc: 0.5016 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6942 - acc: 0.4891 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6939 - acc: 0.4975 - val_loss: 0.7002 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6958 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6940 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6942 - acc: 0.4901 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6937 - acc: 0.5082 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6938 - acc: 0.5055 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6939 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6939 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6939 - acc: 0.5031 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 670us/sample - loss: 0.8479 - acc: 0.4978 - val_loss: 0.6949 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 192us/sample - loss: 0.7365 - acc: 0.4915 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.7063 - acc: 0.5059 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.7015 - acc: 0.4964 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6970 - acc: 0.5011 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6953 - acc: 0.4995 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6952 - acc: 0.4954 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6944 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6942 - acc: 0.4947 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 191us/sample - loss: 0.6942 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 191us/sample - loss: 0.6940 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6939 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6940 - acc: 0.4966 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6942 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6939 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6936 - acc: 0.5078 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6937 - acc: 0.4986 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6939 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6942 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 188us/sample - loss: 0.6943 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6939 - acc: 0.4924 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6939 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6941 - acc: 0.5022 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6936 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6939 - acc: 0.5019 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6940 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 660us/sample - loss: 0.8298 - acc: 0.4914 - val_loss: 0.7028 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.7234 - acc: 0.5002 - val_loss: 0.6945 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.7071 - acc: 0.5005 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6988 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6964 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6951 - acc: 0.5016 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6944 - acc: 0.5023 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6946 - acc: 0.4956 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6942 - acc: 0.4968 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6943 - acc: 0.4986 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6940 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6941 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.4945 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6938 - acc: 0.4970 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6939 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6937 - acc: 0.5029 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6942 - acc: 0.4974 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6938 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6941 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6939 - acc: 0.4906 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6941 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6941 - acc: 0.4937 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6938 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6937 - acc: 0.5064 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6940 - acc: 0.4992 - val_loss: 0.6943 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6937 - acc: 0.5045 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6936 - acc: 0.5058 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6937 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 653us/sample - loss: 0.8279 - acc: 0.5063 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.7307 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.7095 - acc: 0.4989 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6993 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6984 - acc: 0.4921 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6953 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6948 - acc: 0.4991 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6942 - acc: 0.5006 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6944 - acc: 0.4906 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6942 - acc: 0.4975 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6939 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6939 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6937 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6943 - acc: 0.4995 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6937 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6945 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6943 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6938 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.5038 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.5040 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6942 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6937 - acc: 0.5001 - val_loss: 0.6954 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6940 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6939 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6935 - acc: 0.5055 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 688us/sample - loss: 0.8392 - acc: 0.5034 - val_loss: 0.6966 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.7350 - acc: 0.4995 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.7066 - acc: 0.5088 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.7033 - acc: 0.4912 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6974 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6964 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6941 - acc: 0.5021 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6941 - acc: 0.5054 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6942 - acc: 0.4973 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6939 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6937 - acc: 0.5030 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6940 - acc: 0.4982 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6939 - acc: 0.4998 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6937 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6940 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6936 - acc: 0.5048 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6939 - acc: 0.5017 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6942 - acc: 0.4955 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6940 - acc: 0.4939 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6939 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6938 - acc: 0.4993 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6937 - acc: 0.5046 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.6939 - acc: 0.4986 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6938 - acc: 0.5029 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6938 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6933 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6938 - acc: 0.5002 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6941 - acc: 0.4904 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6939 - acc: 0.4968 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 742us/sample - loss: 0.8648 - acc: 0.5093 - val_loss: 0.8575 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.7387 - acc: 0.4992 - val_loss: 0.8785 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.7089 - acc: 0.5049 - val_loss: 0.8639 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.7028 - acc: 0.4962 - val_loss: 0.6978 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6968 - acc: 0.4986 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6951 - acc: 0.5012 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6942 - acc: 0.5025 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6949 - acc: 0.4914 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.6936 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 191us/sample - loss: 0.6943 - acc: 0.5002 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6940 - acc: 0.4932 - val_loss: 0.6966 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6940 - acc: 0.5015 - val_loss: 0.6955 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6940 - acc: 0.4991 - val_loss: 0.7190 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6941 - acc: 0.4976 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.6939 - acc: 0.5041 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6940 - acc: 0.4950 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6939 - acc: 0.5013 - val_loss: 0.6957 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6941 - acc: 0.4924 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6941 - acc: 0.4946 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 192us/sample - loss: 0.6941 - acc: 0.4944 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 191us/sample - loss: 0.6936 - acc: 0.5038 - val_loss: 0.7157 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6941 - acc: 0.4970 - val_loss: 0.6971 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6940 - acc: 0.5016 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6941 - acc: 0.4933 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.6939 - acc: 0.4954 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 200us/sample - loss: 0.6938 - acc: 0.5037 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6938 - acc: 0.5047 - val_loss: 0.6953 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.6935 - acc: 0.5066 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6938 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 709us/sample - loss: 0.8655 - acc: 0.4969 - val_loss: 0.6963 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.7417 - acc: 0.4951 - val_loss: 0.6957 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.7131 - acc: 0.4970 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.7021 - acc: 0.4989 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6984 - acc: 0.5017 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6953 - acc: 0.5028 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6959 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6943 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6946 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6945 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6942 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6936 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 188us/sample - loss: 0.6945 - acc: 0.4922 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6940 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6937 - acc: 0.5021 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6940 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6939 - acc: 0.4955 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6937 - acc: 0.5079 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6940 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6937 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6940 - acc: 0.4945 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6938 - acc: 0.4982 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 188us/sample - loss: 0.6939 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6940 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6936 - acc: 0.5072 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6938 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 732us/sample - loss: 0.8876 - acc: 0.4931 - val_loss: 0.6995 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.7400 - acc: 0.5004 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.7118 - acc: 0.5012 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.7035 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6969 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6954 - acc: 0.4995 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6945 - acc: 0.5026 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.6941 - acc: 0.4992 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6937 - acc: 0.5050 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6942 - acc: 0.5001 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6938 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6940 - acc: 0.4991 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6940 - acc: 0.4948 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6943 - acc: 0.4922 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6937 - acc: 0.4966 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6939 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6942 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.6940 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6937 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6935 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6942 - acc: 0.4956 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6942 - acc: 0.4948 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6943 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6939 - acc: 0.4960 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6937 - acc: 0.5075 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6938 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6940 - acc: 0.4915 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6936 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6941 - acc: 0.4932 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 752us/sample - loss: 0.8551 - acc: 0.4992 - val_loss: 0.6958 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.7365 - acc: 0.5020 - val_loss: 0.7198 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 200us/sample - loss: 0.7103 - acc: 0.5017 - val_loss: 0.7632 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.7011 - acc: 0.4971 - val_loss: 0.7050 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6972 - acc: 0.4978 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6952 - acc: 0.4994 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6945 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6943 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6943 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6941 - acc: 0.4952 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.6939 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6940 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6937 - acc: 0.4995 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.6939 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6937 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6941 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6937 - acc: 0.5036 - val_loss: 0.7021 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6939 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6938 - acc: 0.4951 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6939 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6938 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6940 - acc: 0.4965 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6941 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6935 - acc: 0.5057 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.6943 - acc: 0.4899 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6938 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 200us/sample - loss: 0.6937 - acc: 0.5034 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 791us/sample - loss: 0.8547 - acc: 0.4934 - val_loss: 0.6999 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.7297 - acc: 0.4981 - val_loss: 0.6964 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.7078 - acc: 0.4995 - val_loss: 0.7227 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 4s 212us/sample - loss: 0.6989 - acc: 0.5041 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.6980 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6949 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6950 - acc: 0.4969 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 206us/sample - loss: 0.6943 - acc: 0.4966 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6944 - acc: 0.4996 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6940 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.6937 - acc: 0.4991 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 4s 215us/sample - loss: 0.6942 - acc: 0.4938 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.6936 - acc: 0.5022 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6941 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 206us/sample - loss: 0.6941 - acc: 0.4936 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6940 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.6940 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.6943 - acc: 0.4885 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6933 - acc: 0.5071 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.6939 - acc: 0.5006 - val_loss: 0.6943 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6940 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6939 - acc: 0.5011 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6937 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.6939 - acc: 0.5045 - val_loss: 0.6973 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6939 - acc: 0.4972 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6938 - acc: 0.5019 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.6940 - acc: 0.5005 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6940 - acc: 0.4942 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 4s 216us/sample - loss: 0.6937 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.6939 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4956\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [0,1,2,3]\n",
    "layer_sizes = [15,50,100,150]\n",
    "conv_layers = [0,1,2,3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME =\"LAPPD-Charge-3x3-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\LAPPD\\{}'.format(NAME))\n",
    "        \n",
    "        \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(layer_size,(9,9),strides=1, input_shape= XTrainingC.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "            model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "            for l in range(conv_layer-1):                   \n",
    "                model.add(Conv2D(layer_size,(3,3),padding='same',activation=\"relu\"))              \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))            \n",
    "            #model.add(GlobalAveragePooling2D())\n",
    "            model.add(Flatten())\n",
    "            for l in range(dense_layer-1):\n",
    "                model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "            model.add(Dense(32,activation=\"relu\"))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(2))\n",
    "            model.add(Activation('softmax'))\n",
    "            #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "            model.compile(loss=\"binary_crossentropy\",\n",
    "                         optimizer=\"adam\",\n",
    "                          metrics=['accuracy']\n",
    "                         )   \n",
    "            #filepath=\"LAPPD_Charge_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "            #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "            #model.summary()\n",
    "            history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_147 (Flatten)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_396 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_364 (Dense)            (None, 150)               24150     \n",
      "_________________________________________________________________\n",
      "batch_normalization_397 (Bat (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dropout_429 (Dropout)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_398 (Bat (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense_365 (Dense)            (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_399 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_430 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_366 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_400 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_431 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_367 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_401 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_432 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_368 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 49,592\n",
      "Trainable params: 48,272\n",
      "Non-trainable params: 1,320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=XTrainingC.shape[1:], name='img')\n",
    "x= layers.Flatten()(inputs)\n",
    "\n",
    "x= layers.BatchNormalization()(x)\n",
    "x = layers.Dense(150, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x = layers.Dense(100, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(50, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(50, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "#outputs = layers.Dense(1,activation='sigmoid')(x)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/60\n",
      "17000/17000 [==============================] - 18s 1ms/sample - loss: 0.7725 - acc: 0.5490 - val_loss: 0.6910 - val_acc: 0.5080\n",
      "Epoch 2/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.6965 - acc: 0.5804 - val_loss: 0.6971 - val_acc: 0.4972\n",
      "Epoch 3/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6659 - acc: 0.5988 - val_loss: 0.8294 - val_acc: 0.4916\n",
      "Epoch 4/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6433 - acc: 0.6241 - val_loss: 1.2116 - val_acc: 0.4924\n",
      "Epoch 5/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.6168 - acc: 0.6520 - val_loss: 1.0946 - val_acc: 0.4980\n",
      "Epoch 6/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.5943 - acc: 0.6675 - val_loss: 1.0440 - val_acc: 0.5052\n",
      "Epoch 7/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5803 - acc: 0.6821 - val_loss: 0.8442 - val_acc: 0.5368\n",
      "Epoch 8/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.5629 - acc: 0.6976 - val_loss: 0.7114 - val_acc: 0.5752\n",
      "Epoch 9/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5510 - acc: 0.7059 - val_loss: 0.7817 - val_acc: 0.5684\n",
      "Epoch 10/60\n",
      "17000/17000 [==============================] - 4s 217us/sample - loss: 0.5388 - acc: 0.7194 - val_loss: 0.5805 - val_acc: 0.6780\n",
      "Epoch 11/60\n",
      "17000/17000 [==============================] - 4s 214us/sample - loss: 0.5303 - acc: 0.7227 - val_loss: 0.5940 - val_acc: 0.6692\n",
      "Epoch 12/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.5170 - acc: 0.7356 - val_loss: 0.5833 - val_acc: 0.7000\n",
      "Epoch 13/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5137 - acc: 0.7383 - val_loss: 0.6219 - val_acc: 0.6516\n",
      "Epoch 14/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5070 - acc: 0.7411 - val_loss: 0.6265 - val_acc: 0.6596\n",
      "Epoch 15/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.4968 - acc: 0.7489 - val_loss: 0.6253 - val_acc: 0.6664\n",
      "Epoch 16/60\n",
      "17000/17000 [==============================] - 4s 225us/sample - loss: 0.4932 - acc: 0.7531 - val_loss: 0.5172 - val_acc: 0.7412\n",
      "Epoch 17/60\n",
      "17000/17000 [==============================] - 4s 216us/sample - loss: 0.4886 - acc: 0.7550 - val_loss: 0.5273 - val_acc: 0.7344\n",
      "Epoch 18/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4800 - acc: 0.7628 - val_loss: 0.5229 - val_acc: 0.7344\n",
      "Epoch 19/60\n",
      "17000/17000 [==============================] - 4s 224us/sample - loss: 0.4739 - acc: 0.7686 - val_loss: 0.6194 - val_acc: 0.6628\n",
      "Epoch 20/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4696 - acc: 0.7696 - val_loss: 0.5572 - val_acc: 0.6968\n",
      "Epoch 21/60\n",
      "17000/17000 [==============================] - 4s 219us/sample - loss: 0.4666 - acc: 0.7702 - val_loss: 0.5248 - val_acc: 0.7304\n",
      "Epoch 22/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4589 - acc: 0.7748 - val_loss: 0.6128 - val_acc: 0.6864\n",
      "Epoch 23/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4531 - acc: 0.7789 - val_loss: 0.5372 - val_acc: 0.7136\n",
      "Epoch 24/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4478 - acc: 0.7837 - val_loss: 0.5587 - val_acc: 0.7048\n",
      "Epoch 25/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4407 - acc: 0.7875 - val_loss: 0.5635 - val_acc: 0.7152\n",
      "Epoch 26/60\n",
      "17000/17000 [==============================] - 4s 228us/sample - loss: 0.4455 - acc: 0.7846 - val_loss: 0.5409 - val_acc: 0.7212\n",
      "Epoch 27/60\n",
      "17000/17000 [==============================] - 4s 228us/sample - loss: 0.4327 - acc: 0.7913 - val_loss: 0.5898 - val_acc: 0.6964\n",
      "Epoch 28/60\n",
      "17000/17000 [==============================] - 4s 218us/sample - loss: 0.4282 - acc: 0.7929 - val_loss: 0.4820 - val_acc: 0.7620\n",
      "Epoch 29/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4225 - acc: 0.7973 - val_loss: 0.7286 - val_acc: 0.6396\n",
      "Epoch 30/60\n",
      "17000/17000 [==============================] - 4s 219us/sample - loss: 0.4234 - acc: 0.7972 - val_loss: 0.6614 - val_acc: 0.6588\n",
      "Epoch 31/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4210 - acc: 0.7986 - val_loss: 0.5719 - val_acc: 0.7124\n",
      "Epoch 32/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4142 - acc: 0.8012 - val_loss: 0.6036 - val_acc: 0.7104\n",
      "Epoch 33/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4086 - acc: 0.8086 - val_loss: 0.5979 - val_acc: 0.7024\n",
      "Epoch 34/60\n",
      "17000/17000 [==============================] - 4s 226us/sample - loss: 0.4024 - acc: 0.8143 - val_loss: 0.5356 - val_acc: 0.7376\n",
      "Epoch 35/60\n",
      "17000/17000 [==============================] - 4s 229us/sample - loss: 0.4004 - acc: 0.8108 - val_loss: 0.5683 - val_acc: 0.7284\n",
      "Epoch 36/60\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.3974 - acc: 0.8162 - val_loss: 0.5234 - val_acc: 0.7380\n",
      "Epoch 37/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.4016 - acc: 0.8093 - val_loss: 0.5233 - val_acc: 0.7396\n",
      "Epoch 38/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3890 - acc: 0.8193 - val_loss: 0.5317 - val_acc: 0.7400\n",
      "Epoch 39/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3914 - acc: 0.8185 - val_loss: 0.6418 - val_acc: 0.7000\n",
      "Epoch 40/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3840 - acc: 0.8219 - val_loss: 0.6487 - val_acc: 0.6844\n",
      "Epoch 41/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3811 - acc: 0.8218 - val_loss: 0.4687 - val_acc: 0.7836\n",
      "Epoch 42/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.3787 - acc: 0.8275 - val_loss: 0.5854 - val_acc: 0.7232\n",
      "Epoch 43/60\n",
      "17000/17000 [==============================] - 4s 214us/sample - loss: 0.3798 - acc: 0.8241 - val_loss: 0.6663 - val_acc: 0.6800\n",
      "Epoch 44/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.3764 - acc: 0.8297 - val_loss: 0.4995 - val_acc: 0.7684\n",
      "Epoch 45/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3710 - acc: 0.8273 - val_loss: 0.5634 - val_acc: 0.7296\n",
      "Epoch 46/60\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.3666 - acc: 0.8316 - val_loss: 0.5257 - val_acc: 0.7656\n",
      "Epoch 47/60\n",
      "17000/17000 [==============================] - 3s 206us/sample - loss: 0.3671 - acc: 0.8316 - val_loss: 0.7167 - val_acc: 0.6768\n",
      "Epoch 48/60\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.3614 - acc: 0.8368 - val_loss: 0.5259 - val_acc: 0.7456\n",
      "Epoch 49/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3647 - acc: 0.8304 - val_loss: 0.5340 - val_acc: 0.7488\n",
      "Epoch 50/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3590 - acc: 0.8355 - val_loss: 0.5137 - val_acc: 0.7568\n",
      "Epoch 51/60\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.3590 - acc: 0.8387 - val_loss: 0.5292 - val_acc: 0.7492\n",
      "Epoch 52/60\n",
      "17000/17000 [==============================] - 4s 212us/sample - loss: 0.3514 - acc: 0.8398 - val_loss: 0.8775 - val_acc: 0.6280\n",
      "Epoch 53/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3514 - acc: 0.8401 - val_loss: 0.4859 - val_acc: 0.7756\n",
      "Epoch 54/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3473 - acc: 0.8418 - val_loss: 0.6815 - val_acc: 0.6984\n",
      "Epoch 55/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3466 - acc: 0.8432 - val_loss: 0.4904 - val_acc: 0.7676\n",
      "Epoch 56/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3449 - acc: 0.8418 - val_loss: 0.6592 - val_acc: 0.7016\n",
      "Epoch 57/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3412 - acc: 0.8446 - val_loss: 0.6384 - val_acc: 0.6980\n",
      "Epoch 58/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3340 - acc: 0.8489 - val_loss: 0.7159 - val_acc: 0.6776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3411 - acc: 0.8453 - val_loss: 0.6283 - val_acc: 0.7140\n",
      "Epoch 60/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.3342 - acc: 0.8488 - val_loss: 0.6639 - val_acc: 0.7080\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 60)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_layers = [0,1,2,3]\n",
    "\n",
    "\n",
    "#for dense_layer in dense_layers:\n",
    "\n",
    "\n",
    "NAME =\"LAPPD-Charge-3x3-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "tensorboard = TensorBoard(log_dir = 'logs\\LAPPDPerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "\n",
    "for l in range(3):\n",
    "    model.add(Dense(512-l*50 ,activation=\"relu\" ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "#model.add(Dense(32,activation=\"relu\"))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "              metrics=['accuracy']\n",
    "             )   \n",
    "#filepath=\"LAPPD_Charge_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "#model.summary()\n",
    "#         history=model.fit(XTrainingC,YTraining,\n",
    "#       validation_data=(XValC,Yval)\n",
    "#       ,batch_size=100,\n",
    "#         shuffle=True,\n",
    "#         class_weight='balanced',\n",
    "#         callbacks=[\n",
    "#                     #monitor,\n",
    "#                     #checkpoint,\n",
    "#                     tensorboard \n",
    "#         ],\n",
    "#       epochs= 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0514 14:02:27.200614  2760 deprecation.py:323] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/40\n",
      "17000/17000 [==============================] - 4s 258us/sample - loss: 0.8613 - acc: 0.5003 - val_loss: 0.7572 - val_acc: 0.4956\n",
      "Epoch 2/40\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.7364 - acc: 0.4989 - val_loss: 0.7997 - val_acc: 0.4956\n",
      "Epoch 3/40\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.7093 - acc: 0.5022 - val_loss: 0.7038 - val_acc: 0.4956\n",
      "Epoch 4/40\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.7009 - acc: 0.5011 - val_loss: 0.6967 - val_acc: 0.4956\n",
      "Epoch 5/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6969 - acc: 0.4987 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6950 - acc: 0.5035 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 7/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6942 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6945 - acc: 0.5024 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 9/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6940 - acc: 0.5039 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 10/40\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6940 - acc: 0.5033 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 11/40\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6939 - acc: 0.5005 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 12/40\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.4968 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.4986 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 14/40\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6940 - acc: 0.4962 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 15/40\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6942 - acc: 0.4928 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 16/40\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/40\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6943 - acc: 0.4919 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 18/40\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6934 - acc: 0.5065 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 19/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6942 - acc: 0.5002 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 20/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6940 - acc: 0.4986 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 21/40\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6940 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6940 - acc: 0.4917 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 23/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6939 - acc: 0.4969 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 24/40\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6940 - acc: 0.5019 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 25/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 26/40\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6940 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6939 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6939 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 30/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6940 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 31/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6936 - acc: 0.5023 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 32/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6940 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 33/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6938 - acc: 0.4988 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 34/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6959 - val_acc: 0.4956\n",
      "Epoch 35/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6939 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 36/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6940 - acc: 0.4931 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 37/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6937 - acc: 0.4977 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 38/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5033 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 39/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6938 - acc: 0.5004 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 40/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 40)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMT, dass aussieht wie LAPPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "120k Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "XL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(9x24)_23k_Files.pickle\",\"rb\"))\n",
    "YL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(9x24)_23k_Files.pickle\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "(85000, 3, 8, 2) (20000, 3, 8, 2) (15005, 3, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(XL, YL))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:20]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:85000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[85000:105000]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[105000:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much from one kind, how much from the other: \n",
      " [7542 7463]\n",
      "How do they look like? \n",
      " [[0 1]\n",
      " [1 0]]\n",
      "Percentage of one kind: \n",
      " 49.73675441519494\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(YTest, return_counts=True, axis=0)\n",
    "print(\"How much from one kind, how much from the other: \\n\",counts)\n",
    "print(\"How do they look like? \\n\",unique)\n",
    "print(\"Percentage of one kind: \\n\", 100/(counts[0]+counts[1])*counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2040000 into shape (17000,9,24,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d7f1263e6da3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mXTrainingT\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mXTraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m17000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mXTestT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4052\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mXValT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXVal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mXTrainingC\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mXTraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m17000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mXTestC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4052\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2040000 into shape (17000,9,24,1)"
     ]
    }
   ],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,9,24,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,9,24,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,9,24,1)\n",
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,9,24,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,9,24,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,9,24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.5815 - acc: 0.7127 - val_loss: 0.5033 - val_acc: 0.7552\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.5126 - acc: 0.7488 - val_loss: 0.4834 - val_acc: 0.7666\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4986 - acc: 0.7575 - val_loss: 0.4672 - val_acc: 0.7793\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4870 - acc: 0.7656 - val_loss: 0.4563 - val_acc: 0.7832\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4793 - acc: 0.7708 - val_loss: 0.4519 - val_acc: 0.7868\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4732 - acc: 0.7746 - val_loss: 0.4621 - val_acc: 0.7766\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4694 - acc: 0.7769 - val_loss: 0.4461 - val_acc: 0.7944\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4671 - acc: 0.7797 - val_loss: 0.4653 - val_acc: 0.7778\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4631 - acc: 0.7811 - val_loss: 0.4326 - val_acc: 0.7991\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4614 - acc: 0.7817 - val_loss: 0.4424 - val_acc: 0.7946\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4593 - acc: 0.7834 - val_loss: 0.4402 - val_acc: 0.7942\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4579 - acc: 0.7836 - val_loss: 0.4387 - val_acc: 0.7958\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4566 - acc: 0.7854 - val_loss: 0.4406 - val_acc: 0.7930\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4571 - acc: 0.7860 - val_loss: 0.4374 - val_acc: 0.7959\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4573 - acc: 0.7850 - val_loss: 0.4326 - val_acc: 0.7985\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4534 - acc: 0.7888 - val_loss: 0.4274 - val_acc: 0.8043\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4549 - acc: 0.7856 - val_loss: 0.4272 - val_acc: 0.8015\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4523 - acc: 0.7878 - val_loss: 0.4308 - val_acc: 0.7997\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4515 - acc: 0.7878 - val_loss: 0.4285 - val_acc: 0.8000\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4520 - acc: 0.7876 - val_loss: 0.4257 - val_acc: 0.8018\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4536 - acc: 0.7879 - val_loss: 0.4608 - val_acc: 0.7811\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4518 - acc: 0.7876 - val_loss: 0.4267 - val_acc: 0.8027\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4530 - acc: 0.7872 - val_loss: 0.4322 - val_acc: 0.7988\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4522 - acc: 0.7871 - val_loss: 0.4277 - val_acc: 0.8026\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4505 - acc: 0.7891 - val_loss: 0.4252 - val_acc: 0.8009\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4504 - acc: 0.7894 - val_loss: 0.4389 - val_acc: 0.7951\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4506 - acc: 0.7897 - val_loss: 0.4275 - val_acc: 0.7988\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4500 - acc: 0.7896 - val_loss: 0.4403 - val_acc: 0.7931\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4519 - acc: 0.7869 - val_loss: 0.4224 - val_acc: 0.8048\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4489 - acc: 0.7887 - val_loss: 0.4288 - val_acc: 0.7984\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.5677 - acc: 0.7150 - val_loss: 0.5035 - val_acc: 0.7573\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.5150 - acc: 0.7476 - val_loss: 0.4855 - val_acc: 0.7662\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.5031 - acc: 0.7550 - val_loss: 0.4775 - val_acc: 0.7699\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4899 - acc: 0.7638 - val_loss: 0.4634 - val_acc: 0.7822\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4817 - acc: 0.7694 - val_loss: 0.4552 - val_acc: 0.7861\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4746 - acc: 0.7745 - val_loss: 0.4640 - val_acc: 0.7757\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4721 - acc: 0.7760 - val_loss: 0.4548 - val_acc: 0.7836\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4685 - acc: 0.7776 - val_loss: 0.4559 - val_acc: 0.7857\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4668 - acc: 0.7800 - val_loss: 0.4585 - val_acc: 0.7799\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4656 - acc: 0.7803 - val_loss: 0.4366 - val_acc: 0.7976\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4627 - acc: 0.7824 - val_loss: 0.4450 - val_acc: 0.7934\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4619 - acc: 0.7832 - val_loss: 0.4374 - val_acc: 0.7975\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4620 - acc: 0.7816 - val_loss: 0.4396 - val_acc: 0.7935\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4590 - acc: 0.7839 - val_loss: 0.4573 - val_acc: 0.7842\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4568 - acc: 0.7844 - val_loss: 0.4367 - val_acc: 0.7983\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4566 - acc: 0.7860 - val_loss: 0.4558 - val_acc: 0.7851\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4556 - acc: 0.7859 - val_loss: 0.4403 - val_acc: 0.7936\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4543 - acc: 0.7856 - val_loss: 0.4287 - val_acc: 0.8039\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4543 - acc: 0.7873 - val_loss: 0.4363 - val_acc: 0.7977\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4522 - acc: 0.7884 - val_loss: 0.4323 - val_acc: 0.7976\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4537 - acc: 0.7876 - val_loss: 0.4402 - val_acc: 0.7919\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4526 - acc: 0.7879 - val_loss: 0.4270 - val_acc: 0.7998\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4511 - acc: 0.7890 - val_loss: 0.4251 - val_acc: 0.8020\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4512 - acc: 0.7882 - val_loss: 0.4266 - val_acc: 0.8041\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4511 - acc: 0.7893 - val_loss: 0.4261 - val_acc: 0.8023\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4492 - acc: 0.7896 - val_loss: 0.4272 - val_acc: 0.8011\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4493 - acc: 0.7897 - val_loss: 0.4227 - val_acc: 0.8040\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4478 - acc: 0.7909 - val_loss: 0.4244 - val_acc: 0.8064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4480 - acc: 0.7911 - val_loss: 0.4333 - val_acc: 0.7964\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4494 - acc: 0.7893 - val_loss: 0.4258 - val_acc: 0.8053\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.5946 - acc: 0.6962 - val_loss: 0.5144 - val_acc: 0.7466\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.5307 - acc: 0.7361 - val_loss: 0.4991 - val_acc: 0.7582\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.5166 - acc: 0.7468 - val_loss: 0.4862 - val_acc: 0.7652\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.5066 - acc: 0.7540 - val_loss: 0.4863 - val_acc: 0.7681\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4992 - acc: 0.7574 - val_loss: 0.4714 - val_acc: 0.7735\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4952 - acc: 0.7606 - val_loss: 0.4644 - val_acc: 0.7763\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4885 - acc: 0.7635 - val_loss: 0.4575 - val_acc: 0.7847\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4851 - acc: 0.7683 - val_loss: 0.4553 - val_acc: 0.7852\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4792 - acc: 0.7710 - val_loss: 0.4479 - val_acc: 0.7909\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4774 - acc: 0.7728 - val_loss: 0.4494 - val_acc: 0.7874\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4754 - acc: 0.7732 - val_loss: 0.4431 - val_acc: 0.7910\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4740 - acc: 0.7765 - val_loss: 0.4409 - val_acc: 0.7936\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4728 - acc: 0.7757 - val_loss: 0.4376 - val_acc: 0.7944\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4716 - acc: 0.7754 - val_loss: 0.4402 - val_acc: 0.7922\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4706 - acc: 0.7778 - val_loss: 0.4388 - val_acc: 0.7940\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4691 - acc: 0.7779 - val_loss: 0.4361 - val_acc: 0.7972\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4689 - acc: 0.7767 - val_loss: 0.4368 - val_acc: 0.7957\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4669 - acc: 0.7802 - val_loss: 0.4335 - val_acc: 0.8013\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4654 - acc: 0.7786 - val_loss: 0.4399 - val_acc: 0.7976\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4637 - acc: 0.7809 - val_loss: 0.4338 - val_acc: 0.7979\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4659 - acc: 0.7788 - val_loss: 0.4531 - val_acc: 0.7886\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4656 - acc: 0.7802 - val_loss: 0.4343 - val_acc: 0.7994\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4647 - acc: 0.7807 - val_loss: 0.4316 - val_acc: 0.8040\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4649 - acc: 0.7800 - val_loss: 0.4388 - val_acc: 0.7947\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4641 - acc: 0.7803 - val_loss: 0.4308 - val_acc: 0.7982\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4621 - acc: 0.7809 - val_loss: 0.4545 - val_acc: 0.7829\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4636 - acc: 0.7800 - val_loss: 0.4327 - val_acc: 0.7984\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4619 - acc: 0.7830 - val_loss: 0.4284 - val_acc: 0.7990\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4629 - acc: 0.7826 - val_loss: 0.4305 - val_acc: 0.8006\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4633 - acc: 0.7825 - val_loss: 0.4336 - val_acc: 0.7983\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.6496 - acc: 0.6638 - val_loss: 0.5255 - val_acc: 0.7413\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5446 - acc: 0.7313 - val_loss: 0.5092 - val_acc: 0.7537\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5313 - acc: 0.7395 - val_loss: 0.5013 - val_acc: 0.7583\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5221 - acc: 0.7460 - val_loss: 0.4904 - val_acc: 0.7614\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5175 - acc: 0.7476 - val_loss: 0.4848 - val_acc: 0.7673\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5118 - acc: 0.7516 - val_loss: 0.4856 - val_acc: 0.7687\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.5047 - acc: 0.7550 - val_loss: 0.4757 - val_acc: 0.7739\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4987 - acc: 0.7601 - val_loss: 0.4659 - val_acc: 0.7768\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4957 - acc: 0.7616 - val_loss: 0.4633 - val_acc: 0.7804\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4920 - acc: 0.7636 - val_loss: 0.4602 - val_acc: 0.7842\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4883 - acc: 0.7653 - val_loss: 0.4589 - val_acc: 0.7838\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4847 - acc: 0.7686 - val_loss: 0.4506 - val_acc: 0.7882\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4825 - acc: 0.7700 - val_loss: 0.4558 - val_acc: 0.7839\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4816 - acc: 0.7712 - val_loss: 0.4501 - val_acc: 0.7876\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4799 - acc: 0.7709 - val_loss: 0.4491 - val_acc: 0.7881\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4776 - acc: 0.7736 - val_loss: 0.4481 - val_acc: 0.7905\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4760 - acc: 0.7732 - val_loss: 0.4439 - val_acc: 0.7959\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4762 - acc: 0.7744 - val_loss: 0.4388 - val_acc: 0.7951\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4732 - acc: 0.7761 - val_loss: 0.4392 - val_acc: 0.7933\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4726 - acc: 0.7769 - val_loss: 0.4360 - val_acc: 0.7967\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4737 - acc: 0.7762 - val_loss: 0.4478 - val_acc: 0.7933\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4745 - acc: 0.7740 - val_loss: 0.4354 - val_acc: 0.7964\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4719 - acc: 0.7765 - val_loss: 0.4372 - val_acc: 0.7987\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4707 - acc: 0.7767 - val_loss: 0.4441 - val_acc: 0.7924\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4718 - acc: 0.7759 - val_loss: 0.4340 - val_acc: 0.7954\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4700 - acc: 0.7771 - val_loss: 0.4432 - val_acc: 0.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4695 - acc: 0.7779 - val_loss: 0.4509 - val_acc: 0.7921\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4675 - acc: 0.7791 - val_loss: 0.4318 - val_acc: 0.8003\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4693 - acc: 0.7776 - val_loss: 0.4398 - val_acc: 0.7959\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4692 - acc: 0.7790 - val_loss: 0.4381 - val_acc: 0.7963\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.5401 - acc: 0.7356 - val_loss: 0.4743 - val_acc: 0.7725\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4825 - acc: 0.7676 - val_loss: 0.4637 - val_acc: 0.7814\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4686 - acc: 0.7761 - val_loss: 0.4390 - val_acc: 0.7944\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4565 - acc: 0.7863 - val_loss: 0.4469 - val_acc: 0.7925\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4492 - acc: 0.7896 - val_loss: 0.4399 - val_acc: 0.7922\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4449 - acc: 0.7930 - val_loss: 0.4413 - val_acc: 0.7908\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4402 - acc: 0.7946 - val_loss: 0.4252 - val_acc: 0.8044\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4371 - acc: 0.7967 - val_loss: 0.4148 - val_acc: 0.8081\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4330 - acc: 0.7988 - val_loss: 0.4194 - val_acc: 0.8087\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4310 - acc: 0.8006 - val_loss: 0.4104 - val_acc: 0.8110\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4297 - acc: 0.7998 - val_loss: 0.4162 - val_acc: 0.8071\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4275 - acc: 0.8027 - val_loss: 0.4240 - val_acc: 0.8002\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4269 - acc: 0.8026 - val_loss: 0.4078 - val_acc: 0.8114\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4233 - acc: 0.8046 - val_loss: 0.4084 - val_acc: 0.8141\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4249 - acc: 0.8041 - val_loss: 0.4196 - val_acc: 0.8071\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4226 - acc: 0.8054 - val_loss: 0.4022 - val_acc: 0.8163\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4227 - acc: 0.8049 - val_loss: 0.4054 - val_acc: 0.8163\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4221 - acc: 0.8052 - val_loss: 0.4028 - val_acc: 0.8179\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4201 - acc: 0.8063 - val_loss: 0.4092 - val_acc: 0.8140\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4185 - acc: 0.8068 - val_loss: 0.4057 - val_acc: 0.8144\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4185 - acc: 0.8076 - val_loss: 0.3996 - val_acc: 0.8181\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4180 - acc: 0.8080 - val_loss: 0.4039 - val_acc: 0.8163\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4182 - acc: 0.8086 - val_loss: 0.4031 - val_acc: 0.8163\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4151 - acc: 0.8087 - val_loss: 0.4073 - val_acc: 0.8149\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4167 - acc: 0.8066 - val_loss: 0.4019 - val_acc: 0.8166\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4164 - acc: 0.8084 - val_loss: 0.4051 - val_acc: 0.8174\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4157 - acc: 0.8080 - val_loss: 0.4010 - val_acc: 0.8172\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4158 - acc: 0.8086 - val_loss: 0.3996 - val_acc: 0.8159\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4139 - acc: 0.8107 - val_loss: 0.3960 - val_acc: 0.8199\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4146 - acc: 0.8090 - val_loss: 0.4096 - val_acc: 0.8148\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.5404 - acc: 0.7350 - val_loss: 0.4738 - val_acc: 0.7760\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4811 - acc: 0.7682 - val_loss: 0.4594 - val_acc: 0.7802\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4657 - acc: 0.7788 - val_loss: 0.4391 - val_acc: 0.7962\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4525 - acc: 0.7872 - val_loss: 0.4289 - val_acc: 0.7983\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4473 - acc: 0.7902 - val_loss: 0.4339 - val_acc: 0.7977\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4390 - acc: 0.7944 - val_loss: 0.4302 - val_acc: 0.7970\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4367 - acc: 0.7964 - val_loss: 0.4143 - val_acc: 0.8087\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4338 - acc: 0.7987 - val_loss: 0.4176 - val_acc: 0.8063\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4313 - acc: 0.8012 - val_loss: 0.4283 - val_acc: 0.7994\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4294 - acc: 0.8024 - val_loss: 0.4118 - val_acc: 0.8092\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4288 - acc: 0.8020 - val_loss: 0.4077 - val_acc: 0.8138\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4272 - acc: 0.8031 - val_loss: 0.4157 - val_acc: 0.8064\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4263 - acc: 0.8043 - val_loss: 0.4109 - val_acc: 0.8106\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4258 - acc: 0.8035 - val_loss: 0.4130 - val_acc: 0.8108\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4250 - acc: 0.8035 - val_loss: 0.4016 - val_acc: 0.8138\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4230 - acc: 0.8040 - val_loss: 0.4035 - val_acc: 0.8152\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4234 - acc: 0.8037 - val_loss: 0.4109 - val_acc: 0.8130\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4223 - acc: 0.8052 - val_loss: 0.4038 - val_acc: 0.8141\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4209 - acc: 0.8059 - val_loss: 0.4175 - val_acc: 0.8102\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4188 - acc: 0.8071 - val_loss: 0.4034 - val_acc: 0.8137\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4195 - acc: 0.8066 - val_loss: 0.4044 - val_acc: 0.8133\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4183 - acc: 0.8072 - val_loss: 0.4005 - val_acc: 0.8159\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4172 - acc: 0.8075 - val_loss: 0.4049 - val_acc: 0.8122\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4187 - acc: 0.8067 - val_loss: 0.3997 - val_acc: 0.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4176 - acc: 0.8075 - val_loss: 0.4174 - val_acc: 0.8030\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4173 - acc: 0.8077 - val_loss: 0.3992 - val_acc: 0.8166\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4167 - acc: 0.8089 - val_loss: 0.4182 - val_acc: 0.8086\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4154 - acc: 0.8106 - val_loss: 0.3982 - val_acc: 0.8183\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4179 - acc: 0.8066 - val_loss: 0.3975 - val_acc: 0.8170\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4148 - acc: 0.8097 - val_loss: 0.4005 - val_acc: 0.8166\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.5590 - acc: 0.7276 - val_loss: 0.4802 - val_acc: 0.7691\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4906 - acc: 0.7635 - val_loss: 0.4618 - val_acc: 0.7826\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4715 - acc: 0.7753 - val_loss: 0.4373 - val_acc: 0.7957\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4575 - acc: 0.7843 - val_loss: 0.4341 - val_acc: 0.7944\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4482 - acc: 0.7904 - val_loss: 0.4347 - val_acc: 0.8032\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4405 - acc: 0.7961 - val_loss: 0.4145 - val_acc: 0.8108\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4370 - acc: 0.7981 - val_loss: 0.4116 - val_acc: 0.8110\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4325 - acc: 0.8004 - val_loss: 0.4120 - val_acc: 0.8090\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4297 - acc: 0.8015 - val_loss: 0.4043 - val_acc: 0.8148\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4284 - acc: 0.8024 - val_loss: 0.4036 - val_acc: 0.8163\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4278 - acc: 0.8026 - val_loss: 0.4196 - val_acc: 0.8090\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4257 - acc: 0.8033 - val_loss: 0.4136 - val_acc: 0.8067\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4230 - acc: 0.8064 - val_loss: 0.4045 - val_acc: 0.8179\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4224 - acc: 0.8056 - val_loss: 0.4028 - val_acc: 0.8159\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4208 - acc: 0.8074 - val_loss: 0.4059 - val_acc: 0.8145\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4202 - acc: 0.8072 - val_loss: 0.3996 - val_acc: 0.8166\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4192 - acc: 0.8074 - val_loss: 0.4047 - val_acc: 0.8145\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4183 - acc: 0.8082 - val_loss: 0.3948 - val_acc: 0.8198\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4163 - acc: 0.8092 - val_loss: 0.3945 - val_acc: 0.8202\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4164 - acc: 0.8097 - val_loss: 0.3914 - val_acc: 0.8220\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4151 - acc: 0.8104 - val_loss: 0.4001 - val_acc: 0.8156\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4136 - acc: 0.8117 - val_loss: 0.3964 - val_acc: 0.8192\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4144 - acc: 0.8096 - val_loss: 0.3908 - val_acc: 0.8217\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4122 - acc: 0.8133 - val_loss: 0.4018 - val_acc: 0.8174\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4133 - acc: 0.8115 - val_loss: 0.3995 - val_acc: 0.8175\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4127 - acc: 0.8126 - val_loss: 0.3862 - val_acc: 0.8260\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4109 - acc: 0.8126 - val_loss: 0.4409 - val_acc: 0.7972\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4126 - acc: 0.8122 - val_loss: 0.3957 - val_acc: 0.8217\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4102 - acc: 0.8136 - val_loss: 0.3859 - val_acc: 0.8253\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4096 - acc: 0.8131 - val_loss: 0.3883 - val_acc: 0.8228\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.5610 - acc: 0.7225 - val_loss: 0.4870 - val_acc: 0.7639\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4981 - acc: 0.7613 - val_loss: 0.4651 - val_acc: 0.7793\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4773 - acc: 0.7734 - val_loss: 0.4501 - val_acc: 0.7887\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4639 - acc: 0.7815 - val_loss: 0.4569 - val_acc: 0.7844\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4536 - acc: 0.7881 - val_loss: 0.4187 - val_acc: 0.8066\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4460 - acc: 0.7929 - val_loss: 0.4237 - val_acc: 0.8047\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4397 - acc: 0.7953 - val_loss: 0.4209 - val_acc: 0.8027\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4378 - acc: 0.7977 - val_loss: 0.4159 - val_acc: 0.8078\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4354 - acc: 0.7984 - val_loss: 0.4089 - val_acc: 0.8109\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4313 - acc: 0.8010 - val_loss: 0.4086 - val_acc: 0.8148\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4284 - acc: 0.8020 - val_loss: 0.4020 - val_acc: 0.8160\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4255 - acc: 0.8049 - val_loss: 0.4006 - val_acc: 0.8169\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4276 - acc: 0.8025 - val_loss: 0.4004 - val_acc: 0.8172\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4228 - acc: 0.8058 - val_loss: 0.4127 - val_acc: 0.8084\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4218 - acc: 0.8062 - val_loss: 0.4077 - val_acc: 0.8173\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4208 - acc: 0.8069 - val_loss: 0.4001 - val_acc: 0.8169\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4187 - acc: 0.8077 - val_loss: 0.4008 - val_acc: 0.8205\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4177 - acc: 0.8094 - val_loss: 0.4063 - val_acc: 0.8097\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4169 - acc: 0.8094 - val_loss: 0.3987 - val_acc: 0.8152\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4146 - acc: 0.8105 - val_loss: 0.3973 - val_acc: 0.8203\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4156 - acc: 0.8100 - val_loss: 0.3924 - val_acc: 0.8209\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4138 - acc: 0.8098 - val_loss: 0.3931 - val_acc: 0.8209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4133 - acc: 0.8089 - val_loss: 0.3938 - val_acc: 0.8170\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4145 - acc: 0.8102 - val_loss: 0.3949 - val_acc: 0.8239\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4133 - acc: 0.8108 - val_loss: 0.3899 - val_acc: 0.8212\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4108 - acc: 0.8121 - val_loss: 0.3971 - val_acc: 0.8174\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4112 - acc: 0.8123 - val_loss: 0.3946 - val_acc: 0.8182\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4088 - acc: 0.8152 - val_loss: 0.3913 - val_acc: 0.8247\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4110 - acc: 0.8118 - val_loss: 0.3864 - val_acc: 0.8245\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4074 - acc: 0.8142 - val_loss: 0.3865 - val_acc: 0.8228\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.5225 - acc: 0.7452 - val_loss: 0.4686 - val_acc: 0.7778\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4713 - acc: 0.7750 - val_loss: 0.4513 - val_acc: 0.7875\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4551 - acc: 0.7850 - val_loss: 0.4307 - val_acc: 0.7972\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4450 - acc: 0.7927 - val_loss: 0.4275 - val_acc: 0.7993\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4385 - acc: 0.7970 - val_loss: 0.4263 - val_acc: 0.8020\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4323 - acc: 0.7999 - val_loss: 0.4234 - val_acc: 0.8002\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4265 - acc: 0.8028 - val_loss: 0.4110 - val_acc: 0.8109\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4260 - acc: 0.8038 - val_loss: 0.4113 - val_acc: 0.8105\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4216 - acc: 0.8056 - val_loss: 0.4088 - val_acc: 0.8124\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4202 - acc: 0.8056 - val_loss: 0.4077 - val_acc: 0.8102\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4196 - acc: 0.8074 - val_loss: 0.4006 - val_acc: 0.8133\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4148 - acc: 0.8109 - val_loss: 0.4017 - val_acc: 0.8154\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4137 - acc: 0.8105 - val_loss: 0.3979 - val_acc: 0.8171\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4126 - acc: 0.8106 - val_loss: 0.4023 - val_acc: 0.8174\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4106 - acc: 0.8127 - val_loss: 0.3928 - val_acc: 0.8209\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4085 - acc: 0.8136 - val_loss: 0.3978 - val_acc: 0.8202\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4093 - acc: 0.8116 - val_loss: 0.3942 - val_acc: 0.8190\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4078 - acc: 0.8136 - val_loss: 0.4073 - val_acc: 0.8126\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4090 - acc: 0.8126 - val_loss: 0.4043 - val_acc: 0.8171\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4077 - acc: 0.8140 - val_loss: 0.4003 - val_acc: 0.8121\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4056 - acc: 0.8130 - val_loss: 0.3921 - val_acc: 0.8216\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4038 - acc: 0.8154 - val_loss: 0.3899 - val_acc: 0.8229\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4038 - acc: 0.8152 - val_loss: 0.3961 - val_acc: 0.8185\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4033 - acc: 0.8159 - val_loss: 0.3939 - val_acc: 0.8203\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4022 - acc: 0.8166 - val_loss: 0.3949 - val_acc: 0.8213\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4008 - acc: 0.8167 - val_loss: 0.3866 - val_acc: 0.8239\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4002 - acc: 0.8170 - val_loss: 0.3958 - val_acc: 0.8203\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.3996 - acc: 0.8182 - val_loss: 0.3889 - val_acc: 0.8234\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.3993 - acc: 0.8185 - val_loss: 0.3973 - val_acc: 0.8190\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3992 - acc: 0.8181 - val_loss: 0.3969 - val_acc: 0.8164\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.5224 - acc: 0.7454 - val_loss: 0.4630 - val_acc: 0.7776\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4704 - acc: 0.7752 - val_loss: 0.4612 - val_acc: 0.7785\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4547 - acc: 0.7866 - val_loss: 0.4411 - val_acc: 0.7883\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4443 - acc: 0.7921 - val_loss: 0.4281 - val_acc: 0.7997\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4389 - acc: 0.7974 - val_loss: 0.4281 - val_acc: 0.7999\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4317 - acc: 0.8010 - val_loss: 0.4412 - val_acc: 0.7955\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4283 - acc: 0.8011 - val_loss: 0.4323 - val_acc: 0.7970\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4226 - acc: 0.8061 - val_loss: 0.4089 - val_acc: 0.8098\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4207 - acc: 0.8061 - val_loss: 0.4110 - val_acc: 0.8113\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4208 - acc: 0.8076 - val_loss: 0.4112 - val_acc: 0.8077\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4172 - acc: 0.8082 - val_loss: 0.4049 - val_acc: 0.8133\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4147 - acc: 0.8097 - val_loss: 0.4066 - val_acc: 0.8154\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4137 - acc: 0.8092 - val_loss: 0.4130 - val_acc: 0.8102\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4101 - acc: 0.8122 - val_loss: 0.4019 - val_acc: 0.8162\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4102 - acc: 0.8113 - val_loss: 0.4305 - val_acc: 0.7967\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4101 - acc: 0.8126 - val_loss: 0.4025 - val_acc: 0.8173\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4077 - acc: 0.8134 - val_loss: 0.4181 - val_acc: 0.8080\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4071 - acc: 0.8143 - val_loss: 0.4072 - val_acc: 0.8093\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4066 - acc: 0.8140 - val_loss: 0.3957 - val_acc: 0.8177\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4057 - acc: 0.8151 - val_loss: 0.4172 - val_acc: 0.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4056 - acc: 0.8145 - val_loss: 0.3937 - val_acc: 0.8194\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4031 - acc: 0.8157 - val_loss: 0.4229 - val_acc: 0.8035\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4035 - acc: 0.8162 - val_loss: 0.3929 - val_acc: 0.8245\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4016 - acc: 0.8171 - val_loss: 0.3904 - val_acc: 0.8202\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3999 - acc: 0.8179 - val_loss: 0.3894 - val_acc: 0.8217\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4001 - acc: 0.8175 - val_loss: 0.4023 - val_acc: 0.8146\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3998 - acc: 0.8173 - val_loss: 0.4003 - val_acc: 0.8159\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4012 - acc: 0.8175 - val_loss: 0.3888 - val_acc: 0.8237\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3982 - acc: 0.8186 - val_loss: 0.4019 - val_acc: 0.8184\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3983 - acc: 0.8189 - val_loss: 0.4005 - val_acc: 0.8176\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.5368 - acc: 0.7389 - val_loss: 0.4655 - val_acc: 0.7788\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4698 - acc: 0.7754 - val_loss: 0.4405 - val_acc: 0.7939\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4506 - acc: 0.7892 - val_loss: 0.4468 - val_acc: 0.7918\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4367 - acc: 0.7962 - val_loss: 0.4148 - val_acc: 0.8069\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4302 - acc: 0.8018 - val_loss: 0.4311 - val_acc: 0.8009\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4229 - acc: 0.8059 - val_loss: 0.4108 - val_acc: 0.8132\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4192 - acc: 0.8075 - val_loss: 0.4087 - val_acc: 0.8143\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4144 - acc: 0.8098 - val_loss: 0.3928 - val_acc: 0.8209\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4120 - acc: 0.8121 - val_loss: 0.3921 - val_acc: 0.8183\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4081 - acc: 0.8131 - val_loss: 0.3904 - val_acc: 0.8207\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4057 - acc: 0.8151 - val_loss: 0.3967 - val_acc: 0.8192\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4039 - acc: 0.8148 - val_loss: 0.4017 - val_acc: 0.8162\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4000 - acc: 0.8191 - val_loss: 0.3857 - val_acc: 0.8231\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4010 - acc: 0.8183 - val_loss: 0.3885 - val_acc: 0.8223\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3969 - acc: 0.8201 - val_loss: 0.3836 - val_acc: 0.8273\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3955 - acc: 0.8210 - val_loss: 0.3900 - val_acc: 0.8202\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3944 - acc: 0.8215 - val_loss: 0.3964 - val_acc: 0.8179\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3924 - acc: 0.8224 - val_loss: 0.3793 - val_acc: 0.8261\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3907 - acc: 0.8248 - val_loss: 0.3791 - val_acc: 0.8278\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3904 - acc: 0.8230 - val_loss: 0.3849 - val_acc: 0.8241\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3896 - acc: 0.8240 - val_loss: 0.3849 - val_acc: 0.8266\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3900 - acc: 0.8228 - val_loss: 0.3892 - val_acc: 0.8207\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3868 - acc: 0.8250 - val_loss: 0.3745 - val_acc: 0.8314\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3879 - acc: 0.8237 - val_loss: 0.3742 - val_acc: 0.8324\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3849 - acc: 0.8263 - val_loss: 0.3809 - val_acc: 0.8262\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3835 - acc: 0.8262 - val_loss: 0.3843 - val_acc: 0.8264\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3818 - acc: 0.8270 - val_loss: 0.3765 - val_acc: 0.8310\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3825 - acc: 0.8275 - val_loss: 0.3763 - val_acc: 0.8306\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3837 - acc: 0.8270 - val_loss: 0.3809 - val_acc: 0.8251\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3802 - acc: 0.8293 - val_loss: 0.3754 - val_acc: 0.8307\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.5405 - acc: 0.7384 - val_loss: 0.4722 - val_acc: 0.7729\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4755 - acc: 0.7734 - val_loss: 0.4510 - val_acc: 0.7901\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4542 - acc: 0.7869 - val_loss: 0.4298 - val_acc: 0.8006\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4397 - acc: 0.7957 - val_loss: 0.4468 - val_acc: 0.7893\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4310 - acc: 0.8020 - val_loss: 0.4178 - val_acc: 0.8076\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4227 - acc: 0.8071 - val_loss: 0.4008 - val_acc: 0.8173\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4193 - acc: 0.8078 - val_loss: 0.4211 - val_acc: 0.8027\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4132 - acc: 0.8106 - val_loss: 0.4009 - val_acc: 0.8192\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4112 - acc: 0.8115 - val_loss: 0.4075 - val_acc: 0.8117\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4060 - acc: 0.8147 - val_loss: 0.4162 - val_acc: 0.8045\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4053 - acc: 0.8148 - val_loss: 0.4112 - val_acc: 0.8091\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4008 - acc: 0.8187 - val_loss: 0.3844 - val_acc: 0.8238\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3990 - acc: 0.8183 - val_loss: 0.3880 - val_acc: 0.8254\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 88us/sample - loss: 0.3969 - acc: 0.8196 - val_loss: 0.3832 - val_acc: 0.8232\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3954 - acc: 0.8202 - val_loss: 0.4092 - val_acc: 0.8118\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3922 - acc: 0.8226 - val_loss: 0.3827 - val_acc: 0.8246\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3911 - acc: 0.8236 - val_loss: 0.3805 - val_acc: 0.8260\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3902 - acc: 0.8230 - val_loss: 0.3756 - val_acc: 0.8288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3901 - acc: 0.8233 - val_loss: 0.3882 - val_acc: 0.8228\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3893 - acc: 0.8244 - val_loss: 0.3927 - val_acc: 0.8224\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3873 - acc: 0.8251 - val_loss: 0.3731 - val_acc: 0.8304\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3853 - acc: 0.8253 - val_loss: 0.3860 - val_acc: 0.8207\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3854 - acc: 0.8260 - val_loss: 0.3781 - val_acc: 0.8275\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3816 - acc: 0.8289 - val_loss: 0.3751 - val_acc: 0.8303\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3830 - acc: 0.8281 - val_loss: 0.3868 - val_acc: 0.8210\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3798 - acc: 0.8289 - val_loss: 0.3902 - val_acc: 0.8227\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3803 - acc: 0.8296 - val_loss: 0.3830 - val_acc: 0.8235\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3785 - acc: 0.8298 - val_loss: 0.3710 - val_acc: 0.8339\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3786 - acc: 0.8293 - val_loss: 0.3705 - val_acc: 0.8323\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3772 - acc: 0.8297 - val_loss: 0.3729 - val_acc: 0.8311\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.5136 - acc: 0.7495 - val_loss: 0.4610 - val_acc: 0.7814\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4650 - acc: 0.7804 - val_loss: 0.4363 - val_acc: 0.7958\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4475 - acc: 0.7901 - val_loss: 0.4512 - val_acc: 0.7897\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4372 - acc: 0.7979 - val_loss: 0.4264 - val_acc: 0.8019\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4303 - acc: 0.8015 - val_loss: 0.4184 - val_acc: 0.8059\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4248 - acc: 0.8037 - val_loss: 0.4236 - val_acc: 0.8030\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4211 - acc: 0.8056 - val_loss: 0.4177 - val_acc: 0.8090\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4178 - acc: 0.8091 - val_loss: 0.4640 - val_acc: 0.7849\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4147 - acc: 0.8087 - val_loss: 0.4010 - val_acc: 0.8155\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4110 - acc: 0.8124 - val_loss: 0.4045 - val_acc: 0.8167\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4114 - acc: 0.8117 - val_loss: 0.3948 - val_acc: 0.8198\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4069 - acc: 0.8139 - val_loss: 0.4046 - val_acc: 0.8129\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4068 - acc: 0.8140 - val_loss: 0.3937 - val_acc: 0.8185\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4045 - acc: 0.8151 - val_loss: 0.3924 - val_acc: 0.8212\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4036 - acc: 0.8155 - val_loss: 0.4028 - val_acc: 0.8134\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4016 - acc: 0.8165 - val_loss: 0.4030 - val_acc: 0.8139\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4035 - acc: 0.8149 - val_loss: 0.3920 - val_acc: 0.8207\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3994 - acc: 0.8183 - val_loss: 0.4070 - val_acc: 0.8108\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3978 - acc: 0.8191 - val_loss: 0.3926 - val_acc: 0.8194\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3972 - acc: 0.8194 - val_loss: 0.3873 - val_acc: 0.8242\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3981 - acc: 0.8185 - val_loss: 0.3878 - val_acc: 0.8241\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3956 - acc: 0.8206 - val_loss: 0.3881 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3953 - acc: 0.8197 - val_loss: 0.3831 - val_acc: 0.8274\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3929 - acc: 0.8211 - val_loss: 0.4000 - val_acc: 0.8181\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3933 - acc: 0.8213 - val_loss: 0.3921 - val_acc: 0.8213\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3910 - acc: 0.8211 - val_loss: 0.3832 - val_acc: 0.8257\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3909 - acc: 0.8222 - val_loss: 0.3923 - val_acc: 0.8211\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3933 - acc: 0.8218 - val_loss: 0.3827 - val_acc: 0.8262\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3894 - acc: 0.8232 - val_loss: 0.3860 - val_acc: 0.8219\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3885 - acc: 0.8237 - val_loss: 0.3818 - val_acc: 0.8260\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.5167 - acc: 0.7503 - val_loss: 0.4665 - val_acc: 0.7768\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4660 - acc: 0.7788 - val_loss: 0.4400 - val_acc: 0.7930\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4472 - acc: 0.7900 - val_loss: 0.4297 - val_acc: 0.7990\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4372 - acc: 0.7972 - val_loss: 0.4682 - val_acc: 0.7803\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4306 - acc: 0.8006 - val_loss: 0.4133 - val_acc: 0.8102\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4241 - acc: 0.8039 - val_loss: 0.4094 - val_acc: 0.8087\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4215 - acc: 0.8063 - val_loss: 0.4309 - val_acc: 0.7998\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4181 - acc: 0.8084 - val_loss: 0.4381 - val_acc: 0.7944\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4139 - acc: 0.8090 - val_loss: 0.4109 - val_acc: 0.8098\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4123 - acc: 0.8115 - val_loss: 0.3974 - val_acc: 0.8173\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4105 - acc: 0.8119 - val_loss: 0.4106 - val_acc: 0.8069\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4095 - acc: 0.8117 - val_loss: 0.4157 - val_acc: 0.8049\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4061 - acc: 0.8152 - val_loss: 0.4136 - val_acc: 0.8101\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4051 - acc: 0.8152 - val_loss: 0.4111 - val_acc: 0.8092\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4034 - acc: 0.8160 - val_loss: 0.3962 - val_acc: 0.8174\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4016 - acc: 0.8183 - val_loss: 0.3944 - val_acc: 0.8197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4012 - acc: 0.8178 - val_loss: 0.3975 - val_acc: 0.8181\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3988 - acc: 0.8187 - val_loss: 0.3896 - val_acc: 0.8246\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3981 - acc: 0.8196 - val_loss: 0.3953 - val_acc: 0.8192\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3976 - acc: 0.8174 - val_loss: 0.3972 - val_acc: 0.8202\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3964 - acc: 0.8186 - val_loss: 0.4023 - val_acc: 0.8144\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3972 - acc: 0.8192 - val_loss: 0.3864 - val_acc: 0.8260\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3970 - acc: 0.8175 - val_loss: 0.3926 - val_acc: 0.8231\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3935 - acc: 0.8208 - val_loss: 0.4144 - val_acc: 0.8068\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3932 - acc: 0.8197 - val_loss: 0.3926 - val_acc: 0.8197\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3921 - acc: 0.8206 - val_loss: 0.3871 - val_acc: 0.8239\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3906 - acc: 0.8229 - val_loss: 0.3967 - val_acc: 0.8194\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3918 - acc: 0.8228 - val_loss: 0.3909 - val_acc: 0.8249\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3888 - acc: 0.8234 - val_loss: 0.3862 - val_acc: 0.8261\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3899 - acc: 0.8218 - val_loss: 0.3857 - val_acc: 0.8247\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.5173 - acc: 0.7514 - val_loss: 0.4515 - val_acc: 0.7878\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4568 - acc: 0.7841 - val_loss: 0.4303 - val_acc: 0.7991\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4390 - acc: 0.7957 - val_loss: 0.4277 - val_acc: 0.8022\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4284 - acc: 0.8019 - val_loss: 0.4164 - val_acc: 0.8072\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4215 - acc: 0.8070 - val_loss: 0.4227 - val_acc: 0.8055\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4149 - acc: 0.8110 - val_loss: 0.3995 - val_acc: 0.8169\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4105 - acc: 0.8119 - val_loss: 0.3898 - val_acc: 0.8219\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4048 - acc: 0.8167 - val_loss: 0.3895 - val_acc: 0.8225\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4027 - acc: 0.8162 - val_loss: 0.3938 - val_acc: 0.8209\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3981 - acc: 0.8198 - val_loss: 0.3986 - val_acc: 0.8148\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3969 - acc: 0.8204 - val_loss: 0.3936 - val_acc: 0.8204\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3931 - acc: 0.8212 - val_loss: 0.3949 - val_acc: 0.8183\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3918 - acc: 0.8213 - val_loss: 0.3886 - val_acc: 0.8217\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3878 - acc: 0.8238 - val_loss: 0.3876 - val_acc: 0.8225\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3861 - acc: 0.8256 - val_loss: 0.3846 - val_acc: 0.8267\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3838 - acc: 0.8274 - val_loss: 0.3848 - val_acc: 0.8237\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3835 - acc: 0.8261 - val_loss: 0.3786 - val_acc: 0.8304\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3790 - acc: 0.8302 - val_loss: 0.3838 - val_acc: 0.8265\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3793 - acc: 0.8286 - val_loss: 0.3758 - val_acc: 0.8302\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3768 - acc: 0.8297 - val_loss: 0.4042 - val_acc: 0.8141\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3746 - acc: 0.8319 - val_loss: 0.3808 - val_acc: 0.8262\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3735 - acc: 0.8314 - val_loss: 0.3753 - val_acc: 0.8305\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3723 - acc: 0.8331 - val_loss: 0.3760 - val_acc: 0.8312\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3743 - acc: 0.8316 - val_loss: 0.3711 - val_acc: 0.8322\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3717 - acc: 0.8334 - val_loss: 0.3778 - val_acc: 0.8286\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3700 - acc: 0.8344 - val_loss: 0.3704 - val_acc: 0.8335\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3677 - acc: 0.8341 - val_loss: 0.3852 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3672 - acc: 0.8356 - val_loss: 0.3836 - val_acc: 0.8256\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3656 - acc: 0.8355 - val_loss: 0.3747 - val_acc: 0.8306\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3656 - acc: 0.8361 - val_loss: 0.3800 - val_acc: 0.8278\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.5438 - acc: 0.7390 - val_loss: 0.4647 - val_acc: 0.7807\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4688 - acc: 0.7775 - val_loss: 0.4366 - val_acc: 0.7968\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4459 - acc: 0.7924 - val_loss: 0.4296 - val_acc: 0.8030\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4316 - acc: 0.7991 - val_loss: 0.4194 - val_acc: 0.8027\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4202 - acc: 0.8069 - val_loss: 0.4184 - val_acc: 0.8037\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4145 - acc: 0.8096 - val_loss: 0.4026 - val_acc: 0.8156\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4099 - acc: 0.8137 - val_loss: 0.3974 - val_acc: 0.8161\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4062 - acc: 0.8142 - val_loss: 0.3929 - val_acc: 0.8210\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4017 - acc: 0.8172 - val_loss: 0.3879 - val_acc: 0.8224\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3979 - acc: 0.8180 - val_loss: 0.4071 - val_acc: 0.8137\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3922 - acc: 0.8216 - val_loss: 0.4101 - val_acc: 0.8121\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3910 - acc: 0.8236 - val_loss: 0.3846 - val_acc: 0.8264\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3883 - acc: 0.8246 - val_loss: 0.3793 - val_acc: 0.8282\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3855 - acc: 0.8262 - val_loss: 0.3797 - val_acc: 0.8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3830 - acc: 0.8281 - val_loss: 0.3846 - val_acc: 0.8235\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3809 - acc: 0.8274 - val_loss: 0.3758 - val_acc: 0.8304\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3775 - acc: 0.8293 - val_loss: 0.3894 - val_acc: 0.8235\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3787 - acc: 0.8300 - val_loss: 0.3930 - val_acc: 0.8202\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3767 - acc: 0.8309 - val_loss: 0.3764 - val_acc: 0.8309\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3734 - acc: 0.8320 - val_loss: 0.3709 - val_acc: 0.8307\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.3716 - acc: 0.8340 - val_loss: 0.3743 - val_acc: 0.8300\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3698 - acc: 0.8346 - val_loss: 0.4003 - val_acc: 0.8167\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3678 - acc: 0.8356 - val_loss: 0.3844 - val_acc: 0.8224\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3662 - acc: 0.8355 - val_loss: 0.3802 - val_acc: 0.8289\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3656 - acc: 0.8360 - val_loss: 0.3770 - val_acc: 0.8295\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3623 - acc: 0.8389 - val_loss: 0.3810 - val_acc: 0.8253\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3626 - acc: 0.8373 - val_loss: 0.3682 - val_acc: 0.8339\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.3606 - acc: 0.8388 - val_loss: 0.3734 - val_acc: 0.8328\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.3592 - acc: 0.8409 - val_loss: 0.3742 - val_acc: 0.8299\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.3590 - acc: 0.8394 - val_loss: 0.3703 - val_acc: 0.8313\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.5739 - acc: 0.7152 - val_loss: 0.5055 - val_acc: 0.7558\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.5119 - acc: 0.7469 - val_loss: 0.5058 - val_acc: 0.7520\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4973 - acc: 0.7576 - val_loss: 0.4670 - val_acc: 0.7780\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4851 - acc: 0.7659 - val_loss: 0.4642 - val_acc: 0.7792\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4792 - acc: 0.7712 - val_loss: 0.4715 - val_acc: 0.7765\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4766 - acc: 0.7736 - val_loss: 0.4539 - val_acc: 0.7882\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4722 - acc: 0.7769 - val_loss: 0.4437 - val_acc: 0.7920\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4702 - acc: 0.7776 - val_loss: 0.4438 - val_acc: 0.7924\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4681 - acc: 0.7782 - val_loss: 0.4421 - val_acc: 0.7936\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4648 - acc: 0.7817 - val_loss: 0.4412 - val_acc: 0.7918\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4663 - acc: 0.7788 - val_loss: 0.4358 - val_acc: 0.7972\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4636 - acc: 0.7816 - val_loss: 0.4406 - val_acc: 0.7936\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4638 - acc: 0.7808 - val_loss: 0.4341 - val_acc: 0.7988\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4603 - acc: 0.7842 - val_loss: 0.4342 - val_acc: 0.7972\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4601 - acc: 0.7837 - val_loss: 0.4315 - val_acc: 0.7998\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4612 - acc: 0.7830 - val_loss: 0.4453 - val_acc: 0.7904\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4589 - acc: 0.7838 - val_loss: 0.4352 - val_acc: 0.7958\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4591 - acc: 0.7844 - val_loss: 0.4386 - val_acc: 0.7961\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4593 - acc: 0.7848 - val_loss: 0.4309 - val_acc: 0.7998\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4573 - acc: 0.7839 - val_loss: 0.4424 - val_acc: 0.7918\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4579 - acc: 0.7847 - val_loss: 0.4287 - val_acc: 0.8011\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4567 - acc: 0.7856 - val_loss: 0.4319 - val_acc: 0.7991\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4568 - acc: 0.7852 - val_loss: 0.4330 - val_acc: 0.7980\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4569 - acc: 0.7856 - val_loss: 0.4300 - val_acc: 0.7973\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4562 - acc: 0.7850 - val_loss: 0.4312 - val_acc: 0.7999\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4547 - acc: 0.7878 - val_loss: 0.4310 - val_acc: 0.7989\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4552 - acc: 0.7854 - val_loss: 0.4268 - val_acc: 0.8006\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4544 - acc: 0.7869 - val_loss: 0.4296 - val_acc: 0.7990\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4531 - acc: 0.7869 - val_loss: 0.4325 - val_acc: 0.7950\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4561 - acc: 0.7853 - val_loss: 0.4333 - val_acc: 0.7987\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.5802 - acc: 0.7095 - val_loss: 0.5053 - val_acc: 0.7544\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.5147 - acc: 0.7462 - val_loss: 0.4877 - val_acc: 0.7649\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.5032 - acc: 0.7527 - val_loss: 0.4748 - val_acc: 0.7725\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4922 - acc: 0.7611 - val_loss: 0.4621 - val_acc: 0.7789\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4849 - acc: 0.7672 - val_loss: 0.4615 - val_acc: 0.7821\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4788 - acc: 0.7699 - val_loss: 0.4504 - val_acc: 0.7862\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4741 - acc: 0.7713 - val_loss: 0.4529 - val_acc: 0.7854\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4716 - acc: 0.7761 - val_loss: 0.4563 - val_acc: 0.7804\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4705 - acc: 0.7756 - val_loss: 0.4458 - val_acc: 0.7896\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4685 - acc: 0.7783 - val_loss: 0.4414 - val_acc: 0.7944\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4657 - acc: 0.7786 - val_loss: 0.4394 - val_acc: 0.7957\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4661 - acc: 0.7789 - val_loss: 0.4437 - val_acc: 0.7912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4667 - acc: 0.7789 - val_loss: 0.4387 - val_acc: 0.7957\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4648 - acc: 0.7810 - val_loss: 0.4367 - val_acc: 0.7936\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4642 - acc: 0.7794 - val_loss: 0.4398 - val_acc: 0.7937\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4630 - acc: 0.7813 - val_loss: 0.4386 - val_acc: 0.7984\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4615 - acc: 0.7827 - val_loss: 0.4390 - val_acc: 0.7972\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4622 - acc: 0.7816 - val_loss: 0.4379 - val_acc: 0.7975\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4626 - acc: 0.7804 - val_loss: 0.4341 - val_acc: 0.7983\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4610 - acc: 0.7812 - val_loss: 0.4341 - val_acc: 0.8000\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4622 - acc: 0.7808 - val_loss: 0.4353 - val_acc: 0.7992\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4604 - acc: 0.7831 - val_loss: 0.4325 - val_acc: 0.7998\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4594 - acc: 0.7828 - val_loss: 0.4333 - val_acc: 0.7986\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4596 - acc: 0.7834 - val_loss: 0.4425 - val_acc: 0.7911\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4580 - acc: 0.7831 - val_loss: 0.4327 - val_acc: 0.7962\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4597 - acc: 0.7830 - val_loss: 0.4329 - val_acc: 0.8008\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4591 - acc: 0.7826 - val_loss: 0.4403 - val_acc: 0.7945\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4585 - acc: 0.7831 - val_loss: 0.4310 - val_acc: 0.7980\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4567 - acc: 0.7852 - val_loss: 0.4290 - val_acc: 0.8021\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4563 - acc: 0.7848 - val_loss: 0.4352 - val_acc: 0.7981\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.5843 - acc: 0.7058 - val_loss: 0.5044 - val_acc: 0.7537\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.5212 - acc: 0.7450 - val_loss: 0.4887 - val_acc: 0.7642\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.5088 - acc: 0.7526 - val_loss: 0.4809 - val_acc: 0.7695\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.5008 - acc: 0.7563 - val_loss: 0.4693 - val_acc: 0.7771\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4950 - acc: 0.7606 - val_loss: 0.4660 - val_acc: 0.7792\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4915 - acc: 0.7629 - val_loss: 0.4619 - val_acc: 0.7827\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4847 - acc: 0.7666 - val_loss: 0.4650 - val_acc: 0.7814\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4827 - acc: 0.7689 - val_loss: 0.4526 - val_acc: 0.7868\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4782 - acc: 0.7724 - val_loss: 0.4572 - val_acc: 0.7879\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4767 - acc: 0.7732 - val_loss: 0.4429 - val_acc: 0.7918\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4754 - acc: 0.7738 - val_loss: 0.4404 - val_acc: 0.7951\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4713 - acc: 0.7769 - val_loss: 0.4631 - val_acc: 0.7782\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4714 - acc: 0.7770 - val_loss: 0.4344 - val_acc: 0.7981\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4690 - acc: 0.7776 - val_loss: 0.4356 - val_acc: 0.7990\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4683 - acc: 0.7788 - val_loss: 0.4347 - val_acc: 0.7977\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4664 - acc: 0.7805 - val_loss: 0.4399 - val_acc: 0.7957\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4673 - acc: 0.7796 - val_loss: 0.4354 - val_acc: 0.7963\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4672 - acc: 0.7783 - val_loss: 0.4306 - val_acc: 0.8019\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4646 - acc: 0.7807 - val_loss: 0.4361 - val_acc: 0.7962\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4646 - acc: 0.7801 - val_loss: 0.4430 - val_acc: 0.7984\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4641 - acc: 0.7805 - val_loss: 0.4293 - val_acc: 0.8043\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4644 - acc: 0.7807 - val_loss: 0.4321 - val_acc: 0.7981\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4625 - acc: 0.7820 - val_loss: 0.4401 - val_acc: 0.7950\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4633 - acc: 0.7801 - val_loss: 0.4342 - val_acc: 0.8005\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4621 - acc: 0.7826 - val_loss: 0.4449 - val_acc: 0.7903\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4618 - acc: 0.7830 - val_loss: 0.4268 - val_acc: 0.8053\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4612 - acc: 0.7834 - val_loss: 0.4306 - val_acc: 0.7984\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4615 - acc: 0.7820 - val_loss: 0.4266 - val_acc: 0.8023\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4613 - acc: 0.7833 - val_loss: 0.4346 - val_acc: 0.7951\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4607 - acc: 0.7848 - val_loss: 0.4245 - val_acc: 0.8035\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.6232 - acc: 0.6736 - val_loss: 0.5219 - val_acc: 0.7477\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5401 - acc: 0.7352 - val_loss: 0.5043 - val_acc: 0.7560\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5271 - acc: 0.7423 - val_loss: 0.5054 - val_acc: 0.7495\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5169 - acc: 0.7473 - val_loss: 0.4917 - val_acc: 0.7684\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5102 - acc: 0.7521 - val_loss: 0.4816 - val_acc: 0.7729\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5030 - acc: 0.7568 - val_loss: 0.4760 - val_acc: 0.7782\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4975 - acc: 0.7603 - val_loss: 0.4688 - val_acc: 0.7825\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4939 - acc: 0.7626 - val_loss: 0.4582 - val_acc: 0.7826\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4879 - acc: 0.7669 - val_loss: 0.4570 - val_acc: 0.7872\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4880 - acc: 0.7676 - val_loss: 0.4555 - val_acc: 0.7839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4864 - acc: 0.7686 - val_loss: 0.4541 - val_acc: 0.7887\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4832 - acc: 0.7702 - val_loss: 0.4471 - val_acc: 0.7942\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4800 - acc: 0.7709 - val_loss: 0.4464 - val_acc: 0.7908\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4790 - acc: 0.7734 - val_loss: 0.4456 - val_acc: 0.7966\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4776 - acc: 0.7736 - val_loss: 0.4447 - val_acc: 0.7955\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4789 - acc: 0.7716 - val_loss: 0.4411 - val_acc: 0.7921\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4771 - acc: 0.7741 - val_loss: 0.4425 - val_acc: 0.7914\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4748 - acc: 0.7759 - val_loss: 0.4384 - val_acc: 0.7968\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4745 - acc: 0.7767 - val_loss: 0.4488 - val_acc: 0.7928\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4763 - acc: 0.7749 - val_loss: 0.4467 - val_acc: 0.7904\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4762 - acc: 0.7754 - val_loss: 0.4374 - val_acc: 0.7958\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4759 - acc: 0.7734 - val_loss: 0.4430 - val_acc: 0.7940\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4729 - acc: 0.7774 - val_loss: 0.4426 - val_acc: 0.7952\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4731 - acc: 0.7781 - val_loss: 0.4401 - val_acc: 0.7972\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4700 - acc: 0.7770 - val_loss: 0.4327 - val_acc: 0.7980\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4713 - acc: 0.7765 - val_loss: 0.4402 - val_acc: 0.7972\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4726 - acc: 0.7771 - val_loss: 0.4348 - val_acc: 0.7988\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4720 - acc: 0.7783 - val_loss: 0.4370 - val_acc: 0.7961\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4712 - acc: 0.7769 - val_loss: 0.4334 - val_acc: 0.7982\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4711 - acc: 0.7774 - val_loss: 0.4358 - val_acc: 0.8009\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.5539 - acc: 0.7307 - val_loss: 0.4776 - val_acc: 0.7698\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4862 - acc: 0.7653 - val_loss: 0.4588 - val_acc: 0.7800\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4703 - acc: 0.7756 - val_loss: 0.4525 - val_acc: 0.7842\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4604 - acc: 0.7825 - val_loss: 0.4351 - val_acc: 0.7972\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4523 - acc: 0.7878 - val_loss: 0.4254 - val_acc: 0.8033\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4465 - acc: 0.7912 - val_loss: 0.4220 - val_acc: 0.8059\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4426 - acc: 0.7948 - val_loss: 0.4208 - val_acc: 0.8023\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4389 - acc: 0.7958 - val_loss: 0.4340 - val_acc: 0.7986\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4359 - acc: 0.7983 - val_loss: 0.4278 - val_acc: 0.8004\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4328 - acc: 0.8009 - val_loss: 0.4177 - val_acc: 0.8081\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4316 - acc: 0.7988 - val_loss: 0.4142 - val_acc: 0.8065\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4306 - acc: 0.8010 - val_loss: 0.4236 - val_acc: 0.8002\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4281 - acc: 0.8036 - val_loss: 0.4115 - val_acc: 0.8103\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4262 - acc: 0.8032 - val_loss: 0.4121 - val_acc: 0.8090\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4249 - acc: 0.8037 - val_loss: 0.4041 - val_acc: 0.8105\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4236 - acc: 0.8038 - val_loss: 0.4069 - val_acc: 0.8125\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4239 - acc: 0.8054 - val_loss: 0.4061 - val_acc: 0.8134\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4215 - acc: 0.8045 - val_loss: 0.4077 - val_acc: 0.8122\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4225 - acc: 0.8055 - val_loss: 0.4166 - val_acc: 0.8091\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4208 - acc: 0.8061 - val_loss: 0.4073 - val_acc: 0.8135\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4207 - acc: 0.8054 - val_loss: 0.4024 - val_acc: 0.8152\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4202 - acc: 0.8066 - val_loss: 0.4016 - val_acc: 0.8142\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4190 - acc: 0.8066 - val_loss: 0.4024 - val_acc: 0.8147\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4214 - acc: 0.8060 - val_loss: 0.3990 - val_acc: 0.8173\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4196 - acc: 0.8062 - val_loss: 0.4000 - val_acc: 0.8203\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4166 - acc: 0.8082 - val_loss: 0.3961 - val_acc: 0.8185\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4188 - acc: 0.8082 - val_loss: 0.4002 - val_acc: 0.8160\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4162 - acc: 0.8092 - val_loss: 0.3963 - val_acc: 0.8177\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4153 - acc: 0.8088 - val_loss: 0.4068 - val_acc: 0.8128\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4156 - acc: 0.8090 - val_loss: 0.4062 - val_acc: 0.8145\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.5569 - acc: 0.7279 - val_loss: 0.4807 - val_acc: 0.7706\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4874 - acc: 0.7660 - val_loss: 0.4597 - val_acc: 0.7819\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4692 - acc: 0.7772 - val_loss: 0.4411 - val_acc: 0.7929\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4571 - acc: 0.7845 - val_loss: 0.4301 - val_acc: 0.7987\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4494 - acc: 0.7900 - val_loss: 0.4382 - val_acc: 0.7952\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4429 - acc: 0.7937 - val_loss: 0.4461 - val_acc: 0.7886\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4410 - acc: 0.7924 - val_loss: 0.4221 - val_acc: 0.8007\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4363 - acc: 0.7961 - val_loss: 0.4156 - val_acc: 0.8073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4349 - acc: 0.7979 - val_loss: 0.4314 - val_acc: 0.7965\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4313 - acc: 0.7996 - val_loss: 0.4157 - val_acc: 0.8059\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4300 - acc: 0.8012 - val_loss: 0.4165 - val_acc: 0.8064\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4273 - acc: 0.8022 - val_loss: 0.4111 - val_acc: 0.8077\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4285 - acc: 0.8027 - val_loss: 0.4088 - val_acc: 0.8105\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4252 - acc: 0.8027 - val_loss: 0.4158 - val_acc: 0.8092\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4237 - acc: 0.8041 - val_loss: 0.4098 - val_acc: 0.8098\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4243 - acc: 0.8034 - val_loss: 0.4021 - val_acc: 0.8143\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4220 - acc: 0.8056 - val_loss: 0.4026 - val_acc: 0.8144\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4206 - acc: 0.8062 - val_loss: 0.4082 - val_acc: 0.8120\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4192 - acc: 0.8060 - val_loss: 0.4029 - val_acc: 0.8152\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4204 - acc: 0.8065 - val_loss: 0.4031 - val_acc: 0.8132\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4177 - acc: 0.8082 - val_loss: 0.4014 - val_acc: 0.8154\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4188 - acc: 0.8064 - val_loss: 0.4049 - val_acc: 0.8132\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4158 - acc: 0.8090 - val_loss: 0.4041 - val_acc: 0.8131\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4173 - acc: 0.8062 - val_loss: 0.4033 - val_acc: 0.8136\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4160 - acc: 0.8086 - val_loss: 0.4205 - val_acc: 0.8056\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4163 - acc: 0.8090 - val_loss: 0.4085 - val_acc: 0.8102\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4138 - acc: 0.8086 - val_loss: 0.4061 - val_acc: 0.8129\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4157 - acc: 0.8091 - val_loss: 0.4064 - val_acc: 0.8118\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4135 - acc: 0.8102 - val_loss: 0.4002 - val_acc: 0.8163\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4134 - acc: 0.8102 - val_loss: 0.4075 - val_acc: 0.8123\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.5463 - acc: 0.7326 - val_loss: 0.4810 - val_acc: 0.7669\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4871 - acc: 0.7657 - val_loss: 0.4543 - val_acc: 0.7841\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4720 - acc: 0.7764 - val_loss: 0.4426 - val_acc: 0.7926\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4599 - acc: 0.7845 - val_loss: 0.4301 - val_acc: 0.7965\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4499 - acc: 0.7890 - val_loss: 0.4272 - val_acc: 0.8045\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4439 - acc: 0.7944 - val_loss: 0.4345 - val_acc: 0.8001\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4382 - acc: 0.7973 - val_loss: 0.4213 - val_acc: 0.8056\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4335 - acc: 0.8003 - val_loss: 0.4096 - val_acc: 0.8124\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4312 - acc: 0.8015 - val_loss: 0.4024 - val_acc: 0.8176\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4268 - acc: 0.8033 - val_loss: 0.4107 - val_acc: 0.8108\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4275 - acc: 0.8037 - val_loss: 0.4029 - val_acc: 0.8169\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4236 - acc: 0.8053 - val_loss: 0.4179 - val_acc: 0.8081\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4230 - acc: 0.8058 - val_loss: 0.4048 - val_acc: 0.8145\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4200 - acc: 0.8063 - val_loss: 0.3980 - val_acc: 0.8178\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4208 - acc: 0.8064 - val_loss: 0.3953 - val_acc: 0.8169\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4181 - acc: 0.8089 - val_loss: 0.3961 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4174 - acc: 0.8099 - val_loss: 0.3988 - val_acc: 0.8188\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4158 - acc: 0.8100 - val_loss: 0.4127 - val_acc: 0.8101\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4140 - acc: 0.8110 - val_loss: 0.3982 - val_acc: 0.8181\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4141 - acc: 0.8115 - val_loss: 0.3915 - val_acc: 0.8210\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4147 - acc: 0.8114 - val_loss: 0.3882 - val_acc: 0.8247\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4117 - acc: 0.8112 - val_loss: 0.3951 - val_acc: 0.8217\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4099 - acc: 0.8132 - val_loss: 0.3898 - val_acc: 0.8257\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4124 - acc: 0.8116 - val_loss: 0.3925 - val_acc: 0.8220\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4115 - acc: 0.8126 - val_loss: 0.3896 - val_acc: 0.8221\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4097 - acc: 0.8142 - val_loss: 0.3960 - val_acc: 0.8191\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4097 - acc: 0.8133 - val_loss: 0.3907 - val_acc: 0.8206\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4106 - acc: 0.8122 - val_loss: 0.3872 - val_acc: 0.8238\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4106 - acc: 0.8124 - val_loss: 0.3897 - val_acc: 0.8249\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4099 - acc: 0.8147 - val_loss: 0.3928 - val_acc: 0.8194\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.5868 - acc: 0.7104 - val_loss: 0.4866 - val_acc: 0.7625\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.5004 - acc: 0.7589 - val_loss: 0.4674 - val_acc: 0.7750\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4809 - acc: 0.7696 - val_loss: 0.4477 - val_acc: 0.7897\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4677 - acc: 0.7787 - val_loss: 0.4345 - val_acc: 0.7988\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4571 - acc: 0.7866 - val_loss: 0.4381 - val_acc: 0.7932\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4487 - acc: 0.7901 - val_loss: 0.4228 - val_acc: 0.8025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4431 - acc: 0.7938 - val_loss: 0.4222 - val_acc: 0.8045\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4397 - acc: 0.7953 - val_loss: 0.4215 - val_acc: 0.8030\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4353 - acc: 0.7982 - val_loss: 0.4086 - val_acc: 0.8120\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4339 - acc: 0.7993 - val_loss: 0.4208 - val_acc: 0.8026\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4299 - acc: 0.8011 - val_loss: 0.4035 - val_acc: 0.8158\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4299 - acc: 0.8002 - val_loss: 0.4044 - val_acc: 0.8108\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4251 - acc: 0.8044 - val_loss: 0.4158 - val_acc: 0.8083\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4238 - acc: 0.8038 - val_loss: 0.3953 - val_acc: 0.8191\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4242 - acc: 0.8052 - val_loss: 0.3936 - val_acc: 0.8209\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4214 - acc: 0.8052 - val_loss: 0.4030 - val_acc: 0.8159\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4210 - acc: 0.8043 - val_loss: 0.3966 - val_acc: 0.8204\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4199 - acc: 0.8078 - val_loss: 0.3917 - val_acc: 0.8213\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4189 - acc: 0.8071 - val_loss: 0.3981 - val_acc: 0.8166\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4170 - acc: 0.8075 - val_loss: 0.3920 - val_acc: 0.8206\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4164 - acc: 0.8079 - val_loss: 0.3897 - val_acc: 0.8236\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4171 - acc: 0.8082 - val_loss: 0.3961 - val_acc: 0.8173\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4152 - acc: 0.8108 - val_loss: 0.4101 - val_acc: 0.8116\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4161 - acc: 0.8081 - val_loss: 0.3931 - val_acc: 0.8209\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4135 - acc: 0.8114 - val_loss: 0.3955 - val_acc: 0.8189\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4136 - acc: 0.8098 - val_loss: 0.3949 - val_acc: 0.8164\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4141 - acc: 0.8101 - val_loss: 0.3876 - val_acc: 0.8213\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4116 - acc: 0.8123 - val_loss: 0.4018 - val_acc: 0.8144\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4099 - acc: 0.8124 - val_loss: 0.3995 - val_acc: 0.8167\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4098 - acc: 0.8134 - val_loss: 0.3857 - val_acc: 0.8252\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.5267 - acc: 0.7443 - val_loss: 0.4788 - val_acc: 0.7717\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4674 - acc: 0.7777 - val_loss: 0.4435 - val_acc: 0.7907\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4514 - acc: 0.7886 - val_loss: 0.4301 - val_acc: 0.7975\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4430 - acc: 0.7928 - val_loss: 0.4273 - val_acc: 0.8024\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4348 - acc: 0.7976 - val_loss: 0.4210 - val_acc: 0.8031\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4305 - acc: 0.8008 - val_loss: 0.4094 - val_acc: 0.8099\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4269 - acc: 0.8016 - val_loss: 0.4052 - val_acc: 0.8145\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4239 - acc: 0.8051 - val_loss: 0.4234 - val_acc: 0.8031\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4217 - acc: 0.8052 - val_loss: 0.4039 - val_acc: 0.8170\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4175 - acc: 0.8082 - val_loss: 0.4231 - val_acc: 0.8041\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4165 - acc: 0.8082 - val_loss: 0.4453 - val_acc: 0.7911\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4144 - acc: 0.8095 - val_loss: 0.3998 - val_acc: 0.8170\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4138 - acc: 0.8094 - val_loss: 0.4059 - val_acc: 0.8153\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4107 - acc: 0.8122 - val_loss: 0.3953 - val_acc: 0.8216\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4099 - acc: 0.8120 - val_loss: 0.4056 - val_acc: 0.8129\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4071 - acc: 0.8139 - val_loss: 0.3989 - val_acc: 0.8156\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4067 - acc: 0.8126 - val_loss: 0.4019 - val_acc: 0.8126\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4058 - acc: 0.8142 - val_loss: 0.4075 - val_acc: 0.8153\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4038 - acc: 0.8148 - val_loss: 0.3945 - val_acc: 0.8188\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4051 - acc: 0.8144 - val_loss: 0.3911 - val_acc: 0.8206\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4035 - acc: 0.8150 - val_loss: 0.3927 - val_acc: 0.8202\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4018 - acc: 0.8162 - val_loss: 0.4087 - val_acc: 0.8119\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4015 - acc: 0.8174 - val_loss: 0.4084 - val_acc: 0.8082\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4011 - acc: 0.8166 - val_loss: 0.4022 - val_acc: 0.8155\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4001 - acc: 0.8183 - val_loss: 0.4160 - val_acc: 0.8059\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3987 - acc: 0.8187 - val_loss: 0.3896 - val_acc: 0.8228\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3991 - acc: 0.8192 - val_loss: 0.3942 - val_acc: 0.8217\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3987 - acc: 0.8176 - val_loss: 0.3870 - val_acc: 0.8227\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3980 - acc: 0.8189 - val_loss: 0.3863 - val_acc: 0.8250\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3965 - acc: 0.8191 - val_loss: 0.3908 - val_acc: 0.8234\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.5159 - acc: 0.7487 - val_loss: 0.4717 - val_acc: 0.7768\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4712 - acc: 0.7769 - val_loss: 0.4478 - val_acc: 0.7925\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4557 - acc: 0.7858 - val_loss: 0.4282 - val_acc: 0.8000\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4457 - acc: 0.7919 - val_loss: 0.4553 - val_acc: 0.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4386 - acc: 0.7966 - val_loss: 0.4411 - val_acc: 0.7930\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4321 - acc: 0.8001 - val_loss: 0.4130 - val_acc: 0.8097\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4288 - acc: 0.8021 - val_loss: 0.4106 - val_acc: 0.8105\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4245 - acc: 0.8037 - val_loss: 0.4201 - val_acc: 0.8061\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4231 - acc: 0.8053 - val_loss: 0.4115 - val_acc: 0.8084\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4211 - acc: 0.8062 - val_loss: 0.3976 - val_acc: 0.8194\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4172 - acc: 0.8081 - val_loss: 0.4095 - val_acc: 0.8097\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4155 - acc: 0.8090 - val_loss: 0.4019 - val_acc: 0.8176\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4164 - acc: 0.8088 - val_loss: 0.4022 - val_acc: 0.8164\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4144 - acc: 0.8101 - val_loss: 0.4255 - val_acc: 0.8034\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4120 - acc: 0.8120 - val_loss: 0.3959 - val_acc: 0.8206\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4126 - acc: 0.8106 - val_loss: 0.4062 - val_acc: 0.8126\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4104 - acc: 0.8109 - val_loss: 0.3936 - val_acc: 0.8200\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4090 - acc: 0.8123 - val_loss: 0.4009 - val_acc: 0.8161\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4079 - acc: 0.8135 - val_loss: 0.3929 - val_acc: 0.8205\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4071 - acc: 0.8136 - val_loss: 0.3916 - val_acc: 0.8213\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4065 - acc: 0.8145 - val_loss: 0.4119 - val_acc: 0.8095\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4039 - acc: 0.8148 - val_loss: 0.3977 - val_acc: 0.8183\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4040 - acc: 0.8144 - val_loss: 0.4000 - val_acc: 0.8161\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4045 - acc: 0.8151 - val_loss: 0.3865 - val_acc: 0.8261\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4034 - acc: 0.8159 - val_loss: 0.3900 - val_acc: 0.8235\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4023 - acc: 0.8168 - val_loss: 0.3883 - val_acc: 0.8234\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4014 - acc: 0.8161 - val_loss: 0.4121 - val_acc: 0.8084\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4008 - acc: 0.8178 - val_loss: 0.3851 - val_acc: 0.8262\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3998 - acc: 0.8180 - val_loss: 0.3912 - val_acc: 0.8216\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3989 - acc: 0.8175 - val_loss: 0.3840 - val_acc: 0.8266\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.5541 - acc: 0.7335 - val_loss: 0.4640 - val_acc: 0.7796\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4768 - acc: 0.7728 - val_loss: 0.4514 - val_acc: 0.7883\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4563 - acc: 0.7838 - val_loss: 0.4542 - val_acc: 0.7883\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4416 - acc: 0.7937 - val_loss: 0.4147 - val_acc: 0.8052\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4331 - acc: 0.7986 - val_loss: 0.4309 - val_acc: 0.8032\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4266 - acc: 0.8034 - val_loss: 0.4271 - val_acc: 0.8043\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4204 - acc: 0.8074 - val_loss: 0.3975 - val_acc: 0.8194\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4171 - acc: 0.8077 - val_loss: 0.4025 - val_acc: 0.8167\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4140 - acc: 0.8118 - val_loss: 0.4071 - val_acc: 0.8112\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4085 - acc: 0.8129 - val_loss: 0.3970 - val_acc: 0.8196\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4064 - acc: 0.8154 - val_loss: 0.3891 - val_acc: 0.8234\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4057 - acc: 0.8157 - val_loss: 0.4157 - val_acc: 0.8098\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4011 - acc: 0.8165 - val_loss: 0.4027 - val_acc: 0.8150\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4010 - acc: 0.8174 - val_loss: 0.3924 - val_acc: 0.8223\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3981 - acc: 0.8198 - val_loss: 0.3855 - val_acc: 0.8249\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3963 - acc: 0.8201 - val_loss: 0.3861 - val_acc: 0.8223\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3955 - acc: 0.8201 - val_loss: 0.3822 - val_acc: 0.8260\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3948 - acc: 0.8225 - val_loss: 0.4036 - val_acc: 0.8152\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3916 - acc: 0.8222 - val_loss: 0.3777 - val_acc: 0.8310\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3923 - acc: 0.8227 - val_loss: 0.3832 - val_acc: 0.8256\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3903 - acc: 0.8231 - val_loss: 0.3873 - val_acc: 0.8215\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.3881 - acc: 0.8237 - val_loss: 0.3984 - val_acc: 0.8164\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3878 - acc: 0.8241 - val_loss: 0.3793 - val_acc: 0.8296\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3870 - acc: 0.8238 - val_loss: 0.3750 - val_acc: 0.8309\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3858 - acc: 0.8245 - val_loss: 0.3737 - val_acc: 0.8307\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 88us/sample - loss: 0.3852 - acc: 0.8257 - val_loss: 0.3747 - val_acc: 0.8289\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3856 - acc: 0.8261 - val_loss: 0.3758 - val_acc: 0.8303\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3833 - acc: 0.8276 - val_loss: 0.3848 - val_acc: 0.8237\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3827 - acc: 0.8272 - val_loss: 0.3790 - val_acc: 0.8280\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3818 - acc: 0.8284 - val_loss: 0.3801 - val_acc: 0.8281\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.5330 - acc: 0.7404 - val_loss: 0.4659 - val_acc: 0.7774\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4744 - acc: 0.7743 - val_loss: 0.4398 - val_acc: 0.7914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4514 - acc: 0.7894 - val_loss: 0.4213 - val_acc: 0.8054\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4400 - acc: 0.7952 - val_loss: 0.4089 - val_acc: 0.8117\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.4285 - acc: 0.8034 - val_loss: 0.4112 - val_acc: 0.8088\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 100us/sample - loss: 0.4230 - acc: 0.8052 - val_loss: 0.4004 - val_acc: 0.8175\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4169 - acc: 0.8086 - val_loss: 0.3969 - val_acc: 0.8170\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4139 - acc: 0.8107 - val_loss: 0.3985 - val_acc: 0.8134\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4089 - acc: 0.8132 - val_loss: 0.3922 - val_acc: 0.8205\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4061 - acc: 0.8165 - val_loss: 0.3922 - val_acc: 0.8209\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4040 - acc: 0.8158 - val_loss: 0.3905 - val_acc: 0.8209\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4022 - acc: 0.8179 - val_loss: 0.3884 - val_acc: 0.8202\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3983 - acc: 0.8203 - val_loss: 0.3895 - val_acc: 0.8227\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3970 - acc: 0.8197 - val_loss: 0.3826 - val_acc: 0.8245\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3955 - acc: 0.8197 - val_loss: 0.3966 - val_acc: 0.8169\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3936 - acc: 0.8220 - val_loss: 0.4122 - val_acc: 0.8083\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3922 - acc: 0.8219 - val_loss: 0.3780 - val_acc: 0.8281\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3903 - acc: 0.8238 - val_loss: 0.4086 - val_acc: 0.8113\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3880 - acc: 0.8253 - val_loss: 0.3815 - val_acc: 0.8253\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3867 - acc: 0.8246 - val_loss: 0.3834 - val_acc: 0.8281\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.3877 - acc: 0.8254 - val_loss: 0.3900 - val_acc: 0.8235\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3837 - acc: 0.8263 - val_loss: 0.3785 - val_acc: 0.8256\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3833 - acc: 0.8270 - val_loss: 0.3762 - val_acc: 0.8278\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3856 - acc: 0.8260 - val_loss: 0.3773 - val_acc: 0.8309\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3821 - acc: 0.8278 - val_loss: 0.3814 - val_acc: 0.8246\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3799 - acc: 0.8291 - val_loss: 0.3754 - val_acc: 0.8302\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3799 - acc: 0.8291 - val_loss: 0.4102 - val_acc: 0.8112\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3785 - acc: 0.8294 - val_loss: 0.3790 - val_acc: 0.8286\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3789 - acc: 0.8306 - val_loss: 0.3794 - val_acc: 0.8275\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3770 - acc: 0.8312 - val_loss: 0.3722 - val_acc: 0.8321\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.5230 - acc: 0.7460 - val_loss: 0.4627 - val_acc: 0.7818\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4657 - acc: 0.7763 - val_loss: 0.4446 - val_acc: 0.7899\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4507 - acc: 0.7879 - val_loss: 0.4285 - val_acc: 0.7994\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4401 - acc: 0.7950 - val_loss: 0.4388 - val_acc: 0.7930\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4332 - acc: 0.7997 - val_loss: 0.4691 - val_acc: 0.7829\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4289 - acc: 0.8016 - val_loss: 0.4202 - val_acc: 0.8019\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4252 - acc: 0.8045 - val_loss: 0.4077 - val_acc: 0.8084\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4223 - acc: 0.8051 - val_loss: 0.4369 - val_acc: 0.8004\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4200 - acc: 0.8070 - val_loss: 0.4160 - val_acc: 0.8115\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4179 - acc: 0.8084 - val_loss: 0.4488 - val_acc: 0.7853\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4165 - acc: 0.8082 - val_loss: 0.3981 - val_acc: 0.8158\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4128 - acc: 0.8103 - val_loss: 0.4117 - val_acc: 0.8106\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4125 - acc: 0.8105 - val_loss: 0.4157 - val_acc: 0.8100\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4111 - acc: 0.8118 - val_loss: 0.4002 - val_acc: 0.8171\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4110 - acc: 0.8115 - val_loss: 0.4078 - val_acc: 0.8137\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4062 - acc: 0.8151 - val_loss: 0.3943 - val_acc: 0.8188\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4058 - acc: 0.8145 - val_loss: 0.3975 - val_acc: 0.8183\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4051 - acc: 0.8155 - val_loss: 0.4120 - val_acc: 0.8109\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4048 - acc: 0.8138 - val_loss: 0.3917 - val_acc: 0.8186\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4007 - acc: 0.8168 - val_loss: 0.3965 - val_acc: 0.8173\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4005 - acc: 0.8171 - val_loss: 0.3936 - val_acc: 0.8194\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3998 - acc: 0.8178 - val_loss: 0.3986 - val_acc: 0.8164\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3994 - acc: 0.8173 - val_loss: 0.3984 - val_acc: 0.8188\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3970 - acc: 0.8186 - val_loss: 0.3936 - val_acc: 0.8194\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3956 - acc: 0.8192 - val_loss: 0.3899 - val_acc: 0.8235\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3975 - acc: 0.8181 - val_loss: 0.3861 - val_acc: 0.8223\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3971 - acc: 0.8192 - val_loss: 0.4135 - val_acc: 0.8097\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3945 - acc: 0.8201 - val_loss: 0.3902 - val_acc: 0.8173\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3947 - acc: 0.8208 - val_loss: 0.3850 - val_acc: 0.8231\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3935 - acc: 0.8203 - val_loss: 0.3872 - val_acc: 0.8211\n",
      "Train on 85000 samples, validate on 20000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.5241 - acc: 0.7454 - val_loss: 0.4647 - val_acc: 0.7796\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4684 - acc: 0.7776 - val_loss: 0.4421 - val_acc: 0.7929\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4504 - acc: 0.7879 - val_loss: 0.4938 - val_acc: 0.7603\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4399 - acc: 0.7953 - val_loss: 0.4288 - val_acc: 0.7993\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4319 - acc: 0.7982 - val_loss: 0.4201 - val_acc: 0.8044\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4269 - acc: 0.8020 - val_loss: 0.4231 - val_acc: 0.8065\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4238 - acc: 0.8046 - val_loss: 0.4082 - val_acc: 0.8123\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4187 - acc: 0.8063 - val_loss: 0.4064 - val_acc: 0.8131\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4156 - acc: 0.8097 - val_loss: 0.4064 - val_acc: 0.8134\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4152 - acc: 0.8110 - val_loss: 0.3950 - val_acc: 0.8199\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4117 - acc: 0.8119 - val_loss: 0.4019 - val_acc: 0.8170\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4109 - acc: 0.8115 - val_loss: 0.3965 - val_acc: 0.8178\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4084 - acc: 0.8131 - val_loss: 0.4038 - val_acc: 0.8132\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4070 - acc: 0.8132 - val_loss: 0.4089 - val_acc: 0.8120\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4063 - acc: 0.8150 - val_loss: 0.3983 - val_acc: 0.8184\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4049 - acc: 0.8143 - val_loss: 0.3902 - val_acc: 0.8227\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4016 - acc: 0.8171 - val_loss: 0.4435 - val_acc: 0.7915\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4019 - acc: 0.8165 - val_loss: 0.3979 - val_acc: 0.8203\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4006 - acc: 0.8183 - val_loss: 0.4077 - val_acc: 0.8133\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.3985 - acc: 0.8180 - val_loss: 0.3888 - val_acc: 0.8220\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.3987 - acc: 0.8175 - val_loss: 0.3918 - val_acc: 0.8237\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.3970 - acc: 0.8192 - val_loss: 0.3860 - val_acc: 0.8227\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3976 - acc: 0.8186 - val_loss: 0.3865 - val_acc: 0.8220\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3932 - acc: 0.8217 - val_loss: 0.3849 - val_acc: 0.8256\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3954 - acc: 0.8204 - val_loss: 0.3848 - val_acc: 0.8252\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3939 - acc: 0.8210 - val_loss: 0.3865 - val_acc: 0.8250\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3921 - acc: 0.8223 - val_loss: 0.3865 - val_acc: 0.8232\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3928 - acc: 0.8224 - val_loss: 0.4037 - val_acc: 0.8135\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3923 - acc: 0.8218 - val_loss: 0.4037 - val_acc: 0.8157\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3911 - acc: 0.8219 - val_loss: 0.3841 - val_acc: 0.8266\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.5151 - acc: 0.7496 - val_loss: 0.4698 - val_acc: 0.7764\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4552 - acc: 0.7858 - val_loss: 0.4321 - val_acc: 0.7972\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4349 - acc: 0.7974 - val_loss: 0.4157 - val_acc: 0.8090\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4238 - acc: 0.8050 - val_loss: 0.4018 - val_acc: 0.8165\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4184 - acc: 0.8082 - val_loss: 0.4191 - val_acc: 0.8083\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4128 - acc: 0.8114 - val_loss: 0.4042 - val_acc: 0.8153\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4078 - acc: 0.8129 - val_loss: 0.3991 - val_acc: 0.8158\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4043 - acc: 0.8159 - val_loss: 0.4489 - val_acc: 0.7929\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4021 - acc: 0.8175 - val_loss: 0.3989 - val_acc: 0.8194\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3971 - acc: 0.8201 - val_loss: 0.4135 - val_acc: 0.8074\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3968 - acc: 0.8193 - val_loss: 0.3859 - val_acc: 0.8242\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3927 - acc: 0.8222 - val_loss: 0.4236 - val_acc: 0.8029\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3915 - acc: 0.8225 - val_loss: 0.3918 - val_acc: 0.8196\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3876 - acc: 0.8243 - val_loss: 0.3861 - val_acc: 0.8262\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3862 - acc: 0.8247 - val_loss: 0.3815 - val_acc: 0.8267\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3850 - acc: 0.8263 - val_loss: 0.3871 - val_acc: 0.8223\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3824 - acc: 0.8278 - val_loss: 0.3900 - val_acc: 0.8225\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3808 - acc: 0.8290 - val_loss: 0.3860 - val_acc: 0.8245\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3805 - acc: 0.8277 - val_loss: 0.3748 - val_acc: 0.8292\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3768 - acc: 0.8311 - val_loss: 0.3962 - val_acc: 0.8189\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3753 - acc: 0.8305 - val_loss: 0.3888 - val_acc: 0.8227\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3754 - acc: 0.8306 - val_loss: 0.3866 - val_acc: 0.8236\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3747 - acc: 0.8313 - val_loss: 0.3866 - val_acc: 0.8233\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3704 - acc: 0.8334 - val_loss: 0.3704 - val_acc: 0.8349\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3726 - acc: 0.8335 - val_loss: 0.3836 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3698 - acc: 0.8356 - val_loss: 0.3758 - val_acc: 0.8293\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3692 - acc: 0.8346 - val_loss: 0.3685 - val_acc: 0.8356\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3672 - acc: 0.8354 - val_loss: 0.3771 - val_acc: 0.8293\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3647 - acc: 0.8368 - val_loss: 0.3710 - val_acc: 0.8314\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3675 - acc: 0.8366 - val_loss: 0.3675 - val_acc: 0.8341\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.5289 - acc: 0.7448 - val_loss: 0.4819 - val_acc: 0.7727\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4652 - acc: 0.7797 - val_loss: 0.4329 - val_acc: 0.7996\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4437 - acc: 0.7932 - val_loss: 0.4215 - val_acc: 0.8041\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4292 - acc: 0.8028 - val_loss: 0.4144 - val_acc: 0.8099\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4213 - acc: 0.8057 - val_loss: 0.4093 - val_acc: 0.8100\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4154 - acc: 0.8114 - val_loss: 0.3970 - val_acc: 0.8177\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4095 - acc: 0.8130 - val_loss: 0.4021 - val_acc: 0.8135\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4044 - acc: 0.8167 - val_loss: 0.3890 - val_acc: 0.8220\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4016 - acc: 0.8183 - val_loss: 0.3879 - val_acc: 0.8227\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3976 - acc: 0.8187 - val_loss: 0.3886 - val_acc: 0.8215\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3930 - acc: 0.8231 - val_loss: 0.3948 - val_acc: 0.8176\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3893 - acc: 0.8233 - val_loss: 0.3839 - val_acc: 0.8244\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3877 - acc: 0.8248 - val_loss: 0.3825 - val_acc: 0.8246\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3845 - acc: 0.8267 - val_loss: 0.3902 - val_acc: 0.8231\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3847 - acc: 0.8266 - val_loss: 0.3808 - val_acc: 0.8250\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3808 - acc: 0.8275 - val_loss: 0.3800 - val_acc: 0.8273\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3795 - acc: 0.8285 - val_loss: 0.3761 - val_acc: 0.8284\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3771 - acc: 0.8293 - val_loss: 0.3742 - val_acc: 0.8298\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3740 - acc: 0.8324 - val_loss: 0.3746 - val_acc: 0.8296\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3713 - acc: 0.8335 - val_loss: 0.3703 - val_acc: 0.8318\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3700 - acc: 0.8340 - val_loss: 0.3780 - val_acc: 0.8284\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3698 - acc: 0.8342 - val_loss: 0.4279 - val_acc: 0.8032\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3677 - acc: 0.8356 - val_loss: 0.3732 - val_acc: 0.8331\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3672 - acc: 0.8362 - val_loss: 0.3739 - val_acc: 0.8293\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3656 - acc: 0.8363 - val_loss: 0.3674 - val_acc: 0.8317\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3630 - acc: 0.8371 - val_loss: 0.3699 - val_acc: 0.8322\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3612 - acc: 0.8393 - val_loss: 0.4070 - val_acc: 0.8141\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3619 - acc: 0.8380 - val_loss: 0.3696 - val_acc: 0.8331\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3613 - acc: 0.8386 - val_loss: 0.3712 - val_acc: 0.8324\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3599 - acc: 0.8400 - val_loss: 0.3694 - val_acc: 0.8332\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.5500 - acc: 0.7297 - val_loss: 0.4882 - val_acc: 0.7631\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4990 - acc: 0.7569 - val_loss: 0.4697 - val_acc: 0.7757\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4856 - acc: 0.7655 - val_loss: 0.4538 - val_acc: 0.7842\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4738 - acc: 0.7741 - val_loss: 0.4500 - val_acc: 0.7861\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4635 - acc: 0.7798 - val_loss: 0.4360 - val_acc: 0.7983\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4606 - acc: 0.7816 - val_loss: 0.4360 - val_acc: 0.7973\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4544 - acc: 0.7862 - val_loss: 0.4459 - val_acc: 0.7881\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4501 - acc: 0.7888 - val_loss: 0.4608 - val_acc: 0.7844\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4455 - acc: 0.7914 - val_loss: 0.4332 - val_acc: 0.7994\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4440 - acc: 0.7926 - val_loss: 0.4203 - val_acc: 0.8041\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4403 - acc: 0.7953 - val_loss: 0.4247 - val_acc: 0.8041\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4394 - acc: 0.7951 - val_loss: 0.4173 - val_acc: 0.8055\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4357 - acc: 0.7974 - val_loss: 0.4212 - val_acc: 0.8038\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4320 - acc: 0.8002 - val_loss: 0.4160 - val_acc: 0.8069\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4320 - acc: 0.7991 - val_loss: 0.4195 - val_acc: 0.8061\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4313 - acc: 0.8000 - val_loss: 0.4341 - val_acc: 0.7969\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4278 - acc: 0.8025 - val_loss: 0.4117 - val_acc: 0.8102\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4262 - acc: 0.8031 - val_loss: 0.4256 - val_acc: 0.8031\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4256 - acc: 0.8042 - val_loss: 0.4125 - val_acc: 0.8117\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4246 - acc: 0.8036 - val_loss: 0.4044 - val_acc: 0.8166\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4237 - acc: 0.8043 - val_loss: 0.4183 - val_acc: 0.8070\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4215 - acc: 0.8075 - val_loss: 0.4136 - val_acc: 0.8090\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4200 - acc: 0.8055 - val_loss: 0.4090 - val_acc: 0.8134\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4207 - acc: 0.8064 - val_loss: 0.4198 - val_acc: 0.8059\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4201 - acc: 0.8061 - val_loss: 0.4215 - val_acc: 0.8020\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4188 - acc: 0.8070 - val_loss: 0.4073 - val_acc: 0.8106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4176 - acc: 0.8076 - val_loss: 0.4137 - val_acc: 0.8092\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4155 - acc: 0.8085 - val_loss: 0.4183 - val_acc: 0.8059\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4136 - acc: 0.8099 - val_loss: 0.4173 - val_acc: 0.8058\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4143 - acc: 0.8098 - val_loss: 0.4075 - val_acc: 0.8117\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 143us/sample - loss: 0.5475 - acc: 0.7294 - val_loss: 0.4907 - val_acc: 0.7612\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4945 - acc: 0.7613 - val_loss: 0.4592 - val_acc: 0.7838\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4759 - acc: 0.7727 - val_loss: 0.4514 - val_acc: 0.7861\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4622 - acc: 0.7817 - val_loss: 0.4374 - val_acc: 0.7947\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4563 - acc: 0.7862 - val_loss: 0.4293 - val_acc: 0.8002\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4501 - acc: 0.7893 - val_loss: 0.4284 - val_acc: 0.8007\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4458 - acc: 0.7922 - val_loss: 0.4295 - val_acc: 0.8033\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4428 - acc: 0.7938 - val_loss: 0.4275 - val_acc: 0.8036\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4399 - acc: 0.7952 - val_loss: 0.4197 - val_acc: 0.8069\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4370 - acc: 0.7960 - val_loss: 0.4302 - val_acc: 0.7996\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4344 - acc: 0.7975 - val_loss: 0.4220 - val_acc: 0.8050\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4338 - acc: 0.7984 - val_loss: 0.4144 - val_acc: 0.8101\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4312 - acc: 0.8008 - val_loss: 0.4186 - val_acc: 0.8082\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4307 - acc: 0.8014 - val_loss: 0.4156 - val_acc: 0.8092\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4278 - acc: 0.8018 - val_loss: 0.4155 - val_acc: 0.8088\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4270 - acc: 0.8019 - val_loss: 0.4132 - val_acc: 0.8116\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4250 - acc: 0.8033 - val_loss: 0.4178 - val_acc: 0.8064\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4261 - acc: 0.8043 - val_loss: 0.4080 - val_acc: 0.8131\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4237 - acc: 0.8028 - val_loss: 0.4069 - val_acc: 0.8130\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4228 - acc: 0.8053 - val_loss: 0.4075 - val_acc: 0.8142\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4200 - acc: 0.8072 - val_loss: 0.4089 - val_acc: 0.8129\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4216 - acc: 0.8060 - val_loss: 0.4054 - val_acc: 0.8154\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4196 - acc: 0.8061 - val_loss: 0.4089 - val_acc: 0.8125\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4176 - acc: 0.8069 - val_loss: 0.4164 - val_acc: 0.8080\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4159 - acc: 0.8087 - val_loss: 0.4072 - val_acc: 0.8141\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4144 - acc: 0.8088 - val_loss: 0.4030 - val_acc: 0.8174\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4154 - acc: 0.8098 - val_loss: 0.4097 - val_acc: 0.8113\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4147 - acc: 0.8089 - val_loss: 0.4204 - val_acc: 0.8062\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4143 - acc: 0.8102 - val_loss: 0.4190 - val_acc: 0.8071\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4110 - acc: 0.8117 - val_loss: 0.4057 - val_acc: 0.8144\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.5853 - acc: 0.7050 - val_loss: 0.5125 - val_acc: 0.7498\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.5273 - acc: 0.7396 - val_loss: 0.4947 - val_acc: 0.7617\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.5132 - acc: 0.7504 - val_loss: 0.4930 - val_acc: 0.7615\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.5011 - acc: 0.7568 - val_loss: 0.4725 - val_acc: 0.7726\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4901 - acc: 0.7648 - val_loss: 0.4611 - val_acc: 0.7814\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4832 - acc: 0.7686 - val_loss: 0.4689 - val_acc: 0.7781\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4786 - acc: 0.7737 - val_loss: 0.4528 - val_acc: 0.7836\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4741 - acc: 0.7755 - val_loss: 0.4466 - val_acc: 0.7896\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4718 - acc: 0.7762 - val_loss: 0.4398 - val_acc: 0.7939\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4699 - acc: 0.7773 - val_loss: 0.4338 - val_acc: 0.7966\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4689 - acc: 0.7784 - val_loss: 0.4508 - val_acc: 0.7904\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4682 - acc: 0.7794 - val_loss: 0.4366 - val_acc: 0.7950\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4664 - acc: 0.7798 - val_loss: 0.4452 - val_acc: 0.7922\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4647 - acc: 0.7820 - val_loss: 0.4332 - val_acc: 0.7983\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4658 - acc: 0.7799 - val_loss: 0.4371 - val_acc: 0.7950\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4649 - acc: 0.7811 - val_loss: 0.4354 - val_acc: 0.7987\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4636 - acc: 0.7812 - val_loss: 0.4275 - val_acc: 0.8026\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4625 - acc: 0.7804 - val_loss: 0.4321 - val_acc: 0.7991\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4627 - acc: 0.7815 - val_loss: 0.4479 - val_acc: 0.7883\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4590 - acc: 0.7850 - val_loss: 0.4241 - val_acc: 0.8045\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4623 - acc: 0.7822 - val_loss: 0.4296 - val_acc: 0.8001\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4592 - acc: 0.7833 - val_loss: 0.4235 - val_acc: 0.8041\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4597 - acc: 0.7836 - val_loss: 0.4299 - val_acc: 0.7994\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4593 - acc: 0.7840 - val_loss: 0.4261 - val_acc: 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4589 - acc: 0.7844 - val_loss: 0.4259 - val_acc: 0.8041\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4566 - acc: 0.7844 - val_loss: 0.4310 - val_acc: 0.7995\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4569 - acc: 0.7865 - val_loss: 0.4239 - val_acc: 0.8025\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4567 - acc: 0.7844 - val_loss: 0.4218 - val_acc: 0.8064\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4568 - acc: 0.7847 - val_loss: 0.4230 - val_acc: 0.8077\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4567 - acc: 0.7844 - val_loss: 0.4223 - val_acc: 0.8069\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.6025 - acc: 0.6929 - val_loss: 0.5132 - val_acc: 0.7505\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.5331 - acc: 0.7392 - val_loss: 0.4973 - val_acc: 0.7596\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.5202 - acc: 0.7464 - val_loss: 0.4915 - val_acc: 0.7630\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.5080 - acc: 0.7542 - val_loss: 0.4766 - val_acc: 0.7732\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4998 - acc: 0.7617 - val_loss: 0.4758 - val_acc: 0.7737\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4937 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7589\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4890 - acc: 0.7670 - val_loss: 0.4875 - val_acc: 0.7732\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4868 - acc: 0.7668 - val_loss: 0.4753 - val_acc: 0.7742\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4815 - acc: 0.7699 - val_loss: 0.4581 - val_acc: 0.7860\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4812 - acc: 0.7716 - val_loss: 0.4524 - val_acc: 0.7897\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4795 - acc: 0.7706 - val_loss: 0.4529 - val_acc: 0.7895\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4793 - acc: 0.7722 - val_loss: 0.4518 - val_acc: 0.7905\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4753 - acc: 0.7741 - val_loss: 0.4887 - val_acc: 0.7657\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4737 - acc: 0.7760 - val_loss: 0.4461 - val_acc: 0.7926\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4730 - acc: 0.7756 - val_loss: 0.4613 - val_acc: 0.7833\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4738 - acc: 0.7754 - val_loss: 0.4424 - val_acc: 0.7929\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4718 - acc: 0.7770 - val_loss: 0.4520 - val_acc: 0.7910\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4723 - acc: 0.7765 - val_loss: 0.4403 - val_acc: 0.7943\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4706 - acc: 0.7771 - val_loss: 0.4418 - val_acc: 0.7944\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4710 - acc: 0.7775 - val_loss: 0.4509 - val_acc: 0.7875\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4714 - acc: 0.7767 - val_loss: 0.4489 - val_acc: 0.7837\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4709 - acc: 0.7779 - val_loss: 0.4434 - val_acc: 0.7947\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4688 - acc: 0.7780 - val_loss: 0.4464 - val_acc: 0.7936\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4706 - acc: 0.7760 - val_loss: 0.4444 - val_acc: 0.7964\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4669 - acc: 0.7794 - val_loss: 0.4377 - val_acc: 0.7955\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4684 - acc: 0.7793 - val_loss: 0.4452 - val_acc: 0.7912\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4666 - acc: 0.7807 - val_loss: 0.4413 - val_acc: 0.7952\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4666 - acc: 0.7790 - val_loss: 0.4411 - val_acc: 0.7933\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4668 - acc: 0.7793 - val_loss: 0.4379 - val_acc: 0.7975\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4666 - acc: 0.7795 - val_loss: 0.4456 - val_acc: 0.7937\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.5234 - acc: 0.7454 - val_loss: 0.4625 - val_acc: 0.7762\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4650 - acc: 0.7797 - val_loss: 0.4348 - val_acc: 0.7952\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4449 - acc: 0.7920 - val_loss: 0.4293 - val_acc: 0.8025\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4356 - acc: 0.7973 - val_loss: 0.4140 - val_acc: 0.8082\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4281 - acc: 0.8016 - val_loss: 0.4306 - val_acc: 0.7994\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4237 - acc: 0.8036 - val_loss: 0.4189 - val_acc: 0.8061\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4164 - acc: 0.8092 - val_loss: 0.4093 - val_acc: 0.8110\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4150 - acc: 0.8093 - val_loss: 0.4356 - val_acc: 0.7922\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4092 - acc: 0.8125 - val_loss: 0.3984 - val_acc: 0.8203\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4068 - acc: 0.8148 - val_loss: 0.3958 - val_acc: 0.8169\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4029 - acc: 0.8169 - val_loss: 0.4087 - val_acc: 0.8094\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4008 - acc: 0.8180 - val_loss: 0.4002 - val_acc: 0.8164\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3975 - acc: 0.8180 - val_loss: 0.4127 - val_acc: 0.8107\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3952 - acc: 0.8206 - val_loss: 0.4246 - val_acc: 0.8048\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3911 - acc: 0.8228 - val_loss: 0.3899 - val_acc: 0.8211\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3925 - acc: 0.8226 - val_loss: 0.3947 - val_acc: 0.8191\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3887 - acc: 0.8239 - val_loss: 0.3902 - val_acc: 0.8213\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3862 - acc: 0.8252 - val_loss: 0.3856 - val_acc: 0.8268\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3842 - acc: 0.8252 - val_loss: 0.3915 - val_acc: 0.8227\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3816 - acc: 0.8286 - val_loss: 0.4113 - val_acc: 0.8113\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3792 - acc: 0.8282 - val_loss: 0.4089 - val_acc: 0.8176\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3796 - acc: 0.8285 - val_loss: 0.3952 - val_acc: 0.8217\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3757 - acc: 0.8299 - val_loss: 0.4005 - val_acc: 0.8170\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3752 - acc: 0.8314 - val_loss: 0.3899 - val_acc: 0.8234\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3721 - acc: 0.8326 - val_loss: 0.3996 - val_acc: 0.8171\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3721 - acc: 0.8323 - val_loss: 0.4095 - val_acc: 0.8149\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3710 - acc: 0.8340 - val_loss: 0.3869 - val_acc: 0.8258\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3720 - acc: 0.8332 - val_loss: 0.3883 - val_acc: 0.8237\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3699 - acc: 0.8333 - val_loss: 0.3906 - val_acc: 0.8224\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3680 - acc: 0.8346 - val_loss: 0.3938 - val_acc: 0.8238\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.5309 - acc: 0.7402 - val_loss: 0.4722 - val_acc: 0.7711\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4694 - acc: 0.7765 - val_loss: 0.4387 - val_acc: 0.7957\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4514 - acc: 0.7884 - val_loss: 0.4259 - val_acc: 0.8031\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4402 - acc: 0.7948 - val_loss: 0.4467 - val_acc: 0.7904\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4322 - acc: 0.7990 - val_loss: 0.4183 - val_acc: 0.8079\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4241 - acc: 0.8046 - val_loss: 0.4099 - val_acc: 0.8129\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4209 - acc: 0.8062 - val_loss: 0.4051 - val_acc: 0.8155\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4169 - acc: 0.8079 - val_loss: 0.4000 - val_acc: 0.8158\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4092 - acc: 0.8112 - val_loss: 0.4135 - val_acc: 0.8073\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4086 - acc: 0.8128 - val_loss: 0.4066 - val_acc: 0.8146\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4054 - acc: 0.8139 - val_loss: 0.3952 - val_acc: 0.8210\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4039 - acc: 0.8153 - val_loss: 0.3936 - val_acc: 0.8205\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3986 - acc: 0.8186 - val_loss: 0.3959 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3957 - acc: 0.8193 - val_loss: 0.3911 - val_acc: 0.8231\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3937 - acc: 0.8203 - val_loss: 0.4167 - val_acc: 0.8073\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3920 - acc: 0.8219 - val_loss: 0.3918 - val_acc: 0.8230\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3903 - acc: 0.8219 - val_loss: 0.3874 - val_acc: 0.8239\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3866 - acc: 0.8235 - val_loss: 0.3974 - val_acc: 0.8206\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3860 - acc: 0.8242 - val_loss: 0.3905 - val_acc: 0.8208\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3827 - acc: 0.8270 - val_loss: 0.3886 - val_acc: 0.8212\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3791 - acc: 0.8273 - val_loss: 0.3921 - val_acc: 0.8195\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3788 - acc: 0.8282 - val_loss: 0.3930 - val_acc: 0.8202\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3775 - acc: 0.8295 - val_loss: 0.3949 - val_acc: 0.8192\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3747 - acc: 0.8303 - val_loss: 0.3853 - val_acc: 0.8252\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3753 - acc: 0.8299 - val_loss: 0.3852 - val_acc: 0.8240\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3708 - acc: 0.8328 - val_loss: 0.3864 - val_acc: 0.8253\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3694 - acc: 0.8346 - val_loss: 0.3929 - val_acc: 0.8216\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3697 - acc: 0.8329 - val_loss: 0.4097 - val_acc: 0.8160\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3663 - acc: 0.8351 - val_loss: 0.3913 - val_acc: 0.8236\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3648 - acc: 0.8349 - val_loss: 0.3840 - val_acc: 0.8263\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.5725 - acc: 0.7221 - val_loss: 0.4784 - val_acc: 0.7708\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4871 - acc: 0.7652 - val_loss: 0.4504 - val_acc: 0.7862\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4655 - acc: 0.7800 - val_loss: 0.4354 - val_acc: 0.7994\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4526 - acc: 0.7879 - val_loss: 0.4508 - val_acc: 0.7937\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4446 - acc: 0.7930 - val_loss: 0.4198 - val_acc: 0.8033\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4403 - acc: 0.7954 - val_loss: 0.4437 - val_acc: 0.7917\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4335 - acc: 0.7990 - val_loss: 0.4188 - val_acc: 0.8056\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4293 - acc: 0.8014 - val_loss: 0.4199 - val_acc: 0.8023\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4292 - acc: 0.8009 - val_loss: 0.4207 - val_acc: 0.8055\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4247 - acc: 0.8048 - val_loss: 0.4060 - val_acc: 0.8127\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4214 - acc: 0.8055 - val_loss: 0.4148 - val_acc: 0.8087\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4195 - acc: 0.8083 - val_loss: 0.4094 - val_acc: 0.8098\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4181 - acc: 0.8081 - val_loss: 0.4137 - val_acc: 0.8089\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4170 - acc: 0.8081 - val_loss: 0.3924 - val_acc: 0.8214\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4136 - acc: 0.8096 - val_loss: 0.3927 - val_acc: 0.8202\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4129 - acc: 0.8099 - val_loss: 0.3877 - val_acc: 0.8229\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4110 - acc: 0.8127 - val_loss: 0.4089 - val_acc: 0.8112\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4097 - acc: 0.8137 - val_loss: 0.3860 - val_acc: 0.8237\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4096 - acc: 0.8123 - val_loss: 0.3845 - val_acc: 0.8237\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4075 - acc: 0.8144 - val_loss: 0.3941 - val_acc: 0.8197\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4075 - acc: 0.8138 - val_loss: 0.4278 - val_acc: 0.8022\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4068 - acc: 0.8142 - val_loss: 0.3936 - val_acc: 0.8188\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4058 - acc: 0.8147 - val_loss: 0.3838 - val_acc: 0.8263\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4047 - acc: 0.8159 - val_loss: 0.3981 - val_acc: 0.8160\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4026 - acc: 0.8155 - val_loss: 0.4026 - val_acc: 0.8139\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4025 - acc: 0.8175 - val_loss: 0.3885 - val_acc: 0.8237\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4019 - acc: 0.8170 - val_loss: 0.4009 - val_acc: 0.8159\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4004 - acc: 0.8181 - val_loss: 0.3819 - val_acc: 0.8260\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3994 - acc: 0.8187 - val_loss: 0.3866 - val_acc: 0.8234\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3997 - acc: 0.8184 - val_loss: 0.3849 - val_acc: 0.8242\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.5649 - acc: 0.7206 - val_loss: 0.4851 - val_acc: 0.7721\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4977 - acc: 0.7601 - val_loss: 0.4626 - val_acc: 0.7807\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4774 - acc: 0.7713 - val_loss: 0.4633 - val_acc: 0.7801\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4647 - acc: 0.7797 - val_loss: 0.4450 - val_acc: 0.7936\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4540 - acc: 0.7871 - val_loss: 0.4282 - val_acc: 0.8008\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4467 - acc: 0.7907 - val_loss: 0.4180 - val_acc: 0.8060\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4409 - acc: 0.7954 - val_loss: 0.4124 - val_acc: 0.8138\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4377 - acc: 0.7977 - val_loss: 0.4097 - val_acc: 0.8138\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4336 - acc: 0.8004 - val_loss: 0.4207 - val_acc: 0.8117\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4308 - acc: 0.8002 - val_loss: 0.4056 - val_acc: 0.8127\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4282 - acc: 0.8010 - val_loss: 0.4176 - val_acc: 0.8069\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4256 - acc: 0.8034 - val_loss: 0.4069 - val_acc: 0.8114\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4253 - acc: 0.8036 - val_loss: 0.4003 - val_acc: 0.8187\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4228 - acc: 0.8062 - val_loss: 0.4325 - val_acc: 0.7994\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4208 - acc: 0.8065 - val_loss: 0.4086 - val_acc: 0.8121\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4198 - acc: 0.8067 - val_loss: 0.3955 - val_acc: 0.8210\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4179 - acc: 0.8083 - val_loss: 0.3965 - val_acc: 0.8191\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4161 - acc: 0.8078 - val_loss: 0.4028 - val_acc: 0.8156\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4159 - acc: 0.8098 - val_loss: 0.3946 - val_acc: 0.8196\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4151 - acc: 0.8090 - val_loss: 0.3899 - val_acc: 0.8215\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4145 - acc: 0.8096 - val_loss: 0.3910 - val_acc: 0.8196\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4138 - acc: 0.8118 - val_loss: 0.3894 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4118 - acc: 0.8128 - val_loss: 0.3864 - val_acc: 0.8232\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4115 - acc: 0.8117 - val_loss: 0.3830 - val_acc: 0.8259\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4116 - acc: 0.8117 - val_loss: 0.3838 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4106 - acc: 0.8104 - val_loss: 0.3929 - val_acc: 0.8188\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4092 - acc: 0.8132 - val_loss: 0.3857 - val_acc: 0.8235\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4099 - acc: 0.8131 - val_loss: 0.3904 - val_acc: 0.8222\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4088 - acc: 0.8141 - val_loss: 0.3869 - val_acc: 0.8228\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4091 - acc: 0.8135 - val_loss: 0.3850 - val_acc: 0.8258\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.5291 - acc: 0.7465 - val_loss: 0.4710 - val_acc: 0.7811\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4594 - acc: 0.7838 - val_loss: 0.4352 - val_acc: 0.7961\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4429 - acc: 0.7931 - val_loss: 0.4309 - val_acc: 0.7985\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4307 - acc: 0.8007 - val_loss: 0.4119 - val_acc: 0.8080\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4230 - acc: 0.8055 - val_loss: 0.4054 - val_acc: 0.8156\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4156 - acc: 0.8086 - val_loss: 0.4337 - val_acc: 0.7950\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4104 - acc: 0.8130 - val_loss: 0.4005 - val_acc: 0.8194\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4056 - acc: 0.8146 - val_loss: 0.3982 - val_acc: 0.8173\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4015 - acc: 0.8163 - val_loss: 0.4047 - val_acc: 0.8140\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3978 - acc: 0.8183 - val_loss: 0.3913 - val_acc: 0.8202\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3934 - acc: 0.8222 - val_loss: 0.3945 - val_acc: 0.8198\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3910 - acc: 0.8225 - val_loss: 0.4009 - val_acc: 0.8135\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3868 - acc: 0.8258 - val_loss: 0.4041 - val_acc: 0.8137\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3819 - acc: 0.8278 - val_loss: 0.3835 - val_acc: 0.8248\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3813 - acc: 0.8274 - val_loss: 0.3892 - val_acc: 0.8215\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3790 - acc: 0.8292 - val_loss: 0.3885 - val_acc: 0.8236\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3776 - acc: 0.8292 - val_loss: 0.3828 - val_acc: 0.8269\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3738 - acc: 0.8328 - val_loss: 0.3884 - val_acc: 0.8237\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3722 - acc: 0.8320 - val_loss: 0.4326 - val_acc: 0.7945\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3687 - acc: 0.8340 - val_loss: 0.3841 - val_acc: 0.8255\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3668 - acc: 0.8349 - val_loss: 0.3944 - val_acc: 0.8177\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3630 - acc: 0.8378 - val_loss: 0.3822 - val_acc: 0.8252\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3615 - acc: 0.8382 - val_loss: 0.3940 - val_acc: 0.8208\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3605 - acc: 0.8389 - val_loss: 0.3813 - val_acc: 0.8265\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3549 - acc: 0.8405 - val_loss: 0.3827 - val_acc: 0.8261\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3555 - acc: 0.8409 - val_loss: 0.3910 - val_acc: 0.8220\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3518 - acc: 0.8426 - val_loss: 0.3886 - val_acc: 0.8238\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3497 - acc: 0.8441 - val_loss: 0.4018 - val_acc: 0.8160\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3452 - acc: 0.8455 - val_loss: 0.3838 - val_acc: 0.8245\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3431 - acc: 0.8468 - val_loss: 0.3860 - val_acc: 0.8240\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.5304 - acc: 0.7468 - val_loss: 0.5145 - val_acc: 0.7475\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4568 - acc: 0.7852 - val_loss: 0.4474 - val_acc: 0.7930\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4381 - acc: 0.7962 - val_loss: 0.4302 - val_acc: 0.8005\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4267 - acc: 0.8020 - val_loss: 0.4135 - val_acc: 0.8101\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4197 - acc: 0.8062 - val_loss: 0.4169 - val_acc: 0.8070\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4135 - acc: 0.8097 - val_loss: 0.4099 - val_acc: 0.8116\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4091 - acc: 0.8130 - val_loss: 0.4034 - val_acc: 0.8126\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4041 - acc: 0.8148 - val_loss: 0.3993 - val_acc: 0.8171\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3996 - acc: 0.8172 - val_loss: 0.3920 - val_acc: 0.8205\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3962 - acc: 0.8194 - val_loss: 0.3941 - val_acc: 0.8196\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3915 - acc: 0.8223 - val_loss: 0.3895 - val_acc: 0.8234\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3882 - acc: 0.8253 - val_loss: 0.3902 - val_acc: 0.8248\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3856 - acc: 0.8249 - val_loss: 0.3915 - val_acc: 0.8199\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3815 - acc: 0.8269 - val_loss: 0.4292 - val_acc: 0.7974\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3785 - acc: 0.8287 - val_loss: 0.4140 - val_acc: 0.8129\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3743 - acc: 0.8322 - val_loss: 0.3916 - val_acc: 0.8238\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3723 - acc: 0.8330 - val_loss: 0.3914 - val_acc: 0.8194\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3707 - acc: 0.8325 - val_loss: 0.3938 - val_acc: 0.8208\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3668 - acc: 0.8361 - val_loss: 0.3892 - val_acc: 0.8234\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3629 - acc: 0.8370 - val_loss: 0.4091 - val_acc: 0.8167\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3604 - acc: 0.8382 - val_loss: 0.3932 - val_acc: 0.8224\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3585 - acc: 0.8391 - val_loss: 0.3827 - val_acc: 0.8284\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3584 - acc: 0.8387 - val_loss: 0.3826 - val_acc: 0.8278\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3537 - acc: 0.8420 - val_loss: 0.3877 - val_acc: 0.8243\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3518 - acc: 0.8432 - val_loss: 0.3839 - val_acc: 0.8256\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3489 - acc: 0.8436 - val_loss: 0.3968 - val_acc: 0.8216\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3490 - acc: 0.8451 - val_loss: 0.3880 - val_acc: 0.8262\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3450 - acc: 0.8466 - val_loss: 0.3875 - val_acc: 0.8263\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3428 - acc: 0.8472 - val_loss: 0.3992 - val_acc: 0.8177\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3406 - acc: 0.8482 - val_loss: 0.3941 - val_acc: 0.8271\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.5385 - acc: 0.7401 - val_loss: 0.4600 - val_acc: 0.7821\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4697 - acc: 0.7767 - val_loss: 0.4363 - val_acc: 0.7962\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4474 - acc: 0.7911 - val_loss: 0.4238 - val_acc: 0.8043\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4347 - acc: 0.7982 - val_loss: 0.4221 - val_acc: 0.8054\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4244 - acc: 0.8046 - val_loss: 0.4666 - val_acc: 0.7820\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4194 - acc: 0.8074 - val_loss: 0.4311 - val_acc: 0.7980\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4149 - acc: 0.8092 - val_loss: 0.4014 - val_acc: 0.8154\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4095 - acc: 0.8128 - val_loss: 0.3985 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4074 - acc: 0.8144 - val_loss: 0.4030 - val_acc: 0.8152\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4024 - acc: 0.8162 - val_loss: 0.3987 - val_acc: 0.8166\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4000 - acc: 0.8180 - val_loss: 0.3922 - val_acc: 0.8219\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3963 - acc: 0.8191 - val_loss: 0.3904 - val_acc: 0.8231\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3932 - acc: 0.8206 - val_loss: 0.3818 - val_acc: 0.8277\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3908 - acc: 0.8232 - val_loss: 0.3865 - val_acc: 0.8242\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3896 - acc: 0.8231 - val_loss: 0.3902 - val_acc: 0.8200\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3866 - acc: 0.8247 - val_loss: 0.3773 - val_acc: 0.8299\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3857 - acc: 0.8245 - val_loss: 0.3794 - val_acc: 0.8284\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3846 - acc: 0.8254 - val_loss: 0.3897 - val_acc: 0.8195\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3821 - acc: 0.8273 - val_loss: 0.3721 - val_acc: 0.8323\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3801 - acc: 0.8273 - val_loss: 0.3777 - val_acc: 0.8295\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3786 - acc: 0.8284 - val_loss: 0.3933 - val_acc: 0.8201\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3773 - acc: 0.8290 - val_loss: 0.3833 - val_acc: 0.8260\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3758 - acc: 0.8305 - val_loss: 0.3866 - val_acc: 0.8250\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3728 - acc: 0.8311 - val_loss: 0.3783 - val_acc: 0.8296\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3719 - acc: 0.8330 - val_loss: 0.3675 - val_acc: 0.8342\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3727 - acc: 0.8318 - val_loss: 0.3779 - val_acc: 0.8303\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3711 - acc: 0.8328 - val_loss: 0.3760 - val_acc: 0.8276\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3707 - acc: 0.8340 - val_loss: 0.3702 - val_acc: 0.8342\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3698 - acc: 0.8321 - val_loss: 0.3831 - val_acc: 0.8287\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3674 - acc: 0.8341 - val_loss: 0.3686 - val_acc: 0.8320\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.5409 - acc: 0.7359 - val_loss: 0.4704 - val_acc: 0.7719\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4749 - acc: 0.7740 - val_loss: 0.4387 - val_acc: 0.7946\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4527 - acc: 0.7871 - val_loss: 0.4212 - val_acc: 0.8026\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4386 - acc: 0.7956 - val_loss: 0.4308 - val_acc: 0.8009\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4301 - acc: 0.8016 - val_loss: 0.4311 - val_acc: 0.8020\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4231 - acc: 0.8062 - val_loss: 0.4031 - val_acc: 0.8175\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4195 - acc: 0.8069 - val_loss: 0.3995 - val_acc: 0.8166\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4121 - acc: 0.8117 - val_loss: 0.4217 - val_acc: 0.8016\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4099 - acc: 0.8126 - val_loss: 0.4063 - val_acc: 0.8140\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4068 - acc: 0.8153 - val_loss: 0.3924 - val_acc: 0.8214\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4043 - acc: 0.8163 - val_loss: 0.3911 - val_acc: 0.8215\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3998 - acc: 0.8171 - val_loss: 0.3855 - val_acc: 0.8255\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3973 - acc: 0.8192 - val_loss: 0.3835 - val_acc: 0.8243\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3949 - acc: 0.8214 - val_loss: 0.4023 - val_acc: 0.8141\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3936 - acc: 0.8217 - val_loss: 0.3877 - val_acc: 0.8207\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3938 - acc: 0.8221 - val_loss: 0.4198 - val_acc: 0.8047\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3912 - acc: 0.8227 - val_loss: 0.3935 - val_acc: 0.8218\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3880 - acc: 0.8243 - val_loss: 0.4173 - val_acc: 0.8048\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3884 - acc: 0.8249 - val_loss: 0.3770 - val_acc: 0.8293\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3862 - acc: 0.8265 - val_loss: 0.3774 - val_acc: 0.8296\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3849 - acc: 0.8257 - val_loss: 0.3902 - val_acc: 0.8223\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3853 - acc: 0.8260 - val_loss: 0.3821 - val_acc: 0.8274\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3839 - acc: 0.8260 - val_loss: 0.3745 - val_acc: 0.8306\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3804 - acc: 0.8292 - val_loss: 0.3727 - val_acc: 0.8310\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.3807 - acc: 0.8275 - val_loss: 0.3733 - val_acc: 0.8289\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3806 - acc: 0.8289 - val_loss: 0.3751 - val_acc: 0.8295\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3775 - acc: 0.8305 - val_loss: 0.3718 - val_acc: 0.8314\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3760 - acc: 0.8308 - val_loss: 0.3692 - val_acc: 0.8325\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.3769 - acc: 0.8305 - val_loss: 0.3739 - val_acc: 0.8295\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3756 - acc: 0.8311 - val_loss: 0.3734 - val_acc: 0.8292\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.5099 - acc: 0.7532 - val_loss: 0.4592 - val_acc: 0.7857\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4544 - acc: 0.7859 - val_loss: 0.4500 - val_acc: 0.7851\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4353 - acc: 0.7967 - val_loss: 0.4245 - val_acc: 0.8059\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4233 - acc: 0.8050 - val_loss: 0.4089 - val_acc: 0.8126\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4168 - acc: 0.8078 - val_loss: 0.4027 - val_acc: 0.8140\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4096 - acc: 0.8124 - val_loss: 0.4026 - val_acc: 0.8144\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4051 - acc: 0.8139 - val_loss: 0.4377 - val_acc: 0.7955\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4012 - acc: 0.8159 - val_loss: 0.3976 - val_acc: 0.8184\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3937 - acc: 0.8204 - val_loss: 0.4132 - val_acc: 0.8113\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3921 - acc: 0.8209 - val_loss: 0.3972 - val_acc: 0.8176\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3890 - acc: 0.8230 - val_loss: 0.3911 - val_acc: 0.8213\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3848 - acc: 0.8256 - val_loss: 0.3873 - val_acc: 0.8243\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3802 - acc: 0.8291 - val_loss: 0.3876 - val_acc: 0.8237\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3780 - acc: 0.8286 - val_loss: 0.3866 - val_acc: 0.8235\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3733 - acc: 0.8319 - val_loss: 0.3843 - val_acc: 0.8264\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3715 - acc: 0.8334 - val_loss: 0.4472 - val_acc: 0.7883\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3697 - acc: 0.8328 - val_loss: 0.3857 - val_acc: 0.8253\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3659 - acc: 0.8361 - val_loss: 0.3944 - val_acc: 0.8209\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3601 - acc: 0.8391 - val_loss: 0.3971 - val_acc: 0.8201\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3597 - acc: 0.8387 - val_loss: 0.4015 - val_acc: 0.8220\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3578 - acc: 0.8407 - val_loss: 0.3906 - val_acc: 0.8224\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3554 - acc: 0.8410 - val_loss: 0.3845 - val_acc: 0.8233\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3513 - acc: 0.8422 - val_loss: 0.3873 - val_acc: 0.8221\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3497 - acc: 0.8445 - val_loss: 0.3903 - val_acc: 0.8228\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3461 - acc: 0.8469 - val_loss: 0.3935 - val_acc: 0.8217\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3437 - acc: 0.8460 - val_loss: 0.3934 - val_acc: 0.8196\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3420 - acc: 0.8463 - val_loss: 0.3869 - val_acc: 0.8231\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3379 - acc: 0.8487 - val_loss: 0.3888 - val_acc: 0.8239\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3357 - acc: 0.8502 - val_loss: 0.3999 - val_acc: 0.8169\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3363 - acc: 0.8505 - val_loss: 0.3844 - val_acc: 0.8263\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.5148 - acc: 0.7515 - val_loss: 0.4563 - val_acc: 0.7830\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4530 - acc: 0.7858 - val_loss: 0.4409 - val_acc: 0.7965\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4325 - acc: 0.7986 - val_loss: 0.4133 - val_acc: 0.8105\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4223 - acc: 0.8053 - val_loss: 0.4316 - val_acc: 0.8001\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4135 - acc: 0.8102 - val_loss: 0.4567 - val_acc: 0.7822\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4090 - acc: 0.8130 - val_loss: 0.4136 - val_acc: 0.8097\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4029 - acc: 0.8167 - val_loss: 0.4140 - val_acc: 0.8092\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3994 - acc: 0.8182 - val_loss: 0.3999 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3960 - acc: 0.8206 - val_loss: 0.3967 - val_acc: 0.8192\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3895 - acc: 0.8238 - val_loss: 0.3916 - val_acc: 0.8230\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3859 - acc: 0.8251 - val_loss: 0.4133 - val_acc: 0.8122\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3820 - acc: 0.8265 - val_loss: 0.3886 - val_acc: 0.8235\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3798 - acc: 0.8285 - val_loss: 0.3920 - val_acc: 0.8221\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3786 - acc: 0.8299 - val_loss: 0.3867 - val_acc: 0.8234\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3730 - acc: 0.8320 - val_loss: 0.3810 - val_acc: 0.8260\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3700 - acc: 0.8342 - val_loss: 0.3838 - val_acc: 0.8277\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3663 - acc: 0.8362 - val_loss: 0.3853 - val_acc: 0.8248\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3637 - acc: 0.8367 - val_loss: 0.3902 - val_acc: 0.8193\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3595 - acc: 0.8392 - val_loss: 0.3911 - val_acc: 0.8209\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3566 - acc: 0.8404 - val_loss: 0.3968 - val_acc: 0.8257\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3543 - acc: 0.8419 - val_loss: 0.3818 - val_acc: 0.8263\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3527 - acc: 0.8425 - val_loss: 0.4159 - val_acc: 0.8154\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3495 - acc: 0.8448 - val_loss: 0.3831 - val_acc: 0.8283\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3468 - acc: 0.8446 - val_loss: 0.3814 - val_acc: 0.8283\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3446 - acc: 0.8462 - val_loss: 0.3869 - val_acc: 0.8281\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3423 - acc: 0.8476 - val_loss: 0.3923 - val_acc: 0.8208\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3410 - acc: 0.8483 - val_loss: 0.3906 - val_acc: 0.8241\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3397 - acc: 0.8498 - val_loss: 0.3973 - val_acc: 0.8239\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3356 - acc: 0.8518 - val_loss: 0.4019 - val_acc: 0.8177\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3329 - acc: 0.8536 - val_loss: 0.3916 - val_acc: 0.8238\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.5165 - acc: 0.7522 - val_loss: 0.4674 - val_acc: 0.7785\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4552 - acc: 0.7878 - val_loss: 0.4343 - val_acc: 0.7970\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4345 - acc: 0.7968 - val_loss: 0.4085 - val_acc: 0.8098\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4234 - acc: 0.8065 - val_loss: 0.4262 - val_acc: 0.8054\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4166 - acc: 0.8081 - val_loss: 0.3966 - val_acc: 0.8191\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4094 - acc: 0.8124 - val_loss: 0.3963 - val_acc: 0.8184\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4043 - acc: 0.8153 - val_loss: 0.3891 - val_acc: 0.8246\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3991 - acc: 0.8179 - val_loss: 0.3867 - val_acc: 0.8219\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3951 - acc: 0.8200 - val_loss: 0.3907 - val_acc: 0.8191\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3927 - acc: 0.8214 - val_loss: 0.3812 - val_acc: 0.8270\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3879 - acc: 0.8234 - val_loss: 0.3882 - val_acc: 0.8227\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3857 - acc: 0.8250 - val_loss: 0.3852 - val_acc: 0.8242\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3840 - acc: 0.8251 - val_loss: 0.3975 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3791 - acc: 0.8287 - val_loss: 0.3750 - val_acc: 0.8306\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3772 - acc: 0.8296 - val_loss: 0.3726 - val_acc: 0.8295\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3757 - acc: 0.8300 - val_loss: 0.3766 - val_acc: 0.8317\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3728 - acc: 0.8331 - val_loss: 0.3771 - val_acc: 0.8282\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3702 - acc: 0.8330 - val_loss: 0.3819 - val_acc: 0.8279\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3676 - acc: 0.8351 - val_loss: 0.3807 - val_acc: 0.8274\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3644 - acc: 0.8365 - val_loss: 0.3803 - val_acc: 0.8299\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3644 - acc: 0.8358 - val_loss: 0.3928 - val_acc: 0.8223\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3600 - acc: 0.8386 - val_loss: 0.3720 - val_acc: 0.8319\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3593 - acc: 0.8385 - val_loss: 0.3835 - val_acc: 0.8282\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3571 - acc: 0.8385 - val_loss: 0.3798 - val_acc: 0.8232\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3557 - acc: 0.8422 - val_loss: 0.3734 - val_acc: 0.8316\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3547 - acc: 0.8408 - val_loss: 0.3870 - val_acc: 0.8233\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3517 - acc: 0.8422 - val_loss: 0.3727 - val_acc: 0.8317\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3508 - acc: 0.8436 - val_loss: 0.3752 - val_acc: 0.8318\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3504 - acc: 0.8448 - val_loss: 0.3810 - val_acc: 0.8313\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3471 - acc: 0.8432 - val_loss: 0.3790 - val_acc: 0.8285\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.5445 - acc: 0.7396 - val_loss: 0.4569 - val_acc: 0.7856\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4654 - acc: 0.7815 - val_loss: 0.4425 - val_acc: 0.7931\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4417 - acc: 0.7945 - val_loss: 0.4214 - val_acc: 0.8069\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4276 - acc: 0.8032 - val_loss: 0.4042 - val_acc: 0.8158\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4201 - acc: 0.8072 - val_loss: 0.4008 - val_acc: 0.8170\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4133 - acc: 0.8108 - val_loss: 0.4081 - val_acc: 0.8112\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4085 - acc: 0.8145 - val_loss: 0.4100 - val_acc: 0.8082\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4049 - acc: 0.8145 - val_loss: 0.3873 - val_acc: 0.8209\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4005 - acc: 0.8173 - val_loss: 0.3981 - val_acc: 0.8194\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3965 - acc: 0.8192 - val_loss: 0.3794 - val_acc: 0.8270\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3915 - acc: 0.8218 - val_loss: 0.3889 - val_acc: 0.8219\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3894 - acc: 0.8243 - val_loss: 0.3832 - val_acc: 0.8245\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3862 - acc: 0.8235 - val_loss: 0.3787 - val_acc: 0.8301\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3843 - acc: 0.8254 - val_loss: 0.3796 - val_acc: 0.8295\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3824 - acc: 0.8277 - val_loss: 0.3849 - val_acc: 0.8242\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3798 - acc: 0.8279 - val_loss: 0.3889 - val_acc: 0.8209\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3770 - acc: 0.8307 - val_loss: 0.3972 - val_acc: 0.8184\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3752 - acc: 0.8294 - val_loss: 0.3799 - val_acc: 0.8267\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3725 - acc: 0.8319 - val_loss: 0.3700 - val_acc: 0.8349\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3702 - acc: 0.8340 - val_loss: 0.3787 - val_acc: 0.8276\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3689 - acc: 0.8344 - val_loss: 0.3724 - val_acc: 0.8310\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3669 - acc: 0.8350 - val_loss: 0.3753 - val_acc: 0.8292\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3661 - acc: 0.8352 - val_loss: 0.3744 - val_acc: 0.8311\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3648 - acc: 0.8348 - val_loss: 0.3788 - val_acc: 0.8295\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3618 - acc: 0.8381 - val_loss: 0.3684 - val_acc: 0.8331\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3607 - acc: 0.8395 - val_loss: 0.3736 - val_acc: 0.8295\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3590 - acc: 0.8389 - val_loss: 0.3703 - val_acc: 0.8305\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3588 - acc: 0.8388 - val_loss: 0.3684 - val_acc: 0.8347\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3551 - acc: 0.8405 - val_loss: 0.3830 - val_acc: 0.8246\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3570 - acc: 0.8414 - val_loss: 0.3764 - val_acc: 0.8309\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.5873 - acc: 0.7071 - val_loss: 0.5061 - val_acc: 0.7505\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.5114 - acc: 0.7491 - val_loss: 0.4841 - val_acc: 0.7659\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4898 - acc: 0.7644 - val_loss: 0.4682 - val_acc: 0.7741\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4737 - acc: 0.7756 - val_loss: 0.4531 - val_acc: 0.7885\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4669 - acc: 0.7789 - val_loss: 0.4395 - val_acc: 0.7991\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4621 - acc: 0.7822 - val_loss: 0.4433 - val_acc: 0.7896\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4612 - acc: 0.7836 - val_loss: 0.4465 - val_acc: 0.7933\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4569 - acc: 0.7864 - val_loss: 0.4333 - val_acc: 0.8011\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4548 - acc: 0.7857 - val_loss: 0.4433 - val_acc: 0.7940\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4518 - acc: 0.7881 - val_loss: 0.4275 - val_acc: 0.8019\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4500 - acc: 0.7913 - val_loss: 0.4419 - val_acc: 0.7913\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4482 - acc: 0.7917 - val_loss: 0.4298 - val_acc: 0.8019\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4497 - acc: 0.7900 - val_loss: 0.4310 - val_acc: 0.8022\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4487 - acc: 0.7907 - val_loss: 0.4265 - val_acc: 0.8035\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4469 - acc: 0.7910 - val_loss: 0.4262 - val_acc: 0.8031\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4461 - acc: 0.7924 - val_loss: 0.4293 - val_acc: 0.7976\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.4437 - acc: 0.7936 - val_loss: 0.4264 - val_acc: 0.8033\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.4450 - acc: 0.7922 - val_loss: 0.4247 - val_acc: 0.8072\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4438 - acc: 0.7931 - val_loss: 0.4213 - val_acc: 0.8066\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4430 - acc: 0.7941 - val_loss: 0.4283 - val_acc: 0.7987\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4425 - acc: 0.7943 - val_loss: 0.4215 - val_acc: 0.8019\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4422 - acc: 0.7944 - val_loss: 0.4221 - val_acc: 0.8058\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4408 - acc: 0.7946 - val_loss: 0.4190 - val_acc: 0.8063\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4408 - acc: 0.7953 - val_loss: 0.4196 - val_acc: 0.8091\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4413 - acc: 0.7940 - val_loss: 0.4182 - val_acc: 0.8066\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4393 - acc: 0.7956 - val_loss: 0.4168 - val_acc: 0.8089\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4399 - acc: 0.7949 - val_loss: 0.4155 - val_acc: 0.8069\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4374 - acc: 0.7964 - val_loss: 0.4398 - val_acc: 0.7932\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.4379 - acc: 0.7982 - val_loss: 0.4195 - val_acc: 0.8073\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4386 - acc: 0.7964 - val_loss: 0.4189 - val_acc: 0.8070\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.5734 - acc: 0.7154 - val_loss: 0.4989 - val_acc: 0.7588\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5053 - acc: 0.7524 - val_loss: 0.4805 - val_acc: 0.7693\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4881 - acc: 0.7644 - val_loss: 0.4583 - val_acc: 0.7800\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4750 - acc: 0.7735 - val_loss: 0.4499 - val_acc: 0.7894\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4693 - acc: 0.7777 - val_loss: 0.4397 - val_acc: 0.7951\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4624 - acc: 0.7803 - val_loss: 0.4620 - val_acc: 0.7829\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4582 - acc: 0.7847 - val_loss: 0.4442 - val_acc: 0.7898\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4541 - acc: 0.7877 - val_loss: 0.4338 - val_acc: 0.7987\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4511 - acc: 0.7879 - val_loss: 0.4317 - val_acc: 0.8025\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4498 - acc: 0.7896 - val_loss: 0.4255 - val_acc: 0.8047\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4470 - acc: 0.7913 - val_loss: 0.4332 - val_acc: 0.8023\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4460 - acc: 0.7913 - val_loss: 0.4317 - val_acc: 0.8023\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4447 - acc: 0.7933 - val_loss: 0.4255 - val_acc: 0.8031\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4436 - acc: 0.7930 - val_loss: 0.4204 - val_acc: 0.8069\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4430 - acc: 0.7925 - val_loss: 0.4235 - val_acc: 0.8018\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4427 - acc: 0.7932 - val_loss: 0.4168 - val_acc: 0.8077\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4410 - acc: 0.7946 - val_loss: 0.4207 - val_acc: 0.8073\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4407 - acc: 0.7940 - val_loss: 0.4183 - val_acc: 0.8052\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4386 - acc: 0.7951 - val_loss: 0.4213 - val_acc: 0.8031\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4392 - acc: 0.7950 - val_loss: 0.4169 - val_acc: 0.8104\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4385 - acc: 0.7960 - val_loss: 0.4261 - val_acc: 0.8046\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4383 - acc: 0.7968 - val_loss: 0.4210 - val_acc: 0.8031\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4376 - acc: 0.7959 - val_loss: 0.4175 - val_acc: 0.8068\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4371 - acc: 0.7973 - val_loss: 0.4222 - val_acc: 0.8020\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4386 - acc: 0.7959 - val_loss: 0.4152 - val_acc: 0.8073\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4372 - acc: 0.7965 - val_loss: 0.4239 - val_acc: 0.8023\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4365 - acc: 0.7977 - val_loss: 0.4211 - val_acc: 0.8105\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4359 - acc: 0.7960 - val_loss: 0.4137 - val_acc: 0.8090\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4373 - acc: 0.7965 - val_loss: 0.4176 - val_acc: 0.8109\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4364 - acc: 0.7980 - val_loss: 0.4144 - val_acc: 0.8084\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.5832 - acc: 0.7079 - val_loss: 0.4976 - val_acc: 0.7614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.5109 - acc: 0.7526 - val_loss: 0.4763 - val_acc: 0.7721\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4928 - acc: 0.7634 - val_loss: 0.4664 - val_acc: 0.7786\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4849 - acc: 0.7672 - val_loss: 0.4576 - val_acc: 0.7847\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4775 - acc: 0.7724 - val_loss: 0.4489 - val_acc: 0.7878\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4690 - acc: 0.7779 - val_loss: 0.4440 - val_acc: 0.7925\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4624 - acc: 0.7809 - val_loss: 0.4384 - val_acc: 0.7964\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4614 - acc: 0.7819 - val_loss: 0.4512 - val_acc: 0.7908\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4575 - acc: 0.7851 - val_loss: 0.4330 - val_acc: 0.7991\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4554 - acc: 0.7857 - val_loss: 0.4359 - val_acc: 0.7986\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4542 - acc: 0.7879 - val_loss: 0.4256 - val_acc: 0.8043\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4506 - acc: 0.7894 - val_loss: 0.4331 - val_acc: 0.7976\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4508 - acc: 0.7899 - val_loss: 0.4242 - val_acc: 0.8019\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4497 - acc: 0.7893 - val_loss: 0.4252 - val_acc: 0.8050\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.4474 - acc: 0.7906 - val_loss: 0.4277 - val_acc: 0.8013\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4488 - acc: 0.7902 - val_loss: 0.4285 - val_acc: 0.8026\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4464 - acc: 0.7916 - val_loss: 0.4279 - val_acc: 0.8002\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4461 - acc: 0.7911 - val_loss: 0.4238 - val_acc: 0.8026\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4447 - acc: 0.7916 - val_loss: 0.4205 - val_acc: 0.8076\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4447 - acc: 0.7919 - val_loss: 0.4192 - val_acc: 0.8056\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.4445 - acc: 0.7935 - val_loss: 0.4269 - val_acc: 0.8027\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4436 - acc: 0.7951 - val_loss: 0.4189 - val_acc: 0.8073\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4428 - acc: 0.7945 - val_loss: 0.4187 - val_acc: 0.8073\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4421 - acc: 0.7942 - val_loss: 0.4174 - val_acc: 0.8086\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4416 - acc: 0.7949 - val_loss: 0.4175 - val_acc: 0.8073\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4416 - acc: 0.7934 - val_loss: 0.4572 - val_acc: 0.7799\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4415 - acc: 0.7941 - val_loss: 0.4153 - val_acc: 0.8094\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4406 - acc: 0.7942 - val_loss: 0.4303 - val_acc: 0.7994\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4403 - acc: 0.7951 - val_loss: 0.4288 - val_acc: 0.7999\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4411 - acc: 0.7935 - val_loss: 0.4149 - val_acc: 0.8083\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 183us/sample - loss: 0.5876 - acc: 0.7044 - val_loss: 0.5016 - val_acc: 0.7578\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.5180 - acc: 0.7496 - val_loss: 0.4805 - val_acc: 0.7735\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.5002 - acc: 0.7616 - val_loss: 0.4827 - val_acc: 0.7724\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4886 - acc: 0.7666 - val_loss: 0.4648 - val_acc: 0.7749\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4799 - acc: 0.7719 - val_loss: 0.4496 - val_acc: 0.7928\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4750 - acc: 0.7762 - val_loss: 0.4439 - val_acc: 0.7967\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4682 - acc: 0.7801 - val_loss: 0.4434 - val_acc: 0.7926\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4634 - acc: 0.7817 - val_loss: 0.4293 - val_acc: 0.8011\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4608 - acc: 0.7842 - val_loss: 0.4729 - val_acc: 0.7796\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4574 - acc: 0.7856 - val_loss: 0.4257 - val_acc: 0.8023\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4546 - acc: 0.7863 - val_loss: 0.4249 - val_acc: 0.8026\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4524 - acc: 0.7877 - val_loss: 0.4314 - val_acc: 0.8026\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4519 - acc: 0.7897 - val_loss: 0.4184 - val_acc: 0.8076\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4504 - acc: 0.7911 - val_loss: 0.4305 - val_acc: 0.8007\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4476 - acc: 0.7908 - val_loss: 0.4199 - val_acc: 0.8054\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4466 - acc: 0.7917 - val_loss: 0.4194 - val_acc: 0.8073\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4452 - acc: 0.7927 - val_loss: 0.4227 - val_acc: 0.8052\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4442 - acc: 0.7932 - val_loss: 0.4188 - val_acc: 0.8072\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4448 - acc: 0.7931 - val_loss: 0.4163 - val_acc: 0.8069\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4429 - acc: 0.7943 - val_loss: 0.4227 - val_acc: 0.8073\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4427 - acc: 0.7934 - val_loss: 0.4169 - val_acc: 0.8077\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4406 - acc: 0.7949 - val_loss: 0.4123 - val_acc: 0.8111\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4409 - acc: 0.7961 - val_loss: 0.4263 - val_acc: 0.8054\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4418 - acc: 0.7942 - val_loss: 0.4199 - val_acc: 0.8087\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4423 - acc: 0.7955 - val_loss: 0.4192 - val_acc: 0.8061\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4400 - acc: 0.7966 - val_loss: 0.4140 - val_acc: 0.8096\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4408 - acc: 0.7968 - val_loss: 0.4125 - val_acc: 0.8084\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4399 - acc: 0.7966 - val_loss: 0.4162 - val_acc: 0.8104\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4386 - acc: 0.7968 - val_loss: 0.4124 - val_acc: 0.8086\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.4380 - acc: 0.7972 - val_loss: 0.4132 - val_acc: 0.8100\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.5376 - acc: 0.7373 - val_loss: 0.4714 - val_acc: 0.7751\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4752 - acc: 0.7736 - val_loss: 0.4623 - val_acc: 0.7797\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4581 - acc: 0.7840 - val_loss: 0.4353 - val_acc: 0.7970\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4451 - acc: 0.7921 - val_loss: 0.4355 - val_acc: 0.7972\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4365 - acc: 0.7971 - val_loss: 0.4286 - val_acc: 0.8012\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4316 - acc: 0.8013 - val_loss: 0.4157 - val_acc: 0.8078\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4257 - acc: 0.8037 - val_loss: 0.4140 - val_acc: 0.8105\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4233 - acc: 0.8044 - val_loss: 0.4033 - val_acc: 0.8145\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4197 - acc: 0.8063 - val_loss: 0.4042 - val_acc: 0.8183\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4156 - acc: 0.8095 - val_loss: 0.4055 - val_acc: 0.8126\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4150 - acc: 0.8094 - val_loss: 0.4007 - val_acc: 0.8181\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4102 - acc: 0.8122 - val_loss: 0.4217 - val_acc: 0.8098\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4099 - acc: 0.8113 - val_loss: 0.4134 - val_acc: 0.8080\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4091 - acc: 0.8128 - val_loss: 0.4041 - val_acc: 0.8148\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4080 - acc: 0.8144 - val_loss: 0.3929 - val_acc: 0.8240\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4051 - acc: 0.8143 - val_loss: 0.3970 - val_acc: 0.8199\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4040 - acc: 0.8157 - val_loss: 0.3964 - val_acc: 0.8228\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4024 - acc: 0.8160 - val_loss: 0.3897 - val_acc: 0.8245\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4005 - acc: 0.8175 - val_loss: 0.3905 - val_acc: 0.8243\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4022 - acc: 0.8178 - val_loss: 0.3914 - val_acc: 0.8220\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4000 - acc: 0.8182 - val_loss: 0.3965 - val_acc: 0.8219\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4005 - acc: 0.8176 - val_loss: 0.3919 - val_acc: 0.8255\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3973 - acc: 0.8204 - val_loss: 0.3926 - val_acc: 0.8213\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3964 - acc: 0.8198 - val_loss: 0.3902 - val_acc: 0.8233\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3958 - acc: 0.8202 - val_loss: 0.3860 - val_acc: 0.8265\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3967 - acc: 0.8198 - val_loss: 0.3872 - val_acc: 0.8245\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3952 - acc: 0.8212 - val_loss: 0.3891 - val_acc: 0.8242\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3949 - acc: 0.8207 - val_loss: 0.3932 - val_acc: 0.8217\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3944 - acc: 0.8211 - val_loss: 0.4115 - val_acc: 0.8108\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.3935 - acc: 0.8207 - val_loss: 0.3993 - val_acc: 0.8177\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.5520 - acc: 0.7315 - val_loss: 0.4787 - val_acc: 0.7704\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4803 - acc: 0.7701 - val_loss: 0.4625 - val_acc: 0.7810\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4603 - acc: 0.7823 - val_loss: 0.4628 - val_acc: 0.7800\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4485 - acc: 0.7906 - val_loss: 0.4449 - val_acc: 0.7897\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4405 - acc: 0.7952 - val_loss: 0.4245 - val_acc: 0.8030\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4323 - acc: 0.7987 - val_loss: 0.4243 - val_acc: 0.8045\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4305 - acc: 0.8010 - val_loss: 0.4170 - val_acc: 0.8086\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4252 - acc: 0.8036 - val_loss: 0.4181 - val_acc: 0.8085\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4205 - acc: 0.8074 - val_loss: 0.4023 - val_acc: 0.8158\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4174 - acc: 0.8082 - val_loss: 0.4146 - val_acc: 0.8104\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4170 - acc: 0.8085 - val_loss: 0.4032 - val_acc: 0.8146\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4124 - acc: 0.8117 - val_loss: 0.3947 - val_acc: 0.8190\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4104 - acc: 0.8121 - val_loss: 0.3988 - val_acc: 0.8164\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4097 - acc: 0.8134 - val_loss: 0.4087 - val_acc: 0.8131\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4082 - acc: 0.8133 - val_loss: 0.4050 - val_acc: 0.8126\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4065 - acc: 0.8147 - val_loss: 0.4031 - val_acc: 0.8128\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4059 - acc: 0.8158 - val_loss: 0.4201 - val_acc: 0.8094\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4050 - acc: 0.8159 - val_loss: 0.3927 - val_acc: 0.8189\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4021 - acc: 0.8163 - val_loss: 0.3912 - val_acc: 0.8206\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4024 - acc: 0.8161 - val_loss: 0.3901 - val_acc: 0.8230\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4016 - acc: 0.8175 - val_loss: 0.3975 - val_acc: 0.8178\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4005 - acc: 0.8181 - val_loss: 0.3920 - val_acc: 0.8237\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4007 - acc: 0.8178 - val_loss: 0.4048 - val_acc: 0.8157\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3988 - acc: 0.8182 - val_loss: 0.4008 - val_acc: 0.8180\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.3978 - acc: 0.8184 - val_loss: 0.3914 - val_acc: 0.8214\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3974 - acc: 0.8194 - val_loss: 0.3980 - val_acc: 0.8148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3983 - acc: 0.8194 - val_loss: 0.3855 - val_acc: 0.8232\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.3970 - acc: 0.8190 - val_loss: 0.3868 - val_acc: 0.8251\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3964 - acc: 0.8204 - val_loss: 0.3889 - val_acc: 0.8243\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3964 - acc: 0.8214 - val_loss: 0.3867 - val_acc: 0.8236\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.5539 - acc: 0.7324 - val_loss: 0.4758 - val_acc: 0.7753\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4762 - acc: 0.7731 - val_loss: 0.5029 - val_acc: 0.7511\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4552 - acc: 0.7864 - val_loss: 0.4443 - val_acc: 0.7919\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.4403 - acc: 0.7955 - val_loss: 0.4170 - val_acc: 0.8075\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4294 - acc: 0.8016 - val_loss: 0.4100 - val_acc: 0.8120\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4234 - acc: 0.8045 - val_loss: 0.4121 - val_acc: 0.8127\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4168 - acc: 0.8091 - val_loss: 0.3991 - val_acc: 0.8177\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4125 - acc: 0.8119 - val_loss: 0.4062 - val_acc: 0.8124\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4082 - acc: 0.8131 - val_loss: 0.4411 - val_acc: 0.7954\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4063 - acc: 0.8138 - val_loss: 0.3907 - val_acc: 0.8230\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4044 - acc: 0.8160 - val_loss: 0.4024 - val_acc: 0.8162\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4009 - acc: 0.8171 - val_loss: 0.4022 - val_acc: 0.8165\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3992 - acc: 0.8183 - val_loss: 0.3908 - val_acc: 0.8230\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3963 - acc: 0.8197 - val_loss: 0.3859 - val_acc: 0.8228\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3935 - acc: 0.8222 - val_loss: 0.3949 - val_acc: 0.8206\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3927 - acc: 0.8233 - val_loss: 0.3832 - val_acc: 0.8281\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3914 - acc: 0.8236 - val_loss: 0.3871 - val_acc: 0.8247\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3899 - acc: 0.8243 - val_loss: 0.3881 - val_acc: 0.8234\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3882 - acc: 0.8246 - val_loss: 0.3956 - val_acc: 0.8191\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3885 - acc: 0.8257 - val_loss: 0.4166 - val_acc: 0.8069\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3851 - acc: 0.8287 - val_loss: 0.3789 - val_acc: 0.8285\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3833 - acc: 0.8274 - val_loss: 0.3839 - val_acc: 0.8268\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3835 - acc: 0.8281 - val_loss: 0.3786 - val_acc: 0.8285\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3828 - acc: 0.8276 - val_loss: 0.3899 - val_acc: 0.8223\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.3825 - acc: 0.8275 - val_loss: 0.3795 - val_acc: 0.8274\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3797 - acc: 0.8289 - val_loss: 0.3996 - val_acc: 0.8163\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3794 - acc: 0.8288 - val_loss: 0.3934 - val_acc: 0.8195\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3784 - acc: 0.8307 - val_loss: 0.4018 - val_acc: 0.8146\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3780 - acc: 0.8296 - val_loss: 0.3772 - val_acc: 0.8285\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3779 - acc: 0.8293 - val_loss: 0.3784 - val_acc: 0.8276\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 192us/sample - loss: 0.5564 - acc: 0.7296 - val_loss: 0.4862 - val_acc: 0.7642\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4864 - acc: 0.7673 - val_loss: 0.4516 - val_acc: 0.7888\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4614 - acc: 0.7815 - val_loss: 0.4314 - val_acc: 0.8007\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4461 - acc: 0.7926 - val_loss: 0.4249 - val_acc: 0.8011\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4344 - acc: 0.7993 - val_loss: 0.4113 - val_acc: 0.8099\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4244 - acc: 0.8047 - val_loss: 0.4041 - val_acc: 0.8148\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4192 - acc: 0.8079 - val_loss: 0.4039 - val_acc: 0.8128\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4142 - acc: 0.8117 - val_loss: 0.4057 - val_acc: 0.8148\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4092 - acc: 0.8120 - val_loss: 0.4045 - val_acc: 0.8123\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4055 - acc: 0.8154 - val_loss: 0.3917 - val_acc: 0.8195\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4030 - acc: 0.8154 - val_loss: 0.3983 - val_acc: 0.8202\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4012 - acc: 0.8180 - val_loss: 0.3880 - val_acc: 0.8220\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3972 - acc: 0.8204 - val_loss: 0.3913 - val_acc: 0.8223\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3969 - acc: 0.8202 - val_loss: 0.3864 - val_acc: 0.8227\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3939 - acc: 0.8213 - val_loss: 0.4220 - val_acc: 0.8048\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3909 - acc: 0.8239 - val_loss: 0.3848 - val_acc: 0.8235\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3879 - acc: 0.8254 - val_loss: 0.3841 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3890 - acc: 0.8246 - val_loss: 0.3829 - val_acc: 0.8258\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3862 - acc: 0.8256 - val_loss: 0.3805 - val_acc: 0.8263\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3860 - acc: 0.8269 - val_loss: 0.3918 - val_acc: 0.8209\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3822 - acc: 0.8285 - val_loss: 0.3920 - val_acc: 0.8225\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3817 - acc: 0.8290 - val_loss: 0.3881 - val_acc: 0.8241\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3812 - acc: 0.8291 - val_loss: 0.3807 - val_acc: 0.8271\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3799 - acc: 0.8290 - val_loss: 0.3924 - val_acc: 0.8222\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3779 - acc: 0.8309 - val_loss: 0.3798 - val_acc: 0.8281\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3771 - acc: 0.8309 - val_loss: 0.3786 - val_acc: 0.8274\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3779 - acc: 0.8304 - val_loss: 0.3762 - val_acc: 0.8289\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3770 - acc: 0.8314 - val_loss: 0.3883 - val_acc: 0.8217\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3759 - acc: 0.8322 - val_loss: 0.3849 - val_acc: 0.8241\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3758 - acc: 0.8310 - val_loss: 0.3872 - val_acc: 0.8244\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.5213 - acc: 0.7444 - val_loss: 0.4714 - val_acc: 0.7751\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4692 - acc: 0.7768 - val_loss: 0.4536 - val_acc: 0.7883\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4521 - acc: 0.7883 - val_loss: 0.4445 - val_acc: 0.7926\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4386 - acc: 0.7967 - val_loss: 0.4175 - val_acc: 0.8080\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4316 - acc: 0.7999 - val_loss: 0.4151 - val_acc: 0.8074\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4248 - acc: 0.8035 - val_loss: 0.4278 - val_acc: 0.8026\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4182 - acc: 0.8076 - val_loss: 0.4134 - val_acc: 0.8106\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4121 - acc: 0.8115 - val_loss: 0.4367 - val_acc: 0.8024\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4083 - acc: 0.8123 - val_loss: 0.3981 - val_acc: 0.8171\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4055 - acc: 0.8156 - val_loss: 0.3998 - val_acc: 0.8162\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4017 - acc: 0.8161 - val_loss: 0.4113 - val_acc: 0.8099\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4011 - acc: 0.8187 - val_loss: 0.4148 - val_acc: 0.8081\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3973 - acc: 0.8186 - val_loss: 0.4119 - val_acc: 0.8108\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3960 - acc: 0.8203 - val_loss: 0.3898 - val_acc: 0.8212\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3935 - acc: 0.8212 - val_loss: 0.3897 - val_acc: 0.8241\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3919 - acc: 0.8215 - val_loss: 0.3879 - val_acc: 0.8233\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.3907 - acc: 0.8231 - val_loss: 0.3837 - val_acc: 0.8241\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3893 - acc: 0.8243 - val_loss: 0.3814 - val_acc: 0.8254\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3884 - acc: 0.8255 - val_loss: 0.3988 - val_acc: 0.8156\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3871 - acc: 0.8243 - val_loss: 0.3811 - val_acc: 0.8249\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3855 - acc: 0.8247 - val_loss: 0.3789 - val_acc: 0.8285\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3858 - acc: 0.8254 - val_loss: 0.4062 - val_acc: 0.8149\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3814 - acc: 0.8274 - val_loss: 0.3775 - val_acc: 0.8285\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3824 - acc: 0.8271 - val_loss: 0.3938 - val_acc: 0.8198\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3806 - acc: 0.8270 - val_loss: 0.3796 - val_acc: 0.8283\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3808 - acc: 0.8277 - val_loss: 0.4272 - val_acc: 0.8025\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.3797 - acc: 0.8286 - val_loss: 0.3753 - val_acc: 0.8304\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3786 - acc: 0.8298 - val_loss: 0.3838 - val_acc: 0.8258\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3782 - acc: 0.8297 - val_loss: 0.3782 - val_acc: 0.8272\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3770 - acc: 0.8295 - val_loss: 0.3780 - val_acc: 0.8288\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.5196 - acc: 0.7484 - val_loss: 0.4742 - val_acc: 0.7717\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4645 - acc: 0.7789 - val_loss: 0.4418 - val_acc: 0.7921\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4467 - acc: 0.7915 - val_loss: 0.4246 - val_acc: 0.8036\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4350 - acc: 0.7976 - val_loss: 0.4281 - val_acc: 0.8010\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4258 - acc: 0.8034 - val_loss: 0.4387 - val_acc: 0.7962\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4214 - acc: 0.8062 - val_loss: 0.4084 - val_acc: 0.8136\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4158 - acc: 0.8098 - val_loss: 0.4293 - val_acc: 0.7995\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4105 - acc: 0.8110 - val_loss: 0.4058 - val_acc: 0.8154\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4072 - acc: 0.8146 - val_loss: 0.3963 - val_acc: 0.8190\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4033 - acc: 0.8162 - val_loss: 0.3955 - val_acc: 0.8203\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4026 - acc: 0.8155 - val_loss: 0.3915 - val_acc: 0.8220\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3998 - acc: 0.8177 - val_loss: 0.3973 - val_acc: 0.8209\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3962 - acc: 0.8208 - val_loss: 0.3957 - val_acc: 0.8196\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3955 - acc: 0.8193 - val_loss: 0.3935 - val_acc: 0.8184\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3928 - acc: 0.8212 - val_loss: 0.3924 - val_acc: 0.8202\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3920 - acc: 0.8218 - val_loss: 0.4120 - val_acc: 0.8097\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3903 - acc: 0.8230 - val_loss: 0.3894 - val_acc: 0.8231\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3888 - acc: 0.8243 - val_loss: 0.3840 - val_acc: 0.8256\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3861 - acc: 0.8266 - val_loss: 0.4015 - val_acc: 0.8166\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3865 - acc: 0.8255 - val_loss: 0.4086 - val_acc: 0.8104\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3852 - acc: 0.8263 - val_loss: 0.3937 - val_acc: 0.8210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3825 - acc: 0.8284 - val_loss: 0.3830 - val_acc: 0.8261\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3838 - acc: 0.8270 - val_loss: 0.3974 - val_acc: 0.8177\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3826 - acc: 0.8262 - val_loss: 0.3858 - val_acc: 0.8242\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3800 - acc: 0.8299 - val_loss: 0.3818 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3792 - acc: 0.8286 - val_loss: 0.3851 - val_acc: 0.8254\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3806 - acc: 0.8293 - val_loss: 0.3813 - val_acc: 0.8273\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3792 - acc: 0.8304 - val_loss: 0.3794 - val_acc: 0.8280\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3770 - acc: 0.8302 - val_loss: 0.3807 - val_acc: 0.8278\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3765 - acc: 0.8300 - val_loss: 0.3801 - val_acc: 0.8282\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.5170 - acc: 0.7500 - val_loss: 0.4626 - val_acc: 0.7837\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4552 - acc: 0.7864 - val_loss: 0.4349 - val_acc: 0.7991\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4354 - acc: 0.7979 - val_loss: 0.4431 - val_acc: 0.7906\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4228 - acc: 0.8063 - val_loss: 0.4236 - val_acc: 0.8045\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4159 - acc: 0.8088 - val_loss: 0.4098 - val_acc: 0.8109\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4084 - acc: 0.8136 - val_loss: 0.3964 - val_acc: 0.8185\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4014 - acc: 0.8176 - val_loss: 0.4023 - val_acc: 0.8155\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3964 - acc: 0.8211 - val_loss: 0.3913 - val_acc: 0.8212\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.3911 - acc: 0.8232 - val_loss: 0.3954 - val_acc: 0.8211\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3884 - acc: 0.8250 - val_loss: 0.4179 - val_acc: 0.8054\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3829 - acc: 0.8276 - val_loss: 0.3877 - val_acc: 0.8249\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3800 - acc: 0.8289 - val_loss: 0.3941 - val_acc: 0.8207\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3762 - acc: 0.8324 - val_loss: 0.3809 - val_acc: 0.8270\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.3732 - acc: 0.8323 - val_loss: 0.3794 - val_acc: 0.8302\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3695 - acc: 0.8356 - val_loss: 0.3940 - val_acc: 0.8230\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3673 - acc: 0.8362 - val_loss: 0.4037 - val_acc: 0.8198\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3648 - acc: 0.8373 - val_loss: 0.3822 - val_acc: 0.8286\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3624 - acc: 0.8373 - val_loss: 0.3962 - val_acc: 0.8217\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3599 - acc: 0.8396 - val_loss: 0.3838 - val_acc: 0.8265\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3579 - acc: 0.8411 - val_loss: 0.3787 - val_acc: 0.8309\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3541 - acc: 0.8427 - val_loss: 0.3794 - val_acc: 0.8295\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3529 - acc: 0.8436 - val_loss: 0.3818 - val_acc: 0.8282\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3502 - acc: 0.8449 - val_loss: 0.3871 - val_acc: 0.8262\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3482 - acc: 0.8457 - val_loss: 0.3801 - val_acc: 0.8274\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3460 - acc: 0.8472 - val_loss: 0.3827 - val_acc: 0.8278\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.3459 - acc: 0.8469 - val_loss: 0.3789 - val_acc: 0.8292\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3418 - acc: 0.8486 - val_loss: 0.3845 - val_acc: 0.8274\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3408 - acc: 0.8496 - val_loss: 0.3771 - val_acc: 0.8298\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3395 - acc: 0.8500 - val_loss: 0.3784 - val_acc: 0.8317\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3396 - acc: 0.8483 - val_loss: 0.3799 - val_acc: 0.8292\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.5207 - acc: 0.7503 - val_loss: 0.4847 - val_acc: 0.7734\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4534 - acc: 0.7883 - val_loss: 0.4439 - val_acc: 0.7912\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4346 - acc: 0.7995 - val_loss: 0.4135 - val_acc: 0.8090\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4201 - acc: 0.8069 - val_loss: 0.4224 - val_acc: 0.8021\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4133 - acc: 0.8120 - val_loss: 0.3948 - val_acc: 0.8202\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4058 - acc: 0.8158 - val_loss: 0.4127 - val_acc: 0.8101\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3982 - acc: 0.8188 - val_loss: 0.3923 - val_acc: 0.8219\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3932 - acc: 0.8219 - val_loss: 0.3924 - val_acc: 0.8197\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3884 - acc: 0.8249 - val_loss: 0.3909 - val_acc: 0.8249\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3827 - acc: 0.8284 - val_loss: 0.3775 - val_acc: 0.8280\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3788 - acc: 0.8298 - val_loss: 0.4392 - val_acc: 0.7951\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3732 - acc: 0.8327 - val_loss: 0.3792 - val_acc: 0.8299\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3707 - acc: 0.8328 - val_loss: 0.3811 - val_acc: 0.8264\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3668 - acc: 0.8358 - val_loss: 0.3805 - val_acc: 0.8249\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3633 - acc: 0.8377 - val_loss: 0.3914 - val_acc: 0.8182\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3609 - acc: 0.8389 - val_loss: 0.3787 - val_acc: 0.8284\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3583 - acc: 0.8407 - val_loss: 0.3758 - val_acc: 0.8288\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3546 - acc: 0.8421 - val_loss: 0.3828 - val_acc: 0.8246\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3527 - acc: 0.8430 - val_loss: 0.3825 - val_acc: 0.8268\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3487 - acc: 0.8456 - val_loss: 0.3832 - val_acc: 0.8285\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3468 - acc: 0.8466 - val_loss: 0.3840 - val_acc: 0.8270\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3464 - acc: 0.8457 - val_loss: 0.3787 - val_acc: 0.8284\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3430 - acc: 0.8494 - val_loss: 0.3855 - val_acc: 0.8237\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3396 - acc: 0.8507 - val_loss: 0.3887 - val_acc: 0.8264\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3375 - acc: 0.8515 - val_loss: 0.3779 - val_acc: 0.8293\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3380 - acc: 0.8510 - val_loss: 0.3876 - val_acc: 0.8273\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3363 - acc: 0.8526 - val_loss: 0.3787 - val_acc: 0.8262\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3337 - acc: 0.8540 - val_loss: 0.3865 - val_acc: 0.8246\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3306 - acc: 0.8550 - val_loss: 0.3732 - val_acc: 0.8347\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3296 - acc: 0.8547 - val_loss: 0.3757 - val_acc: 0.8299\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.5224 - acc: 0.7487 - val_loss: 0.4610 - val_acc: 0.7813\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4595 - acc: 0.7839 - val_loss: 0.4455 - val_acc: 0.7922\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4416 - acc: 0.7949 - val_loss: 0.4464 - val_acc: 0.7925\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4304 - acc: 0.8001 - val_loss: 0.4124 - val_acc: 0.8119\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4223 - acc: 0.8051 - val_loss: 0.4127 - val_acc: 0.8091\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4154 - acc: 0.8081 - val_loss: 0.4487 - val_acc: 0.7875\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4092 - acc: 0.8144 - val_loss: 0.4331 - val_acc: 0.7994\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4035 - acc: 0.8157 - val_loss: 0.4041 - val_acc: 0.8144\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3999 - acc: 0.8180 - val_loss: 0.4016 - val_acc: 0.8151\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3965 - acc: 0.8204 - val_loss: 0.3864 - val_acc: 0.8253\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3927 - acc: 0.8215 - val_loss: 0.4072 - val_acc: 0.8137\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3906 - acc: 0.8242 - val_loss: 0.3955 - val_acc: 0.8213\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3894 - acc: 0.8232 - val_loss: 0.3839 - val_acc: 0.8280\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 100us/sample - loss: 0.3858 - acc: 0.8248 - val_loss: 0.3861 - val_acc: 0.8240\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3857 - acc: 0.8262 - val_loss: 0.3853 - val_acc: 0.8259\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3840 - acc: 0.8268 - val_loss: 0.3810 - val_acc: 0.8303\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3816 - acc: 0.8278 - val_loss: 0.3868 - val_acc: 0.8245\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3791 - acc: 0.8296 - val_loss: 0.3907 - val_acc: 0.8214\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3774 - acc: 0.8307 - val_loss: 0.3850 - val_acc: 0.8258\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3770 - acc: 0.8301 - val_loss: 0.3876 - val_acc: 0.8237\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3742 - acc: 0.8315 - val_loss: 0.3858 - val_acc: 0.8267\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3736 - acc: 0.8314 - val_loss: 0.3769 - val_acc: 0.8335\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3720 - acc: 0.8326 - val_loss: 0.3840 - val_acc: 0.8271\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 100us/sample - loss: 0.3703 - acc: 0.8324 - val_loss: 0.3786 - val_acc: 0.8310\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 100us/sample - loss: 0.3686 - acc: 0.8336 - val_loss: 0.3872 - val_acc: 0.8260\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3687 - acc: 0.8343 - val_loss: 0.3830 - val_acc: 0.8260\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3670 - acc: 0.8353 - val_loss: 0.3844 - val_acc: 0.8258\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3671 - acc: 0.8357 - val_loss: 0.3775 - val_acc: 0.8282\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.3641 - acc: 0.8364 - val_loss: 0.3795 - val_acc: 0.8274\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3643 - acc: 0.8363 - val_loss: 0.3805 - val_acc: 0.8254\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 183us/sample - loss: 0.5282 - acc: 0.7465 - val_loss: 0.4570 - val_acc: 0.7833\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4606 - acc: 0.7826 - val_loss: 0.4392 - val_acc: 0.7955\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4428 - acc: 0.7947 - val_loss: 0.4216 - val_acc: 0.8069\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4291 - acc: 0.8018 - val_loss: 0.4131 - val_acc: 0.8080\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4203 - acc: 0.8057 - val_loss: 0.4094 - val_acc: 0.8115\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4121 - acc: 0.8107 - val_loss: 0.4110 - val_acc: 0.8135\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4091 - acc: 0.8124 - val_loss: 0.4035 - val_acc: 0.8167\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4039 - acc: 0.8159 - val_loss: 0.3969 - val_acc: 0.8201\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3992 - acc: 0.8193 - val_loss: 0.3956 - val_acc: 0.8153\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3972 - acc: 0.8187 - val_loss: 0.4213 - val_acc: 0.8088\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3938 - acc: 0.8216 - val_loss: 0.3959 - val_acc: 0.8192\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.3922 - acc: 0.8218 - val_loss: 0.3899 - val_acc: 0.8181\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3881 - acc: 0.8245 - val_loss: 0.3879 - val_acc: 0.8207\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3864 - acc: 0.8240 - val_loss: 0.3895 - val_acc: 0.8245\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3845 - acc: 0.8268 - val_loss: 0.3848 - val_acc: 0.8281\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3840 - acc: 0.8271 - val_loss: 0.3892 - val_acc: 0.8228\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3805 - acc: 0.8295 - val_loss: 0.4015 - val_acc: 0.8163\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3797 - acc: 0.8297 - val_loss: 0.3901 - val_acc: 0.8231\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3795 - acc: 0.8289 - val_loss: 0.3799 - val_acc: 0.8291\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3753 - acc: 0.8324 - val_loss: 0.3797 - val_acc: 0.8278\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3742 - acc: 0.8303 - val_loss: 0.3785 - val_acc: 0.8314\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3736 - acc: 0.8326 - val_loss: 0.3856 - val_acc: 0.8266\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3714 - acc: 0.8332 - val_loss: 0.3773 - val_acc: 0.8318\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3690 - acc: 0.8347 - val_loss: 0.3805 - val_acc: 0.8306\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3681 - acc: 0.8360 - val_loss: 0.3748 - val_acc: 0.8322\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3685 - acc: 0.8342 - val_loss: 0.3887 - val_acc: 0.8254\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3673 - acc: 0.8344 - val_loss: 0.3764 - val_acc: 0.8281\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3675 - acc: 0.8354 - val_loss: 0.3767 - val_acc: 0.8306\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3640 - acc: 0.8371 - val_loss: 0.3750 - val_acc: 0.8313\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3653 - acc: 0.8366 - val_loss: 0.3756 - val_acc: 0.8322\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.5215 - acc: 0.7498 - val_loss: 0.4638 - val_acc: 0.7824\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4474 - acc: 0.7886 - val_loss: 0.4291 - val_acc: 0.8022\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4266 - acc: 0.8038 - val_loss: 0.4306 - val_acc: 0.7984\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4145 - acc: 0.8090 - val_loss: 0.4018 - val_acc: 0.8172\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4055 - acc: 0.8160 - val_loss: 0.4045 - val_acc: 0.8150\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3973 - acc: 0.8196 - val_loss: 0.3996 - val_acc: 0.8164\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3918 - acc: 0.8226 - val_loss: 0.3926 - val_acc: 0.8209\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3832 - acc: 0.8280 - val_loss: 0.4196 - val_acc: 0.8076\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3799 - acc: 0.8294 - val_loss: 0.3842 - val_acc: 0.8249\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3752 - acc: 0.8321 - val_loss: 0.3921 - val_acc: 0.8209\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3684 - acc: 0.8338 - val_loss: 0.3897 - val_acc: 0.8233\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3648 - acc: 0.8373 - val_loss: 0.3927 - val_acc: 0.8215\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3604 - acc: 0.8386 - val_loss: 0.3875 - val_acc: 0.8236\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3559 - acc: 0.8419 - val_loss: 0.3997 - val_acc: 0.8187\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3505 - acc: 0.8432 - val_loss: 0.3839 - val_acc: 0.8280\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3470 - acc: 0.8468 - val_loss: 0.3872 - val_acc: 0.8242\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3434 - acc: 0.8482 - val_loss: 0.4048 - val_acc: 0.8210\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3385 - acc: 0.8498 - val_loss: 0.3835 - val_acc: 0.8278\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3357 - acc: 0.8504 - val_loss: 0.3854 - val_acc: 0.8299\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3327 - acc: 0.8522 - val_loss: 0.3853 - val_acc: 0.8295\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3295 - acc: 0.8541 - val_loss: 0.3849 - val_acc: 0.8296\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3251 - acc: 0.8553 - val_loss: 0.3877 - val_acc: 0.8288\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3224 - acc: 0.8573 - val_loss: 0.3916 - val_acc: 0.8266\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3206 - acc: 0.8581 - val_loss: 0.3995 - val_acc: 0.8267\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3182 - acc: 0.8600 - val_loss: 0.3928 - val_acc: 0.8299\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3151 - acc: 0.8611 - val_loss: 0.3854 - val_acc: 0.8288\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3108 - acc: 0.8638 - val_loss: 0.3890 - val_acc: 0.8288\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3068 - acc: 0.8647 - val_loss: 0.3960 - val_acc: 0.8301\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3062 - acc: 0.8652 - val_loss: 0.3958 - val_acc: 0.8249\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3036 - acc: 0.8666 - val_loss: 0.4111 - val_acc: 0.8249\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 213us/sample - loss: 0.5168 - acc: 0.7543 - val_loss: 0.4842 - val_acc: 0.7649\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4473 - acc: 0.7922 - val_loss: 0.4659 - val_acc: 0.7839\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4252 - acc: 0.8046 - val_loss: 0.4535 - val_acc: 0.7850\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4130 - acc: 0.8107 - val_loss: 0.4010 - val_acc: 0.8162\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4032 - acc: 0.8174 - val_loss: 0.4363 - val_acc: 0.7990\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3944 - acc: 0.8204 - val_loss: 0.3889 - val_acc: 0.8262\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3882 - acc: 0.8244 - val_loss: 0.4003 - val_acc: 0.8174\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3811 - acc: 0.8282 - val_loss: 0.3853 - val_acc: 0.8253\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3745 - acc: 0.8318 - val_loss: 0.3892 - val_acc: 0.8226\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3700 - acc: 0.8341 - val_loss: 0.3917 - val_acc: 0.8224\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3642 - acc: 0.8380 - val_loss: 0.3837 - val_acc: 0.8246\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3587 - acc: 0.8404 - val_loss: 0.3795 - val_acc: 0.8282\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3539 - acc: 0.8431 - val_loss: 0.3767 - val_acc: 0.8302\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3499 - acc: 0.8456 - val_loss: 0.3755 - val_acc: 0.8289\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3444 - acc: 0.8472 - val_loss: 0.3934 - val_acc: 0.8191\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3379 - acc: 0.8517 - val_loss: 0.3883 - val_acc: 0.8247\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3338 - acc: 0.8532 - val_loss: 0.4209 - val_acc: 0.8145\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3307 - acc: 0.8554 - val_loss: 0.3829 - val_acc: 0.8253\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3252 - acc: 0.8566 - val_loss: 0.3843 - val_acc: 0.8303\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3222 - acc: 0.8598 - val_loss: 0.3834 - val_acc: 0.8288\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3194 - acc: 0.8611 - val_loss: 0.4115 - val_acc: 0.8145\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3155 - acc: 0.8627 - val_loss: 0.4007 - val_acc: 0.8245\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3109 - acc: 0.8656 - val_loss: 0.3959 - val_acc: 0.8291\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3075 - acc: 0.8658 - val_loss: 0.3915 - val_acc: 0.8236\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3028 - acc: 0.8684 - val_loss: 0.4021 - val_acc: 0.8214\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.2990 - acc: 0.8705 - val_loss: 0.3981 - val_acc: 0.8250\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.2965 - acc: 0.8704 - val_loss: 0.3966 - val_acc: 0.8236\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.2934 - acc: 0.8730 - val_loss: 0.3925 - val_acc: 0.8264\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.2927 - acc: 0.8732 - val_loss: 0.3982 - val_acc: 0.8234\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.2881 - acc: 0.8759 - val_loss: 0.4052 - val_acc: 0.8287\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 181us/sample - loss: 0.5696 - acc: 0.7145 - val_loss: 0.5021 - val_acc: 0.7570\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.5093 - acc: 0.7506 - val_loss: 0.4777 - val_acc: 0.7709\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4936 - acc: 0.7602 - val_loss: 0.4664 - val_acc: 0.7792\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4829 - acc: 0.7678 - val_loss: 0.4699 - val_acc: 0.7738\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4737 - acc: 0.7738 - val_loss: 0.4490 - val_acc: 0.7893\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4640 - acc: 0.7805 - val_loss: 0.4342 - val_acc: 0.7961\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4572 - acc: 0.7848 - val_loss: 0.4393 - val_acc: 0.7959\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4534 - acc: 0.7867 - val_loss: 0.4390 - val_acc: 0.7934\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4493 - acc: 0.7901 - val_loss: 0.4278 - val_acc: 0.8048\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4480 - acc: 0.7908 - val_loss: 0.4508 - val_acc: 0.7880\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4448 - acc: 0.7921 - val_loss: 0.4219 - val_acc: 0.8059\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4441 - acc: 0.7920 - val_loss: 0.4258 - val_acc: 0.8015\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4415 - acc: 0.7934 - val_loss: 0.4201 - val_acc: 0.8074\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4404 - acc: 0.7946 - val_loss: 0.4323 - val_acc: 0.7965\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4393 - acc: 0.7941 - val_loss: 0.4218 - val_acc: 0.8065\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4382 - acc: 0.7963 - val_loss: 0.4173 - val_acc: 0.8116\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4390 - acc: 0.7956 - val_loss: 0.4156 - val_acc: 0.8112\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4354 - acc: 0.7970 - val_loss: 0.4321 - val_acc: 0.7950\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4364 - acc: 0.7969 - val_loss: 0.4135 - val_acc: 0.8105\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4356 - acc: 0.7963 - val_loss: 0.4143 - val_acc: 0.8133\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4365 - acc: 0.7975 - val_loss: 0.4129 - val_acc: 0.8112\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4325 - acc: 0.7985 - val_loss: 0.4116 - val_acc: 0.8113\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4328 - acc: 0.7988 - val_loss: 0.4120 - val_acc: 0.8091\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4340 - acc: 0.7978 - val_loss: 0.4113 - val_acc: 0.8112\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4329 - acc: 0.7994 - val_loss: 0.4280 - val_acc: 0.7992\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4315 - acc: 0.7990 - val_loss: 0.4192 - val_acc: 0.8049\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4321 - acc: 0.8004 - val_loss: 0.4108 - val_acc: 0.8114\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4327 - acc: 0.7992 - val_loss: 0.4097 - val_acc: 0.8110\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4311 - acc: 0.8007 - val_loss: 0.4092 - val_acc: 0.8117\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4301 - acc: 0.7997 - val_loss: 0.4102 - val_acc: 0.8115\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.5868 - acc: 0.7100 - val_loss: 0.4966 - val_acc: 0.7624\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.5070 - acc: 0.7532 - val_loss: 0.4756 - val_acc: 0.7727\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4861 - acc: 0.7659 - val_loss: 0.4634 - val_acc: 0.7787\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4758 - acc: 0.7727 - val_loss: 0.4492 - val_acc: 0.7898\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4674 - acc: 0.7791 - val_loss: 0.4491 - val_acc: 0.7869\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4612 - acc: 0.7821 - val_loss: 0.4539 - val_acc: 0.7839\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4573 - acc: 0.7851 - val_loss: 0.4346 - val_acc: 0.7968\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4529 - acc: 0.7875 - val_loss: 0.4359 - val_acc: 0.7982\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4486 - acc: 0.7895 - val_loss: 0.4599 - val_acc: 0.7756\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4459 - acc: 0.7912 - val_loss: 0.4364 - val_acc: 0.7940\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4453 - acc: 0.7918 - val_loss: 0.4414 - val_acc: 0.7910\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4446 - acc: 0.7919 - val_loss: 0.4236 - val_acc: 0.8037\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4438 - acc: 0.7935 - val_loss: 0.4192 - val_acc: 0.8032\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4418 - acc: 0.7947 - val_loss: 0.4216 - val_acc: 0.8037\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4397 - acc: 0.7962 - val_loss: 0.4233 - val_acc: 0.8019\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4394 - acc: 0.7958 - val_loss: 0.4205 - val_acc: 0.8037\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4384 - acc: 0.7958 - val_loss: 0.4208 - val_acc: 0.8061\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4367 - acc: 0.7966 - val_loss: 0.4176 - val_acc: 0.8048\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4375 - acc: 0.7961 - val_loss: 0.4211 - val_acc: 0.8036\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4362 - acc: 0.7979 - val_loss: 0.4226 - val_acc: 0.8025\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4359 - acc: 0.7982 - val_loss: 0.4272 - val_acc: 0.8007\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4358 - acc: 0.7969 - val_loss: 0.4183 - val_acc: 0.8048\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4355 - acc: 0.7979 - val_loss: 0.4199 - val_acc: 0.8067\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4337 - acc: 0.7999 - val_loss: 0.4138 - val_acc: 0.8092\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4344 - acc: 0.7984 - val_loss: 0.4422 - val_acc: 0.7894\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4335 - acc: 0.7994 - val_loss: 0.4126 - val_acc: 0.8080\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4324 - acc: 0.7998 - val_loss: 0.4124 - val_acc: 0.8070\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4320 - acc: 0.7995 - val_loss: 0.4144 - val_acc: 0.8097\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4328 - acc: 0.7983 - val_loss: 0.4114 - val_acc: 0.8080\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4327 - acc: 0.7988 - val_loss: 0.4133 - val_acc: 0.8104\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 203us/sample - loss: 0.5919 - acc: 0.6994 - val_loss: 0.4969 - val_acc: 0.7617\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.5128 - acc: 0.7489 - val_loss: 0.4761 - val_acc: 0.7716\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4960 - acc: 0.7590 - val_loss: 0.4650 - val_acc: 0.7793\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4845 - acc: 0.7677 - val_loss: 0.4545 - val_acc: 0.7868\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4771 - acc: 0.7726 - val_loss: 0.4553 - val_acc: 0.7845\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4712 - acc: 0.7760 - val_loss: 0.4478 - val_acc: 0.7932\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4670 - acc: 0.7780 - val_loss: 0.4410 - val_acc: 0.7949\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4619 - acc: 0.7822 - val_loss: 0.4371 - val_acc: 0.7946\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4602 - acc: 0.7842 - val_loss: 0.4422 - val_acc: 0.7959\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4571 - acc: 0.7845 - val_loss: 0.4342 - val_acc: 0.8002\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4548 - acc: 0.7862 - val_loss: 0.4284 - val_acc: 0.8020\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4536 - acc: 0.7860 - val_loss: 0.4316 - val_acc: 0.7963\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4539 - acc: 0.7869 - val_loss: 0.4266 - val_acc: 0.8054\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4517 - acc: 0.7890 - val_loss: 0.4264 - val_acc: 0.8036\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4477 - acc: 0.7916 - val_loss: 0.4276 - val_acc: 0.8044\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4490 - acc: 0.7906 - val_loss: 0.4211 - val_acc: 0.8059\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4466 - acc: 0.7923 - val_loss: 0.4179 - val_acc: 0.8070\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4460 - acc: 0.7913 - val_loss: 0.4232 - val_acc: 0.8069\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4448 - acc: 0.7928 - val_loss: 0.4277 - val_acc: 0.8050\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4429 - acc: 0.7929 - val_loss: 0.4147 - val_acc: 0.8084\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4420 - acc: 0.7952 - val_loss: 0.4153 - val_acc: 0.8080\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4417 - acc: 0.7954 - val_loss: 0.4185 - val_acc: 0.8077\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4419 - acc: 0.7942 - val_loss: 0.4165 - val_acc: 0.8096\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4404 - acc: 0.7946 - val_loss: 0.4143 - val_acc: 0.8105\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4399 - acc: 0.7945 - val_loss: 0.4197 - val_acc: 0.8094\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4394 - acc: 0.7957 - val_loss: 0.4747 - val_acc: 0.7674\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4391 - acc: 0.7968 - val_loss: 0.4145 - val_acc: 0.8106\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4397 - acc: 0.7951 - val_loss: 0.4203 - val_acc: 0.8080\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4386 - acc: 0.7952 - val_loss: 0.4115 - val_acc: 0.8134\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4401 - acc: 0.7954 - val_loss: 0.4147 - val_acc: 0.8102\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 215us/sample - loss: 0.6170 - acc: 0.6869 - val_loss: 0.5032 - val_acc: 0.7544\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.5189 - acc: 0.7478 - val_loss: 0.4818 - val_acc: 0.7689\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4997 - acc: 0.7598 - val_loss: 0.4714 - val_acc: 0.7772\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4882 - acc: 0.7652 - val_loss: 0.4597 - val_acc: 0.7840\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4794 - acc: 0.7725 - val_loss: 0.4484 - val_acc: 0.7890\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4714 - acc: 0.7768 - val_loss: 0.4492 - val_acc: 0.7885\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4640 - acc: 0.7821 - val_loss: 0.4330 - val_acc: 0.7997\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4607 - acc: 0.7836 - val_loss: 0.4406 - val_acc: 0.7965\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4579 - acc: 0.7854 - val_loss: 0.4282 - val_acc: 0.8033\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4547 - acc: 0.7876 - val_loss: 0.4356 - val_acc: 0.8019\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4527 - acc: 0.7897 - val_loss: 0.4267 - val_acc: 0.8055\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4502 - acc: 0.7900 - val_loss: 0.4250 - val_acc: 0.8073\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4504 - acc: 0.7910 - val_loss: 0.4221 - val_acc: 0.8071\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4480 - acc: 0.7908 - val_loss: 0.4179 - val_acc: 0.8094\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4454 - acc: 0.7939 - val_loss: 0.5077 - val_acc: 0.7511\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4457 - acc: 0.7934 - val_loss: 0.4149 - val_acc: 0.8119\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4455 - acc: 0.7937 - val_loss: 0.4170 - val_acc: 0.8104\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4445 - acc: 0.7925 - val_loss: 0.4277 - val_acc: 0.8028\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4429 - acc: 0.7955 - val_loss: 0.4190 - val_acc: 0.8081\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4417 - acc: 0.7942 - val_loss: 0.4314 - val_acc: 0.8007\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4411 - acc: 0.7963 - val_loss: 0.4144 - val_acc: 0.8104\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4419 - acc: 0.7954 - val_loss: 0.4277 - val_acc: 0.8006\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4394 - acc: 0.7966 - val_loss: 0.4197 - val_acc: 0.8072\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4401 - acc: 0.7961 - val_loss: 0.4043 - val_acc: 0.8178\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4395 - acc: 0.7972 - val_loss: 0.4069 - val_acc: 0.8142\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4374 - acc: 0.7991 - val_loss: 0.4093 - val_acc: 0.8153\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4378 - acc: 0.7977 - val_loss: 0.4086 - val_acc: 0.8133\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4371 - acc: 0.7981 - val_loss: 0.4090 - val_acc: 0.8151\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4368 - acc: 0.7981 - val_loss: 0.4059 - val_acc: 0.8165\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4358 - acc: 0.8003 - val_loss: 0.4542 - val_acc: 0.7902\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.5519 - acc: 0.7321 - val_loss: 0.4858 - val_acc: 0.7662\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4822 - acc: 0.7692 - val_loss: 0.4554 - val_acc: 0.7847\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4642 - acc: 0.7785 - val_loss: 0.4606 - val_acc: 0.7825\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4489 - acc: 0.7907 - val_loss: 0.4524 - val_acc: 0.7898\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4400 - acc: 0.7956 - val_loss: 0.4202 - val_acc: 0.8084\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4314 - acc: 0.8006 - val_loss: 0.4124 - val_acc: 0.8111\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4273 - acc: 0.8031 - val_loss: 0.4237 - val_acc: 0.8026\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4230 - acc: 0.8063 - val_loss: 0.4050 - val_acc: 0.8145\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4195 - acc: 0.8087 - val_loss: 0.4044 - val_acc: 0.8148\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4157 - acc: 0.8102 - val_loss: 0.3983 - val_acc: 0.8184\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4151 - acc: 0.8087 - val_loss: 0.3970 - val_acc: 0.8198\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4117 - acc: 0.8100 - val_loss: 0.4059 - val_acc: 0.8145\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4088 - acc: 0.8133 - val_loss: 0.4197 - val_acc: 0.8090\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4080 - acc: 0.8137 - val_loss: 0.3974 - val_acc: 0.8191\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4067 - acc: 0.8151 - val_loss: 0.3980 - val_acc: 0.8194\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4072 - acc: 0.8151 - val_loss: 0.4288 - val_acc: 0.8059\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4054 - acc: 0.8151 - val_loss: 0.4012 - val_acc: 0.8165\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4047 - acc: 0.8158 - val_loss: 0.3921 - val_acc: 0.8217\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4021 - acc: 0.8182 - val_loss: 0.3922 - val_acc: 0.8224\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4014 - acc: 0.8179 - val_loss: 0.4055 - val_acc: 0.8184\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4004 - acc: 0.8185 - val_loss: 0.3855 - val_acc: 0.8242\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3986 - acc: 0.8202 - val_loss: 0.3972 - val_acc: 0.8181\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3985 - acc: 0.8180 - val_loss: 0.4031 - val_acc: 0.8159\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4000 - acc: 0.8185 - val_loss: 0.3862 - val_acc: 0.8241\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3980 - acc: 0.8182 - val_loss: 0.3895 - val_acc: 0.8217\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3971 - acc: 0.8196 - val_loss: 0.3976 - val_acc: 0.8160\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3958 - acc: 0.8206 - val_loss: 0.3880 - val_acc: 0.8227\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3946 - acc: 0.8217 - val_loss: 0.3840 - val_acc: 0.8256\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3951 - acc: 0.8204 - val_loss: 0.3890 - val_acc: 0.8234\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3944 - acc: 0.8210 - val_loss: 0.3985 - val_acc: 0.8174\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.5342 - acc: 0.7384 - val_loss: 0.4753 - val_acc: 0.7740\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4752 - acc: 0.7728 - val_loss: 0.4490 - val_acc: 0.7887\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4563 - acc: 0.7843 - val_loss: 0.4471 - val_acc: 0.7921\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4453 - acc: 0.7924 - val_loss: 0.4467 - val_acc: 0.7877\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4374 - acc: 0.7964 - val_loss: 0.4171 - val_acc: 0.8067\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4306 - acc: 0.8007 - val_loss: 0.4471 - val_acc: 0.7940\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4252 - acc: 0.8042 - val_loss: 0.4168 - val_acc: 0.8105\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4206 - acc: 0.8059 - val_loss: 0.4072 - val_acc: 0.8127\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4177 - acc: 0.8094 - val_loss: 0.3999 - val_acc: 0.8160\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4157 - acc: 0.8100 - val_loss: 0.4698 - val_acc: 0.7764\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4123 - acc: 0.8114 - val_loss: 0.4047 - val_acc: 0.8147\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4115 - acc: 0.8114 - val_loss: 0.4290 - val_acc: 0.7979\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4099 - acc: 0.8133 - val_loss: 0.4034 - val_acc: 0.8181\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4081 - acc: 0.8134 - val_loss: 0.4031 - val_acc: 0.8174\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4060 - acc: 0.8142 - val_loss: 0.4082 - val_acc: 0.8116\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4050 - acc: 0.8150 - val_loss: 0.3976 - val_acc: 0.8166\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4053 - acc: 0.8157 - val_loss: 0.3928 - val_acc: 0.8189\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4030 - acc: 0.8165 - val_loss: 0.3960 - val_acc: 0.8185\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4023 - acc: 0.8166 - val_loss: 0.4092 - val_acc: 0.8091\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4027 - acc: 0.8157 - val_loss: 0.3905 - val_acc: 0.8224\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4003 - acc: 0.8170 - val_loss: 0.3910 - val_acc: 0.8210\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4010 - acc: 0.8169 - val_loss: 0.3889 - val_acc: 0.8235\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4029 - acc: 0.8162 - val_loss: 0.3927 - val_acc: 0.8204\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3989 - acc: 0.8185 - val_loss: 0.4096 - val_acc: 0.8112\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3973 - acc: 0.8207 - val_loss: 0.3878 - val_acc: 0.8234\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3976 - acc: 0.8193 - val_loss: 0.3893 - val_acc: 0.8228\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3962 - acc: 0.8198 - val_loss: 0.3927 - val_acc: 0.8201\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3961 - acc: 0.8194 - val_loss: 0.3869 - val_acc: 0.8253\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3961 - acc: 0.8209 - val_loss: 0.3885 - val_acc: 0.8221\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3957 - acc: 0.8205 - val_loss: 0.3935 - val_acc: 0.8199\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.5453 - acc: 0.7343 - val_loss: 0.4724 - val_acc: 0.7736\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4757 - acc: 0.7739 - val_loss: 0.4465 - val_acc: 0.7885\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4537 - acc: 0.7872 - val_loss: 0.4286 - val_acc: 0.8015\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4392 - acc: 0.7948 - val_loss: 0.4293 - val_acc: 0.7997\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4293 - acc: 0.8008 - val_loss: 0.4200 - val_acc: 0.8057\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4204 - acc: 0.8064 - val_loss: 0.4117 - val_acc: 0.8075\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4169 - acc: 0.8083 - val_loss: 0.4003 - val_acc: 0.8178\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4124 - acc: 0.8112 - val_loss: 0.4039 - val_acc: 0.8151\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4087 - acc: 0.8141 - val_loss: 0.4102 - val_acc: 0.8116\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4059 - acc: 0.8139 - val_loss: 0.4015 - val_acc: 0.8176\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4002 - acc: 0.8191 - val_loss: 0.3947 - val_acc: 0.8193\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4007 - acc: 0.8194 - val_loss: 0.3871 - val_acc: 0.8235\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3967 - acc: 0.8205 - val_loss: 0.3937 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3959 - acc: 0.8212 - val_loss: 0.4103 - val_acc: 0.8112\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3932 - acc: 0.8210 - val_loss: 0.3875 - val_acc: 0.8230\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3923 - acc: 0.8232 - val_loss: 0.3838 - val_acc: 0.8257\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3881 - acc: 0.8248 - val_loss: 0.3980 - val_acc: 0.8155\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.3892 - acc: 0.8242 - val_loss: 0.3860 - val_acc: 0.8256\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3870 - acc: 0.8262 - val_loss: 0.3879 - val_acc: 0.8232\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3855 - acc: 0.8266 - val_loss: 0.3846 - val_acc: 0.8268\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3842 - acc: 0.8271 - val_loss: 0.3781 - val_acc: 0.8285\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3838 - acc: 0.8265 - val_loss: 0.3809 - val_acc: 0.8260\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3829 - acc: 0.8275 - val_loss: 0.4035 - val_acc: 0.8166\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3812 - acc: 0.8289 - val_loss: 0.3786 - val_acc: 0.8267\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3808 - acc: 0.8286 - val_loss: 0.3961 - val_acc: 0.8184\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3805 - acc: 0.8280 - val_loss: 0.3765 - val_acc: 0.8289\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3779 - acc: 0.8297 - val_loss: 0.3811 - val_acc: 0.8280\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.3792 - acc: 0.8289 - val_loss: 0.3765 - val_acc: 0.8288\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3768 - acc: 0.8317 - val_loss: 0.3761 - val_acc: 0.8303\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3771 - acc: 0.8313 - val_loss: 0.3803 - val_acc: 0.8284\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 19s 223us/sample - loss: 0.5455 - acc: 0.7322 - val_loss: 0.4714 - val_acc: 0.7763\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4804 - acc: 0.7724 - val_loss: 0.4482 - val_acc: 0.7899\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4596 - acc: 0.7837 - val_loss: 0.4418 - val_acc: 0.7962\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4461 - acc: 0.7929 - val_loss: 0.4185 - val_acc: 0.8062\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4339 - acc: 0.7994 - val_loss: 0.4203 - val_acc: 0.8051\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4270 - acc: 0.8041 - val_loss: 0.4361 - val_acc: 0.7980\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4200 - acc: 0.8084 - val_loss: 0.3994 - val_acc: 0.8167\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4152 - acc: 0.8096 - val_loss: 0.3984 - val_acc: 0.8177\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 135us/sample - loss: 0.4105 - acc: 0.8125 - val_loss: 0.4180 - val_acc: 0.8049\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4064 - acc: 0.8158 - val_loss: 0.4146 - val_acc: 0.8064\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4033 - acc: 0.8176 - val_loss: 0.4205 - val_acc: 0.8074\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4011 - acc: 0.8184 - val_loss: 0.3944 - val_acc: 0.8189\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3977 - acc: 0.8210 - val_loss: 0.3932 - val_acc: 0.8194\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3951 - acc: 0.8214 - val_loss: 0.3851 - val_acc: 0.8249\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3949 - acc: 0.8234 - val_loss: 0.3879 - val_acc: 0.8227\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3917 - acc: 0.8232 - val_loss: 0.3854 - val_acc: 0.8266\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3908 - acc: 0.8236 - val_loss: 0.3856 - val_acc: 0.8246\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3868 - acc: 0.8259 - val_loss: 0.3883 - val_acc: 0.8201\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3882 - acc: 0.8242 - val_loss: 0.4047 - val_acc: 0.8159\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3871 - acc: 0.8254 - val_loss: 0.3794 - val_acc: 0.8284\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3861 - acc: 0.8261 - val_loss: 0.3798 - val_acc: 0.8275\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3841 - acc: 0.8260 - val_loss: 0.3765 - val_acc: 0.8267\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3821 - acc: 0.8292 - val_loss: 0.3884 - val_acc: 0.8227\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3817 - acc: 0.8288 - val_loss: 0.3917 - val_acc: 0.8214\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3816 - acc: 0.8286 - val_loss: 0.3806 - val_acc: 0.8270\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3803 - acc: 0.8301 - val_loss: 0.3778 - val_acc: 0.8286\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3781 - acc: 0.8302 - val_loss: 0.3830 - val_acc: 0.8276\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.3762 - acc: 0.8319 - val_loss: 0.3899 - val_acc: 0.8208\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3775 - acc: 0.8309 - val_loss: 0.3773 - val_acc: 0.8300\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3758 - acc: 0.8308 - val_loss: 0.3757 - val_acc: 0.8279\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.5534 - acc: 0.7352 - val_loss: 0.4856 - val_acc: 0.7692\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4694 - acc: 0.7766 - val_loss: 0.4482 - val_acc: 0.7889\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4514 - acc: 0.7873 - val_loss: 0.4650 - val_acc: 0.7811\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4392 - acc: 0.7966 - val_loss: 0.4426 - val_acc: 0.7959\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4300 - acc: 0.8008 - val_loss: 0.4375 - val_acc: 0.7955\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4229 - acc: 0.8048 - val_loss: 0.4266 - val_acc: 0.7978\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4168 - acc: 0.8087 - val_loss: 0.4073 - val_acc: 0.8130\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4128 - acc: 0.8090 - val_loss: 0.4006 - val_acc: 0.8183\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4070 - acc: 0.8139 - val_loss: 0.3973 - val_acc: 0.8184\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4038 - acc: 0.8166 - val_loss: 0.4052 - val_acc: 0.8138\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4013 - acc: 0.8184 - val_loss: 0.4115 - val_acc: 0.8120\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3968 - acc: 0.8208 - val_loss: 0.3907 - val_acc: 0.8219\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3963 - acc: 0.8187 - val_loss: 0.4036 - val_acc: 0.8152\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3928 - acc: 0.8214 - val_loss: 0.3963 - val_acc: 0.8200\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3905 - acc: 0.8223 - val_loss: 0.4207 - val_acc: 0.8063\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3906 - acc: 0.8235 - val_loss: 0.3863 - val_acc: 0.8230\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3868 - acc: 0.8248 - val_loss: 0.3839 - val_acc: 0.8286\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3861 - acc: 0.8245 - val_loss: 0.3858 - val_acc: 0.8233\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3855 - acc: 0.8269 - val_loss: 0.4010 - val_acc: 0.8156\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3837 - acc: 0.8274 - val_loss: 0.3804 - val_acc: 0.8277\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3817 - acc: 0.8275 - val_loss: 0.3822 - val_acc: 0.8264\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3829 - acc: 0.8274 - val_loss: 0.3802 - val_acc: 0.8285\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3800 - acc: 0.8287 - val_loss: 0.3816 - val_acc: 0.8261\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3788 - acc: 0.8298 - val_loss: 0.3789 - val_acc: 0.8277\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3787 - acc: 0.8279 - val_loss: 0.3813 - val_acc: 0.8272\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3766 - acc: 0.8321 - val_loss: 0.3799 - val_acc: 0.8261\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3755 - acc: 0.8305 - val_loss: 0.3901 - val_acc: 0.8216\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3757 - acc: 0.8305 - val_loss: 0.3837 - val_acc: 0.8256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3746 - acc: 0.8321 - val_loss: 0.3806 - val_acc: 0.8260\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3728 - acc: 0.8322 - val_loss: 0.3824 - val_acc: 0.8267\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.5298 - acc: 0.7446 - val_loss: 0.4628 - val_acc: 0.7827\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4661 - acc: 0.7790 - val_loss: 0.4482 - val_acc: 0.7890\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4470 - acc: 0.7905 - val_loss: 0.4369 - val_acc: 0.7982\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4355 - acc: 0.7972 - val_loss: 0.4310 - val_acc: 0.8020\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4268 - acc: 0.8031 - val_loss: 0.4309 - val_acc: 0.7994\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4212 - acc: 0.8041 - val_loss: 0.4296 - val_acc: 0.8030\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4140 - acc: 0.8109 - val_loss: 0.4083 - val_acc: 0.8082\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4110 - acc: 0.8122 - val_loss: 0.4096 - val_acc: 0.8087\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4076 - acc: 0.8135 - val_loss: 0.3939 - val_acc: 0.8205\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4044 - acc: 0.8157 - val_loss: 0.4002 - val_acc: 0.8180\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4014 - acc: 0.8168 - val_loss: 0.4189 - val_acc: 0.8016\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3991 - acc: 0.8180 - val_loss: 0.3873 - val_acc: 0.8235\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3958 - acc: 0.8197 - val_loss: 0.3895 - val_acc: 0.8232\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3942 - acc: 0.8204 - val_loss: 0.3845 - val_acc: 0.8263\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3930 - acc: 0.8213 - val_loss: 0.3902 - val_acc: 0.8202\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3904 - acc: 0.8224 - val_loss: 0.3971 - val_acc: 0.8171\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3895 - acc: 0.8242 - val_loss: 0.4245 - val_acc: 0.8076\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3876 - acc: 0.8244 - val_loss: 0.3818 - val_acc: 0.8279\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3855 - acc: 0.8249 - val_loss: 0.3851 - val_acc: 0.8239\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3849 - acc: 0.8265 - val_loss: 0.3810 - val_acc: 0.8287\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3831 - acc: 0.8265 - val_loss: 0.3963 - val_acc: 0.8192\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3829 - acc: 0.8275 - val_loss: 0.3897 - val_acc: 0.8213\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3815 - acc: 0.8277 - val_loss: 0.3819 - val_acc: 0.8271\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3800 - acc: 0.8293 - val_loss: 0.3893 - val_acc: 0.8246\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.3800 - acc: 0.8286 - val_loss: 0.3868 - val_acc: 0.8257\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3787 - acc: 0.8290 - val_loss: 0.4077 - val_acc: 0.8141\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3776 - acc: 0.8292 - val_loss: 0.3968 - val_acc: 0.8188\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3767 - acc: 0.8298 - val_loss: 0.3943 - val_acc: 0.8199\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3761 - acc: 0.8313 - val_loss: 0.3775 - val_acc: 0.8299\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3754 - acc: 0.8320 - val_loss: 0.3795 - val_acc: 0.8298\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 19s 221us/sample - loss: 0.5359 - acc: 0.7425 - val_loss: 0.4551 - val_acc: 0.7889\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4568 - acc: 0.7853 - val_loss: 0.4295 - val_acc: 0.8011\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4348 - acc: 0.7991 - val_loss: 0.4207 - val_acc: 0.8045\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4231 - acc: 0.8059 - val_loss: 0.4208 - val_acc: 0.8045\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4144 - acc: 0.8117 - val_loss: 0.4020 - val_acc: 0.8146\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4072 - acc: 0.8139 - val_loss: 0.4059 - val_acc: 0.8140\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4016 - acc: 0.8175 - val_loss: 0.4068 - val_acc: 0.8152\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3972 - acc: 0.8200 - val_loss: 0.3953 - val_acc: 0.8203\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3913 - acc: 0.8233 - val_loss: 0.4021 - val_acc: 0.8145\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3866 - acc: 0.8251 - val_loss: 0.3929 - val_acc: 0.8204\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3835 - acc: 0.8271 - val_loss: 0.3825 - val_acc: 0.8275\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3787 - acc: 0.8303 - val_loss: 0.3864 - val_acc: 0.8224\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3773 - acc: 0.8312 - val_loss: 0.3926 - val_acc: 0.8227\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3719 - acc: 0.8336 - val_loss: 0.3831 - val_acc: 0.8273\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3699 - acc: 0.8346 - val_loss: 0.3786 - val_acc: 0.8288\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3660 - acc: 0.8360 - val_loss: 0.3925 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3641 - acc: 0.8373 - val_loss: 0.3790 - val_acc: 0.8291\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3608 - acc: 0.8386 - val_loss: 0.3752 - val_acc: 0.8288\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3594 - acc: 0.8391 - val_loss: 0.3787 - val_acc: 0.8294\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3565 - acc: 0.8409 - val_loss: 0.3870 - val_acc: 0.8253\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3532 - acc: 0.8422 - val_loss: 0.3784 - val_acc: 0.8281\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3515 - acc: 0.8450 - val_loss: 0.3806 - val_acc: 0.8277\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3511 - acc: 0.8443 - val_loss: 0.3783 - val_acc: 0.8292\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3478 - acc: 0.8456 - val_loss: 0.3751 - val_acc: 0.8314\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3467 - acc: 0.8463 - val_loss: 0.3876 - val_acc: 0.8286\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3460 - acc: 0.8466 - val_loss: 0.3794 - val_acc: 0.8271\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3417 - acc: 0.8475 - val_loss: 0.3802 - val_acc: 0.8300\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3416 - acc: 0.8481 - val_loss: 0.3764 - val_acc: 0.8293\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3385 - acc: 0.8492 - val_loss: 0.3799 - val_acc: 0.8288\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3388 - acc: 0.8502 - val_loss: 0.3924 - val_acc: 0.8185\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 20s 234us/sample - loss: 0.5320 - acc: 0.7452 - val_loss: 0.4565 - val_acc: 0.7862\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4574 - acc: 0.7860 - val_loss: 0.4241 - val_acc: 0.8037\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4380 - acc: 0.7975 - val_loss: 0.4261 - val_acc: 0.8028\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4231 - acc: 0.8066 - val_loss: 0.4041 - val_acc: 0.8154\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 136us/sample - loss: 0.4137 - acc: 0.8106 - val_loss: 0.4038 - val_acc: 0.8149\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4056 - acc: 0.8152 - val_loss: 0.4077 - val_acc: 0.8131\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4014 - acc: 0.8174 - val_loss: 0.3866 - val_acc: 0.8217\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3949 - acc: 0.8208 - val_loss: 0.4072 - val_acc: 0.8158\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3893 - acc: 0.8240 - val_loss: 0.3818 - val_acc: 0.8270\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3850 - acc: 0.8265 - val_loss: 0.4403 - val_acc: 0.7996\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3792 - acc: 0.8293 - val_loss: 0.3851 - val_acc: 0.8245\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3753 - acc: 0.8327 - val_loss: 0.3801 - val_acc: 0.8264\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3715 - acc: 0.8352 - val_loss: 0.4045 - val_acc: 0.8156\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3690 - acc: 0.8344 - val_loss: 0.3754 - val_acc: 0.8281\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3661 - acc: 0.8358 - val_loss: 0.3759 - val_acc: 0.8308\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 135us/sample - loss: 0.3614 - acc: 0.8394 - val_loss: 0.3789 - val_acc: 0.8280\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3596 - acc: 0.8404 - val_loss: 0.3899 - val_acc: 0.8239\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3561 - acc: 0.8407 - val_loss: 0.3795 - val_acc: 0.8268\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3542 - acc: 0.8418 - val_loss: 0.3716 - val_acc: 0.8334\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3513 - acc: 0.8441 - val_loss: 0.3964 - val_acc: 0.8219\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3468 - acc: 0.8451 - val_loss: 0.3835 - val_acc: 0.8229\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3441 - acc: 0.8477 - val_loss: 0.3745 - val_acc: 0.8309\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3456 - acc: 0.8471 - val_loss: 0.3752 - val_acc: 0.8338\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3410 - acc: 0.8485 - val_loss: 0.3746 - val_acc: 0.8283\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3411 - acc: 0.8489 - val_loss: 0.3838 - val_acc: 0.8280\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 135us/sample - loss: 0.3365 - acc: 0.8520 - val_loss: 0.3776 - val_acc: 0.8298\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3343 - acc: 0.8527 - val_loss: 0.3767 - val_acc: 0.8292\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3328 - acc: 0.8534 - val_loss: 0.3824 - val_acc: 0.8306\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3307 - acc: 0.8539 - val_loss: 0.3829 - val_acc: 0.8271\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3288 - acc: 0.8562 - val_loss: 0.3821 - val_acc: 0.8260\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.5156 - acc: 0.7496 - val_loss: 0.4594 - val_acc: 0.7819\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4589 - acc: 0.7833 - val_loss: 0.4509 - val_acc: 0.7870\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4395 - acc: 0.7943 - val_loss: 0.4322 - val_acc: 0.8002\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4300 - acc: 0.8003 - val_loss: 0.4189 - val_acc: 0.8077\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4215 - acc: 0.8061 - val_loss: 0.4102 - val_acc: 0.8138\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4166 - acc: 0.8080 - val_loss: 0.4031 - val_acc: 0.8148\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4113 - acc: 0.8119 - val_loss: 0.4000 - val_acc: 0.8163\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4058 - acc: 0.8146 - val_loss: 0.4184 - val_acc: 0.8071\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.4031 - acc: 0.8162 - val_loss: 0.3934 - val_acc: 0.8223\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3992 - acc: 0.8186 - val_loss: 0.3960 - val_acc: 0.8194\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3959 - acc: 0.8186 - val_loss: 0.3934 - val_acc: 0.8207\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3933 - acc: 0.8218 - val_loss: 0.4106 - val_acc: 0.8102\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3910 - acc: 0.8221 - val_loss: 0.3901 - val_acc: 0.8248\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3878 - acc: 0.8241 - val_loss: 0.3923 - val_acc: 0.8219\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3858 - acc: 0.8247 - val_loss: 0.3846 - val_acc: 0.8238\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3852 - acc: 0.8263 - val_loss: 0.3874 - val_acc: 0.8237\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3835 - acc: 0.8259 - val_loss: 0.4183 - val_acc: 0.8074\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3807 - acc: 0.8280 - val_loss: 0.3790 - val_acc: 0.8268\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3785 - acc: 0.8290 - val_loss: 0.3865 - val_acc: 0.8261\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3770 - acc: 0.8306 - val_loss: 0.3819 - val_acc: 0.8249\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3752 - acc: 0.8308 - val_loss: 0.3841 - val_acc: 0.8262\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3758 - acc: 0.8307 - val_loss: 0.3780 - val_acc: 0.8310\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3729 - acc: 0.8337 - val_loss: 0.3809 - val_acc: 0.8284\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3714 - acc: 0.8329 - val_loss: 0.3884 - val_acc: 0.8241\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3708 - acc: 0.8341 - val_loss: 0.3885 - val_acc: 0.8220\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3720 - acc: 0.8334 - val_loss: 0.3881 - val_acc: 0.8260\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3695 - acc: 0.8336 - val_loss: 0.3839 - val_acc: 0.8258\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3691 - acc: 0.8338 - val_loss: 0.3738 - val_acc: 0.8321\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3674 - acc: 0.8350 - val_loss: 0.3742 - val_acc: 0.8306\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3649 - acc: 0.8370 - val_loss: 0.3940 - val_acc: 0.8206\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 211us/sample - loss: 0.5238 - acc: 0.7477 - val_loss: 0.4626 - val_acc: 0.7862\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4587 - acc: 0.7827 - val_loss: 0.4387 - val_acc: 0.7936\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4392 - acc: 0.7954 - val_loss: 0.4819 - val_acc: 0.7702\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4278 - acc: 0.8022 - val_loss: 0.4127 - val_acc: 0.8130\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4186 - acc: 0.8082 - val_loss: 0.4037 - val_acc: 0.8122\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4133 - acc: 0.8108 - val_loss: 0.4186 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4078 - acc: 0.8138 - val_loss: 0.4088 - val_acc: 0.8108\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4040 - acc: 0.8166 - val_loss: 0.3965 - val_acc: 0.8180\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4028 - acc: 0.8160 - val_loss: 0.3933 - val_acc: 0.8200\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3978 - acc: 0.8193 - val_loss: 0.3916 - val_acc: 0.8209\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3956 - acc: 0.8206 - val_loss: 0.3885 - val_acc: 0.8251\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3923 - acc: 0.8232 - val_loss: 0.3969 - val_acc: 0.8184\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3914 - acc: 0.8224 - val_loss: 0.3854 - val_acc: 0.8243\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3882 - acc: 0.8231 - val_loss: 0.3868 - val_acc: 0.8253\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3859 - acc: 0.8265 - val_loss: 0.4445 - val_acc: 0.7944\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3847 - acc: 0.8262 - val_loss: 0.4011 - val_acc: 0.8181\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3812 - acc: 0.8274 - val_loss: 0.3907 - val_acc: 0.8219\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3800 - acc: 0.8288 - val_loss: 0.3840 - val_acc: 0.8268\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3774 - acc: 0.8296 - val_loss: 0.3820 - val_acc: 0.8291\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3806 - acc: 0.8284 - val_loss: 0.3801 - val_acc: 0.8278\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3758 - acc: 0.8303 - val_loss: 0.3790 - val_acc: 0.8300\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3747 - acc: 0.8312 - val_loss: 0.4140 - val_acc: 0.8112\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3731 - acc: 0.8322 - val_loss: 0.3778 - val_acc: 0.8291\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3704 - acc: 0.8329 - val_loss: 0.3879 - val_acc: 0.8227\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3714 - acc: 0.8325 - val_loss: 0.3870 - val_acc: 0.8286\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3701 - acc: 0.8342 - val_loss: 0.3917 - val_acc: 0.8242\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3661 - acc: 0.8347 - val_loss: 0.3880 - val_acc: 0.8221\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3691 - acc: 0.8345 - val_loss: 0.3808 - val_acc: 0.8281\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3658 - acc: 0.8354 - val_loss: 0.3925 - val_acc: 0.8230\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3666 - acc: 0.8358 - val_loss: 0.3764 - val_acc: 0.8311\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 20s 232us/sample - loss: 0.5225 - acc: 0.7479 - val_loss: 0.4594 - val_acc: 0.7836\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4484 - acc: 0.7899 - val_loss: 0.4343 - val_acc: 0.7957\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4272 - acc: 0.8036 - val_loss: 0.4066 - val_acc: 0.8155\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4148 - acc: 0.8099 - val_loss: 0.4467 - val_acc: 0.7907\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4069 - acc: 0.8144 - val_loss: 0.4000 - val_acc: 0.8173\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4001 - acc: 0.8185 - val_loss: 0.4011 - val_acc: 0.8162\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3908 - acc: 0.8232 - val_loss: 0.4234 - val_acc: 0.8042\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3868 - acc: 0.8265 - val_loss: 0.3935 - val_acc: 0.8199\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3788 - acc: 0.8295 - val_loss: 0.3913 - val_acc: 0.8224\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3754 - acc: 0.8327 - val_loss: 0.4057 - val_acc: 0.8177\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3708 - acc: 0.8322 - val_loss: 0.3799 - val_acc: 0.8274\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3658 - acc: 0.8362 - val_loss: 0.3830 - val_acc: 0.8253\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3622 - acc: 0.8389 - val_loss: 0.3878 - val_acc: 0.8256\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3574 - acc: 0.8405 - val_loss: 0.3837 - val_acc: 0.8247\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3521 - acc: 0.8436 - val_loss: 0.3899 - val_acc: 0.8237\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3497 - acc: 0.8455 - val_loss: 0.3808 - val_acc: 0.8281\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3451 - acc: 0.8474 - val_loss: 0.4091 - val_acc: 0.8184\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3409 - acc: 0.8502 - val_loss: 0.3872 - val_acc: 0.8280\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3374 - acc: 0.8508 - val_loss: 0.3769 - val_acc: 0.8304\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3334 - acc: 0.8524 - val_loss: 0.3858 - val_acc: 0.8263\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3306 - acc: 0.8530 - val_loss: 0.3915 - val_acc: 0.8277\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3281 - acc: 0.8560 - val_loss: 0.3818 - val_acc: 0.8306\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3258 - acc: 0.8558 - val_loss: 0.4231 - val_acc: 0.8155\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3219 - acc: 0.8583 - val_loss: 0.3844 - val_acc: 0.8304\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3178 - acc: 0.8585 - val_loss: 0.4023 - val_acc: 0.8232\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3144 - acc: 0.8616 - val_loss: 0.3857 - val_acc: 0.8325\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3120 - acc: 0.8628 - val_loss: 0.3977 - val_acc: 0.8254\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3098 - acc: 0.8628 - val_loss: 0.3831 - val_acc: 0.8296\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3081 - acc: 0.8653 - val_loss: 0.3905 - val_acc: 0.8274\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3047 - acc: 0.8662 - val_loss: 0.4002 - val_acc: 0.8274\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 21s 245us/sample - loss: 0.5059 - acc: 0.7562 - val_loss: 0.4519 - val_acc: 0.7850\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4498 - acc: 0.7898 - val_loss: 0.4297 - val_acc: 0.7997\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4284 - acc: 0.8033 - val_loss: 0.4400 - val_acc: 0.8002\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4156 - acc: 0.8098 - val_loss: 0.4245 - val_acc: 0.8051\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4054 - acc: 0.8155 - val_loss: 0.4309 - val_acc: 0.8010\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3955 - acc: 0.8205 - val_loss: 0.4045 - val_acc: 0.8127\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3916 - acc: 0.8234 - val_loss: 0.4048 - val_acc: 0.8120\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3836 - acc: 0.8279 - val_loss: 0.4156 - val_acc: 0.8064\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3769 - acc: 0.8306 - val_loss: 0.3781 - val_acc: 0.8286\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3716 - acc: 0.8337 - val_loss: 0.3911 - val_acc: 0.8223\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3655 - acc: 0.8362 - val_loss: 0.3843 - val_acc: 0.8249\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3587 - acc: 0.8394 - val_loss: 0.3807 - val_acc: 0.8245\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3543 - acc: 0.8431 - val_loss: 0.3930 - val_acc: 0.8231\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3495 - acc: 0.8437 - val_loss: 0.3811 - val_acc: 0.8256\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3421 - acc: 0.8488 - val_loss: 0.3805 - val_acc: 0.8284\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3393 - acc: 0.8502 - val_loss: 0.3822 - val_acc: 0.8271\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3338 - acc: 0.8531 - val_loss: 0.3830 - val_acc: 0.8270\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3287 - acc: 0.8559 - val_loss: 0.3986 - val_acc: 0.8253\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3254 - acc: 0.8565 - val_loss: 0.3779 - val_acc: 0.8286\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3227 - acc: 0.8580 - val_loss: 0.3829 - val_acc: 0.8278\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3165 - acc: 0.8614 - val_loss: 0.3844 - val_acc: 0.8284\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3139 - acc: 0.8615 - val_loss: 0.3970 - val_acc: 0.8262\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3108 - acc: 0.8639 - val_loss: 0.3869 - val_acc: 0.8293\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3062 - acc: 0.8663 - val_loss: 0.3914 - val_acc: 0.8273\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3034 - acc: 0.8660 - val_loss: 0.4049 - val_acc: 0.8210\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.2986 - acc: 0.8705 - val_loss: 0.4070 - val_acc: 0.8236\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.2951 - acc: 0.8715 - val_loss: 0.3953 - val_acc: 0.8210\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.2932 - acc: 0.8723 - val_loss: 0.4040 - val_acc: 0.8252\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.2913 - acc: 0.8744 - val_loss: 0.4040 - val_acc: 0.8314\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.2855 - acc: 0.8762 - val_loss: 0.4028 - val_acc: 0.8221\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 21s 243us/sample - loss: 0.5607 - acc: 0.7244 - val_loss: 0.4914 - val_acc: 0.7643\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4946 - acc: 0.7621 - val_loss: 0.4592 - val_acc: 0.7802\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4715 - acc: 0.7762 - val_loss: 0.4671 - val_acc: 0.7766\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4615 - acc: 0.7838 - val_loss: 0.4386 - val_acc: 0.7954\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4541 - acc: 0.7867 - val_loss: 0.4292 - val_acc: 0.8008\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4473 - acc: 0.7914 - val_loss: 0.4275 - val_acc: 0.8002\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4420 - acc: 0.7953 - val_loss: 0.4430 - val_acc: 0.7916\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4384 - acc: 0.7961 - val_loss: 0.4195 - val_acc: 0.8065\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4363 - acc: 0.7985 - val_loss: 0.4170 - val_acc: 0.8069\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4332 - acc: 0.7996 - val_loss: 0.4175 - val_acc: 0.8087\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4292 - acc: 0.8012 - val_loss: 0.4252 - val_acc: 0.8033\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4283 - acc: 0.8032 - val_loss: 0.4170 - val_acc: 0.8081\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4252 - acc: 0.8036 - val_loss: 0.4124 - val_acc: 0.8126\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4228 - acc: 0.8060 - val_loss: 0.4066 - val_acc: 0.8130\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4210 - acc: 0.8058 - val_loss: 0.4077 - val_acc: 0.8134\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4181 - acc: 0.8069 - val_loss: 0.4151 - val_acc: 0.8083\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4157 - acc: 0.8085 - val_loss: 0.4134 - val_acc: 0.8079\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4168 - acc: 0.8079 - val_loss: 0.4170 - val_acc: 0.8090\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4142 - acc: 0.8096 - val_loss: 0.4212 - val_acc: 0.8084\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4122 - acc: 0.8108 - val_loss: 0.4037 - val_acc: 0.8166\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4126 - acc: 0.8108 - val_loss: 0.4054 - val_acc: 0.8150\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4104 - acc: 0.8114 - val_loss: 0.4061 - val_acc: 0.8145\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4083 - acc: 0.8129 - val_loss: 0.3989 - val_acc: 0.8177\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4056 - acc: 0.8128 - val_loss: 0.4154 - val_acc: 0.8120\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4075 - acc: 0.8136 - val_loss: 0.4072 - val_acc: 0.8149\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4058 - acc: 0.8149 - val_loss: 0.4004 - val_acc: 0.8170\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4048 - acc: 0.8155 - val_loss: 0.4015 - val_acc: 0.8166\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4049 - acc: 0.8149 - val_loss: 0.4010 - val_acc: 0.8177\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4027 - acc: 0.8164 - val_loss: 0.4052 - val_acc: 0.8142\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4011 - acc: 0.8178 - val_loss: 0.4006 - val_acc: 0.8173\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 21s 245us/sample - loss: 0.5447 - acc: 0.7306 - val_loss: 0.4821 - val_acc: 0.7668\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4892 - acc: 0.7628 - val_loss: 0.4614 - val_acc: 0.7794\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4736 - acc: 0.7740 - val_loss: 0.4442 - val_acc: 0.7933\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4621 - acc: 0.7823 - val_loss: 0.4324 - val_acc: 0.7983\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4494 - acc: 0.7901 - val_loss: 0.4306 - val_acc: 0.8000\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4434 - acc: 0.7934 - val_loss: 0.4263 - val_acc: 0.7993\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4391 - acc: 0.7946 - val_loss: 0.4667 - val_acc: 0.7770\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4354 - acc: 0.7988 - val_loss: 0.4103 - val_acc: 0.8094\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4299 - acc: 0.8008 - val_loss: 0.4139 - val_acc: 0.8113\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4281 - acc: 0.8023 - val_loss: 0.4381 - val_acc: 0.7905\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4237 - acc: 0.8044 - val_loss: 0.4098 - val_acc: 0.8112\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4226 - acc: 0.8046 - val_loss: 0.4093 - val_acc: 0.8125\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4188 - acc: 0.8079 - val_loss: 0.4072 - val_acc: 0.8127\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4157 - acc: 0.8098 - val_loss: 0.4074 - val_acc: 0.8084\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4183 - acc: 0.8072 - val_loss: 0.4051 - val_acc: 0.8148\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4142 - acc: 0.8095 - val_loss: 0.4021 - val_acc: 0.8138\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4131 - acc: 0.8111 - val_loss: 0.4054 - val_acc: 0.8116\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4133 - acc: 0.8097 - val_loss: 0.4091 - val_acc: 0.8134\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4078 - acc: 0.8145 - val_loss: 0.4072 - val_acc: 0.8153\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4085 - acc: 0.8125 - val_loss: 0.4036 - val_acc: 0.8142\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4079 - acc: 0.8145 - val_loss: 0.4005 - val_acc: 0.8176\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4043 - acc: 0.8160 - val_loss: 0.4088 - val_acc: 0.8105\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4034 - acc: 0.8166 - val_loss: 0.4065 - val_acc: 0.8133\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4038 - acc: 0.8161 - val_loss: 0.3977 - val_acc: 0.8198\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4020 - acc: 0.8161 - val_loss: 0.4022 - val_acc: 0.8152\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3999 - acc: 0.8182 - val_loss: 0.4070 - val_acc: 0.8151\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3998 - acc: 0.8169 - val_loss: 0.3965 - val_acc: 0.8187\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3971 - acc: 0.8192 - val_loss: 0.3999 - val_acc: 0.8176\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3974 - acc: 0.8187 - val_loss: 0.3938 - val_acc: 0.8209\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3974 - acc: 0.8179 - val_loss: 0.3925 - val_acc: 0.8220\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 22s 257us/sample - loss: 0.5748 - acc: 0.7132 - val_loss: 0.4948 - val_acc: 0.7614\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.5111 - acc: 0.7521 - val_loss: 0.4926 - val_acc: 0.7652\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4943 - acc: 0.7625 - val_loss: 0.4683 - val_acc: 0.7804\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4857 - acc: 0.7693 - val_loss: 0.4585 - val_acc: 0.7845\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4765 - acc: 0.7736 - val_loss: 0.4482 - val_acc: 0.7885\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4691 - acc: 0.7790 - val_loss: 0.4415 - val_acc: 0.7929\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4625 - acc: 0.7824 - val_loss: 0.4442 - val_acc: 0.7932\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4571 - acc: 0.7860 - val_loss: 0.4368 - val_acc: 0.7952\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4531 - acc: 0.7884 - val_loss: 0.4514 - val_acc: 0.7918\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4507 - acc: 0.7904 - val_loss: 0.4289 - val_acc: 0.7997\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4472 - acc: 0.7922 - val_loss: 0.4201 - val_acc: 0.8062\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4448 - acc: 0.7944 - val_loss: 0.4239 - val_acc: 0.8066\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4436 - acc: 0.7945 - val_loss: 0.4190 - val_acc: 0.8073\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4430 - acc: 0.7941 - val_loss: 0.4212 - val_acc: 0.8040\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4416 - acc: 0.7961 - val_loss: 0.4138 - val_acc: 0.8097\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4407 - acc: 0.7944 - val_loss: 0.4284 - val_acc: 0.7996\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4397 - acc: 0.7952 - val_loss: 0.4220 - val_acc: 0.8058\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4387 - acc: 0.7956 - val_loss: 0.4157 - val_acc: 0.8099\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4379 - acc: 0.7960 - val_loss: 0.4145 - val_acc: 0.8105\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4379 - acc: 0.7959 - val_loss: 0.4114 - val_acc: 0.8110\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4374 - acc: 0.7968 - val_loss: 0.4119 - val_acc: 0.8116\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4364 - acc: 0.7992 - val_loss: 0.4073 - val_acc: 0.8140\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4374 - acc: 0.7973 - val_loss: 0.4321 - val_acc: 0.8019\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4350 - acc: 0.7996 - val_loss: 0.4239 - val_acc: 0.8049\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4352 - acc: 0.7985 - val_loss: 0.4109 - val_acc: 0.8095\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4330 - acc: 0.7994 - val_loss: 0.4058 - val_acc: 0.8129\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4318 - acc: 0.7998 - val_loss: 0.4072 - val_acc: 0.8152\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4324 - acc: 0.8011 - val_loss: 0.4202 - val_acc: 0.8082\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4311 - acc: 0.8007 - val_loss: 0.4068 - val_acc: 0.8152\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4305 - acc: 0.8017 - val_loss: 0.4045 - val_acc: 0.8148\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 272us/sample - loss: 0.6055 - acc: 0.6864 - val_loss: 0.5023 - val_acc: 0.7581\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.5188 - acc: 0.7473 - val_loss: 0.5253 - val_acc: 0.7449\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4998 - acc: 0.7593 - val_loss: 0.4651 - val_acc: 0.7808\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4868 - acc: 0.7674 - val_loss: 0.4600 - val_acc: 0.7855\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4794 - acc: 0.7723 - val_loss: 0.4514 - val_acc: 0.7855\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4745 - acc: 0.7741 - val_loss: 0.4431 - val_acc: 0.7948\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4693 - acc: 0.7776 - val_loss: 0.4390 - val_acc: 0.7958\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4652 - acc: 0.7802 - val_loss: 0.4367 - val_acc: 0.7951\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4614 - acc: 0.7826 - val_loss: 0.4299 - val_acc: 0.8015\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4581 - acc: 0.7827 - val_loss: 0.4381 - val_acc: 0.7966\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4575 - acc: 0.7840 - val_loss: 0.4495 - val_acc: 0.7879\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4545 - acc: 0.7855 - val_loss: 0.4246 - val_acc: 0.8048\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4537 - acc: 0.7882 - val_loss: 0.4424 - val_acc: 0.7914\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4527 - acc: 0.7876 - val_loss: 0.4289 - val_acc: 0.8062\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4520 - acc: 0.7887 - val_loss: 0.4228 - val_acc: 0.8042\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4486 - acc: 0.7911 - val_loss: 0.4222 - val_acc: 0.8074\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4490 - acc: 0.7898 - val_loss: 0.4387 - val_acc: 0.7946\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4462 - acc: 0.7906 - val_loss: 0.4181 - val_acc: 0.8099\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4465 - acc: 0.7915 - val_loss: 0.4387 - val_acc: 0.7964\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4452 - acc: 0.7924 - val_loss: 0.4234 - val_acc: 0.8051\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4446 - acc: 0.7927 - val_loss: 0.4155 - val_acc: 0.8097\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4465 - acc: 0.7901 - val_loss: 0.4301 - val_acc: 0.8025\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4427 - acc: 0.7932 - val_loss: 0.4235 - val_acc: 0.8049\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4440 - acc: 0.7917 - val_loss: 0.4186 - val_acc: 0.8078\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4424 - acc: 0.7937 - val_loss: 0.4160 - val_acc: 0.8110\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4434 - acc: 0.7947 - val_loss: 0.4380 - val_acc: 0.7984\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4396 - acc: 0.7952 - val_loss: 0.4299 - val_acc: 0.8036\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4414 - acc: 0.7935 - val_loss: 0.4199 - val_acc: 0.8104\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4400 - acc: 0.7960 - val_loss: 0.4238 - val_acc: 0.8088\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4399 - acc: 0.7948 - val_loss: 0.4227 - val_acc: 0.8062\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 22s 255us/sample - loss: 0.5364 - acc: 0.7386 - val_loss: 0.4668 - val_acc: 0.7758\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4670 - acc: 0.7786 - val_loss: 0.4450 - val_acc: 0.7895\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4474 - acc: 0.7900 - val_loss: 0.4338 - val_acc: 0.7987\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4335 - acc: 0.7994 - val_loss: 0.4258 - val_acc: 0.8031\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4238 - acc: 0.8060 - val_loss: 0.4450 - val_acc: 0.7908\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4167 - acc: 0.8080 - val_loss: 0.4029 - val_acc: 0.8162\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4108 - acc: 0.8132 - val_loss: 0.4068 - val_acc: 0.8136\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4039 - acc: 0.8167 - val_loss: 0.4060 - val_acc: 0.8131\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3989 - acc: 0.8182 - val_loss: 0.4017 - val_acc: 0.8166\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3965 - acc: 0.8196 - val_loss: 0.4111 - val_acc: 0.8112\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3918 - acc: 0.8220 - val_loss: 0.3876 - val_acc: 0.8225\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3892 - acc: 0.8231 - val_loss: 0.4054 - val_acc: 0.8133\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3846 - acc: 0.8257 - val_loss: 0.4032 - val_acc: 0.8139\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3814 - acc: 0.8273 - val_loss: 0.3897 - val_acc: 0.8221\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3752 - acc: 0.8311 - val_loss: 0.4145 - val_acc: 0.8133\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3746 - acc: 0.8320 - val_loss: 0.3919 - val_acc: 0.8222\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3720 - acc: 0.8323 - val_loss: 0.3910 - val_acc: 0.8220\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3699 - acc: 0.8335 - val_loss: 0.3970 - val_acc: 0.8185\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3668 - acc: 0.8359 - val_loss: 0.3908 - val_acc: 0.8220\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3643 - acc: 0.8364 - val_loss: 0.3920 - val_acc: 0.8199\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3611 - acc: 0.8395 - val_loss: 0.3918 - val_acc: 0.8204\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3579 - acc: 0.8399 - val_loss: 0.3900 - val_acc: 0.8238\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3579 - acc: 0.8386 - val_loss: 0.3911 - val_acc: 0.8212\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3533 - acc: 0.8427 - val_loss: 0.3980 - val_acc: 0.8195\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3511 - acc: 0.8438 - val_loss: 0.4068 - val_acc: 0.8179\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3496 - acc: 0.8435 - val_loss: 0.3964 - val_acc: 0.8228\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3478 - acc: 0.8444 - val_loss: 0.4057 - val_acc: 0.8204\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3453 - acc: 0.8462 - val_loss: 0.3905 - val_acc: 0.8243\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3428 - acc: 0.8463 - val_loss: 0.4018 - val_acc: 0.8224\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3400 - acc: 0.8502 - val_loss: 0.3943 - val_acc: 0.8231\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 22s 258us/sample - loss: 0.5150 - acc: 0.7485 - val_loss: 0.4582 - val_acc: 0.7814\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4617 - acc: 0.7820 - val_loss: 0.4367 - val_acc: 0.7962\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4416 - acc: 0.7937 - val_loss: 0.4294 - val_acc: 0.8015\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4298 - acc: 0.8018 - val_loss: 0.4194 - val_acc: 0.8056\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4191 - acc: 0.8068 - val_loss: 0.4108 - val_acc: 0.8124\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4107 - acc: 0.8105 - val_loss: 0.4206 - val_acc: 0.8051\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4045 - acc: 0.8165 - val_loss: 0.4048 - val_acc: 0.8124\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3999 - acc: 0.8185 - val_loss: 0.3979 - val_acc: 0.8189\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3953 - acc: 0.8196 - val_loss: 0.3974 - val_acc: 0.8193\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3904 - acc: 0.8232 - val_loss: 0.3919 - val_acc: 0.8217\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3861 - acc: 0.8252 - val_loss: 0.3999 - val_acc: 0.8181\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3839 - acc: 0.8265 - val_loss: 0.4002 - val_acc: 0.8177\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3789 - acc: 0.8303 - val_loss: 0.3855 - val_acc: 0.8261\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3741 - acc: 0.8314 - val_loss: 0.4173 - val_acc: 0.8090\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3720 - acc: 0.8334 - val_loss: 0.3964 - val_acc: 0.8195\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3695 - acc: 0.8347 - val_loss: 0.3874 - val_acc: 0.8232\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3681 - acc: 0.8362 - val_loss: 0.3975 - val_acc: 0.8215\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3635 - acc: 0.8380 - val_loss: 0.3844 - val_acc: 0.8243\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3590 - acc: 0.8400 - val_loss: 0.4000 - val_acc: 0.8196\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3588 - acc: 0.8391 - val_loss: 0.3906 - val_acc: 0.8235\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3562 - acc: 0.8410 - val_loss: 0.3923 - val_acc: 0.8238\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3540 - acc: 0.8428 - val_loss: 0.4011 - val_acc: 0.8209\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3494 - acc: 0.8437 - val_loss: 0.3902 - val_acc: 0.8214\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3474 - acc: 0.8455 - val_loss: 0.4035 - val_acc: 0.8200\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3447 - acc: 0.8458 - val_loss: 0.4003 - val_acc: 0.8201\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3447 - acc: 0.8471 - val_loss: 0.3958 - val_acc: 0.8216\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3382 - acc: 0.8500 - val_loss: 0.4002 - val_acc: 0.8224\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3391 - acc: 0.8502 - val_loss: 0.3940 - val_acc: 0.8217\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3361 - acc: 0.8515 - val_loss: 0.4052 - val_acc: 0.8188\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3356 - acc: 0.8511 - val_loss: 0.3927 - val_acc: 0.8243\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 271us/sample - loss: 0.5574 - acc: 0.7307 - val_loss: 0.4794 - val_acc: 0.7732\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4757 - acc: 0.7739 - val_loss: 0.4606 - val_acc: 0.7814\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4517 - acc: 0.7876 - val_loss: 0.4579 - val_acc: 0.7837\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4380 - acc: 0.7969 - val_loss: 0.4196 - val_acc: 0.8077\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4285 - acc: 0.8026 - val_loss: 0.4117 - val_acc: 0.8105\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4231 - acc: 0.8056 - val_loss: 0.4116 - val_acc: 0.8124\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4167 - acc: 0.8087 - val_loss: 0.4001 - val_acc: 0.8191\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4116 - acc: 0.8119 - val_loss: 0.3980 - val_acc: 0.8189\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4094 - acc: 0.8131 - val_loss: 0.3907 - val_acc: 0.8230\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4058 - acc: 0.8141 - val_loss: 0.3968 - val_acc: 0.8210\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4012 - acc: 0.8175 - val_loss: 0.4355 - val_acc: 0.8018\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4003 - acc: 0.8182 - val_loss: 0.4109 - val_acc: 0.8101\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3949 - acc: 0.8213 - val_loss: 0.3905 - val_acc: 0.8233\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3937 - acc: 0.8220 - val_loss: 0.3944 - val_acc: 0.8214\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3893 - acc: 0.8234 - val_loss: 0.3874 - val_acc: 0.8236\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3899 - acc: 0.8220 - val_loss: 0.4334 - val_acc: 0.8033\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3870 - acc: 0.8242 - val_loss: 0.3848 - val_acc: 0.8249\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3864 - acc: 0.8247 - val_loss: 0.3850 - val_acc: 0.8266\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3849 - acc: 0.8252 - val_loss: 0.3848 - val_acc: 0.8229\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3829 - acc: 0.8266 - val_loss: 0.3930 - val_acc: 0.8209\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3809 - acc: 0.8276 - val_loss: 0.3842 - val_acc: 0.8260\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3798 - acc: 0.8292 - val_loss: 0.3767 - val_acc: 0.8306\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3789 - acc: 0.8294 - val_loss: 0.3795 - val_acc: 0.8275\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3800 - acc: 0.8270 - val_loss: 0.3938 - val_acc: 0.8219\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3776 - acc: 0.8294 - val_loss: 0.3777 - val_acc: 0.8256\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3776 - acc: 0.8286 - val_loss: 0.3779 - val_acc: 0.8294\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3752 - acc: 0.8313 - val_loss: 0.3848 - val_acc: 0.8256\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3741 - acc: 0.8307 - val_loss: 0.3788 - val_acc: 0.8271\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3746 - acc: 0.8320 - val_loss: 0.3805 - val_acc: 0.8269\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3732 - acc: 0.8303 - val_loss: 0.3788 - val_acc: 0.8271\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 280us/sample - loss: 0.5588 - acc: 0.7286 - val_loss: 0.4777 - val_acc: 0.7724\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4840 - acc: 0.7690 - val_loss: 0.4674 - val_acc: 0.7754\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4634 - acc: 0.7831 - val_loss: 0.4426 - val_acc: 0.7923\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4485 - acc: 0.7906 - val_loss: 0.4303 - val_acc: 0.7974\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4361 - acc: 0.7992 - val_loss: 0.4192 - val_acc: 0.8065\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4292 - acc: 0.8029 - val_loss: 0.4146 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4228 - acc: 0.8060 - val_loss: 0.4242 - val_acc: 0.8017\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4184 - acc: 0.8071 - val_loss: 0.4035 - val_acc: 0.8141\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4126 - acc: 0.8116 - val_loss: 0.3986 - val_acc: 0.8191\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4088 - acc: 0.8141 - val_loss: 0.4208 - val_acc: 0.8048\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4046 - acc: 0.8172 - val_loss: 0.4056 - val_acc: 0.8123\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4030 - acc: 0.8180 - val_loss: 0.3960 - val_acc: 0.8170\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4005 - acc: 0.8189 - val_loss: 0.3875 - val_acc: 0.8243\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3965 - acc: 0.8196 - val_loss: 0.3906 - val_acc: 0.8263\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3962 - acc: 0.8202 - val_loss: 0.4069 - val_acc: 0.8112\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3939 - acc: 0.8216 - val_loss: 0.4175 - val_acc: 0.8115\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3901 - acc: 0.8244 - val_loss: 0.3796 - val_acc: 0.8296\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3890 - acc: 0.8237 - val_loss: 0.3857 - val_acc: 0.8260\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3889 - acc: 0.8233 - val_loss: 0.3842 - val_acc: 0.8255\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3874 - acc: 0.8262 - val_loss: 0.3830 - val_acc: 0.8271\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3847 - acc: 0.8256 - val_loss: 0.4007 - val_acc: 0.8199\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3837 - acc: 0.8274 - val_loss: 0.3825 - val_acc: 0.8269\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3830 - acc: 0.8271 - val_loss: 0.4084 - val_acc: 0.8138\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3816 - acc: 0.8280 - val_loss: 0.3815 - val_acc: 0.8274\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3800 - acc: 0.8290 - val_loss: 0.3825 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3786 - acc: 0.8296 - val_loss: 0.3868 - val_acc: 0.8241\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3775 - acc: 0.8293 - val_loss: 0.3987 - val_acc: 0.8212\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3791 - acc: 0.8295 - val_loss: 0.3760 - val_acc: 0.8295\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3796 - acc: 0.8281 - val_loss: 0.3859 - val_acc: 0.8242\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3776 - acc: 0.8304 - val_loss: 0.4096 - val_acc: 0.8130\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 265us/sample - loss: 0.5177 - acc: 0.7494 - val_loss: 0.4529 - val_acc: 0.7872\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4541 - acc: 0.7869 - val_loss: 0.4710 - val_acc: 0.7801\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4338 - acc: 0.7967 - val_loss: 0.4168 - val_acc: 0.8075\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4189 - acc: 0.8075 - val_loss: 0.4065 - val_acc: 0.8141\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4101 - acc: 0.8121 - val_loss: 0.4146 - val_acc: 0.8084\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4034 - acc: 0.8170 - val_loss: 0.3968 - val_acc: 0.8195\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3974 - acc: 0.8201 - val_loss: 0.3928 - val_acc: 0.8210\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3897 - acc: 0.8239 - val_loss: 0.4043 - val_acc: 0.8137\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3852 - acc: 0.8266 - val_loss: 0.4058 - val_acc: 0.8138\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3798 - acc: 0.8292 - val_loss: 0.3904 - val_acc: 0.8220\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3758 - acc: 0.8309 - val_loss: 0.3995 - val_acc: 0.8167\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3704 - acc: 0.8343 - val_loss: 0.3802 - val_acc: 0.8257\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3640 - acc: 0.8365 - val_loss: 0.3942 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3614 - acc: 0.8386 - val_loss: 0.3911 - val_acc: 0.8227\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3573 - acc: 0.8405 - val_loss: 0.3851 - val_acc: 0.8278\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3525 - acc: 0.8428 - val_loss: 0.3912 - val_acc: 0.8228\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3467 - acc: 0.8457 - val_loss: 0.3819 - val_acc: 0.8260\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3446 - acc: 0.8478 - val_loss: 0.3935 - val_acc: 0.8237\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3400 - acc: 0.8483 - val_loss: 0.3945 - val_acc: 0.8209\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3358 - acc: 0.8510 - val_loss: 0.3976 - val_acc: 0.8201\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3321 - acc: 0.8529 - val_loss: 0.3948 - val_acc: 0.8224\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3264 - acc: 0.8564 - val_loss: 0.4058 - val_acc: 0.8215\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3251 - acc: 0.8566 - val_loss: 0.3877 - val_acc: 0.8287\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3188 - acc: 0.8591 - val_loss: 0.3951 - val_acc: 0.8268\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3161 - acc: 0.8593 - val_loss: 0.3948 - val_acc: 0.8217\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3118 - acc: 0.8616 - val_loss: 0.3988 - val_acc: 0.8246\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3091 - acc: 0.8642 - val_loss: 0.4065 - val_acc: 0.8274\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.3031 - acc: 0.8659 - val_loss: 0.4038 - val_acc: 0.8249\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3053 - acc: 0.8646 - val_loss: 0.4016 - val_acc: 0.8257\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3014 - acc: 0.8673 - val_loss: 0.4108 - val_acc: 0.8245\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 268us/sample - loss: 0.5142 - acc: 0.7510 - val_loss: 0.4468 - val_acc: 0.7901\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4519 - acc: 0.7876 - val_loss: 0.4502 - val_acc: 0.7876\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4308 - acc: 0.8010 - val_loss: 0.4191 - val_acc: 0.8036\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4205 - acc: 0.8067 - val_loss: 0.4077 - val_acc: 0.8121\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4103 - acc: 0.8130 - val_loss: 0.4023 - val_acc: 0.8184\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4015 - acc: 0.8163 - val_loss: 0.3950 - val_acc: 0.8225\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3964 - acc: 0.8199 - val_loss: 0.3965 - val_acc: 0.8177\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3914 - acc: 0.8233 - val_loss: 0.3921 - val_acc: 0.8239\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3836 - acc: 0.8271 - val_loss: 0.3976 - val_acc: 0.8185\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3790 - acc: 0.8296 - val_loss: 0.3870 - val_acc: 0.8249\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3728 - acc: 0.8331 - val_loss: 0.3936 - val_acc: 0.8197\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3677 - acc: 0.8362 - val_loss: 0.3875 - val_acc: 0.8252\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3647 - acc: 0.8360 - val_loss: 0.3851 - val_acc: 0.8267\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3593 - acc: 0.8386 - val_loss: 0.3857 - val_acc: 0.8253\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3555 - acc: 0.8412 - val_loss: 0.3806 - val_acc: 0.8286\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3505 - acc: 0.8436 - val_loss: 0.3845 - val_acc: 0.8260\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3476 - acc: 0.8462 - val_loss: 0.3890 - val_acc: 0.8228\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3424 - acc: 0.8472 - val_loss: 0.3908 - val_acc: 0.8270\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3393 - acc: 0.8493 - val_loss: 0.4039 - val_acc: 0.8180\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3365 - acc: 0.8508 - val_loss: 0.3938 - val_acc: 0.8278\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3329 - acc: 0.8532 - val_loss: 0.3917 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3284 - acc: 0.8543 - val_loss: 0.4001 - val_acc: 0.8214\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3247 - acc: 0.8562 - val_loss: 0.3929 - val_acc: 0.8234\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3191 - acc: 0.8590 - val_loss: 0.4097 - val_acc: 0.8156\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3178 - acc: 0.8604 - val_loss: 0.3937 - val_acc: 0.8281\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3136 - acc: 0.8621 - val_loss: 0.3964 - val_acc: 0.8268\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3082 - acc: 0.8642 - val_loss: 0.3953 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3068 - acc: 0.8651 - val_loss: 0.3980 - val_acc: 0.8255\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3030 - acc: 0.8668 - val_loss: 0.3957 - val_acc: 0.8237\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3002 - acc: 0.8681 - val_loss: 0.4093 - val_acc: 0.8212\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 281us/sample - loss: 0.5315 - acc: 0.7450 - val_loss: 0.4551 - val_acc: 0.7856\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4562 - acc: 0.7859 - val_loss: 0.4283 - val_acc: 0.8016\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4341 - acc: 0.7992 - val_loss: 0.4233 - val_acc: 0.8033\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4223 - acc: 0.8055 - val_loss: 0.4167 - val_acc: 0.8110\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4103 - acc: 0.8130 - val_loss: 0.4042 - val_acc: 0.8169\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4047 - acc: 0.8151 - val_loss: 0.4142 - val_acc: 0.8089\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3973 - acc: 0.8191 - val_loss: 0.4411 - val_acc: 0.7976\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3917 - acc: 0.8225 - val_loss: 0.3897 - val_acc: 0.8198\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3858 - acc: 0.8256 - val_loss: 0.3895 - val_acc: 0.8226\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3822 - acc: 0.8276 - val_loss: 0.4043 - val_acc: 0.8188\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3759 - acc: 0.8321 - val_loss: 0.3819 - val_acc: 0.8252\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3721 - acc: 0.8332 - val_loss: 0.3781 - val_acc: 0.8279\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3682 - acc: 0.8351 - val_loss: 0.3911 - val_acc: 0.8232\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3653 - acc: 0.8373 - val_loss: 0.3759 - val_acc: 0.8299\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3631 - acc: 0.8382 - val_loss: 0.3899 - val_acc: 0.8224\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3594 - acc: 0.8404 - val_loss: 0.3735 - val_acc: 0.8310\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3549 - acc: 0.8420 - val_loss: 0.3851 - val_acc: 0.8245\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3518 - acc: 0.8438 - val_loss: 0.3756 - val_acc: 0.8310\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3490 - acc: 0.8439 - val_loss: 0.3942 - val_acc: 0.8203\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3460 - acc: 0.8445 - val_loss: 0.3741 - val_acc: 0.8328\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3442 - acc: 0.8483 - val_loss: 0.3918 - val_acc: 0.8288\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3422 - acc: 0.8472 - val_loss: 0.3790 - val_acc: 0.8300\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3398 - acc: 0.8503 - val_loss: 0.3771 - val_acc: 0.8294\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3362 - acc: 0.8511 - val_loss: 0.3814 - val_acc: 0.8293\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3345 - acc: 0.8510 - val_loss: 0.3723 - val_acc: 0.8344\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3308 - acc: 0.8542 - val_loss: 0.4104 - val_acc: 0.8206\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3274 - acc: 0.8546 - val_loss: 0.3816 - val_acc: 0.8309\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3272 - acc: 0.8553 - val_loss: 0.3896 - val_acc: 0.8295\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3252 - acc: 0.8564 - val_loss: 0.3859 - val_acc: 0.8288\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3236 - acc: 0.8566 - val_loss: 0.3895 - val_acc: 0.8291\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 293us/sample - loss: 0.5387 - acc: 0.7398 - val_loss: 0.4624 - val_acc: 0.7802\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4626 - acc: 0.7827 - val_loss: 0.4429 - val_acc: 0.7947\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.4400 - acc: 0.7945 - val_loss: 0.4183 - val_acc: 0.8056\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4252 - acc: 0.8042 - val_loss: 0.4152 - val_acc: 0.8076\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4149 - acc: 0.8100 - val_loss: 0.4474 - val_acc: 0.7917\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4085 - acc: 0.8133 - val_loss: 0.3970 - val_acc: 0.8196\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4005 - acc: 0.8166 - val_loss: 0.3948 - val_acc: 0.8202\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3943 - acc: 0.8226 - val_loss: 0.4072 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3885 - acc: 0.8254 - val_loss: 0.3887 - val_acc: 0.8219\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3826 - acc: 0.8270 - val_loss: 0.3886 - val_acc: 0.8234\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3794 - acc: 0.8298 - val_loss: 0.3769 - val_acc: 0.8302\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3751 - acc: 0.8322 - val_loss: 0.3830 - val_acc: 0.8235\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3705 - acc: 0.8345 - val_loss: 0.3794 - val_acc: 0.8273\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3658 - acc: 0.8374 - val_loss: 0.3943 - val_acc: 0.8194\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3639 - acc: 0.8369 - val_loss: 0.3807 - val_acc: 0.8285\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3596 - acc: 0.8391 - val_loss: 0.3781 - val_acc: 0.8293\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3585 - acc: 0.8410 - val_loss: 0.3728 - val_acc: 0.8326\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3545 - acc: 0.8417 - val_loss: 0.3747 - val_acc: 0.8316\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3505 - acc: 0.8441 - val_loss: 0.3875 - val_acc: 0.8270\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3493 - acc: 0.8449 - val_loss: 0.4019 - val_acc: 0.8184\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3452 - acc: 0.8464 - val_loss: 0.3883 - val_acc: 0.8256\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3417 - acc: 0.8500 - val_loss: 0.3888 - val_acc: 0.8243\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3403 - acc: 0.8494 - val_loss: 0.3865 - val_acc: 0.8255\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3393 - acc: 0.8497 - val_loss: 0.3764 - val_acc: 0.8298\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.3363 - acc: 0.8505 - val_loss: 0.3860 - val_acc: 0.8257\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3359 - acc: 0.8509 - val_loss: 0.3727 - val_acc: 0.8325\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3314 - acc: 0.8536 - val_loss: 0.3810 - val_acc: 0.8334\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3322 - acc: 0.8529 - val_loss: 0.3839 - val_acc: 0.8276\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3273 - acc: 0.8557 - val_loss: 0.3826 - val_acc: 0.8289\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3277 - acc: 0.8555 - val_loss: 0.3789 - val_acc: 0.8339\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 277us/sample - loss: 0.5120 - acc: 0.7531 - val_loss: 0.4602 - val_acc: 0.7806\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4480 - acc: 0.7902 - val_loss: 0.4251 - val_acc: 0.8037\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4291 - acc: 0.8021 - val_loss: 0.4255 - val_acc: 0.8026\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4164 - acc: 0.8079 - val_loss: 0.4196 - val_acc: 0.8072\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4092 - acc: 0.8126 - val_loss: 0.4804 - val_acc: 0.7725\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3998 - acc: 0.8180 - val_loss: 0.4024 - val_acc: 0.8164\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3913 - acc: 0.8224 - val_loss: 0.4056 - val_acc: 0.8140\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3859 - acc: 0.8252 - val_loss: 0.3944 - val_acc: 0.8187\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3790 - acc: 0.8302 - val_loss: 0.3888 - val_acc: 0.8232\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3744 - acc: 0.8316 - val_loss: 0.3827 - val_acc: 0.8260\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3685 - acc: 0.8344 - val_loss: 0.3850 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3650 - acc: 0.8357 - val_loss: 0.4051 - val_acc: 0.8104\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3583 - acc: 0.8404 - val_loss: 0.3945 - val_acc: 0.8178\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3521 - acc: 0.8424 - val_loss: 0.3870 - val_acc: 0.8244\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3480 - acc: 0.8454 - val_loss: 0.3859 - val_acc: 0.8253\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.3418 - acc: 0.8481 - val_loss: 0.3895 - val_acc: 0.8263\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3390 - acc: 0.8510 - val_loss: 0.4143 - val_acc: 0.8164\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3343 - acc: 0.8527 - val_loss: 0.3891 - val_acc: 0.8231\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.3284 - acc: 0.8554 - val_loss: 0.3999 - val_acc: 0.8196\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3264 - acc: 0.8567 - val_loss: 0.4003 - val_acc: 0.8252\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3225 - acc: 0.8580 - val_loss: 0.4003 - val_acc: 0.8172\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3163 - acc: 0.8613 - val_loss: 0.4149 - val_acc: 0.8186\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3120 - acc: 0.8629 - val_loss: 0.4078 - val_acc: 0.8209\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3083 - acc: 0.8654 - val_loss: 0.3964 - val_acc: 0.8220\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3018 - acc: 0.8675 - val_loss: 0.4011 - val_acc: 0.8220\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.2988 - acc: 0.8700 - val_loss: 0.4087 - val_acc: 0.8232\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.2964 - acc: 0.8718 - val_loss: 0.4077 - val_acc: 0.8238\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.2917 - acc: 0.8745 - val_loss: 0.4125 - val_acc: 0.8134\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.2896 - acc: 0.8744 - val_loss: 0.4322 - val_acc: 0.8212\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.2828 - acc: 0.8773 - val_loss: 0.4339 - val_acc: 0.8152\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 280us/sample - loss: 0.5119 - acc: 0.7512 - val_loss: 0.4996 - val_acc: 0.7530\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4494 - acc: 0.7891 - val_loss: 0.4597 - val_acc: 0.7782\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4290 - acc: 0.8023 - val_loss: 0.4259 - val_acc: 0.8040\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4185 - acc: 0.8080 - val_loss: 0.4034 - val_acc: 0.8164\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4077 - acc: 0.8134 - val_loss: 0.3991 - val_acc: 0.8158\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3997 - acc: 0.8180 - val_loss: 0.4009 - val_acc: 0.8163\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3937 - acc: 0.8213 - val_loss: 0.4004 - val_acc: 0.8149\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3863 - acc: 0.8249 - val_loss: 0.4076 - val_acc: 0.8120\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3828 - acc: 0.8288 - val_loss: 0.4167 - val_acc: 0.8117\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3752 - acc: 0.8309 - val_loss: 0.4453 - val_acc: 0.7937\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3697 - acc: 0.8349 - val_loss: 0.3797 - val_acc: 0.8275\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3638 - acc: 0.8374 - val_loss: 0.3904 - val_acc: 0.8231\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3607 - acc: 0.8391 - val_loss: 0.3864 - val_acc: 0.8223\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3556 - acc: 0.8418 - val_loss: 0.4017 - val_acc: 0.8164\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3504 - acc: 0.8438 - val_loss: 0.3824 - val_acc: 0.8260\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3456 - acc: 0.8468 - val_loss: 0.4008 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3445 - acc: 0.8457 - val_loss: 0.3869 - val_acc: 0.8225\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3362 - acc: 0.8507 - val_loss: 0.4209 - val_acc: 0.8156\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3316 - acc: 0.8539 - val_loss: 0.3934 - val_acc: 0.8202\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3269 - acc: 0.8560 - val_loss: 0.3940 - val_acc: 0.8246\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3213 - acc: 0.8568 - val_loss: 0.3941 - val_acc: 0.8270\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.3169 - acc: 0.8602 - val_loss: 0.4002 - val_acc: 0.8209\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3150 - acc: 0.8618 - val_loss: 0.3936 - val_acc: 0.8278\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3081 - acc: 0.8651 - val_loss: 0.4199 - val_acc: 0.8187\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3075 - acc: 0.8652 - val_loss: 0.4182 - val_acc: 0.8181\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3006 - acc: 0.8686 - val_loss: 0.4270 - val_acc: 0.8198\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.2989 - acc: 0.8699 - val_loss: 0.4002 - val_acc: 0.8217\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.2936 - acc: 0.8727 - val_loss: 0.4228 - val_acc: 0.8203\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.2919 - acc: 0.8720 - val_loss: 0.4089 - val_acc: 0.8242\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.2860 - acc: 0.8754 - val_loss: 0.4199 - val_acc: 0.8209\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 293us/sample - loss: 0.5105 - acc: 0.7571 - val_loss: 0.4486 - val_acc: 0.7910\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.4471 - acc: 0.7909 - val_loss: 0.4815 - val_acc: 0.7661\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.4265 - acc: 0.8041 - val_loss: 0.4098 - val_acc: 0.8121\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.4137 - acc: 0.8112 - val_loss: 0.4917 - val_acc: 0.7630\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.4050 - acc: 0.8152 - val_loss: 0.3975 - val_acc: 0.8172\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3986 - acc: 0.8202 - val_loss: 0.4034 - val_acc: 0.8135\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3900 - acc: 0.8234 - val_loss: 0.3905 - val_acc: 0.8213\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3838 - acc: 0.8284 - val_loss: 0.3846 - val_acc: 0.8234\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3777 - acc: 0.8313 - val_loss: 0.3908 - val_acc: 0.8184\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3705 - acc: 0.8340 - val_loss: 0.3895 - val_acc: 0.8206\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3662 - acc: 0.8367 - val_loss: 0.3774 - val_acc: 0.8266\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3612 - acc: 0.8403 - val_loss: 0.3951 - val_acc: 0.8212\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3560 - acc: 0.8415 - val_loss: 0.3871 - val_acc: 0.8254\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3506 - acc: 0.8436 - val_loss: 0.3800 - val_acc: 0.8308\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3444 - acc: 0.8477 - val_loss: 0.3827 - val_acc: 0.8279\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3400 - acc: 0.8492 - val_loss: 0.3827 - val_acc: 0.8275\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3370 - acc: 0.8518 - val_loss: 0.3866 - val_acc: 0.8306\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3309 - acc: 0.8547 - val_loss: 0.3846 - val_acc: 0.8237\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3271 - acc: 0.8560 - val_loss: 0.3879 - val_acc: 0.8231\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3223 - acc: 0.8588 - val_loss: 0.3827 - val_acc: 0.8299\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3172 - acc: 0.8609 - val_loss: 0.3827 - val_acc: 0.8256\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3150 - acc: 0.8620 - val_loss: 0.3920 - val_acc: 0.8241\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3098 - acc: 0.8647 - val_loss: 0.3937 - val_acc: 0.8272\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3084 - acc: 0.8647 - val_loss: 0.3929 - val_acc: 0.8268\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3045 - acc: 0.8658 - val_loss: 0.3916 - val_acc: 0.8298\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3002 - acc: 0.8687 - val_loss: 0.3936 - val_acc: 0.8302\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.2957 - acc: 0.8705 - val_loss: 0.4165 - val_acc: 0.8215\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.2915 - acc: 0.8740 - val_loss: 0.3932 - val_acc: 0.8274\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.2892 - acc: 0.8743 - val_loss: 0.4096 - val_acc: 0.8260\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.2871 - acc: 0.8742 - val_loss: 0.4139 - val_acc: 0.8271\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 308us/sample - loss: 0.5196 - acc: 0.7514 - val_loss: 0.4604 - val_acc: 0.7844\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4507 - acc: 0.7888 - val_loss: 0.4308 - val_acc: 0.7984\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4305 - acc: 0.8009 - val_loss: 0.4138 - val_acc: 0.8098\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4154 - acc: 0.8085 - val_loss: 0.4041 - val_acc: 0.8166\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4052 - acc: 0.8154 - val_loss: 0.4625 - val_acc: 0.7954\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3976 - acc: 0.8197 - val_loss: 0.4012 - val_acc: 0.8166\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3892 - acc: 0.8239 - val_loss: 0.3898 - val_acc: 0.8213\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3833 - acc: 0.8276 - val_loss: 0.3852 - val_acc: 0.8275\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3767 - acc: 0.8297 - val_loss: 0.4394 - val_acc: 0.8029\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3704 - acc: 0.8342 - val_loss: 0.3778 - val_acc: 0.8303\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3651 - acc: 0.8376 - val_loss: 0.3904 - val_acc: 0.8253\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3612 - acc: 0.8378 - val_loss: 0.3845 - val_acc: 0.8260\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3550 - acc: 0.8416 - val_loss: 0.3756 - val_acc: 0.8324\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3490 - acc: 0.8438 - val_loss: 0.3793 - val_acc: 0.8292\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3448 - acc: 0.8456 - val_loss: 0.4112 - val_acc: 0.8156\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3397 - acc: 0.8482 - val_loss: 0.3795 - val_acc: 0.8301\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3348 - acc: 0.8522 - val_loss: 0.3894 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3319 - acc: 0.8539 - val_loss: 0.3800 - val_acc: 0.8353\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3246 - acc: 0.8567 - val_loss: 0.3830 - val_acc: 0.8302\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3221 - acc: 0.8593 - val_loss: 0.3822 - val_acc: 0.8304\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3184 - acc: 0.8599 - val_loss: 0.3834 - val_acc: 0.8293\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3132 - acc: 0.8621 - val_loss: 0.3839 - val_acc: 0.8309\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3101 - acc: 0.8629 - val_loss: 0.3994 - val_acc: 0.8309\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3061 - acc: 0.8667 - val_loss: 0.4026 - val_acc: 0.8212\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3025 - acc: 0.8686 - val_loss: 0.3943 - val_acc: 0.8295\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3005 - acc: 0.8693 - val_loss: 0.3942 - val_acc: 0.8268\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.2984 - acc: 0.8699 - val_loss: 0.4029 - val_acc: 0.8302\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.2919 - acc: 0.8725 - val_loss: 0.4178 - val_acc: 0.8237\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.2924 - acc: 0.8726 - val_loss: 0.4157 - val_acc: 0.8127\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.2878 - acc: 0.8736 - val_loss: 0.4176 - val_acc: 0.8288\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 22s 260us/sample - loss: 0.5740 - acc: 0.7174 - val_loss: 0.4934 - val_acc: 0.7634\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.5013 - acc: 0.7560 - val_loss: 0.4732 - val_acc: 0.7737\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4862 - acc: 0.7663 - val_loss: 0.4831 - val_acc: 0.7656\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4761 - acc: 0.7730 - val_loss: 0.4654 - val_acc: 0.7797\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4681 - acc: 0.7777 - val_loss: 0.4821 - val_acc: 0.7668\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4597 - acc: 0.7827 - val_loss: 0.4402 - val_acc: 0.7986\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4537 - acc: 0.7875 - val_loss: 0.4416 - val_acc: 0.7912\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4496 - acc: 0.7893 - val_loss: 0.4307 - val_acc: 0.7991\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4454 - acc: 0.7916 - val_loss: 0.4334 - val_acc: 0.7973\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4418 - acc: 0.7952 - val_loss: 0.4237 - val_acc: 0.8032\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4388 - acc: 0.7959 - val_loss: 0.4308 - val_acc: 0.8023\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4370 - acc: 0.7975 - val_loss: 0.4267 - val_acc: 0.8041\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4362 - acc: 0.7976 - val_loss: 0.4150 - val_acc: 0.8094\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4354 - acc: 0.7977 - val_loss: 0.4263 - val_acc: 0.8052\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4355 - acc: 0.7970 - val_loss: 0.4291 - val_acc: 0.7993\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4339 - acc: 0.7988 - val_loss: 0.4157 - val_acc: 0.8098\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4311 - acc: 0.7993 - val_loss: 0.4128 - val_acc: 0.8113\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4327 - acc: 0.7998 - val_loss: 0.4143 - val_acc: 0.8094\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4297 - acc: 0.8019 - val_loss: 0.4173 - val_acc: 0.8096\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4297 - acc: 0.8017 - val_loss: 0.4173 - val_acc: 0.8082\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4323 - acc: 0.7994 - val_loss: 0.4100 - val_acc: 0.8117\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4285 - acc: 0.8012 - val_loss: 0.4135 - val_acc: 0.8115\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4297 - acc: 0.8022 - val_loss: 0.4183 - val_acc: 0.8067\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4275 - acc: 0.8033 - val_loss: 0.4080 - val_acc: 0.8134\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4272 - acc: 0.8024 - val_loss: 0.4195 - val_acc: 0.8090\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4273 - acc: 0.8015 - val_loss: 0.4681 - val_acc: 0.7743\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4272 - acc: 0.8025 - val_loss: 0.4088 - val_acc: 0.8130\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4274 - acc: 0.8023 - val_loss: 0.4119 - val_acc: 0.8145\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4258 - acc: 0.8022 - val_loss: 0.4108 - val_acc: 0.8100\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4263 - acc: 0.8025 - val_loss: 0.4078 - val_acc: 0.8139\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 273us/sample - loss: 0.5870 - acc: 0.7082 - val_loss: 0.5305 - val_acc: 0.7375\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.5057 - acc: 0.7537 - val_loss: 0.4772 - val_acc: 0.7707\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4899 - acc: 0.7640 - val_loss: 0.4666 - val_acc: 0.7783\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4794 - acc: 0.7688 - val_loss: 0.4588 - val_acc: 0.7822\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4680 - acc: 0.7765 - val_loss: 0.4746 - val_acc: 0.7675\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4593 - acc: 0.7832 - val_loss: 0.4390 - val_acc: 0.7944\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4527 - acc: 0.7876 - val_loss: 0.4343 - val_acc: 0.7959\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4478 - acc: 0.7897 - val_loss: 0.4456 - val_acc: 0.7890\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4448 - acc: 0.7909 - val_loss: 0.4228 - val_acc: 0.8038\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4438 - acc: 0.7935 - val_loss: 0.4436 - val_acc: 0.7877\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4391 - acc: 0.7952 - val_loss: 0.4226 - val_acc: 0.8062\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4379 - acc: 0.7961 - val_loss: 0.4167 - val_acc: 0.8108\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4363 - acc: 0.7965 - val_loss: 0.4160 - val_acc: 0.8095\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4360 - acc: 0.7980 - val_loss: 0.4140 - val_acc: 0.8116\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4359 - acc: 0.7976 - val_loss: 0.4132 - val_acc: 0.8122\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4346 - acc: 0.7987 - val_loss: 0.4148 - val_acc: 0.8094\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4345 - acc: 0.7979 - val_loss: 0.4176 - val_acc: 0.8102\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4315 - acc: 0.7998 - val_loss: 0.4157 - val_acc: 0.8088\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4322 - acc: 0.7994 - val_loss: 0.4129 - val_acc: 0.8121\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4315 - acc: 0.7993 - val_loss: 0.4095 - val_acc: 0.8114\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4313 - acc: 0.7998 - val_loss: 0.4148 - val_acc: 0.8101\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4299 - acc: 0.8013 - val_loss: 0.4113 - val_acc: 0.8137\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4280 - acc: 0.8018 - val_loss: 0.4131 - val_acc: 0.8117\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4282 - acc: 0.8030 - val_loss: 0.4295 - val_acc: 0.7991\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4276 - acc: 0.8030 - val_loss: 0.4200 - val_acc: 0.8048\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4264 - acc: 0.8036 - val_loss: 0.4088 - val_acc: 0.8130\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4250 - acc: 0.8047 - val_loss: 0.4148 - val_acc: 0.8067\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4266 - acc: 0.8020 - val_loss: 0.4052 - val_acc: 0.8152\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4264 - acc: 0.8041 - val_loss: 0.4121 - val_acc: 0.8083\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4247 - acc: 0.8035 - val_loss: 0.4055 - val_acc: 0.8147\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 284us/sample - loss: 0.5859 - acc: 0.7086 - val_loss: 0.4957 - val_acc: 0.7622\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.5113 - acc: 0.7527 - val_loss: 0.4819 - val_acc: 0.7689\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4942 - acc: 0.7623 - val_loss: 0.4815 - val_acc: 0.7721\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4807 - acc: 0.7704 - val_loss: 0.4582 - val_acc: 0.7818\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4700 - acc: 0.7781 - val_loss: 0.4511 - val_acc: 0.7870\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4613 - acc: 0.7822 - val_loss: 0.4399 - val_acc: 0.7987\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4522 - acc: 0.7896 - val_loss: 0.4316 - val_acc: 0.8011\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4484 - acc: 0.7916 - val_loss: 0.4324 - val_acc: 0.8030\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4454 - acc: 0.7942 - val_loss: 0.4220 - val_acc: 0.8080\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4439 - acc: 0.7945 - val_loss: 0.4186 - val_acc: 0.8074\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.4396 - acc: 0.7958 - val_loss: 0.4181 - val_acc: 0.8073\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4396 - acc: 0.7962 - val_loss: 0.4173 - val_acc: 0.8099\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4381 - acc: 0.7971 - val_loss: 0.4167 - val_acc: 0.8078\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4362 - acc: 0.7981 - val_loss: 0.4147 - val_acc: 0.8099\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4356 - acc: 0.7985 - val_loss: 0.4182 - val_acc: 0.8086\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.4351 - acc: 0.7990 - val_loss: 0.4116 - val_acc: 0.8133\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4333 - acc: 0.8002 - val_loss: 0.4151 - val_acc: 0.8106\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.4348 - acc: 0.7999 - val_loss: 0.4286 - val_acc: 0.7982\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4309 - acc: 0.8007 - val_loss: 0.4128 - val_acc: 0.8088\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4312 - acc: 0.8007 - val_loss: 0.4222 - val_acc: 0.8041\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4309 - acc: 0.8011 - val_loss: 0.4081 - val_acc: 0.8140\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4296 - acc: 0.8021 - val_loss: 0.4062 - val_acc: 0.8130\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4296 - acc: 0.8030 - val_loss: 0.4099 - val_acc: 0.8114\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4288 - acc: 0.8027 - val_loss: 0.4161 - val_acc: 0.8079\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4283 - acc: 0.8019 - val_loss: 0.4064 - val_acc: 0.8141\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4287 - acc: 0.8037 - val_loss: 0.4066 - val_acc: 0.8127\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4270 - acc: 0.8033 - val_loss: 0.4109 - val_acc: 0.8128\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4272 - acc: 0.8032 - val_loss: 0.4084 - val_acc: 0.8106\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.4269 - acc: 0.8026 - val_loss: 0.4058 - val_acc: 0.8137\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4281 - acc: 0.8025 - val_loss: 0.4062 - val_acc: 0.8124\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 311us/sample - loss: 0.6008 - acc: 0.6945 - val_loss: 0.4991 - val_acc: 0.7607\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.5149 - acc: 0.7508 - val_loss: 0.4880 - val_acc: 0.7656\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.4975 - acc: 0.7626 - val_loss: 0.4688 - val_acc: 0.7781\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4859 - acc: 0.7688 - val_loss: 0.4577 - val_acc: 0.7840\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4764 - acc: 0.7745 - val_loss: 0.4521 - val_acc: 0.7903\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.4682 - acc: 0.7808 - val_loss: 0.4476 - val_acc: 0.7911\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4616 - acc: 0.7846 - val_loss: 0.4365 - val_acc: 0.7959\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4594 - acc: 0.7853 - val_loss: 0.4367 - val_acc: 0.7952\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4561 - acc: 0.7868 - val_loss: 0.4372 - val_acc: 0.7958\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4521 - acc: 0.7881 - val_loss: 0.4301 - val_acc: 0.8034\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4498 - acc: 0.7909 - val_loss: 0.4323 - val_acc: 0.7994\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4460 - acc: 0.7926 - val_loss: 0.4387 - val_acc: 0.7972\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4456 - acc: 0.7933 - val_loss: 0.4213 - val_acc: 0.8038\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4423 - acc: 0.7955 - val_loss: 0.4157 - val_acc: 0.8089\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4430 - acc: 0.7947 - val_loss: 0.4210 - val_acc: 0.8061\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4402 - acc: 0.7962 - val_loss: 0.4151 - val_acc: 0.8087\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4386 - acc: 0.7971 - val_loss: 0.4224 - val_acc: 0.8063\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4374 - acc: 0.7979 - val_loss: 0.4150 - val_acc: 0.8080\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4369 - acc: 0.7984 - val_loss: 0.4222 - val_acc: 0.8032\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4378 - acc: 0.7985 - val_loss: 0.4255 - val_acc: 0.8057\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4339 - acc: 0.7999 - val_loss: 0.4160 - val_acc: 0.8094\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4334 - acc: 0.8000 - val_loss: 0.4191 - val_acc: 0.8056\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4325 - acc: 0.8004 - val_loss: 0.4107 - val_acc: 0.8072\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4325 - acc: 0.8009 - val_loss: 0.4238 - val_acc: 0.8069\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4314 - acc: 0.8023 - val_loss: 0.4252 - val_acc: 0.7979\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4313 - acc: 0.8009 - val_loss: 0.4074 - val_acc: 0.8135\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4307 - acc: 0.8012 - val_loss: 0.4034 - val_acc: 0.8132\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4298 - acc: 0.8022 - val_loss: 0.4181 - val_acc: 0.8067\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4301 - acc: 0.8010 - val_loss: 0.4206 - val_acc: 0.8027\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4279 - acc: 0.8023 - val_loss: 0.4029 - val_acc: 0.8139\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 267us/sample - loss: 0.5207 - acc: 0.7426 - val_loss: 0.5758 - val_acc: 0.7054\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4712 - acc: 0.7749 - val_loss: 0.4461 - val_acc: 0.7936\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4528 - acc: 0.7876 - val_loss: 0.4466 - val_acc: 0.7861\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4405 - acc: 0.7934 - val_loss: 0.4269 - val_acc: 0.8013\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4332 - acc: 0.7984 - val_loss: 0.4234 - val_acc: 0.8048\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4240 - acc: 0.8042 - val_loss: 0.4228 - val_acc: 0.8091\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4202 - acc: 0.8054 - val_loss: 0.4082 - val_acc: 0.8127\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4169 - acc: 0.8073 - val_loss: 0.4061 - val_acc: 0.8138\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4123 - acc: 0.8112 - val_loss: 0.4003 - val_acc: 0.8183\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4098 - acc: 0.8136 - val_loss: 0.4182 - val_acc: 0.8062\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4070 - acc: 0.8146 - val_loss: 0.4098 - val_acc: 0.8135\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4072 - acc: 0.8140 - val_loss: 0.4210 - val_acc: 0.8039\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4041 - acc: 0.8155 - val_loss: 0.3959 - val_acc: 0.8189\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4029 - acc: 0.8172 - val_loss: 0.3938 - val_acc: 0.8206\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4006 - acc: 0.8179 - val_loss: 0.3993 - val_acc: 0.8181\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3989 - acc: 0.8197 - val_loss: 0.3879 - val_acc: 0.8244\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3967 - acc: 0.8211 - val_loss: 0.4178 - val_acc: 0.8037\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3968 - acc: 0.8204 - val_loss: 0.3907 - val_acc: 0.8221\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3944 - acc: 0.8219 - val_loss: 0.3968 - val_acc: 0.8189\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3934 - acc: 0.8216 - val_loss: 0.3899 - val_acc: 0.8221\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3938 - acc: 0.8222 - val_loss: 0.3871 - val_acc: 0.8241\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3913 - acc: 0.8230 - val_loss: 0.4028 - val_acc: 0.8129\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3926 - acc: 0.8233 - val_loss: 0.3986 - val_acc: 0.8156\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3920 - acc: 0.8223 - val_loss: 0.3869 - val_acc: 0.8215\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3910 - acc: 0.8246 - val_loss: 0.3975 - val_acc: 0.8181\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3897 - acc: 0.8245 - val_loss: 0.4090 - val_acc: 0.8102\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3898 - acc: 0.8249 - val_loss: 0.3855 - val_acc: 0.8237\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3891 - acc: 0.8251 - val_loss: 0.3981 - val_acc: 0.8153\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3871 - acc: 0.8250 - val_loss: 0.3915 - val_acc: 0.8187\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3863 - acc: 0.8249 - val_loss: 0.4007 - val_acc: 0.8175\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 268us/sample - loss: 0.5538 - acc: 0.7309 - val_loss: 0.4919 - val_acc: 0.7636\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4780 - acc: 0.7707 - val_loss: 0.4521 - val_acc: 0.7862\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4585 - acc: 0.7840 - val_loss: 0.4352 - val_acc: 0.7957\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4463 - acc: 0.7908 - val_loss: 0.4285 - val_acc: 0.8004\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4352 - acc: 0.7986 - val_loss: 0.4849 - val_acc: 0.7666\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4291 - acc: 0.8021 - val_loss: 0.4178 - val_acc: 0.8104\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4234 - acc: 0.8057 - val_loss: 0.4072 - val_acc: 0.8152\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4204 - acc: 0.8067 - val_loss: 0.4048 - val_acc: 0.8156\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4154 - acc: 0.8092 - val_loss: 0.4009 - val_acc: 0.8184\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4135 - acc: 0.8107 - val_loss: 0.4031 - val_acc: 0.8180\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4111 - acc: 0.8124 - val_loss: 0.3983 - val_acc: 0.8192\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4083 - acc: 0.8148 - val_loss: 0.3964 - val_acc: 0.8196\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4084 - acc: 0.8135 - val_loss: 0.3982 - val_acc: 0.8189\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4054 - acc: 0.8160 - val_loss: 0.3944 - val_acc: 0.8207\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4044 - acc: 0.8171 - val_loss: 0.3975 - val_acc: 0.8213\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4009 - acc: 0.8177 - val_loss: 0.3994 - val_acc: 0.8175\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3999 - acc: 0.8190 - val_loss: 0.3922 - val_acc: 0.8208\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3995 - acc: 0.8187 - val_loss: 0.3900 - val_acc: 0.8234\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3970 - acc: 0.8195 - val_loss: 0.3953 - val_acc: 0.8180\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3964 - acc: 0.8212 - val_loss: 0.4067 - val_acc: 0.8117\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3964 - acc: 0.8197 - val_loss: 0.4184 - val_acc: 0.8104\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3943 - acc: 0.8209 - val_loss: 0.3986 - val_acc: 0.8187\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3937 - acc: 0.8221 - val_loss: 0.3952 - val_acc: 0.8191\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3937 - acc: 0.8227 - val_loss: 0.4154 - val_acc: 0.8099\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3916 - acc: 0.8227 - val_loss: 0.3877 - val_acc: 0.8257\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3919 - acc: 0.8236 - val_loss: 0.4153 - val_acc: 0.8102\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3911 - acc: 0.8226 - val_loss: 0.3875 - val_acc: 0.8260\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3889 - acc: 0.8238 - val_loss: 0.3888 - val_acc: 0.8243\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3889 - acc: 0.8248 - val_loss: 0.3977 - val_acc: 0.8182\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3886 - acc: 0.8246 - val_loss: 0.3905 - val_acc: 0.8227\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 292us/sample - loss: 0.5377 - acc: 0.7390 - val_loss: 0.4836 - val_acc: 0.7686\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4721 - acc: 0.7749 - val_loss: 0.4433 - val_acc: 0.7909\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4511 - acc: 0.7884 - val_loss: 0.4334 - val_acc: 0.7976\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4365 - acc: 0.7971 - val_loss: 0.4548 - val_acc: 0.7857\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4275 - acc: 0.8028 - val_loss: 0.4143 - val_acc: 0.8104\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4218 - acc: 0.8055 - val_loss: 0.4129 - val_acc: 0.8096\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.4160 - acc: 0.8090 - val_loss: 0.4173 - val_acc: 0.8076\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4119 - acc: 0.8123 - val_loss: 0.4252 - val_acc: 0.8046\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4057 - acc: 0.8151 - val_loss: 0.4126 - val_acc: 0.8119\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4024 - acc: 0.8166 - val_loss: 0.4000 - val_acc: 0.8178\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3996 - acc: 0.8185 - val_loss: 0.3971 - val_acc: 0.8191\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.3952 - acc: 0.8215 - val_loss: 0.3988 - val_acc: 0.8187\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3928 - acc: 0.8217 - val_loss: 0.3966 - val_acc: 0.8199\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3917 - acc: 0.8223 - val_loss: 0.3929 - val_acc: 0.8224\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3889 - acc: 0.8251 - val_loss: 0.4161 - val_acc: 0.8109\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3860 - acc: 0.8262 - val_loss: 0.3923 - val_acc: 0.8218\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3856 - acc: 0.8262 - val_loss: 0.3870 - val_acc: 0.8229\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3831 - acc: 0.8277 - val_loss: 0.4033 - val_acc: 0.8158\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3823 - acc: 0.8283 - val_loss: 0.3912 - val_acc: 0.8231\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3802 - acc: 0.8286 - val_loss: 0.3989 - val_acc: 0.8178\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3794 - acc: 0.8295 - val_loss: 0.3940 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3759 - acc: 0.8306 - val_loss: 0.3905 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.3745 - acc: 0.8318 - val_loss: 0.3807 - val_acc: 0.8277\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3741 - acc: 0.8323 - val_loss: 0.3961 - val_acc: 0.8199\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3740 - acc: 0.8316 - val_loss: 0.3836 - val_acc: 0.8258\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3717 - acc: 0.8343 - val_loss: 0.3851 - val_acc: 0.8264\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3716 - acc: 0.8338 - val_loss: 0.3856 - val_acc: 0.8254\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3716 - acc: 0.8334 - val_loss: 0.3819 - val_acc: 0.8266\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3707 - acc: 0.8330 - val_loss: 0.3812 - val_acc: 0.8262\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3694 - acc: 0.8348 - val_loss: 0.3912 - val_acc: 0.8225\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 307us/sample - loss: 0.5322 - acc: 0.7404 - val_loss: 0.4732 - val_acc: 0.7708\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4730 - acc: 0.7749 - val_loss: 0.4480 - val_acc: 0.7887\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4541 - acc: 0.7871 - val_loss: 0.4340 - val_acc: 0.7965\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4406 - acc: 0.7951 - val_loss: 0.4234 - val_acc: 0.8026\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4291 - acc: 0.8022 - val_loss: 0.4285 - val_acc: 0.8041\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4212 - acc: 0.8072 - val_loss: 0.4060 - val_acc: 0.8130\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4164 - acc: 0.8093 - val_loss: 0.4095 - val_acc: 0.8100\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4122 - acc: 0.8118 - val_loss: 0.3990 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4051 - acc: 0.8165 - val_loss: 0.3963 - val_acc: 0.8183\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4020 - acc: 0.8162 - val_loss: 0.3939 - val_acc: 0.8212\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3987 - acc: 0.8194 - val_loss: 0.3901 - val_acc: 0.8228\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3938 - acc: 0.8226 - val_loss: 0.4015 - val_acc: 0.8191\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3924 - acc: 0.8228 - val_loss: 0.3892 - val_acc: 0.8224\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3910 - acc: 0.8230 - val_loss: 0.4011 - val_acc: 0.8163\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3874 - acc: 0.8246 - val_loss: 0.3878 - val_acc: 0.8195\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3849 - acc: 0.8268 - val_loss: 0.3872 - val_acc: 0.8238\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3844 - acc: 0.8265 - val_loss: 0.4110 - val_acc: 0.8108\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3826 - acc: 0.8281 - val_loss: 0.3865 - val_acc: 0.8252\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3800 - acc: 0.8288 - val_loss: 0.3786 - val_acc: 0.8286\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3780 - acc: 0.8297 - val_loss: 0.4080 - val_acc: 0.8121\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3771 - acc: 0.8300 - val_loss: 0.3819 - val_acc: 0.8291\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3758 - acc: 0.8305 - val_loss: 0.3818 - val_acc: 0.8263\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3744 - acc: 0.8322 - val_loss: 0.3756 - val_acc: 0.8295\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3711 - acc: 0.8338 - val_loss: 0.3898 - val_acc: 0.8216\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3715 - acc: 0.8342 - val_loss: 0.3861 - val_acc: 0.8243\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3712 - acc: 0.8345 - val_loss: 0.3960 - val_acc: 0.8203\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3696 - acc: 0.8343 - val_loss: 0.3779 - val_acc: 0.8267\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3693 - acc: 0.8343 - val_loss: 0.3792 - val_acc: 0.8263\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3659 - acc: 0.8365 - val_loss: 0.3812 - val_acc: 0.8293\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3679 - acc: 0.8358 - val_loss: 0.4038 - val_acc: 0.8184\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 279us/sample - loss: 0.5264 - acc: 0.7430 - val_loss: 0.4705 - val_acc: 0.7804\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4650 - acc: 0.7787 - val_loss: 0.4497 - val_acc: 0.7891\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4467 - acc: 0.7915 - val_loss: 0.4601 - val_acc: 0.7832\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4343 - acc: 0.7980 - val_loss: 0.4450 - val_acc: 0.7896\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4250 - acc: 0.8036 - val_loss: 0.4086 - val_acc: 0.8111\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4186 - acc: 0.8087 - val_loss: 0.4089 - val_acc: 0.8145\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4127 - acc: 0.8109 - val_loss: 0.4643 - val_acc: 0.7793\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4098 - acc: 0.8117 - val_loss: 0.4125 - val_acc: 0.8125\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4053 - acc: 0.8153 - val_loss: 0.4237 - val_acc: 0.8030\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4027 - acc: 0.8177 - val_loss: 0.4056 - val_acc: 0.8179\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3981 - acc: 0.8183 - val_loss: 0.4071 - val_acc: 0.8112\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3952 - acc: 0.8212 - val_loss: 0.4049 - val_acc: 0.8159\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3932 - acc: 0.8212 - val_loss: 0.3905 - val_acc: 0.8239\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3890 - acc: 0.8254 - val_loss: 0.3901 - val_acc: 0.8224\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3879 - acc: 0.8252 - val_loss: 0.3958 - val_acc: 0.8216\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3856 - acc: 0.8260 - val_loss: 0.3818 - val_acc: 0.8304\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3851 - acc: 0.8260 - val_loss: 0.3878 - val_acc: 0.8238\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3830 - acc: 0.8266 - val_loss: 0.3922 - val_acc: 0.8229\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3817 - acc: 0.8291 - val_loss: 0.3900 - val_acc: 0.8256\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3796 - acc: 0.8290 - val_loss: 0.3977 - val_acc: 0.8170\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3790 - acc: 0.8305 - val_loss: 0.3849 - val_acc: 0.8270\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3765 - acc: 0.8311 - val_loss: 0.3935 - val_acc: 0.8177\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3774 - acc: 0.8294 - val_loss: 0.3804 - val_acc: 0.8278\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3744 - acc: 0.8312 - val_loss: 0.3813 - val_acc: 0.8291\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3728 - acc: 0.8330 - val_loss: 0.3915 - val_acc: 0.8218\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3719 - acc: 0.8336 - val_loss: 0.3910 - val_acc: 0.8213\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3717 - acc: 0.8344 - val_loss: 0.3826 - val_acc: 0.8278\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3702 - acc: 0.8339 - val_loss: 0.3790 - val_acc: 0.8278\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3693 - acc: 0.8342 - val_loss: 0.3840 - val_acc: 0.8266\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3667 - acc: 0.8351 - val_loss: 0.3787 - val_acc: 0.8281\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 279us/sample - loss: 0.5247 - acc: 0.7449 - val_loss: 0.5270 - val_acc: 0.7448\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4666 - acc: 0.7778 - val_loss: 0.4519 - val_acc: 0.7838\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4473 - acc: 0.7898 - val_loss: 0.4319 - val_acc: 0.7983\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4349 - acc: 0.7974 - val_loss: 0.4316 - val_acc: 0.8024\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4251 - acc: 0.8037 - val_loss: 0.4150 - val_acc: 0.8067\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4186 - acc: 0.8078 - val_loss: 0.4186 - val_acc: 0.8069\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4140 - acc: 0.8107 - val_loss: 0.4029 - val_acc: 0.8159\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4089 - acc: 0.8141 - val_loss: 0.4051 - val_acc: 0.8108\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4048 - acc: 0.8160 - val_loss: 0.3972 - val_acc: 0.8202\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4027 - acc: 0.8177 - val_loss: 0.3996 - val_acc: 0.8174\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4001 - acc: 0.8185 - val_loss: 0.3947 - val_acc: 0.8213\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3965 - acc: 0.8206 - val_loss: 0.3966 - val_acc: 0.8213\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3929 - acc: 0.8225 - val_loss: 0.3953 - val_acc: 0.8229\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3918 - acc: 0.8239 - val_loss: 0.3895 - val_acc: 0.8242\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3904 - acc: 0.8252 - val_loss: 0.4196 - val_acc: 0.8053\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3865 - acc: 0.8261 - val_loss: 0.3865 - val_acc: 0.8235\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3857 - acc: 0.8258 - val_loss: 0.3969 - val_acc: 0.8178\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3845 - acc: 0.8270 - val_loss: 0.3887 - val_acc: 0.8245\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3818 - acc: 0.8284 - val_loss: 0.3984 - val_acc: 0.8181\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3809 - acc: 0.8280 - val_loss: 0.3818 - val_acc: 0.8242\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3778 - acc: 0.8310 - val_loss: 0.3813 - val_acc: 0.8275\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3761 - acc: 0.8305 - val_loss: 0.3841 - val_acc: 0.8260\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3759 - acc: 0.8320 - val_loss: 0.3832 - val_acc: 0.8247\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3751 - acc: 0.8320 - val_loss: 0.3830 - val_acc: 0.8267\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3722 - acc: 0.8341 - val_loss: 0.3811 - val_acc: 0.8271\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3713 - acc: 0.8341 - val_loss: 0.3783 - val_acc: 0.8283\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3707 - acc: 0.8327 - val_loss: 0.3825 - val_acc: 0.8252\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3704 - acc: 0.8345 - val_loss: 0.3795 - val_acc: 0.8283\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3674 - acc: 0.8368 - val_loss: 0.3810 - val_acc: 0.8278\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3667 - acc: 0.8365 - val_loss: 0.3946 - val_acc: 0.8209\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 303us/sample - loss: 0.5420 - acc: 0.7416 - val_loss: 0.4654 - val_acc: 0.7789\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4622 - acc: 0.7808 - val_loss: 0.4649 - val_acc: 0.7760\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4390 - acc: 0.7950 - val_loss: 0.4211 - val_acc: 0.8077\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4240 - acc: 0.8047 - val_loss: 0.4147 - val_acc: 0.8100\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4152 - acc: 0.8109 - val_loss: 0.4095 - val_acc: 0.8143\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4072 - acc: 0.8140 - val_loss: 0.3953 - val_acc: 0.8207\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4022 - acc: 0.8181 - val_loss: 0.4009 - val_acc: 0.8188\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3972 - acc: 0.8207 - val_loss: 0.4262 - val_acc: 0.8019\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3903 - acc: 0.8239 - val_loss: 0.3903 - val_acc: 0.8209\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3852 - acc: 0.8267 - val_loss: 0.3901 - val_acc: 0.8235\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.3825 - acc: 0.8288 - val_loss: 0.3816 - val_acc: 0.8278\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3745 - acc: 0.8330 - val_loss: 0.3825 - val_acc: 0.8255\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3724 - acc: 0.8330 - val_loss: 0.4176 - val_acc: 0.8059\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3701 - acc: 0.8347 - val_loss: 0.3892 - val_acc: 0.8246\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3646 - acc: 0.8377 - val_loss: 0.3827 - val_acc: 0.8270\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3614 - acc: 0.8386 - val_loss: 0.3809 - val_acc: 0.8274\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3561 - acc: 0.8405 - val_loss: 0.3882 - val_acc: 0.8274\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.3536 - acc: 0.8426 - val_loss: 0.3957 - val_acc: 0.8185\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3504 - acc: 0.8448 - val_loss: 0.3796 - val_acc: 0.8272\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3473 - acc: 0.8459 - val_loss: 0.3824 - val_acc: 0.8264\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3436 - acc: 0.8480 - val_loss: 0.4005 - val_acc: 0.8171\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3407 - acc: 0.8489 - val_loss: 0.3905 - val_acc: 0.8266\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3386 - acc: 0.8499 - val_loss: 0.3835 - val_acc: 0.8248\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3339 - acc: 0.8520 - val_loss: 0.3910 - val_acc: 0.8259\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3331 - acc: 0.8535 - val_loss: 0.3953 - val_acc: 0.8223\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3318 - acc: 0.8527 - val_loss: 0.3913 - val_acc: 0.8254\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3284 - acc: 0.8543 - val_loss: 0.3849 - val_acc: 0.8256\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3271 - acc: 0.8550 - val_loss: 0.3951 - val_acc: 0.8275\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3232 - acc: 0.8562 - val_loss: 0.3926 - val_acc: 0.8227\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.3196 - acc: 0.8587 - val_loss: 0.4031 - val_acc: 0.8234\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 317us/sample - loss: 0.5291 - acc: 0.7448 - val_loss: 0.5067 - val_acc: 0.7503\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4602 - acc: 0.7839 - val_loss: 0.4728 - val_acc: 0.7776\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.4386 - acc: 0.7965 - val_loss: 0.4448 - val_acc: 0.7917\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4233 - acc: 0.8048 - val_loss: 0.4103 - val_acc: 0.8130\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4133 - acc: 0.8122 - val_loss: 0.4333 - val_acc: 0.8017\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4036 - acc: 0.8161 - val_loss: 0.4044 - val_acc: 0.8130\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3983 - acc: 0.8196 - val_loss: 0.3944 - val_acc: 0.8213\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3907 - acc: 0.8227 - val_loss: 0.3905 - val_acc: 0.8249\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3844 - acc: 0.8282 - val_loss: 0.3860 - val_acc: 0.8251\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3807 - acc: 0.8293 - val_loss: 0.3846 - val_acc: 0.8265\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3758 - acc: 0.8318 - val_loss: 0.3777 - val_acc: 0.8273\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3698 - acc: 0.8349 - val_loss: 0.3906 - val_acc: 0.8224\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3664 - acc: 0.8357 - val_loss: 0.4211 - val_acc: 0.8017\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3627 - acc: 0.8386 - val_loss: 0.4216 - val_acc: 0.8065\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3576 - acc: 0.8411 - val_loss: 0.3827 - val_acc: 0.8243\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3515 - acc: 0.8434 - val_loss: 0.3895 - val_acc: 0.8254\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3509 - acc: 0.8441 - val_loss: 0.3828 - val_acc: 0.8300\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3437 - acc: 0.8474 - val_loss: 0.4066 - val_acc: 0.8140\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3438 - acc: 0.8473 - val_loss: 0.3826 - val_acc: 0.8284\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3390 - acc: 0.8509 - val_loss: 0.3796 - val_acc: 0.8279\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3342 - acc: 0.8518 - val_loss: 0.3988 - val_acc: 0.8198\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3334 - acc: 0.8529 - val_loss: 0.3847 - val_acc: 0.8249\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3301 - acc: 0.8550 - val_loss: 0.3951 - val_acc: 0.8256\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3247 - acc: 0.8570 - val_loss: 0.3859 - val_acc: 0.8293\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3222 - acc: 0.8577 - val_loss: 0.3917 - val_acc: 0.8236\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3217 - acc: 0.8584 - val_loss: 0.3865 - val_acc: 0.8292\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3172 - acc: 0.8607 - val_loss: 0.4015 - val_acc: 0.8223\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3143 - acc: 0.8622 - val_loss: 0.3927 - val_acc: 0.8238\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3123 - acc: 0.8638 - val_loss: 0.3977 - val_acc: 0.8267\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3105 - acc: 0.8646 - val_loss: 0.3921 - val_acc: 0.8271\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 287us/sample - loss: 0.5245 - acc: 0.7454 - val_loss: 0.4726 - val_acc: 0.7775\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4683 - acc: 0.7780 - val_loss: 0.4504 - val_acc: 0.7876\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4463 - acc: 0.7908 - val_loss: 0.4633 - val_acc: 0.7763\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4327 - acc: 0.8004 - val_loss: 0.4254 - val_acc: 0.8026\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4216 - acc: 0.8059 - val_loss: 0.4368 - val_acc: 0.7971\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4154 - acc: 0.8101 - val_loss: 0.4087 - val_acc: 0.8127\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4094 - acc: 0.8133 - val_loss: 0.4181 - val_acc: 0.8091\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4063 - acc: 0.8147 - val_loss: 0.4399 - val_acc: 0.7950\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4027 - acc: 0.8172 - val_loss: 0.4150 - val_acc: 0.8070\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3994 - acc: 0.8189 - val_loss: 0.3961 - val_acc: 0.8206\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3952 - acc: 0.8215 - val_loss: 0.4228 - val_acc: 0.8073\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3928 - acc: 0.8214 - val_loss: 0.3959 - val_acc: 0.8234\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3890 - acc: 0.8233 - val_loss: 0.3851 - val_acc: 0.8253\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3863 - acc: 0.8258 - val_loss: 0.3904 - val_acc: 0.8239\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3844 - acc: 0.8271 - val_loss: 0.3998 - val_acc: 0.8173\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3801 - acc: 0.8285 - val_loss: 0.3848 - val_acc: 0.8254\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3794 - acc: 0.8289 - val_loss: 0.3909 - val_acc: 0.8231\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3789 - acc: 0.8296 - val_loss: 0.3909 - val_acc: 0.8191\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3767 - acc: 0.8305 - val_loss: 0.3816 - val_acc: 0.8269\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3740 - acc: 0.8316 - val_loss: 0.3801 - val_acc: 0.8285\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3738 - acc: 0.8309 - val_loss: 0.3861 - val_acc: 0.8233\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3702 - acc: 0.8340 - val_loss: 0.3806 - val_acc: 0.8292\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3697 - acc: 0.8341 - val_loss: 0.3796 - val_acc: 0.8297\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3693 - acc: 0.8339 - val_loss: 0.3791 - val_acc: 0.8292\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3667 - acc: 0.8358 - val_loss: 0.3820 - val_acc: 0.8273\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3672 - acc: 0.8354 - val_loss: 0.3904 - val_acc: 0.8209\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3652 - acc: 0.8369 - val_loss: 0.3813 - val_acc: 0.8278\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3636 - acc: 0.8375 - val_loss: 0.4080 - val_acc: 0.8195\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3608 - acc: 0.8383 - val_loss: 0.3818 - val_acc: 0.8282\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3605 - acc: 0.8390 - val_loss: 0.3785 - val_acc: 0.8302\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 287us/sample - loss: 0.5221 - acc: 0.7474 - val_loss: 0.5283 - val_acc: 0.7422\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4605 - acc: 0.7831 - val_loss: 0.4593 - val_acc: 0.7868\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4416 - acc: 0.7939 - val_loss: 0.4370 - val_acc: 0.8001\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4273 - acc: 0.8007 - val_loss: 0.4145 - val_acc: 0.8095\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4203 - acc: 0.8055 - val_loss: 0.4101 - val_acc: 0.8129\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4128 - acc: 0.8106 - val_loss: 0.4030 - val_acc: 0.8183\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4075 - acc: 0.8140 - val_loss: 0.3977 - val_acc: 0.8178\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4029 - acc: 0.8167 - val_loss: 0.4003 - val_acc: 0.8178\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.3996 - acc: 0.8176 - val_loss: 0.3994 - val_acc: 0.8153\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3951 - acc: 0.8209 - val_loss: 0.4088 - val_acc: 0.8120\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3910 - acc: 0.8226 - val_loss: 0.3884 - val_acc: 0.8230\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3891 - acc: 0.8252 - val_loss: 0.3968 - val_acc: 0.8179\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3852 - acc: 0.8259 - val_loss: 0.3888 - val_acc: 0.8219\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3827 - acc: 0.8278 - val_loss: 0.3861 - val_acc: 0.8252\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3806 - acc: 0.8290 - val_loss: 0.3828 - val_acc: 0.8256\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3774 - acc: 0.8305 - val_loss: 0.4057 - val_acc: 0.8149\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3753 - acc: 0.8317 - val_loss: 0.3880 - val_acc: 0.8244\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3737 - acc: 0.8318 - val_loss: 0.3840 - val_acc: 0.8263\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3725 - acc: 0.8330 - val_loss: 0.3943 - val_acc: 0.8227\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3712 - acc: 0.8338 - val_loss: 0.3828 - val_acc: 0.8276\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3691 - acc: 0.8342 - val_loss: 0.3836 - val_acc: 0.8239\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3665 - acc: 0.8365 - val_loss: 0.3820 - val_acc: 0.8277\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3644 - acc: 0.8356 - val_loss: 0.4312 - val_acc: 0.7964\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3625 - acc: 0.8378 - val_loss: 0.3867 - val_acc: 0.8249\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3608 - acc: 0.8381 - val_loss: 0.3829 - val_acc: 0.8273\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3614 - acc: 0.8383 - val_loss: 0.3811 - val_acc: 0.8270\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3605 - acc: 0.8398 - val_loss: 0.3818 - val_acc: 0.8293\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3571 - acc: 0.8404 - val_loss: 0.3840 - val_acc: 0.8267\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3577 - acc: 0.8410 - val_loss: 0.3983 - val_acc: 0.8215\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3555 - acc: 0.8420 - val_loss: 0.3805 - val_acc: 0.8316\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 315us/sample - loss: 0.5134 - acc: 0.7536 - val_loss: 0.4457 - val_acc: 0.7899\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4477 - acc: 0.7907 - val_loss: 0.4623 - val_acc: 0.7867\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4290 - acc: 0.8021 - val_loss: 0.4143 - val_acc: 0.8126\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4126 - acc: 0.8106 - val_loss: 0.3991 - val_acc: 0.8169\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4026 - acc: 0.8185 - val_loss: 0.4071 - val_acc: 0.8099\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3945 - acc: 0.8214 - val_loss: 0.3977 - val_acc: 0.8186\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3884 - acc: 0.8260 - val_loss: 0.4108 - val_acc: 0.8133\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3801 - acc: 0.8289 - val_loss: 0.3831 - val_acc: 0.8261\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3752 - acc: 0.8320 - val_loss: 0.3944 - val_acc: 0.8234\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3693 - acc: 0.8343 - val_loss: 0.3790 - val_acc: 0.8270\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3615 - acc: 0.8379 - val_loss: 0.3855 - val_acc: 0.8245\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3566 - acc: 0.8411 - val_loss: 0.3912 - val_acc: 0.8227\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3527 - acc: 0.8433 - val_loss: 0.3911 - val_acc: 0.8206\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3462 - acc: 0.8469 - val_loss: 0.3961 - val_acc: 0.8199\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3404 - acc: 0.8490 - val_loss: 0.3921 - val_acc: 0.8243\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3362 - acc: 0.8510 - val_loss: 0.3923 - val_acc: 0.8260\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3328 - acc: 0.8532 - val_loss: 0.3811 - val_acc: 0.8310\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3257 - acc: 0.8545 - val_loss: 0.3847 - val_acc: 0.8267\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3204 - acc: 0.8591 - val_loss: 0.4133 - val_acc: 0.8167\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3157 - acc: 0.8616 - val_loss: 0.3988 - val_acc: 0.8212\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3109 - acc: 0.8640 - val_loss: 0.4225 - val_acc: 0.8216\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3062 - acc: 0.8663 - val_loss: 0.3919 - val_acc: 0.8278\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2992 - acc: 0.8681 - val_loss: 0.4033 - val_acc: 0.8241\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2972 - acc: 0.8704 - val_loss: 0.3930 - val_acc: 0.8274\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.2933 - acc: 0.8714 - val_loss: 0.3965 - val_acc: 0.8276\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2881 - acc: 0.8746 - val_loss: 0.4125 - val_acc: 0.8260\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2839 - acc: 0.8763 - val_loss: 0.4134 - val_acc: 0.8229\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2822 - acc: 0.8772 - val_loss: 0.4223 - val_acc: 0.8220\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2798 - acc: 0.8790 - val_loss: 0.4166 - val_acc: 0.8184\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2781 - acc: 0.8792 - val_loss: 0.4122 - val_acc: 0.8249\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 28s 331us/sample - loss: 0.5186 - acc: 0.7526 - val_loss: 0.4805 - val_acc: 0.7690\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 166us/sample - loss: 0.4500 - acc: 0.7912 - val_loss: 0.4238 - val_acc: 0.8020\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.4278 - acc: 0.8033 - val_loss: 0.4674 - val_acc: 0.7857\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4145 - acc: 0.8118 - val_loss: 0.4152 - val_acc: 0.8105\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4040 - acc: 0.8158 - val_loss: 0.4139 - val_acc: 0.8099\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3961 - acc: 0.8205 - val_loss: 0.4155 - val_acc: 0.8110\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3875 - acc: 0.8236 - val_loss: 0.3877 - val_acc: 0.8235\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3802 - acc: 0.8302 - val_loss: 0.3904 - val_acc: 0.8229\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3737 - acc: 0.8323 - val_loss: 0.3809 - val_acc: 0.8285\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3651 - acc: 0.8359 - val_loss: 0.3796 - val_acc: 0.8292\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3600 - acc: 0.8397 - val_loss: 0.3948 - val_acc: 0.8221\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3536 - acc: 0.8430 - val_loss: 0.3915 - val_acc: 0.8248\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3468 - acc: 0.8445 - val_loss: 0.3885 - val_acc: 0.8267\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3409 - acc: 0.8488 - val_loss: 0.3982 - val_acc: 0.8173\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3330 - acc: 0.8529 - val_loss: 0.3796 - val_acc: 0.8322\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3270 - acc: 0.8564 - val_loss: 0.3800 - val_acc: 0.8286\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3219 - acc: 0.8579 - val_loss: 0.3984 - val_acc: 0.8255\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3153 - acc: 0.8617 - val_loss: 0.4213 - val_acc: 0.8187\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3095 - acc: 0.8655 - val_loss: 0.3999 - val_acc: 0.8257\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3037 - acc: 0.8676 - val_loss: 0.4047 - val_acc: 0.8231\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2997 - acc: 0.8704 - val_loss: 0.4229 - val_acc: 0.8180\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2954 - acc: 0.8712 - val_loss: 0.4079 - val_acc: 0.8230\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2890 - acc: 0.8745 - val_loss: 0.4056 - val_acc: 0.8129\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2847 - acc: 0.8769 - val_loss: 0.4281 - val_acc: 0.8123\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2798 - acc: 0.8803 - val_loss: 0.4217 - val_acc: 0.8222\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2761 - acc: 0.8813 - val_loss: 0.4265 - val_acc: 0.8229\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2695 - acc: 0.8845 - val_loss: 0.4253 - val_acc: 0.8159\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2677 - acc: 0.8846 - val_loss: 0.4255 - val_acc: 0.8212\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2622 - acc: 0.8880 - val_loss: 0.4213 - val_acc: 0.8203\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2582 - acc: 0.8893 - val_loss: 0.4333 - val_acc: 0.8242\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 295us/sample - loss: 0.5864 - acc: 0.7103 - val_loss: 0.5018 - val_acc: 0.7584\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.5075 - acc: 0.7504 - val_loss: 0.4840 - val_acc: 0.7700\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4894 - acc: 0.7623 - val_loss: 0.4865 - val_acc: 0.7677\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4781 - acc: 0.7700 - val_loss: 0.4526 - val_acc: 0.7876\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4676 - acc: 0.7781 - val_loss: 0.4628 - val_acc: 0.7825\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4595 - acc: 0.7831 - val_loss: 0.4363 - val_acc: 0.7972\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4530 - acc: 0.7883 - val_loss: 0.4331 - val_acc: 0.8001\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4495 - acc: 0.7892 - val_loss: 0.4358 - val_acc: 0.7952\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4463 - acc: 0.7918 - val_loss: 0.4539 - val_acc: 0.7913\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4446 - acc: 0.7925 - val_loss: 0.4420 - val_acc: 0.7932\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4411 - acc: 0.7956 - val_loss: 0.4277 - val_acc: 0.8001\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4401 - acc: 0.7955 - val_loss: 0.4448 - val_acc: 0.7899\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4398 - acc: 0.7944 - val_loss: 0.4244 - val_acc: 0.8017\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4378 - acc: 0.7973 - val_loss: 0.4176 - val_acc: 0.8066\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4357 - acc: 0.7982 - val_loss: 0.4191 - val_acc: 0.8067\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4333 - acc: 0.7995 - val_loss: 0.4131 - val_acc: 0.8094\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4329 - acc: 0.8009 - val_loss: 0.4144 - val_acc: 0.8098\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4335 - acc: 0.7996 - val_loss: 0.4123 - val_acc: 0.8090\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4318 - acc: 0.7987 - val_loss: 0.4174 - val_acc: 0.8041\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4299 - acc: 0.8010 - val_loss: 0.4147 - val_acc: 0.8082\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4301 - acc: 0.8013 - val_loss: 0.4103 - val_acc: 0.8104\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4287 - acc: 0.8014 - val_loss: 0.4179 - val_acc: 0.8091\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4280 - acc: 0.8018 - val_loss: 0.4152 - val_acc: 0.8063\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4295 - acc: 0.8023 - val_loss: 0.4077 - val_acc: 0.8134\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4287 - acc: 0.8010 - val_loss: 0.4160 - val_acc: 0.8112\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4253 - acc: 0.8051 - val_loss: 0.4254 - val_acc: 0.8027\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4283 - acc: 0.8019 - val_loss: 0.4061 - val_acc: 0.8124\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4261 - acc: 0.8031 - val_loss: 0.4146 - val_acc: 0.8082\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4269 - acc: 0.8018 - val_loss: 0.4048 - val_acc: 0.8159\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4246 - acc: 0.8045 - val_loss: 0.4043 - val_acc: 0.8145\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 296us/sample - loss: 0.5689 - acc: 0.7174 - val_loss: 0.4947 - val_acc: 0.7613\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.5010 - acc: 0.7547 - val_loss: 0.4726 - val_acc: 0.7760\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4847 - acc: 0.7659 - val_loss: 0.4783 - val_acc: 0.7689\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4742 - acc: 0.7728 - val_loss: 0.4584 - val_acc: 0.7818\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4663 - acc: 0.7786 - val_loss: 0.4543 - val_acc: 0.7882\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4600 - acc: 0.7828 - val_loss: 0.4374 - val_acc: 0.7970\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4553 - acc: 0.7854 - val_loss: 0.4395 - val_acc: 0.7972\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4526 - acc: 0.7881 - val_loss: 0.4369 - val_acc: 0.7935\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4484 - acc: 0.7905 - val_loss: 0.4627 - val_acc: 0.7775\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4468 - acc: 0.7898 - val_loss: 0.4328 - val_acc: 0.8002\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4454 - acc: 0.7926 - val_loss: 0.4306 - val_acc: 0.7975\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4414 - acc: 0.7956 - val_loss: 0.4205 - val_acc: 0.8073\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4423 - acc: 0.7950 - val_loss: 0.4277 - val_acc: 0.8036\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4411 - acc: 0.7956 - val_loss: 0.4221 - val_acc: 0.8061\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4407 - acc: 0.7965 - val_loss: 0.4225 - val_acc: 0.8033\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4375 - acc: 0.7974 - val_loss: 0.4214 - val_acc: 0.8044\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4359 - acc: 0.7986 - val_loss: 0.4128 - val_acc: 0.8103\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4372 - acc: 0.7967 - val_loss: 0.4167 - val_acc: 0.8078\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4354 - acc: 0.7988 - val_loss: 0.4140 - val_acc: 0.8109\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4347 - acc: 0.7987 - val_loss: 0.4220 - val_acc: 0.8061\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4334 - acc: 0.7994 - val_loss: 0.4177 - val_acc: 0.8087\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4324 - acc: 0.8000 - val_loss: 0.4101 - val_acc: 0.8109\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4329 - acc: 0.8003 - val_loss: 0.4140 - val_acc: 0.8108\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4326 - acc: 0.7991 - val_loss: 0.4157 - val_acc: 0.8091\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4320 - acc: 0.7997 - val_loss: 0.4199 - val_acc: 0.8043\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4316 - acc: 0.8000 - val_loss: 0.4158 - val_acc: 0.8098\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 135us/sample - loss: 0.4316 - acc: 0.8017 - val_loss: 0.4117 - val_acc: 0.8120\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4299 - acc: 0.8020 - val_loss: 0.4108 - val_acc: 0.8112\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4304 - acc: 0.8010 - val_loss: 0.4141 - val_acc: 0.8120\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4300 - acc: 0.8017 - val_loss: 0.4117 - val_acc: 0.8112\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 317us/sample - loss: 0.5872 - acc: 0.7006 - val_loss: 0.4997 - val_acc: 0.7555\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.5080 - acc: 0.7539 - val_loss: 0.4807 - val_acc: 0.7709\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4879 - acc: 0.7650 - val_loss: 0.4600 - val_acc: 0.7847\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4749 - acc: 0.7726 - val_loss: 0.4494 - val_acc: 0.7875\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4672 - acc: 0.7786 - val_loss: 0.4450 - val_acc: 0.7897\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4618 - acc: 0.7828 - val_loss: 0.4529 - val_acc: 0.7854\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4553 - acc: 0.7859 - val_loss: 0.4356 - val_acc: 0.8014\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4509 - acc: 0.7893 - val_loss: 0.4290 - val_acc: 0.8022\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4492 - acc: 0.7902 - val_loss: 0.4244 - val_acc: 0.8037\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4463 - acc: 0.7922 - val_loss: 0.4250 - val_acc: 0.8065\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4435 - acc: 0.7929 - val_loss: 0.4336 - val_acc: 0.7968\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4415 - acc: 0.7952 - val_loss: 0.4189 - val_acc: 0.8064\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4389 - acc: 0.7973 - val_loss: 0.4190 - val_acc: 0.8051\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4380 - acc: 0.7972 - val_loss: 0.4158 - val_acc: 0.8082\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4364 - acc: 0.7993 - val_loss: 0.4204 - val_acc: 0.8056\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4352 - acc: 0.7998 - val_loss: 0.4125 - val_acc: 0.8112\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4345 - acc: 0.7991 - val_loss: 0.4239 - val_acc: 0.8065\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4329 - acc: 0.7990 - val_loss: 0.4188 - val_acc: 0.8086\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4328 - acc: 0.8015 - val_loss: 0.4072 - val_acc: 0.8114\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4312 - acc: 0.8016 - val_loss: 0.4093 - val_acc: 0.8112\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4299 - acc: 0.8022 - val_loss: 0.4219 - val_acc: 0.8079\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4285 - acc: 0.8037 - val_loss: 0.4079 - val_acc: 0.8137\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4276 - acc: 0.8020 - val_loss: 0.4115 - val_acc: 0.8088\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4284 - acc: 0.8034 - val_loss: 0.4047 - val_acc: 0.8152\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4273 - acc: 0.8028 - val_loss: 0.4042 - val_acc: 0.8137\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4271 - acc: 0.8045 - val_loss: 0.4085 - val_acc: 0.8122\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4271 - acc: 0.8032 - val_loss: 0.4036 - val_acc: 0.8146\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4256 - acc: 0.8041 - val_loss: 0.4102 - val_acc: 0.8125\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4258 - acc: 0.8038 - val_loss: 0.4029 - val_acc: 0.8160\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4265 - acc: 0.8041 - val_loss: 0.4009 - val_acc: 0.8170\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 28s 332us/sample - loss: 0.5954 - acc: 0.6993 - val_loss: 0.5016 - val_acc: 0.7541\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.5162 - acc: 0.7477 - val_loss: 0.4798 - val_acc: 0.7708- loss:\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4961 - acc: 0.7623 - val_loss: 0.4692 - val_acc: 0.7768\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4816 - acc: 0.7705 - val_loss: 0.4529 - val_acc: 0.7863\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4719 - acc: 0.7756 - val_loss: 0.4554 - val_acc: 0.7837\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4671 - acc: 0.7796 - val_loss: 0.4402 - val_acc: 0.7966\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4597 - acc: 0.7834 - val_loss: 0.4343 - val_acc: 0.7957\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4538 - acc: 0.7862 - val_loss: 0.4385 - val_acc: 0.7954\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4513 - acc: 0.7890 - val_loss: 0.4218 - val_acc: 0.8062\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.4472 - acc: 0.7922 - val_loss: 0.4242 - val_acc: 0.8044\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4442 - acc: 0.7924 - val_loss: 0.4233 - val_acc: 0.8029\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4428 - acc: 0.7943 - val_loss: 0.4155 - val_acc: 0.8098\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.4387 - acc: 0.7970 - val_loss: 0.4169 - val_acc: 0.8092\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4388 - acc: 0.7966 - val_loss: 0.4179 - val_acc: 0.8115\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4376 - acc: 0.7962 - val_loss: 0.4132 - val_acc: 0.8114\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4366 - acc: 0.7979 - val_loss: 0.4169 - val_acc: 0.8081\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4347 - acc: 0.7985 - val_loss: 0.4316 - val_acc: 0.7991\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4345 - acc: 0.8001 - val_loss: 0.4122 - val_acc: 0.8085\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4331 - acc: 0.7992 - val_loss: 0.4151 - val_acc: 0.8100\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4334 - acc: 0.7993 - val_loss: 0.4177 - val_acc: 0.8063\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4320 - acc: 0.8002 - val_loss: 0.4135 - val_acc: 0.8092\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4311 - acc: 0.8008 - val_loss: 0.4104 - val_acc: 0.8120\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4319 - acc: 0.8003 - val_loss: 0.4088 - val_acc: 0.8115\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4313 - acc: 0.8015 - val_loss: 0.4189 - val_acc: 0.8083\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4288 - acc: 0.8031 - val_loss: 0.4110 - val_acc: 0.8120\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4282 - acc: 0.8025 - val_loss: 0.4058 - val_acc: 0.8136\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.4288 - acc: 0.8021 - val_loss: 0.4129 - val_acc: 0.8087\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4261 - acc: 0.8035 - val_loss: 0.4121 - val_acc: 0.8092\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4282 - acc: 0.8034 - val_loss: 0.4045 - val_acc: 0.8150\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4282 - acc: 0.8022 - val_loss: 0.4073 - val_acc: 0.8151\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 302us/sample - loss: 0.5454 - acc: 0.7325 - val_loss: 0.4816 - val_acc: 0.7675\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4806 - acc: 0.7694 - val_loss: 0.4791 - val_acc: 0.7693\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4623 - acc: 0.7808 - val_loss: 0.4430 - val_acc: 0.7940\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4470 - acc: 0.7904 - val_loss: 0.4422 - val_acc: 0.7936\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4360 - acc: 0.7978 - val_loss: 0.4418 - val_acc: 0.7919\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4311 - acc: 0.8017 - val_loss: 0.4149 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4243 - acc: 0.8042 - val_loss: 0.4111 - val_acc: 0.8106\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4179 - acc: 0.8083 - val_loss: 0.4043 - val_acc: 0.8142\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4153 - acc: 0.8102 - val_loss: 0.4143 - val_acc: 0.8104\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4140 - acc: 0.8118 - val_loss: 0.4210 - val_acc: 0.8057\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4108 - acc: 0.8129 - val_loss: 0.4060 - val_acc: 0.8131\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4075 - acc: 0.8143 - val_loss: 0.4038 - val_acc: 0.8141\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4047 - acc: 0.8158 - val_loss: 0.3957 - val_acc: 0.8163\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4049 - acc: 0.8155 - val_loss: 0.4363 - val_acc: 0.8037\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4020 - acc: 0.8175 - val_loss: 0.3940 - val_acc: 0.8194\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3997 - acc: 0.8193 - val_loss: 0.3967 - val_acc: 0.8183\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3984 - acc: 0.8197 - val_loss: 0.3887 - val_acc: 0.8228\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3979 - acc: 0.8190 - val_loss: 0.4134 - val_acc: 0.8077\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3962 - acc: 0.8209 - val_loss: 0.3913 - val_acc: 0.8224\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3956 - acc: 0.8221 - val_loss: 0.3968 - val_acc: 0.8195\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3949 - acc: 0.8212 - val_loss: 0.4015 - val_acc: 0.8159\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3920 - acc: 0.8212 - val_loss: 0.3875 - val_acc: 0.8237\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3917 - acc: 0.8224 - val_loss: 0.3870 - val_acc: 0.8231\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3919 - acc: 0.8234 - val_loss: 0.3909 - val_acc: 0.8209\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3902 - acc: 0.8241 - val_loss: 0.3832 - val_acc: 0.8249\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3901 - acc: 0.8238 - val_loss: 0.3963 - val_acc: 0.8176\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3901 - acc: 0.8242 - val_loss: 0.3828 - val_acc: 0.8249\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3890 - acc: 0.8242 - val_loss: 0.3954 - val_acc: 0.8177\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3880 - acc: 0.8258 - val_loss: 0.3851 - val_acc: 0.8235\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3868 - acc: 0.8254 - val_loss: 0.3850 - val_acc: 0.8262\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 307us/sample - loss: 0.5392 - acc: 0.7363 - val_loss: 0.4835 - val_acc: 0.7711\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4764 - acc: 0.7716 - val_loss: 0.4628 - val_acc: 0.7817\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4576 - acc: 0.7843 - val_loss: 0.4555 - val_acc: 0.7814\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4453 - acc: 0.7925 - val_loss: 0.4351 - val_acc: 0.7998\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4372 - acc: 0.7977 - val_loss: 0.4189 - val_acc: 0.8066\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4309 - acc: 0.8014 - val_loss: 0.4140 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4277 - acc: 0.8028 - val_loss: 0.4158 - val_acc: 0.8083\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4226 - acc: 0.8050 - val_loss: 0.4082 - val_acc: 0.8108\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4183 - acc: 0.8079 - val_loss: 0.4108 - val_acc: 0.8102\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4146 - acc: 0.8102 - val_loss: 0.4460 - val_acc: 0.7929\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4119 - acc: 0.8120 - val_loss: 0.4242 - val_acc: 0.8062\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4081 - acc: 0.8138 - val_loss: 0.4126 - val_acc: 0.8099\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4073 - acc: 0.8146 - val_loss: 0.4004 - val_acc: 0.8189\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4042 - acc: 0.8161 - val_loss: 0.3967 - val_acc: 0.8173\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4027 - acc: 0.8173 - val_loss: 0.3931 - val_acc: 0.8209\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4018 - acc: 0.8171 - val_loss: 0.4069 - val_acc: 0.8115\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4000 - acc: 0.8171 - val_loss: 0.3905 - val_acc: 0.8231\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3987 - acc: 0.8200 - val_loss: 0.4024 - val_acc: 0.8153\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3984 - acc: 0.8193 - val_loss: 0.3878 - val_acc: 0.8237\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3974 - acc: 0.8185 - val_loss: 0.3999 - val_acc: 0.8169\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3968 - acc: 0.8202 - val_loss: 0.3881 - val_acc: 0.8227\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3954 - acc: 0.8206 - val_loss: 0.3899 - val_acc: 0.8201\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3927 - acc: 0.8223 - val_loss: 0.3847 - val_acc: 0.8253\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3922 - acc: 0.8238 - val_loss: 0.4021 - val_acc: 0.8163\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3900 - acc: 0.8235 - val_loss: 0.4197 - val_acc: 0.8032\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3901 - acc: 0.8237 - val_loss: 0.3926 - val_acc: 0.8218\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3893 - acc: 0.8238 - val_loss: 0.3880 - val_acc: 0.8238\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3894 - acc: 0.8234 - val_loss: 0.3867 - val_acc: 0.8229\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3897 - acc: 0.8231 - val_loss: 0.3864 - val_acc: 0.8256\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3874 - acc: 0.8251 - val_loss: 0.3866 - val_acc: 0.8220\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 28s 329us/sample - loss: 0.5497 - acc: 0.7325 - val_loss: 0.4725 - val_acc: 0.7737\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4735 - acc: 0.7740 - val_loss: 0.4407 - val_acc: 0.7942\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4521 - acc: 0.7870 - val_loss: 0.5382 - val_acc: 0.7281\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4367 - acc: 0.7970 - val_loss: 0.4246 - val_acc: 0.8044\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4289 - acc: 0.8024 - val_loss: 0.4195 - val_acc: 0.8051\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4214 - acc: 0.8067 - val_loss: 0.4117 - val_acc: 0.8116\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4164 - acc: 0.8099 - val_loss: 0.4109 - val_acc: 0.8098\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4127 - acc: 0.8126 - val_loss: 0.4030 - val_acc: 0.8153\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.4073 - acc: 0.8157 - val_loss: 0.4157 - val_acc: 0.8122\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4037 - acc: 0.8173 - val_loss: 0.3947 - val_acc: 0.8175\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4005 - acc: 0.8184 - val_loss: 0.4045 - val_acc: 0.8159\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3967 - acc: 0.8208 - val_loss: 0.3961 - val_acc: 0.8192\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3940 - acc: 0.8224 - val_loss: 0.3903 - val_acc: 0.8222\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3926 - acc: 0.8228 - val_loss: 0.3877 - val_acc: 0.8243\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3889 - acc: 0.8242 - val_loss: 0.3857 - val_acc: 0.8246\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3881 - acc: 0.8250 - val_loss: 0.4041 - val_acc: 0.8140\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3864 - acc: 0.8249 - val_loss: 0.3991 - val_acc: 0.8199\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3835 - acc: 0.8271 - val_loss: 0.3932 - val_acc: 0.8178\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3812 - acc: 0.8279 - val_loss: 0.3846 - val_acc: 0.8274\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3796 - acc: 0.8289 - val_loss: 0.4038 - val_acc: 0.8169\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3790 - acc: 0.8298 - val_loss: 0.3832 - val_acc: 0.8271\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3785 - acc: 0.8309 - val_loss: 0.3865 - val_acc: 0.8242\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3757 - acc: 0.8313 - val_loss: 0.3853 - val_acc: 0.8256\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3755 - acc: 0.8308 - val_loss: 0.3877 - val_acc: 0.8249\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3740 - acc: 0.8330 - val_loss: 0.3856 - val_acc: 0.8249\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3707 - acc: 0.8352 - val_loss: 0.3870 - val_acc: 0.8243\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3720 - acc: 0.8326 - val_loss: 0.3758 - val_acc: 0.8291\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3699 - acc: 0.8345 - val_loss: 0.3828 - val_acc: 0.8267\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3691 - acc: 0.8334 - val_loss: 0.3851 - val_acc: 0.8275\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3675 - acc: 0.8364 - val_loss: 0.3794 - val_acc: 0.8280\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 29s 346us/sample - loss: 0.5430 - acc: 0.7370 - val_loss: 0.4902 - val_acc: 0.7637\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4766 - acc: 0.7738 - val_loss: 0.4452 - val_acc: 0.7901\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4533 - acc: 0.7893 - val_loss: 0.4252 - val_acc: 0.8036\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4387 - acc: 0.7960 - val_loss: 0.4190 - val_acc: 0.8049\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4285 - acc: 0.8012 - val_loss: 0.4371 - val_acc: 0.7898- acc: 0.801\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4200 - acc: 0.8074 - val_loss: 0.4138 - val_acc: 0.8108\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4158 - acc: 0.8091 - val_loss: 0.4074 - val_acc: 0.8126\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4097 - acc: 0.8128 - val_loss: 0.4012 - val_acc: 0.8163\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4051 - acc: 0.8167 - val_loss: 0.4005 - val_acc: 0.8159\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4037 - acc: 0.8176 - val_loss: 0.4029 - val_acc: 0.8167\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3994 - acc: 0.8186 - val_loss: 0.3952 - val_acc: 0.8197\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3978 - acc: 0.8204 - val_loss: 0.3947 - val_acc: 0.8202\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3945 - acc: 0.8208 - val_loss: 0.3854 - val_acc: 0.8249\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3898 - acc: 0.8237 - val_loss: 0.3981 - val_acc: 0.8159\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3888 - acc: 0.8250 - val_loss: 0.4165 - val_acc: 0.8067\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3860 - acc: 0.8268 - val_loss: 0.3893 - val_acc: 0.8223\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3852 - acc: 0.8255 - val_loss: 0.4082 - val_acc: 0.8105\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3819 - acc: 0.8272 - val_loss: 0.3866 - val_acc: 0.8228\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3812 - acc: 0.8286 - val_loss: 0.3836 - val_acc: 0.8260\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3801 - acc: 0.8281 - val_loss: 0.4098 - val_acc: 0.8105\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3769 - acc: 0.8308 - val_loss: 0.3843 - val_acc: 0.8243\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3771 - acc: 0.8309 - val_loss: 0.3868 - val_acc: 0.8248\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3748 - acc: 0.8313 - val_loss: 0.3823 - val_acc: 0.8238\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3742 - acc: 0.8321 - val_loss: 0.3830 - val_acc: 0.8248\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3714 - acc: 0.8334 - val_loss: 0.3784 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3694 - acc: 0.8340 - val_loss: 0.3796 - val_acc: 0.8255\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3698 - acc: 0.8349 - val_loss: 0.3807 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3693 - acc: 0.8343 - val_loss: 0.4074 - val_acc: 0.8162\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3676 - acc: 0.8350 - val_loss: 0.3803 - val_acc: 0.8261\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3663 - acc: 0.8369 - val_loss: 0.3872 - val_acc: 0.8232\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 309us/sample - loss: 0.5240 - acc: 0.7446 - val_loss: 0.4648 - val_acc: 0.7801\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4626 - acc: 0.7797 - val_loss: 0.4503 - val_acc: 0.7860\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4450 - acc: 0.7922 - val_loss: 0.4335 - val_acc: 0.7973\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4328 - acc: 0.7996 - val_loss: 0.4222 - val_acc: 0.8036\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4242 - acc: 0.8032 - val_loss: 0.4197 - val_acc: 0.8056\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4154 - acc: 0.8084 - val_loss: 0.4253 - val_acc: 0.8064\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4126 - acc: 0.8112 - val_loss: 0.4056 - val_acc: 0.8132\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4084 - acc: 0.8143 - val_loss: 0.3983 - val_acc: 0.8173\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4015 - acc: 0.8170 - val_loss: 0.3972 - val_acc: 0.8179\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4011 - acc: 0.8180 - val_loss: 0.3943 - val_acc: 0.8172\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3962 - acc: 0.8195 - val_loss: 0.3914 - val_acc: 0.8199\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3948 - acc: 0.8223 - val_loss: 0.3923 - val_acc: 0.8202\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3919 - acc: 0.8221 - val_loss: 0.3906 - val_acc: 0.8210\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3904 - acc: 0.8241 - val_loss: 0.3981 - val_acc: 0.8177\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3876 - acc: 0.8240 - val_loss: 0.3850 - val_acc: 0.8248\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3863 - acc: 0.8243 - val_loss: 0.3889 - val_acc: 0.8231\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3832 - acc: 0.8269 - val_loss: 0.3850 - val_acc: 0.8246\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3825 - acc: 0.8267 - val_loss: 0.3910 - val_acc: 0.8207\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3794 - acc: 0.8302 - val_loss: 0.3986 - val_acc: 0.8177\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3766 - acc: 0.8323 - val_loss: 0.4012 - val_acc: 0.8169\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3782 - acc: 0.8290 - val_loss: 0.3890 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3762 - acc: 0.8312 - val_loss: 0.3931 - val_acc: 0.8214\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3746 - acc: 0.8321 - val_loss: 0.3846 - val_acc: 0.8257\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3743 - acc: 0.8321 - val_loss: 0.3915 - val_acc: 0.8213\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3730 - acc: 0.8328 - val_loss: 0.3848 - val_acc: 0.8249\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3737 - acc: 0.8327 - val_loss: 0.4140 - val_acc: 0.8120\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3713 - acc: 0.8334 - val_loss: 0.3841 - val_acc: 0.8248\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3716 - acc: 0.8325 - val_loss: 0.3842 - val_acc: 0.8224\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3704 - acc: 0.8344 - val_loss: 0.3848 - val_acc: 0.8259\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3676 - acc: 0.8345 - val_loss: 0.3952 - val_acc: 0.8177\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 312us/sample - loss: 0.5332 - acc: 0.7433 - val_loss: 0.4704 - val_acc: 0.7753\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4654 - acc: 0.7790 - val_loss: 0.4442 - val_acc: 0.7909\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4489 - acc: 0.7904 - val_loss: 0.4399 - val_acc: 0.7930\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4357 - acc: 0.7981 - val_loss: 0.4355 - val_acc: 0.7982\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4266 - acc: 0.8031 - val_loss: 0.4303 - val_acc: 0.8003\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4205 - acc: 0.8064 - val_loss: 0.4117 - val_acc: 0.8080\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4127 - acc: 0.8116 - val_loss: 0.4228 - val_acc: 0.8046\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4083 - acc: 0.8127 - val_loss: 0.3976 - val_acc: 0.8177\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4045 - acc: 0.8156 - val_loss: 0.4139 - val_acc: 0.8105\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4015 - acc: 0.8170 - val_loss: 0.4120 - val_acc: 0.8100\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3977 - acc: 0.8201 - val_loss: 0.3907 - val_acc: 0.8209\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3955 - acc: 0.8204 - val_loss: 0.3962 - val_acc: 0.8173\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3929 - acc: 0.8226 - val_loss: 0.3880 - val_acc: 0.8238\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3913 - acc: 0.8221 - val_loss: 0.3949 - val_acc: 0.8198\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3899 - acc: 0.8237 - val_loss: 0.3868 - val_acc: 0.8235\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3865 - acc: 0.8250 - val_loss: 0.3901 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3845 - acc: 0.8264 - val_loss: 0.3873 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3815 - acc: 0.8271 - val_loss: 0.3878 - val_acc: 0.8259\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3822 - acc: 0.8282 - val_loss: 0.3901 - val_acc: 0.8220\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3809 - acc: 0.8278 - val_loss: 0.3863 - val_acc: 0.8242\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3771 - acc: 0.8306 - val_loss: 0.3827 - val_acc: 0.8260\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3773 - acc: 0.8303 - val_loss: 0.4015 - val_acc: 0.8171\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3768 - acc: 0.8314 - val_loss: 0.4122 - val_acc: 0.8122\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 143us/sample - loss: 0.3751 - acc: 0.8314 - val_loss: 0.4014 - val_acc: 0.8182\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3733 - acc: 0.8325 - val_loss: 0.3832 - val_acc: 0.8255\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3732 - acc: 0.8331 - val_loss: 0.3861 - val_acc: 0.8252\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3714 - acc: 0.8340 - val_loss: 0.3865 - val_acc: 0.8257\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3708 - acc: 0.8339 - val_loss: 0.3819 - val_acc: 0.8270\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3679 - acc: 0.8347 - val_loss: 0.3963 - val_acc: 0.8191\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3684 - acc: 0.8348 - val_loss: 0.3812 - val_acc: 0.8285\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 29s 338us/sample - loss: 0.5241 - acc: 0.7475 - val_loss: 0.4574 - val_acc: 0.7849\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4571 - acc: 0.7840 - val_loss: 0.4671 - val_acc: 0.7865\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4364 - acc: 0.7986 - val_loss: 0.4238 - val_acc: 0.8040\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4226 - acc: 0.8067 - val_loss: 0.4239 - val_acc: 0.8040\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4113 - acc: 0.8122 - val_loss: 0.4107 - val_acc: 0.8104\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4044 - acc: 0.8151 - val_loss: 0.4293 - val_acc: 0.8000\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3969 - acc: 0.8210 - val_loss: 0.4052 - val_acc: 0.8126\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3925 - acc: 0.8222 - val_loss: 0.4067 - val_acc: 0.8147\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3870 - acc: 0.8252 - val_loss: 0.4099 - val_acc: 0.8138\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3811 - acc: 0.8281 - val_loss: 0.3905 - val_acc: 0.8242\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3787 - acc: 0.8310 - val_loss: 0.3826 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3727 - acc: 0.8335 - val_loss: 0.3890 - val_acc: 0.8238\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3670 - acc: 0.8366 - val_loss: 0.3882 - val_acc: 0.8267\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3653 - acc: 0.8378 - val_loss: 0.3846 - val_acc: 0.8262\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3600 - acc: 0.8410 - val_loss: 0.3838 - val_acc: 0.8283\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3569 - acc: 0.8412 - val_loss: 0.3899 - val_acc: 0.8237\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3530 - acc: 0.8438 - val_loss: 0.3798 - val_acc: 0.8270\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3501 - acc: 0.8448 - val_loss: 0.3834 - val_acc: 0.8291\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3483 - acc: 0.8448 - val_loss: 0.3835 - val_acc: 0.8263\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3437 - acc: 0.8489 - val_loss: 0.3805 - val_acc: 0.8301\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3387 - acc: 0.8510 - val_loss: 0.4059 - val_acc: 0.8193\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3370 - acc: 0.8511 - val_loss: 0.3875 - val_acc: 0.8264\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3342 - acc: 0.8531 - val_loss: 0.3858 - val_acc: 0.8269\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3300 - acc: 0.8541 - val_loss: 0.4068 - val_acc: 0.8172\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3314 - acc: 0.8538 - val_loss: 0.3944 - val_acc: 0.8235 - acc: 0.\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3243 - acc: 0.8580 - val_loss: 0.4092 - val_acc: 0.8191\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3228 - acc: 0.8581 - val_loss: 0.3983 - val_acc: 0.8239\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3229 - acc: 0.8580 - val_loss: 0.3912 - val_acc: 0.8243\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3184 - acc: 0.8603 - val_loss: 0.3872 - val_acc: 0.8281\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3179 - acc: 0.8604 - val_loss: 0.3970 - val_acc: 0.8281\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 30s 354us/sample - loss: 0.5157 - acc: 0.7485 - val_loss: 0.5509 - val_acc: 0.7215\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.4503 - acc: 0.7894 - val_loss: 0.4271 - val_acc: 0.8026\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.4312 - acc: 0.8012 - val_loss: 0.4161 - val_acc: 0.8086\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.4194 - acc: 0.8065 - val_loss: 0.4135 - val_acc: 0.8065\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4103 - acc: 0.8129 - val_loss: 0.4035 - val_acc: 0.8191\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.4016 - acc: 0.8180 - val_loss: 0.4099 - val_acc: 0.8149\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3944 - acc: 0.8212 - val_loss: 0.3869 - val_acc: 0.8251\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3898 - acc: 0.8237 - val_loss: 0.4072 - val_acc: 0.8175\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3824 - acc: 0.8283 - val_loss: 0.3837 - val_acc: 0.8249\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3774 - acc: 0.8303 - val_loss: 0.4099 - val_acc: 0.8159\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3724 - acc: 0.8329 - val_loss: 0.3821 - val_acc: 0.8255\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3685 - acc: 0.8352 - val_loss: 0.3889 - val_acc: 0.8249\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3632 - acc: 0.8368 - val_loss: 0.3835 - val_acc: 0.8257\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3606 - acc: 0.8386 - val_loss: 0.3843 - val_acc: 0.8263\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3565 - acc: 0.8414 - val_loss: 0.4140 - val_acc: 0.8183\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3517 - acc: 0.8434 - val_loss: 0.3896 - val_acc: 0.8274\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3486 - acc: 0.8456 - val_loss: 0.3844 - val_acc: 0.8285\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3442 - acc: 0.8473 - val_loss: 0.3793 - val_acc: 0.8277\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3400 - acc: 0.8493 - val_loss: 0.4217 - val_acc: 0.8163\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3385 - acc: 0.8498 - val_loss: 0.3903 - val_acc: 0.8277\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3363 - acc: 0.8505 - val_loss: 0.3922 - val_acc: 0.8264\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3305 - acc: 0.8553 - val_loss: 0.3860 - val_acc: 0.8284\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3288 - acc: 0.8530 - val_loss: 0.3970 - val_acc: 0.8232\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3253 - acc: 0.8557 - val_loss: 0.3890 - val_acc: 0.8235\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3226 - acc: 0.8569 - val_loss: 0.3970 - val_acc: 0.8283\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3213 - acc: 0.8570 - val_loss: 0.3871 - val_acc: 0.8244\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3170 - acc: 0.8606 - val_loss: 0.4174 - val_acc: 0.8127\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3165 - acc: 0.8614 - val_loss: 0.3887 - val_acc: 0.8276\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3127 - acc: 0.8626 - val_loss: 0.4019 - val_acc: 0.8256\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3102 - acc: 0.8647 - val_loss: 0.3924 - val_acc: 0.8285\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 321us/sample - loss: 0.5197 - acc: 0.7486 - val_loss: 0.4702 - val_acc: 0.7777\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4660 - acc: 0.7796 - val_loss: 0.4534 - val_acc: 0.7867\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4480 - acc: 0.7911 - val_loss: 0.4581 - val_acc: 0.7822\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4334 - acc: 0.7987 - val_loss: 0.4140 - val_acc: 0.8105\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4242 - acc: 0.8050 - val_loss: 0.4252 - val_acc: 0.7998\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4165 - acc: 0.8105 - val_loss: 0.4312 - val_acc: 0.8016\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4106 - acc: 0.8121 - val_loss: 0.4040 - val_acc: 0.8156\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4067 - acc: 0.8149 - val_loss: 0.3943 - val_acc: 0.8223\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4030 - acc: 0.8168 - val_loss: 0.4300 - val_acc: 0.7984\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3984 - acc: 0.8190 - val_loss: 0.4466 - val_acc: 0.7937\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3946 - acc: 0.8213 - val_loss: 0.4006 - val_acc: 0.8178\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3928 - acc: 0.8228 - val_loss: 0.4262 - val_acc: 0.8030\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3897 - acc: 0.8245 - val_loss: 0.4039 - val_acc: 0.8130\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3863 - acc: 0.8258 - val_loss: 0.3882 - val_acc: 0.8247\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3835 - acc: 0.8285 - val_loss: 0.3883 - val_acc: 0.8241\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3822 - acc: 0.8275 - val_loss: 0.3850 - val_acc: 0.8262\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3796 - acc: 0.8285 - val_loss: 0.4015 - val_acc: 0.8134\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3767 - acc: 0.8309 - val_loss: 0.3847 - val_acc: 0.8275\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3742 - acc: 0.8321 - val_loss: 0.3830 - val_acc: 0.8270\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3733 - acc: 0.8322 - val_loss: 0.3830 - val_acc: 0.8286\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3709 - acc: 0.8321 - val_loss: 0.3920 - val_acc: 0.8221\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3702 - acc: 0.8343 - val_loss: 0.3908 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3669 - acc: 0.8354 - val_loss: 0.3936 - val_acc: 0.8228\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3648 - acc: 0.8375 - val_loss: 0.3862 - val_acc: 0.8262\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3634 - acc: 0.8377 - val_loss: 0.3794 - val_acc: 0.8312\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3622 - acc: 0.8385 - val_loss: 0.3791 - val_acc: 0.8282\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3609 - acc: 0.8400 - val_loss: 0.3857 - val_acc: 0.8255\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3614 - acc: 0.8382 - val_loss: 0.3800 - val_acc: 0.8279\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3579 - acc: 0.8400 - val_loss: 0.3811 - val_acc: 0.8283\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3556 - acc: 0.8419 - val_loss: 0.3837 - val_acc: 0.8263\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 320us/sample - loss: 0.5207 - acc: 0.7491 - val_loss: 0.4571 - val_acc: 0.7814\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4606 - acc: 0.7816 - val_loss: 0.4409 - val_acc: 0.7954\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4408 - acc: 0.7944 - val_loss: 0.4703 - val_acc: 0.7757\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4291 - acc: 0.8001 - val_loss: 0.4157 - val_acc: 0.8082\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4201 - acc: 0.8069 - val_loss: 0.4118 - val_acc: 0.8110\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4125 - acc: 0.8107 - val_loss: 0.4076 - val_acc: 0.8119\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4080 - acc: 0.8137 - val_loss: 0.4024 - val_acc: 0.8141\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4041 - acc: 0.8160 - val_loss: 0.4178 - val_acc: 0.8066\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4007 - acc: 0.8182 - val_loss: 0.4102 - val_acc: 0.8111\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3960 - acc: 0.8207 - val_loss: 0.3932 - val_acc: 0.8227\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3920 - acc: 0.8228 - val_loss: 0.3924 - val_acc: 0.8224\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3892 - acc: 0.8234 - val_loss: 0.4024 - val_acc: 0.8143\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3874 - acc: 0.8256 - val_loss: 0.3965 - val_acc: 0.8191\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3852 - acc: 0.8265 - val_loss: 0.3891 - val_acc: 0.8230\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3846 - acc: 0.8275 - val_loss: 0.3862 - val_acc: 0.8255\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3805 - acc: 0.8303 - val_loss: 0.3929 - val_acc: 0.8235\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3794 - acc: 0.8289 - val_loss: 0.3942 - val_acc: 0.8228\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3768 - acc: 0.8309 - val_loss: 0.3869 - val_acc: 0.8262\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3736 - acc: 0.8315 - val_loss: 0.3834 - val_acc: 0.8259\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3735 - acc: 0.8322 - val_loss: 0.3864 - val_acc: 0.8224\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3722 - acc: 0.8336 - val_loss: 0.3915 - val_acc: 0.8227\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3701 - acc: 0.8342 - val_loss: 0.3970 - val_acc: 0.8199\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3678 - acc: 0.8349 - val_loss: 0.3874 - val_acc: 0.8231\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3676 - acc: 0.8355 - val_loss: 0.3882 - val_acc: 0.8242\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3663 - acc: 0.8357 - val_loss: 0.3869 - val_acc: 0.8245\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3631 - acc: 0.8369 - val_loss: 0.3873 - val_acc: 0.8248\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3603 - acc: 0.8390 - val_loss: 0.4020 - val_acc: 0.8165\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3607 - acc: 0.8387 - val_loss: 0.3841 - val_acc: 0.8255\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3595 - acc: 0.8397 - val_loss: 0.3844 - val_acc: 0.8249\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3556 - acc: 0.8417 - val_loss: 0.3830 - val_acc: 0.8281\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 30s 351us/sample - loss: 0.5240 - acc: 0.7495 - val_loss: 0.4526 - val_acc: 0.7832\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4505 - acc: 0.7891 - val_loss: 0.5959 - val_acc: 0.7148\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4291 - acc: 0.8024 - val_loss: 0.4399 - val_acc: 0.7986\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.4171 - acc: 0.8104 - val_loss: 0.4183 - val_acc: 0.8097\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.4066 - acc: 0.8138 - val_loss: 0.4730 - val_acc: 0.7873\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3974 - acc: 0.8193 - val_loss: 0.3960 - val_acc: 0.8206\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3888 - acc: 0.8233 - val_loss: 0.3998 - val_acc: 0.8176\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3835 - acc: 0.8285 - val_loss: 0.3877 - val_acc: 0.8239\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3764 - acc: 0.8319 - val_loss: 0.3992 - val_acc: 0.8178\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3709 - acc: 0.8343 - val_loss: 0.4046 - val_acc: 0.8147\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3653 - acc: 0.8370 - val_loss: 0.3821 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3585 - acc: 0.8403 - val_loss: 0.3828 - val_acc: 0.8284\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3538 - acc: 0.8425 - val_loss: 0.3818 - val_acc: 0.8281\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3473 - acc: 0.8467 - val_loss: 0.3850 - val_acc: 0.8255\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3422 - acc: 0.8488 - val_loss: 0.3832 - val_acc: 0.8274\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3380 - acc: 0.8490 - val_loss: 0.3921 - val_acc: 0.8275\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3317 - acc: 0.8535 - val_loss: 0.3924 - val_acc: 0.8261\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3268 - acc: 0.8553 - val_loss: 0.3862 - val_acc: 0.8282\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3213 - acc: 0.8590 - val_loss: 0.3912 - val_acc: 0.8281\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.3174 - acc: 0.8606 - val_loss: 0.3957 - val_acc: 0.8199\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.3111 - acc: 0.8630 - val_loss: 0.3957 - val_acc: 0.8256\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.3064 - acc: 0.8664 - val_loss: 0.4034 - val_acc: 0.8205\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.3036 - acc: 0.8669 - val_loss: 0.4080 - val_acc: 0.8258\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2984 - acc: 0.8701 - val_loss: 0.4098 - val_acc: 0.8207\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2955 - acc: 0.8716 - val_loss: 0.3982 - val_acc: 0.8273\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2905 - acc: 0.8738 - val_loss: 0.4089 - val_acc: 0.8217\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2864 - acc: 0.8749 - val_loss: 0.4070 - val_acc: 0.8248\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.2851 - acc: 0.8760 - val_loss: 0.4141 - val_acc: 0.8219\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2812 - acc: 0.8780 - val_loss: 0.4134 - val_acc: 0.8229\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2769 - acc: 0.8804 - val_loss: 0.4172 - val_acc: 0.8206\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 31s 367us/sample - loss: 0.5172 - acc: 0.7528 - val_loss: 0.4626 - val_acc: 0.7761\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4512 - acc: 0.7894 - val_loss: 0.4344 - val_acc: 0.7998\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4312 - acc: 0.8021 - val_loss: 0.4137 - val_acc: 0.8105\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4164 - acc: 0.8099 - val_loss: 0.4168 - val_acc: 0.8091\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4090 - acc: 0.8148 - val_loss: 0.4161 - val_acc: 0.8072\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3998 - acc: 0.8185 - val_loss: 0.4143 - val_acc: 0.8081\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3916 - acc: 0.8229 - val_loss: 0.3925 - val_acc: 0.8208\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3832 - acc: 0.8286 - val_loss: 0.3922 - val_acc: 0.8254\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3756 - acc: 0.8317 - val_loss: 0.3916 - val_acc: 0.8220\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3702 - acc: 0.8343 - val_loss: 0.3843 - val_acc: 0.8246\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3630 - acc: 0.8383 - val_loss: 0.3826 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3566 - acc: 0.8406 - val_loss: 0.3880 - val_acc: 0.8250\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3507 - acc: 0.8441 - val_loss: 0.3853 - val_acc: 0.8280\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3447 - acc: 0.8472 - val_loss: 0.3942 - val_acc: 0.8214\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3391 - acc: 0.8505 - val_loss: 0.3815 - val_acc: 0.8259\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3328 - acc: 0.8535 - val_loss: 0.3829 - val_acc: 0.8298\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3255 - acc: 0.8569 - val_loss: 0.3912 - val_acc: 0.8288\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3200 - acc: 0.8612 - val_loss: 0.3920 - val_acc: 0.8266\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3136 - acc: 0.8632 - val_loss: 0.3984 - val_acc: 0.8245\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3090 - acc: 0.8647 - val_loss: 0.4317 - val_acc: 0.8142\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3052 - acc: 0.8663 - val_loss: 0.3949 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2981 - acc: 0.8716 - val_loss: 0.3984 - val_acc: 0.8245\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2935 - acc: 0.8735 - val_loss: 0.4113 - val_acc: 0.8256\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.2891 - acc: 0.8743 - val_loss: 0.4110 - val_acc: 0.8203\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2849 - acc: 0.8768 - val_loss: 0.4141 - val_acc: 0.8220\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2781 - acc: 0.8806 - val_loss: 0.4316 - val_acc: 0.8213\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.2762 - acc: 0.8812 - val_loss: 0.4253 - val_acc: 0.8220\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 180us/sample - loss: 0.2691 - acc: 0.8850 - val_loss: 0.4354 - val_acc: 0.8165\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2658 - acc: 0.8858 - val_loss: 0.4264 - val_acc: 0.8263\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.2646 - acc: 0.8869 - val_loss: 0.4293 - val_acc: 0.8232\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 31s 366us/sample - loss: 0.5511 - acc: 0.7296 - val_loss: 0.4826 - val_acc: 0.7667\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4912 - acc: 0.7620 - val_loss: 0.4646 - val_acc: 0.7783\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4733 - acc: 0.7743 - val_loss: 0.4495 - val_acc: 0.7876\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4621 - acc: 0.7817 - val_loss: 0.4658 - val_acc: 0.7752\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4538 - acc: 0.7858 - val_loss: 0.4341 - val_acc: 0.7991\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4459 - acc: 0.7893 - val_loss: 0.4299 - val_acc: 0.8008\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4405 - acc: 0.7948 - val_loss: 0.4188 - val_acc: 0.8045\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4361 - acc: 0.7976 - val_loss: 0.4285 - val_acc: 0.7997\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4315 - acc: 0.8002 - val_loss: 0.4291 - val_acc: 0.8001\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4276 - acc: 0.8022 - val_loss: 0.4157 - val_acc: 0.8095\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4252 - acc: 0.8034 - val_loss: 0.4179 - val_acc: 0.8071\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4231 - acc: 0.8043 - val_loss: 0.4102 - val_acc: 0.8130\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4215 - acc: 0.8052 - val_loss: 0.4137 - val_acc: 0.8112\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4188 - acc: 0.8073 - val_loss: 0.4257 - val_acc: 0.8041\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4148 - acc: 0.8104 - val_loss: 0.4012 - val_acc: 0.8163\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4154 - acc: 0.8097 - val_loss: 0.4179 - val_acc: 0.8073\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4124 - acc: 0.8109 - val_loss: 0.4035 - val_acc: 0.8158\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4102 - acc: 0.8109 - val_loss: 0.4068 - val_acc: 0.8122\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4090 - acc: 0.8139 - val_loss: 0.4101 - val_acc: 0.8105 loss: 0.4091 - acc: \n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4079 - acc: 0.8139 - val_loss: 0.4171 - val_acc: 0.8109\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4065 - acc: 0.8146 - val_loss: 0.4002 - val_acc: 0.8178\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4065 - acc: 0.8134 - val_loss: 0.3991 - val_acc: 0.8181\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4058 - acc: 0.8149 - val_loss: 0.4020 - val_acc: 0.8159\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4032 - acc: 0.8153 - val_loss: 0.3990 - val_acc: 0.8152- ETA: 0s - loss: 0.402\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4014 - acc: 0.8162 - val_loss: 0.4003 - val_acc: 0.8151\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4009 - acc: 0.8166 - val_loss: 0.3974 - val_acc: 0.8167\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.3998 - acc: 0.8189 - val_loss: 0.4006 - val_acc: 0.8177\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.3993 - acc: 0.8178 - val_loss: 0.3981 - val_acc: 0.8185\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.3968 - acc: 0.8182 - val_loss: 0.3949 - val_acc: 0.8194\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.3980 - acc: 0.8193 - val_loss: 0.4024 - val_acc: 0.8148\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 31s 365us/sample - loss: 0.5541 - acc: 0.7239 - val_loss: 0.4851 - val_acc: 0.7662\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4893 - acc: 0.7632 - val_loss: 0.4635 - val_acc: 0.7817\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4697 - acc: 0.7760 - val_loss: 0.4571 - val_acc: 0.7839\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4582 - acc: 0.7833 - val_loss: 0.4427 - val_acc: 0.7901\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 180us/sample - loss: 0.4489 - acc: 0.7910 - val_loss: 0.4326 - val_acc: 0.7971\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4437 - acc: 0.7937 - val_loss: 0.4272 - val_acc: 0.8019\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4393 - acc: 0.7960 - val_loss: 0.4238 - val_acc: 0.8038\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4324 - acc: 0.8000 - val_loss: 0.4238 - val_acc: 0.8037\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4304 - acc: 0.8011 - val_loss: 0.4247 - val_acc: 0.8041\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4260 - acc: 0.8033 - val_loss: 0.4136 - val_acc: 0.8104\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4240 - acc: 0.8042 - val_loss: 0.4139 - val_acc: 0.8083\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4203 - acc: 0.8070 - val_loss: 0.4092 - val_acc: 0.8123\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4196 - acc: 0.8068 - val_loss: 0.4127 - val_acc: 0.8091\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4164 - acc: 0.8091 - val_loss: 0.4049 - val_acc: 0.8145\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4142 - acc: 0.8099 - val_loss: 0.4052 - val_acc: 0.8123\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4128 - acc: 0.8099 - val_loss: 0.4072 - val_acc: 0.8121\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4094 - acc: 0.8126 - val_loss: 0.4048 - val_acc: 0.8141\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4100 - acc: 0.8127 - val_loss: 0.4174 - val_acc: 0.8066\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4077 - acc: 0.8135 - val_loss: 0.4172 - val_acc: 0.8065\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4062 - acc: 0.8146 - val_loss: 0.3977 - val_acc: 0.8194\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4045 - acc: 0.8149 - val_loss: 0.4003 - val_acc: 0.8152\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4010 - acc: 0.8179 - val_loss: 0.4012 - val_acc: 0.8144\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4024 - acc: 0.8166 - val_loss: 0.4020 - val_acc: 0.8145\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4014 - acc: 0.8167 - val_loss: 0.4060 - val_acc: 0.8141\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.3990 - acc: 0.8182 - val_loss: 0.4005 - val_acc: 0.8150\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3999 - acc: 0.8183 - val_loss: 0.3982 - val_acc: 0.8174\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3965 - acc: 0.8196 - val_loss: 0.3969 - val_acc: 0.8184\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3948 - acc: 0.8212 - val_loss: 0.4072 - val_acc: 0.8119\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3957 - acc: 0.8191 - val_loss: 0.3996 - val_acc: 0.8162\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3948 - acc: 0.8200 - val_loss: 0.3978 - val_acc: 0.8173\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 33s 383us/sample - loss: 0.5675 - acc: 0.7193 - val_loss: 0.4868 - val_acc: 0.7691\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.5001 - acc: 0.7584 - val_loss: 0.4666 - val_acc: 0.7786\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4845 - acc: 0.7679 - val_loss: 0.4625 - val_acc: 0.7801\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4713 - acc: 0.7757 - val_loss: 0.4401 - val_acc: 0.7953\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4612 - acc: 0.7844 - val_loss: 0.4434 - val_acc: 0.7916\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4563 - acc: 0.7863 - val_loss: 0.4292 - val_acc: 0.8005\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4522 - acc: 0.7884 - val_loss: 0.4266 - val_acc: 0.8042\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4457 - acc: 0.7929 - val_loss: 0.4278 - val_acc: 0.8037\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4455 - acc: 0.7928 - val_loss: 0.4249 - val_acc: 0.8059\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4423 - acc: 0.7949 - val_loss: 0.4195 - val_acc: 0.8076\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4404 - acc: 0.7960 - val_loss: 0.4191 - val_acc: 0.8074\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4365 - acc: 0.7977 - val_loss: 0.4154 - val_acc: 0.8091\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4360 - acc: 0.7977 - val_loss: 0.4183 - val_acc: 0.8076\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4321 - acc: 0.7995 - val_loss: 0.4291 - val_acc: 0.8005\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4330 - acc: 0.7998 - val_loss: 0.4195 - val_acc: 0.8058\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4324 - acc: 0.8007 - val_loss: 0.4089 - val_acc: 0.8117\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4323 - acc: 0.7990 - val_loss: 0.4118 - val_acc: 0.8114\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4302 - acc: 0.8020 - val_loss: 0.4135 - val_acc: 0.8094\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4299 - acc: 0.8005 - val_loss: 0.4247 - val_acc: 0.8002\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4274 - acc: 0.8039 - val_loss: 0.4100 - val_acc: 0.8134\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4282 - acc: 0.8010 - val_loss: 0.4163 - val_acc: 0.8048\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4266 - acc: 0.8031 - val_loss: 0.4030 - val_acc: 0.8165\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4249 - acc: 0.8046 - val_loss: 0.4147 - val_acc: 0.8101\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4248 - acc: 0.8045 - val_loss: 0.4050 - val_acc: 0.8137\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4257 - acc: 0.8032 - val_loss: 0.4044 - val_acc: 0.8156\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4256 - acc: 0.8039 - val_loss: 0.4058 - val_acc: 0.8143\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4227 - acc: 0.8057 - val_loss: 0.4066 - val_acc: 0.8145\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4232 - acc: 0.8058 - val_loss: 0.4058 - val_acc: 0.8124\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4236 - acc: 0.8061 - val_loss: 0.4022 - val_acc: 0.8155\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4227 - acc: 0.8045 - val_loss: 0.4100 - val_acc: 0.8135\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 34s 395us/sample - loss: 0.5843 - acc: 0.7071 - val_loss: 0.5004 - val_acc: 0.7574\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 192us/sample - loss: 0.5158 - acc: 0.7496 - val_loss: 0.4902 - val_acc: 0.7591\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4969 - acc: 0.7596 - val_loss: 0.4678 - val_acc: 0.7775\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4855 - acc: 0.7680 - val_loss: 0.4683 - val_acc: 0.7777\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4759 - acc: 0.7741 - val_loss: 0.4521 - val_acc: 0.7890\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4682 - acc: 0.7786 - val_loss: 0.4601 - val_acc: 0.7821\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4614 - acc: 0.7820 - val_loss: 0.4364 - val_acc: 0.7987\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4547 - acc: 0.7878 - val_loss: 0.4653 - val_acc: 0.7754\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4515 - acc: 0.7890 - val_loss: 0.4304 - val_acc: 0.8015\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4455 - acc: 0.7918 - val_loss: 0.4433 - val_acc: 0.7842\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4445 - acc: 0.7926 - val_loss: 0.4357 - val_acc: 0.7969\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4423 - acc: 0.7941 - val_loss: 0.4262 - val_acc: 0.8022\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4396 - acc: 0.7961 - val_loss: 0.4230 - val_acc: 0.8060\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4379 - acc: 0.7977 - val_loss: 0.4204 - val_acc: 0.8062\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4376 - acc: 0.7971 - val_loss: 0.4197 - val_acc: 0.8082\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4360 - acc: 0.7980 - val_loss: 0.4217 - val_acc: 0.8055\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4355 - acc: 0.7986 - val_loss: 0.4148 - val_acc: 0.8088\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4347 - acc: 0.8001 - val_loss: 0.4193 - val_acc: 0.8097\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4341 - acc: 0.7978 - val_loss: 0.4326 - val_acc: 0.8015\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4325 - acc: 0.7992 - val_loss: 0.4113 - val_acc: 0.8118\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4321 - acc: 0.7996 - val_loss: 0.4209 - val_acc: 0.8044\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4306 - acc: 0.8012 - val_loss: 0.4181 - val_acc: 0.8064\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4294 - acc: 0.8006 - val_loss: 0.4136 - val_acc: 0.8077\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4292 - acc: 0.8014 - val_loss: 0.4139 - val_acc: 0.8084\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4289 - acc: 0.8012 - val_loss: 0.4317 - val_acc: 0.7985\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4287 - acc: 0.8026 - val_loss: 0.4103 - val_acc: 0.8145\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4295 - acc: 0.8009 - val_loss: 0.4115 - val_acc: 0.8130\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4265 - acc: 0.8041 - val_loss: 0.4081 - val_acc: 0.8127\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4283 - acc: 0.8039 - val_loss: 0.4095 - val_acc: 0.8137\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4285 - acc: 0.8034 - val_loss: 0.4192 - val_acc: 0.8101\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 32s 379us/sample - loss: 0.5335 - acc: 0.7400 - val_loss: 0.4811 - val_acc: 0.7714\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4680 - acc: 0.7777 - val_loss: 0.4613 - val_acc: 0.7839\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4456 - acc: 0.7919 - val_loss: 0.4315 - val_acc: 0.7968\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4313 - acc: 0.7998 - val_loss: 0.4122 - val_acc: 0.8094\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4221 - acc: 0.8063 - val_loss: 0.4086 - val_acc: 0.8125\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4133 - acc: 0.8103 - val_loss: 0.4168 - val_acc: 0.8094\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4076 - acc: 0.8136 - val_loss: 0.4104 - val_acc: 0.8097\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4032 - acc: 0.8158 - val_loss: 0.4003 - val_acc: 0.8178\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3989 - acc: 0.8184 - val_loss: 0.4154 - val_acc: 0.8090\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3931 - acc: 0.8224 - val_loss: 0.4400 - val_acc: 0.7929\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 180us/sample - loss: 0.3880 - acc: 0.8244 - val_loss: 0.4039 - val_acc: 0.8164\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3838 - acc: 0.8271 - val_loss: 0.3926 - val_acc: 0.8207\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3804 - acc: 0.8282 - val_loss: 0.3930 - val_acc: 0.8212\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3761 - acc: 0.8317 - val_loss: 0.3897 - val_acc: 0.8219\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3734 - acc: 0.8323 - val_loss: 0.3953 - val_acc: 0.8195\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3699 - acc: 0.8335 - val_loss: 0.3934 - val_acc: 0.8198\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3657 - acc: 0.8348 - val_loss: 0.3992 - val_acc: 0.8147\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3614 - acc: 0.8375 - val_loss: 0.3899 - val_acc: 0.8239\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3621 - acc: 0.8384 - val_loss: 0.3901 - val_acc: 0.8204\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3568 - acc: 0.8407 - val_loss: 0.3922 - val_acc: 0.8199 - ETA: 2 - ETA: 0s - loss: 0.3566\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3546 - acc: 0.8425 - val_loss: 0.3925 - val_acc: 0.8224\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3507 - acc: 0.8445 - val_loss: 0.3859 - val_acc: 0.8253\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 177us/sample - loss: 0.3497 - acc: 0.8442 - val_loss: 0.4023 - val_acc: 0.8144\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3475 - acc: 0.8454 - val_loss: 0.3929 - val_acc: 0.8199 loss: 0.3456 - acc: 0.846 - ETA: 6s - loss: 0.34 - ETA: 2s - loss: 0.3479 - \n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3439 - acc: 0.8464 - val_loss: 0.3931 - val_acc: 0.8228\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3402 - acc: 0.8478 - val_loss: 0.3891 - val_acc: 0.8231\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3390 - acc: 0.8498 - val_loss: 0.3939 - val_acc: 0.8230\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3352 - acc: 0.8510 - val_loss: 0.3973 - val_acc: 0.8231\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3340 - acc: 0.8517 - val_loss: 0.3979 - val_acc: 0.8180\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3322 - acc: 0.8537 - val_loss: 0.4016 - val_acc: 0.8192\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 32s 379us/sample - loss: 0.5432 - acc: 0.7361 - val_loss: 0.4637 - val_acc: 0.7793\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4696 - acc: 0.7771 - val_loss: 0.4717 - val_acc: 0.7763\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4480 - acc: 0.7902 - val_loss: 0.4431 - val_acc: 0.7930s - loss\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4337 - acc: 0.7982 - val_loss: 0.4292 - val_acc: 0.8006\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4239 - acc: 0.8057 - val_loss: 0.4137 - val_acc: 0.8091\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4155 - acc: 0.8094 - val_loss: 0.4094 - val_acc: 0.8134\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4102 - acc: 0.8128 - val_loss: 0.4005 - val_acc: 0.8166\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4028 - acc: 0.8164 - val_loss: 0.4078 - val_acc: 0.8156\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3985 - acc: 0.8193 - val_loss: 0.4020 - val_acc: 0.8166\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3928 - acc: 0.8219 - val_loss: 0.4184 - val_acc: 0.8054\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3907 - acc: 0.8241 - val_loss: 0.4032 - val_acc: 0.8152\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3851 - acc: 0.8259 - val_loss: 0.3935 - val_acc: 0.8221\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3818 - acc: 0.8283 - val_loss: 0.3978 - val_acc: 0.8183\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3799 - acc: 0.8293 - val_loss: 0.3973 - val_acc: 0.8167\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3735 - acc: 0.8318 - val_loss: 0.3955 - val_acc: 0.8192\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3710 - acc: 0.8339 - val_loss: 0.4036 - val_acc: 0.8162\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3678 - acc: 0.8349 - val_loss: 0.3923 - val_acc: 0.8198\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3652 - acc: 0.8364 - val_loss: 0.3904 - val_acc: 0.8233\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3623 - acc: 0.8380 - val_loss: 0.3939 - val_acc: 0.8198\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 180us/sample - loss: 0.3588 - acc: 0.8403 - val_loss: 0.3911 - val_acc: 0.8223\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3577 - acc: 0.8395 - val_loss: 0.3993 - val_acc: 0.8181\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3533 - acc: 0.8431 - val_loss: 0.3940 - val_acc: 0.8199\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3517 - acc: 0.8434 - val_loss: 0.3900 - val_acc: 0.8217\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3465 - acc: 0.8453 - val_loss: 0.3942 - val_acc: 0.8207\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3441 - acc: 0.8458 - val_loss: 0.4357 - val_acc: 0.8041\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3428 - acc: 0.8467 - val_loss: 0.4059 - val_acc: 0.8156\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3404 - acc: 0.8468 - val_loss: 0.3993 - val_acc: 0.8231\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3367 - acc: 0.8504 - val_loss: 0.3918 - val_acc: 0.8241\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3352 - acc: 0.8518 - val_loss: 0.3936 - val_acc: 0.8220\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3349 - acc: 0.8495 - val_loss: 0.3996 - val_acc: 0.8185\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 33s 393us/sample - loss: 0.5413 - acc: 0.7365 - val_loss: 0.4653 - val_acc: 0.7782\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4684 - acc: 0.7792 - val_loss: 0.4459 - val_acc: 0.7906s: 0.4687 - acc: \n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4490 - acc: 0.7900 - val_loss: 0.4468 - val_acc: 0.7947\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4353 - acc: 0.7995 - val_loss: 0.4171 - val_acc: 0.8072\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4263 - acc: 0.8049 - val_loss: 0.4093 - val_acc: 0.8144\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4194 - acc: 0.8072 - val_loss: 0.4205 - val_acc: 0.8035\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4135 - acc: 0.8116 - val_loss: 0.4210 - val_acc: 0.8040\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4095 - acc: 0.8123 - val_loss: 0.4076 - val_acc: 0.8134\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4052 - acc: 0.8158 - val_loss: 0.3937 - val_acc: 0.8202TA: 1s - loss:\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4016 - acc: 0.8179 - val_loss: 0.4258 - val_acc: 0.8041\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3977 - acc: 0.8186 - val_loss: 0.3961 - val_acc: 0.8180\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3944 - acc: 0.8214 - val_loss: 0.3918 - val_acc: 0.8219\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3906 - acc: 0.8229 - val_loss: 0.3933 - val_acc: 0.8219\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3902 - acc: 0.8228 - val_loss: 0.3891 - val_acc: 0.8233\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3882 - acc: 0.8238 - val_loss: 0.3878 - val_acc: 0.8217\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3850 - acc: 0.8254 - val_loss: 0.3930 - val_acc: 0.8217\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3828 - acc: 0.8277 - val_loss: 0.3976 - val_acc: 0.8166\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3791 - acc: 0.8298 - val_loss: 0.3833 - val_acc: 0.8260\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3793 - acc: 0.8293 - val_loss: 0.3815 - val_acc: 0.8269\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3757 - acc: 0.8310 - val_loss: 0.3870 - val_acc: 0.8261\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3750 - acc: 0.8329 - val_loss: 0.3871 - val_acc: 0.8242\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3732 - acc: 0.8331 - val_loss: 0.3990 - val_acc: 0.8194\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3723 - acc: 0.8324 - val_loss: 0.3967 - val_acc: 0.8181\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3695 - acc: 0.8345 - val_loss: 0.3860 - val_acc: 0.8253\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3682 - acc: 0.8361 - val_loss: 0.3893 - val_acc: 0.8233\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3679 - acc: 0.8345 - val_loss: 0.3821 - val_acc: 0.8263\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3660 - acc: 0.8364 - val_loss: 0.3808 - val_acc: 0.8266\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3661 - acc: 0.8358 - val_loss: 0.3866 - val_acc: 0.8271\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3633 - acc: 0.8379 - val_loss: 0.3887 - val_acc: 0.8222\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3640 - acc: 0.8378 - val_loss: 0.3846 - val_acc: 0.8263\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 35s 410us/sample - loss: 0.5605 - acc: 0.7272 - val_loss: 0.4761 - val_acc: 0.7682\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4803 - acc: 0.7718 - val_loss: 0.4536 - val_acc: 0.7850\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.4601 - acc: 0.7827 - val_loss: 0.4532 - val_acc: 0.7859\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.4471 - acc: 0.7914 - val_loss: 0.4649 - val_acc: 0.7826\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.4367 - acc: 0.7982 - val_loss: 0.4234 - val_acc: 0.8087\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4271 - acc: 0.8025 - val_loss: 0.4229 - val_acc: 0.8026\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4203 - acc: 0.8063 - val_loss: 0.4048 - val_acc: 0.8160\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4151 - acc: 0.8095 - val_loss: 0.4180 - val_acc: 0.8066\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4099 - acc: 0.8132 - val_loss: 0.4385 - val_acc: 0.7946\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4057 - acc: 0.8156 - val_loss: 0.3962 - val_acc: 0.8168\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4025 - acc: 0.8176 - val_loss: 0.3955 - val_acc: 0.8163\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3990 - acc: 0.8184 - val_loss: 0.3925 - val_acc: 0.8190\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3946 - acc: 0.8212 - val_loss: 0.3838 - val_acc: 0.8249\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3938 - acc: 0.8227 - val_loss: 0.3869 - val_acc: 0.8220\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3895 - acc: 0.8241 - val_loss: 0.4036 - val_acc: 0.8151\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3856 - acc: 0.8266 - val_loss: 0.3919 - val_acc: 0.8224\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3850 - acc: 0.8250 - val_loss: 0.3870 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3833 - acc: 0.8260 - val_loss: 0.3841 - val_acc: 0.8251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3813 - acc: 0.8286 - val_loss: 0.4035 - val_acc: 0.8142\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3797 - acc: 0.8285 - val_loss: 0.3898 - val_acc: 0.8209\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3769 - acc: 0.8301 - val_loss: 0.3826 - val_acc: 0.8250\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3746 - acc: 0.8324 - val_loss: 0.3909 - val_acc: 0.8209\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3755 - acc: 0.8314 - val_loss: 0.3890 - val_acc: 0.8236\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3720 - acc: 0.8337 - val_loss: 0.3821 - val_acc: 0.8255\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3727 - acc: 0.8329 - val_loss: 0.3891 - val_acc: 0.8216\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3716 - acc: 0.8335 - val_loss: 0.3755 - val_acc: 0.8292\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3706 - acc: 0.8342 - val_loss: 0.3804 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3688 - acc: 0.8348 - val_loss: 0.3820 - val_acc: 0.8269\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3672 - acc: 0.8358 - val_loss: 0.3813 - val_acc: 0.8256\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3676 - acc: 0.8350 - val_loss: 0.3753 - val_acc: 0.8266\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 33s 391us/sample - loss: 0.5157 - acc: 0.7499 - val_loss: 0.4730 - val_acc: 0.7750\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4540 - acc: 0.7864 - val_loss: 0.4281 - val_acc: 0.8000\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4318 - acc: 0.7999 - val_loss: 0.4250 - val_acc: 0.8040\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4208 - acc: 0.8069 - val_loss: 0.4314 - val_acc: 0.7966\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4106 - acc: 0.8127 - val_loss: 0.4063 - val_acc: 0.8127\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4030 - acc: 0.8156 - val_loss: 0.3982 - val_acc: 0.8163\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3949 - acc: 0.8213 - val_loss: 0.4072 - val_acc: 0.8170\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3908 - acc: 0.8238 - val_loss: 0.3942 - val_acc: 0.8196\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3848 - acc: 0.8269 - val_loss: 0.4010 - val_acc: 0.8140\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3788 - acc: 0.8308 - val_loss: 0.3931 - val_acc: 0.8218\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3728 - acc: 0.8346 - val_loss: 0.3904 - val_acc: 0.8231\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3680 - acc: 0.8355 - val_loss: 0.3872 - val_acc: 0.8260\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3637 - acc: 0.8368 - val_loss: 0.4559 - val_acc: 0.7916\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3599 - acc: 0.8395 - val_loss: 0.3859 - val_acc: 0.8260\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3550 - acc: 0.8414 - val_loss: 0.4044 - val_acc: 0.8125\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3522 - acc: 0.8446 - val_loss: 0.3947 - val_acc: 0.8229\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3457 - acc: 0.8463 - val_loss: 0.3947 - val_acc: 0.8217\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3444 - acc: 0.8476 - val_loss: 0.4130 - val_acc: 0.8184\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3383 - acc: 0.8501 - val_loss: 0.4231 - val_acc: 0.8073\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3361 - acc: 0.8499 - val_loss: 0.3959 - val_acc: 0.8228\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3288 - acc: 0.8550 - val_loss: 0.4083 - val_acc: 0.8178\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3259 - acc: 0.8562 - val_loss: 0.3945 - val_acc: 0.8242\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3216 - acc: 0.8575 - val_loss: 0.3988 - val_acc: 0.8263\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3178 - acc: 0.8603 - val_loss: 0.3956 - val_acc: 0.8233\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3156 - acc: 0.8602 - val_loss: 0.4189 - val_acc: 0.8177\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3097 - acc: 0.8648 - val_loss: 0.3937 - val_acc: 0.8263\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3091 - acc: 0.8633 - val_loss: 0.4175 - val_acc: 0.8187\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3037 - acc: 0.8670 - val_loss: 0.4191 - val_acc: 0.8148\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.2991 - acc: 0.8693 - val_loss: 0.4108 - val_acc: 0.8235\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.2964 - acc: 0.8696 - val_loss: 0.4168 - val_acc: 0.8190\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 34s 398us/sample - loss: 0.5150 - acc: 0.7497 - val_loss: 0.4818 - val_acc: 0.7673\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4544 - acc: 0.7861 - val_loss: 0.4368 - val_acc: 0.7964\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.4333 - acc: 0.7991 - val_loss: 0.4192 - val_acc: 0.8076\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4217 - acc: 0.8062 - val_loss: 0.4045 - val_acc: 0.8159\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4095 - acc: 0.8121 - val_loss: 0.4096 - val_acc: 0.8122\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4009 - acc: 0.8179 - val_loss: 0.4103 - val_acc: 0.8107\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3951 - acc: 0.8205 - val_loss: 0.4174 - val_acc: 0.8072\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3893 - acc: 0.8247 - val_loss: 0.4255 - val_acc: 0.8033\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3843 - acc: 0.8264 - val_loss: 0.4133 - val_acc: 0.8112\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3793 - acc: 0.8297 - val_loss: 0.3876 - val_acc: 0.8257\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3737 - acc: 0.8319 - val_loss: 0.3872 - val_acc: 0.8226\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3695 - acc: 0.8342 - val_loss: 0.3837 - val_acc: 0.8260\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3646 - acc: 0.8371 - val_loss: 0.3977 - val_acc: 0.8192\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3586 - acc: 0.8404 - val_loss: 0.3921 - val_acc: 0.8239 0.840\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3553 - acc: 0.8415 - val_loss: 0.3894 - val_acc: 0.8250\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3495 - acc: 0.8442 - val_loss: 0.3886 - val_acc: 0.8252\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3458 - acc: 0.8467 - val_loss: 0.3902 - val_acc: 0.8230\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3398 - acc: 0.8497 - val_loss: 0.3929 - val_acc: 0.8219\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3381 - acc: 0.8508 - val_loss: 0.3901 - val_acc: 0.8281\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3323 - acc: 0.8537 - val_loss: 0.3998 - val_acc: 0.8214\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3279 - acc: 0.8548 - val_loss: 0.3957 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3248 - acc: 0.8557 - val_loss: 0.4049 - val_acc: 0.8220\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3179 - acc: 0.8603 - val_loss: 0.4069 - val_acc: 0.8189\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3152 - acc: 0.8618 - val_loss: 0.3997 - val_acc: 0.8216\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3129 - acc: 0.8630 - val_loss: 0.3982 - val_acc: 0.8246\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3061 - acc: 0.8670 - val_loss: 0.4137 - val_acc: 0.8193\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3038 - acc: 0.8671 - val_loss: 0.4031 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3015 - acc: 0.8678 - val_loss: 0.4158 - val_acc: 0.8166\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.2956 - acc: 0.8706 - val_loss: 0.4107 - val_acc: 0.8174\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.2910 - acc: 0.8729 - val_loss: 0.4130 - val_acc: 0.8185\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 35s 409us/sample - loss: 0.5217 - acc: 0.7482 - val_loss: 0.4611 - val_acc: 0.7829\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4574 - acc: 0.7853 - val_loss: 0.4271 - val_acc: 0.8009\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.4362 - acc: 0.7988 - val_loss: 0.4581 - val_acc: 0.7844\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.4230 - acc: 0.8053 - val_loss: 0.4060 - val_acc: 0.8140\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4114 - acc: 0.8136 - val_loss: 0.4062 - val_acc: 0.8135\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4049 - acc: 0.8162 - val_loss: 0.3950 - val_acc: 0.8205\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3971 - acc: 0.8204 - val_loss: 0.4004 - val_acc: 0.8158\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3913 - acc: 0.8238 - val_loss: 0.4017 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3851 - acc: 0.8258 - val_loss: 0.4501 - val_acc: 0.7929\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3791 - acc: 0.8303 - val_loss: 0.3927 - val_acc: 0.8207\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3746 - acc: 0.8331 - val_loss: 0.3917 - val_acc: 0.8199\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3709 - acc: 0.8347 - val_loss: 0.3809 - val_acc: 0.8275\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3644 - acc: 0.8373 - val_loss: 0.3966 - val_acc: 0.8235\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.3584 - acc: 0.8398 - val_loss: 0.3863 - val_acc: 0.8267\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3566 - acc: 0.8411 - val_loss: 0.3890 - val_acc: 0.8245\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3511 - acc: 0.8438 - val_loss: 0.4202 - val_acc: 0.8066\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3481 - acc: 0.8452 - val_loss: 0.3859 - val_acc: 0.8267\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3442 - acc: 0.8475 - val_loss: 0.3874 - val_acc: 0.8277\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3396 - acc: 0.8507 - val_loss: 0.3929 - val_acc: 0.8244\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3371 - acc: 0.8506 - val_loss: 0.3898 - val_acc: 0.8271\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3326 - acc: 0.8541 - val_loss: 0.3905 - val_acc: 0.8236\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3309 - acc: 0.8542 - val_loss: 0.3847 - val_acc: 0.8271\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3270 - acc: 0.8568 - val_loss: 0.4007 - val_acc: 0.8205\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3248 - acc: 0.8574 - val_loss: 0.4028 - val_acc: 0.8192\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3214 - acc: 0.8593 - val_loss: 0.3946 - val_acc: 0.8230\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3165 - acc: 0.8623 - val_loss: 0.3915 - val_acc: 0.8262\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3157 - acc: 0.8629 - val_loss: 0.3991 - val_acc: 0.8254\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3126 - acc: 0.8630 - val_loss: 0.4040 - val_acc: 0.8195\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3092 - acc: 0.8644 - val_loss: 0.4009 - val_acc: 0.8258\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3085 - acc: 0.8659 - val_loss: 0.3962 - val_acc: 0.8257\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 36s 427us/sample - loss: 0.5261 - acc: 0.7454 - val_loss: 0.4783 - val_acc: 0.7732\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.4622 - acc: 0.7818 - val_loss: 0.4473 - val_acc: 0.7911\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.4405 - acc: 0.7955 - val_loss: 0.4162 - val_acc: 0.8087\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.4274 - acc: 0.8029 - val_loss: 0.4113 - val_acc: 0.8115\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.4159 - acc: 0.8090 - val_loss: 0.4074 - val_acc: 0.8134\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.4069 - acc: 0.8152 - val_loss: 0.4049 - val_acc: 0.8172\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.3998 - acc: 0.8197 - val_loss: 0.3941 - val_acc: 0.8230\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3931 - acc: 0.8234 - val_loss: 0.4207 - val_acc: 0.8043\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3869 - acc: 0.8256 - val_loss: 0.3982 - val_acc: 0.8184\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.3806 - acc: 0.8294 - val_loss: 0.3877 - val_acc: 0.8263\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3770 - acc: 0.8320 - val_loss: 0.4226 - val_acc: 0.8098\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 17s 206us/sample - loss: 0.3705 - acc: 0.8333 - val_loss: 0.3815 - val_acc: 0.8266\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3671 - acc: 0.8360 - val_loss: 0.3855 - val_acc: 0.8211\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3627 - acc: 0.8380 - val_loss: 0.3775 - val_acc: 0.8274\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.3582 - acc: 0.8400 - val_loss: 0.3891 - val_acc: 0.8232\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 17s 206us/sample - loss: 0.3547 - acc: 0.8419 - val_loss: 0.3854 - val_acc: 0.8228\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3518 - acc: 0.8422 - val_loss: 0.3863 - val_acc: 0.8271\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3486 - acc: 0.8453 - val_loss: 0.3819 - val_acc: 0.8292\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3437 - acc: 0.8482 - val_loss: 0.3827 - val_acc: 0.8304\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3390 - acc: 0.8501 - val_loss: 0.3990 - val_acc: 0.8210\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3359 - acc: 0.8512 - val_loss: 0.3893 - val_acc: 0.8263\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3333 - acc: 0.8531 - val_loss: 0.3957 - val_acc: 0.8216\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3300 - acc: 0.8559 - val_loss: 0.3984 - val_acc: 0.8266\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3263 - acc: 0.8563 - val_loss: 0.3841 - val_acc: 0.8292\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3226 - acc: 0.8579 - val_loss: 0.3931 - val_acc: 0.8195\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3217 - acc: 0.8602 - val_loss: 0.3865 - val_acc: 0.8291\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3193 - acc: 0.8605 - val_loss: 0.3820 - val_acc: 0.8285\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3166 - acc: 0.8613 - val_loss: 0.3891 - val_acc: 0.8244\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3152 - acc: 0.8628 - val_loss: 0.3913 - val_acc: 0.8278\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3098 - acc: 0.8659 - val_loss: 0.3886 - val_acc: 0.8270\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 35s 407us/sample - loss: 0.5218 - acc: 0.7505 - val_loss: 0.4615 - val_acc: 0.7790\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4552 - acc: 0.7863 - val_loss: 0.4855 - val_acc: 0.7637\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4335 - acc: 0.7985 - val_loss: 0.4230 - val_acc: 0.8027\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4202 - acc: 0.8070 - val_loss: 0.4097 - val_acc: 0.8098\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4097 - acc: 0.8123 - val_loss: 0.4079 - val_acc: 0.8115\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4010 - acc: 0.8169 - val_loss: 0.3941 - val_acc: 0.8207\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3948 - acc: 0.8214 - val_loss: 0.3977 - val_acc: 0.8188\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 190us/sample - loss: 0.3875 - acc: 0.8261 - val_loss: 0.4167 - val_acc: 0.8084\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3824 - acc: 0.8276 - val_loss: 0.3876 - val_acc: 0.8243s - loss: 0. - ETA: 0s - loss: 0.3820 \n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3741 - acc: 0.8335 - val_loss: 0.3931 - val_acc: 0.8210\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3712 - acc: 0.8350 - val_loss: 0.3848 - val_acc: 0.8242\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3663 - acc: 0.8372 - val_loss: 0.3919 - val_acc: 0.8249\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3593 - acc: 0.8398 - val_loss: 0.4051 - val_acc: 0.8191\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3542 - acc: 0.8427 - val_loss: 0.3902 - val_acc: 0.8214 loss: 0. - ETA: 2s - loss: 0.3543 - acc: 0. - ETA: 2s - loss: 0.3545 -  - ETA: 1\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3486 - acc: 0.8458 - val_loss: 0.3864 - val_acc: 0.8252\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3429 - acc: 0.8491 - val_loss: 0.3948 - val_acc: 0.8215\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3392 - acc: 0.8510 - val_loss: 0.4000 - val_acc: 0.8227\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3306 - acc: 0.8547 - val_loss: 0.3870 - val_acc: 0.8264\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3282 - acc: 0.8561 - val_loss: 0.4035 - val_acc: 0.8202\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3233 - acc: 0.8596 - val_loss: 0.4031 - val_acc: 0.8191\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3178 - acc: 0.8610 - val_loss: 0.3980 - val_acc: 0.8238\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3136 - acc: 0.8629 - val_loss: 0.3927 - val_acc: 0.8237\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3071 - acc: 0.8661 - val_loss: 0.3998 - val_acc: 0.8229\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3028 - acc: 0.8681 - val_loss: 0.4070 - val_acc: 0.8215\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2961 - acc: 0.8710 - val_loss: 0.4103 - val_acc: 0.8227\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2940 - acc: 0.8713 - val_loss: 0.4243 - val_acc: 0.8180\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2896 - acc: 0.8756 - val_loss: 0.4196 - val_acc: 0.8137 ETA: 1s - lo\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2840 - acc: 0.8773 - val_loss: 0.4256 - val_acc: 0.8181\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2812 - acc: 0.8785 - val_loss: 0.4221 - val_acc: 0.8158\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2741 - acc: 0.8822 - val_loss: 0.4316 - val_acc: 0.8198\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 35s 410us/sample - loss: 0.5200 - acc: 0.7504 - val_loss: 0.4571 - val_acc: 0.7817\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4523 - acc: 0.7878 - val_loss: 0.4334 - val_acc: 0.7988\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4337 - acc: 0.7992 - val_loss: 0.4266 - val_acc: 0.8016\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4204 - acc: 0.8073 - val_loss: 0.4340 - val_acc: 0.7991\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4096 - acc: 0.8139 - val_loss: 0.4678 - val_acc: 0.7788\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4010 - acc: 0.8176 - val_loss: 0.4079 - val_acc: 0.8133ETA: 2s - loss: 0.4021 - acc: 0 - E\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3948 - acc: 0.8215 - val_loss: 0.5255 - val_acc: 0.7539\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3890 - acc: 0.8254 - val_loss: 0.4204 - val_acc: 0.8041\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3820 - acc: 0.8287 - val_loss: 0.4153 - val_acc: 0.8109\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3778 - acc: 0.8314 - val_loss: 0.3977 - val_acc: 0.8198\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3722 - acc: 0.8334 - val_loss: 0.3950 - val_acc: 0.8206\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3674 - acc: 0.8352 - val_loss: 0.3951 - val_acc: 0.8184\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3615 - acc: 0.8380 - val_loss: 0.4103 - val_acc: 0.8140\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3575 - acc: 0.8418 - val_loss: 0.4233 - val_acc: 0.8038\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3499 - acc: 0.8451 - val_loss: 0.3888 - val_acc: 0.8266\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3462 - acc: 0.8472 - val_loss: 0.3874 - val_acc: 0.8288\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3406 - acc: 0.8486 - val_loss: 0.3874 - val_acc: 0.8263\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3347 - acc: 0.8524 - val_loss: 0.4160 - val_acc: 0.8125\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3296 - acc: 0.8550 - val_loss: 0.3950 - val_acc: 0.8216\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3257 - acc: 0.8552 - val_loss: 0.3928 - val_acc: 0.8232.32 - ETA: 5s - loss:  - E\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3206 - acc: 0.8593 - val_loss: 0.3912 - val_acc: 0.8266\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3171 - acc: 0.8608 - val_loss: 0.3991 - val_acc: 0.8214\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3109 - acc: 0.8628 - val_loss: 0.3989 - val_acc: 0.823531\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3064 - acc: 0.8679 - val_loss: 0.4106 - val_acc: 0.8229\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3034 - acc: 0.8666 - val_loss: 0.4014 - val_acc: 0.8203\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.2988 - acc: 0.8708 - val_loss: 0.4203 - val_acc: 0.8180\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.2912 - acc: 0.8723 - val_loss: 0.4122 - val_acc: 0.8215\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.2904 - acc: 0.8728 - val_loss: 0.4054 - val_acc: 0.8218\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.2856 - acc: 0.8750 - val_loss: 0.4146 - val_acc: 0.8194\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.2800 - acc: 0.8788 - val_loss: 0.4192 - val_acc: 0.8158\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 36s 426us/sample - loss: 0.5259 - acc: 0.7485 - val_loss: 0.4545 - val_acc: 0.7865\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.4513 - acc: 0.7882 - val_loss: 0.4293 - val_acc: 0.8016\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.4294 - acc: 0.8026 - val_loss: 0.4282 - val_acc: 0.8012\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.4157 - acc: 0.8099 - val_loss: 0.4199 - val_acc: 0.8045\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.4060 - acc: 0.8144 - val_loss: 0.4082 - val_acc: 0.8149\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3991 - acc: 0.8195 - val_loss: 0.3957 - val_acc: 0.8183\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3894 - acc: 0.8248 - val_loss: 0.3947 - val_acc: 0.8213\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.3832 - acc: 0.8282 - val_loss: 0.4306 - val_acc: 0.8023\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3760 - acc: 0.8309 - val_loss: 0.3927 - val_acc: 0.8233\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.3682 - acc: 0.8352 - val_loss: 0.3852 - val_acc: 0.8260\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3637 - acc: 0.8375 - val_loss: 0.4017 - val_acc: 0.8149\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3575 - acc: 0.8411 - val_loss: 0.3846 - val_acc: 0.8246\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3496 - acc: 0.8444 - val_loss: 0.3974 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3445 - acc: 0.8466 - val_loss: 0.3942 - val_acc: 0.8235\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3390 - acc: 0.8505 - val_loss: 0.3845 - val_acc: 0.8263\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3317 - acc: 0.8541 - val_loss: 0.4065 - val_acc: 0.8163\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3249 - acc: 0.8564 - val_loss: 0.3896 - val_acc: 0.8273\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3199 - acc: 0.8589 - val_loss: 0.4187 - val_acc: 0.8128\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3142 - acc: 0.8627 - val_loss: 0.3995 - val_acc: 0.8289\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3088 - acc: 0.8649 - val_loss: 0.4126 - val_acc: 0.8120\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3020 - acc: 0.8685 - val_loss: 0.4163 - val_acc: 0.8165\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2985 - acc: 0.8701 - val_loss: 0.4029 - val_acc: 0.8242 \n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2931 - acc: 0.8721 - val_loss: 0.4139 - val_acc: 0.8220\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2883 - acc: 0.8753 - val_loss: 0.4012 - val_acc: 0.8266\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2834 - acc: 0.8770 - val_loss: 0.4355 - val_acc: 0.8158\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2784 - acc: 0.8789 - val_loss: 0.4125 - val_acc: 0.8219\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2745 - acc: 0.8817 - val_loss: 0.4177 - val_acc: 0.8239\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2707 - acc: 0.8834 - val_loss: 0.4155 - val_acc: 0.8223\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2642 - acc: 0.8864 - val_loss: 0.4462 - val_acc: 0.8184\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2598 - acc: 0.8889 - val_loss: 0.4281 - val_acc: 0.8256\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 38s 445us/sample - loss: 0.5327 - acc: 0.7450 - val_loss: 0.4748 - val_acc: 0.7742\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.4565 - acc: 0.7863 - val_loss: 0.4530 - val_acc: 0.7908\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.4341 - acc: 0.7993 - val_loss: 0.4269 - val_acc: 0.7997\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.4202 - acc: 0.8076 - val_loss: 0.4998 - val_acc: 0.7653\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.4109 - acc: 0.8127 - val_loss: 0.4083 - val_acc: 0.8117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3993 - acc: 0.8195 - val_loss: 0.4099 - val_acc: 0.8093\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3920 - acc: 0.8225 - val_loss: 0.4101 - val_acc: 0.8130\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 18s 207us/sample - loss: 0.3851 - acc: 0.8272 - val_loss: 0.4086 - val_acc: 0.8133\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3778 - acc: 0.8309 - val_loss: 0.4004 - val_acc: 0.8152\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3728 - acc: 0.8317 - val_loss: 0.3942 - val_acc: 0.8205\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3639 - acc: 0.8370 - val_loss: 0.3962 - val_acc: 0.8235\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3595 - acc: 0.8399 - val_loss: 0.3830 - val_acc: 0.8270\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3508 - acc: 0.8440 - val_loss: 0.3879 - val_acc: 0.8220\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3450 - acc: 0.8481 - val_loss: 0.3906 - val_acc: 0.8233\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3382 - acc: 0.8501 - val_loss: 0.4004 - val_acc: 0.8210\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.3345 - acc: 0.8518 - val_loss: 0.3843 - val_acc: 0.8293\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3286 - acc: 0.8559 - val_loss: 0.3908 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3197 - acc: 0.8604 - val_loss: 0.3953 - val_acc: 0.8241\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.3161 - acc: 0.8620 - val_loss: 0.3923 - val_acc: 0.8253\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3080 - acc: 0.8667 - val_loss: 0.4124 - val_acc: 0.8270\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3048 - acc: 0.8672 - val_loss: 0.3997 - val_acc: 0.8260\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2970 - acc: 0.8715 - val_loss: 0.4030 - val_acc: 0.8183\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2937 - acc: 0.8721 - val_loss: 0.4109 - val_acc: 0.8244\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2881 - acc: 0.8761 - val_loss: 0.4132 - val_acc: 0.8209\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2827 - acc: 0.8787 - val_loss: 0.4231 - val_acc: 0.8260\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2814 - acc: 0.8783 - val_loss: 0.4176 - val_acc: 0.8233\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2750 - acc: 0.8820 - val_loss: 0.4246 - val_acc: 0.8217\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2699 - acc: 0.8835 - val_loss: 0.4471 - val_acc: 0.8172\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2657 - acc: 0.8854 - val_loss: 0.4482 - val_acc: 0.8166\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2609 - acc: 0.8877 - val_loss: 0.4319 - val_acc: 0.8205\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [0,1,2]\n",
    "layer_sizes = [15,50,100,150]\n",
    "conv_layers = [0,1,2,3]\n",
    "kernal_size = [(2,2),(3,3),(4,4)]\n",
    "for filter_size in kernal_size:\n",
    "    for dense_layer in dense_layers:\n",
    "        for layer_size in layer_sizes:\n",
    "            for conv_layer in conv_layers:\n",
    "\n",
    "                NAME =\"PMT-MuEl-{}-filter_size-{}-conv-{}-nodes-{}-dense\".format(filter_size,conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "                tensorboard = TensorBoard(log_dir = 'logs\\PMTsmall\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(layer_size,filter_size,strides=1, input_shape= XL.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                for l in range(conv_layer-1):                   \n",
    "                    model.add(Conv2D(layer_size,filter_size,padding='same',activation=\"relu\"))              \n",
    "                    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))            \n",
    "                #model.add(GlobalAveragePooling2D())\n",
    "                model.add(Flatten())\n",
    "                for l in range(dense_layer-1):\n",
    "                    model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))\n",
    "                model.add(Dense(32,activation=\"relu\"))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(2))\n",
    "                model.add(Activation('softmax'))\n",
    "                #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "                model.compile(loss=\"binary_crossentropy\",\n",
    "                             optimizer=\"adam\",\n",
    "                              metrics=['accuracy']\n",
    "                             )   \n",
    "                #filepath=\"LAPPD_Charge_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "                #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "                #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "                #model.summary()\n",
    "                history=model.fit(XTraining,YTraining,\n",
    "              validation_data=(XVal,Yval)\n",
    "              ,batch_size=100,\n",
    "                shuffle=True,\n",
    "                class_weight='balanced',\n",
    "                callbacks=[\n",
    "                            #monitor,\n",
    "                            #checkpoint,\n",
    "                            tensorboard \n",
    "                ],\n",
    "              epochs= 30)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bestes Modell 24-PMT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_171\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_294 (Conv2D)          (None, 3, 8, 150)         1350      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_294 (MaxPoolin (None, 2, 4, 150)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_512 (Bat (None, 2, 4, 150)         600       \n",
      "_________________________________________________________________\n",
      "dropout_512 (Dropout)        (None, 2, 4, 150)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_295 (Conv2D)          (None, 2, 4, 150)         90150     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_295 (MaxPoolin (None, 1, 2, 150)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_513 (Bat (None, 1, 2, 150)         600       \n",
      "_________________________________________________________________\n",
      "dropout_513 (Dropout)        (None, 1, 2, 150)         0         \n",
      "_________________________________________________________________\n",
      "flatten_170 (Flatten)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 32)                9632      \n",
      "_________________________________________________________________\n",
      "batch_normalization_514 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_514 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 102,526\n",
      "Trainable params: 101,862\n",
      "Non-trainable params: 664\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.7506\n",
      "Epoch 00001: val_acc improved from -inf to 0.78000, saving model to PMT_24_PID_120k-improvement-val-acc_0.78.model\n",
      "85000/85000 [==============================] - 127s 1ms/sample - loss: 0.5135 - acc: 0.7508 - val_loss: 0.4625 - val_acc: 0.7800\n",
      "Epoch 2/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.7856-\n",
      "Epoch 00002: val_acc improved from 0.78000 to 0.80055, saving model to PMT_24_PID_120k-improvement-val-acc_0.80.model\n",
      "85000/85000 [==============================] - 18s 215us/sample - loss: 0.4552 - acc: 0.7856 - val_loss: 0.4279 - val_acc: 0.8005\n",
      "Epoch 3/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.7975- ETA: 4s - - - ETA: 0s - loss: 0.4362 - acc: - ETA: 0s - loss: 0.4363 - acc: 0.7975\n",
      "Epoch 00003: val_acc did not improve from 0.80055\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.4364 - acc: 0.7974 - val_loss: 0.4444 - val_acc: 0.7936\n",
      "Epoch 4/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8027\n",
      "Epoch 00004: val_acc improved from 0.80055 to 0.81240, saving model to PMT_24_PID_120k-improvement-val-acc_0.81.model\n",
      "85000/85000 [==============================] - 17s 203us/sample - loss: 0.4265 - acc: 0.8027 - val_loss: 0.4092 - val_acc: 0.8124\n",
      "Epoch 5/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.8085\n",
      "Epoch 00005: val_acc improved from 0.81240 to 0.81405, saving model to PMT_24_PID_120k-improvement-val-acc_0.81.model\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.4176 - acc: 0.8084 - val_loss: 0.4048 - val_acc: 0.8141\n",
      "Epoch 6/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8100\n",
      "Epoch 00006: val_acc did not improve from 0.81405\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4138 - acc: 0.8100 - val_loss: 0.4069 - val_acc: 0.8133\n",
      "Epoch 7/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8133\n",
      "Epoch 00007: val_acc improved from 0.81405 to 0.82055, saving model to PMT_24_PID_120k-improvement-val-acc_0.82.model\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.4098 - acc: 0.8133 - val_loss: 0.3958 - val_acc: 0.8206\n",
      "Epoch 8/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8157- ETA: 5s - loss: 0 - ETA: 2s - - ETA: 0s - loss: 0.4048 - a\n",
      "Epoch 00008: val_acc did not improve from 0.82055\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4044 - acc: 0.8157 - val_loss: 0.4008 - val_acc: 0.8174\n",
      "Epoch 9/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8185\n",
      "Epoch 00009: val_acc improved from 0.82055 to 0.82810, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 17s 206us/sample - loss: 0.4014 - acc: 0.8185 - val_loss: 0.3860 - val_acc: 0.8281\n",
      "Epoch 10/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8197\n",
      "Epoch 00010: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.3984 - acc: 0.8197 - val_loss: 0.3885 - val_acc: 0.8239\n",
      "Epoch 11/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8203- ETA: 1s - los\n",
      "Epoch 00011: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.3948 - acc: 0.8202 - val_loss: 0.3882 - val_acc: 0.8241\n",
      "Epoch 12/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8202\n",
      "Epoch 00012: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3954 - acc: 0.8203 - val_loss: 0.3837 - val_acc: 0.8271\n",
      "Epoch 13/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8245\n",
      "Epoch 00013: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3894 - acc: 0.8243 - val_loss: 0.3820 - val_acc: 0.8260\n",
      "Epoch 14/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8247- ET - ETA: 1s - loss: 0.3899 - acc:  - ETA: 1s - l\n",
      "Epoch 00014: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3894 - acc: 0.8246 - val_loss: 0.3817 - val_acc: 0.8266\n",
      "Epoch 15/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8253\n",
      "Epoch 00015: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.3872 - acc: 0.8253 - val_loss: 0.3832 - val_acc: 0.8250\n",
      "Epoch 16/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8264\n",
      "Epoch 00016: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 202us/sample - loss: 0.3853 - acc: 0.8263 - val_loss: 0.3786 - val_acc: 0.8279\n",
      "Epoch 17/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8274\n",
      "Epoch 00017: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 192us/sample - loss: 0.3827 - acc: 0.8273 - val_loss: 0.3925 - val_acc: 0.8178\n",
      "Epoch 18/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3822 - acc: 0.8282\n",
      "Epoch 00018: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 202us/sample - loss: 0.3825 - acc: 0.8281 - val_loss: 0.3876 - val_acc: 0.8245\n",
      "Epoch 19/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8287\n",
      "Epoch 00019: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.3799 - acc: 0.8287 - val_loss: 0.3869 - val_acc: 0.8266\n",
      "Epoch 20/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8296\n",
      "Epoch 00020: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3783 - acc: 0.8296 - val_loss: 0.3865 - val_acc: 0.8255\n",
      "Epoch 21/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8317\n",
      "Epoch 00021: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3757 - acc: 0.8318 - val_loss: 0.3858 - val_acc: 0.8250\n",
      "Epoch 22/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8326\n",
      "Epoch 00022: val_acc improved from 0.82810 to 0.82895, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3743 - acc: 0.8325 - val_loss: 0.3773 - val_acc: 0.8289\n",
      "Epoch 23/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3740 - acc: 0.8318\n",
      "Epoch 00023: val_acc improved from 0.82895 to 0.83020, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.3740 - acc: 0.8318 - val_loss: 0.3758 - val_acc: 0.8302\n",
      "Epoch 24/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8325\n",
      "Epoch 00024: val_acc improved from 0.83020 to 0.83195, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3731 - acc: 0.8325 - val_loss: 0.3710 - val_acc: 0.8320\n",
      "Epoch 25/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8336\n",
      "Epoch 00025: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3707 - acc: 0.8336 - val_loss: 0.3790 - val_acc: 0.8270\n",
      "Epoch 26/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8344\n",
      "Epoch 00026: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3689 - acc: 0.8344 - val_loss: 0.3791 - val_acc: 0.8266\n",
      "Epoch 27/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8345\n",
      "Epoch 00027: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3681 - acc: 0.8345 - val_loss: 0.3749 - val_acc: 0.8292\n",
      "Epoch 28/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8360\n",
      "Epoch 00028: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 16s 190us/sample - loss: 0.3674 - acc: 0.8361 - val_loss: 0.3731 - val_acc: 0.8316\n",
      "Epoch 29/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8360\n",
      "Epoch 00029: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3672 - acc: 0.8360 - val_loss: 0.3849 - val_acc: 0.8251\n",
      "Epoch 30/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8370- ETA: 0s - loss: 0.3644 \n",
      "Epoch 00030: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3647 - acc: 0.8370 - val_loss: 0.3706 - val_acc: 0.8306\n",
      "Epoch 31/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8372\n",
      "Epoch 00031: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3622 - acc: 0.8372 - val_loss: 0.3804 - val_acc: 0.8275\n",
      "Epoch 32/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8385\n",
      "Epoch 00032: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 18s 211us/sample - loss: 0.3630 - acc: 0.8385 - val_loss: 0.3877 - val_acc: 0.8263\n",
      "Epoch 33/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8364\n",
      "Epoch 00033: val_acc improved from 0.83195 to 0.83340, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 28s 329us/sample - loss: 0.3643 - acc: 0.8364 - val_loss: 0.3673 - val_acc: 0.8334\n",
      "Epoch 34/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.8388- ETA: 1s - loss: 0.360\n",
      "Epoch 00034: val_acc improved from 0.83340 to 0.83415, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 30s 353us/sample - loss: 0.3604 - acc: 0.8389 - val_loss: 0.3685 - val_acc: 0.8342\n",
      "Epoch 35/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8374- ETA: 1s - loss: \n",
      "Epoch 00035: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 21s 252us/sample - loss: 0.3621 - acc: 0.8375 - val_loss: 0.3753 - val_acc: 0.8315\n",
      "Epoch 36/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8378\n",
      "Epoch 00036: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.3612 - acc: 0.8378 - val_loss: 0.3702 - val_acc: 0.8305\n",
      "Epoch 37/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8392\n",
      "Epoch 00037: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3601 - acc: 0.8392 - val_loss: 0.3819 - val_acc: 0.8249\n",
      "Epoch 38/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8406\n",
      "Epoch 00038: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3582 - acc: 0.8406 - val_loss: 0.3689 - val_acc: 0.8309\n",
      "Epoch 39/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8392\n",
      "Epoch 00039: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 190us/sample - loss: 0.3583 - acc: 0.8392 - val_loss: 0.3746 - val_acc: 0.8302\n",
      "Epoch 40/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8424\n",
      "Epoch 00040: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3564 - acc: 0.8424 - val_loss: 0.3721 - val_acc: 0.8320\n",
      "Epoch 41/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8412\n",
      "Epoch 00041: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3565 - acc: 0.8411 - val_loss: 0.3711 - val_acc: 0.8314\n",
      "Epoch 42/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8415\n",
      "Epoch 00042: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 203us/sample - loss: 0.3556 - acc: 0.8416 - val_loss: 0.3720 - val_acc: 0.8291\n",
      "Epoch 43/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8418\n",
      "Epoch 00043: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3551 - acc: 0.8418 - val_loss: 0.3705 - val_acc: 0.8310\n",
      "Epoch 44/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8424- ETA: 0s - loss: 0.3539 -\n",
      "Epoch 00044: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 22s 259us/sample - loss: 0.3541 - acc: 0.8425 - val_loss: 0.3715 - val_acc: 0.8325\n",
      "Epoch 45/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8430- ETA: 0s - loss: 0.3512 - acc:  - ETA: 0s - loss: 0.3510 - acc: 0.84 - ETA: 0s - loss: 0.3512 - acc:\n",
      "Epoch 00045: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3515 - acc: 0.8430 - val_loss: 0.3693 - val_acc: 0.8331\n",
      "Epoch 46/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8436- ETA: 0s - loss: 0.3516\n",
      "Epoch 00046: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3521 - acc: 0.8436 - val_loss: 0.3730 - val_acc: 0.8317\n",
      "Epoch 47/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8441\n",
      "Epoch 00047: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3510 - acc: 0.8440 - val_loss: 0.3702 - val_acc: 0.8321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8443\n",
      "Epoch 00048: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3522 - acc: 0.8443 - val_loss: 0.3820 - val_acc: 0.8255\n",
      "Epoch 49/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8431- ETA: 0s - loss: 0.3516 - acc: 0.8\n",
      "Epoch 00049: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3513 - acc: 0.8430 - val_loss: 0.3698 - val_acc: 0.8325\n",
      "Epoch 50/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8442\n",
      "Epoch 00050: val_acc improved from 0.83415 to 0.83505, saving model to PMT_24_PID_120k-improvement-val-acc_0.84.model\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.3507 - acc: 0.8441 - val_loss: 0.3676 - val_acc: 0.8350\n",
      "Epoch 51/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8455\n",
      "Epoch 00051: val_acc did not improve from 0.83505\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3495 - acc: 0.8454 - val_loss: 0.3771 - val_acc: 0.8270\n",
      "Epoch 52/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8445\n",
      "Epoch 00052: val_acc did not improve from 0.83505\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3493 - acc: 0.8444 - val_loss: 0.3675 - val_acc: 0.8339\n",
      "Epoch 53/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8456- ETA: 0s - loss: 0.3487 -\n",
      "Epoch 00053: val_acc did not improve from 0.83505\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3487 - acc: 0.8455 - val_loss: 0.3704 - val_acc: 0.8317\n",
      "Epoch 54/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8452\n",
      "Epoch 00054: val_acc improved from 0.83505 to 0.83615, saving model to PMT_24_PID_120k-improvement-val-acc_0.84.model\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3493 - acc: 0.8452 - val_loss: 0.3675 - val_acc: 0.8361\n",
      "Epoch 55/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8460\n",
      "Epoch 00055: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.3485 - acc: 0.8460 - val_loss: 0.3707 - val_acc: 0.8336\n",
      "Epoch 56/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8467- ETA: 0s - loss:\n",
      "Epoch 00056: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3470 - acc: 0.8467 - val_loss: 0.3681 - val_acc: 0.8340\n",
      "Epoch 57/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8458\n",
      "Epoch 00057: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3485 - acc: 0.8459 - val_loss: 0.3730 - val_acc: 0.8277\n",
      "Epoch 58/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8472\n",
      "Epoch 00058: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3468 - acc: 0.8471 - val_loss: 0.3690 - val_acc: 0.8320\n",
      "Epoch 59/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.8458\n",
      "Epoch 00059: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3461 - acc: 0.8459 - val_loss: 0.3731 - val_acc: 0.8303\n",
      "Epoch 60/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8459- ETA: 4\n",
      "Epoch 00060: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3448 - acc: 0.8458 - val_loss: 0.3758 - val_acc: 0.8289\n",
      "Epoch 61/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8487\n",
      "Epoch 00061: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3441 - acc: 0.8486 - val_loss: 0.3788 - val_acc: 0.8312\n",
      "Epoch 62/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8467- ETA\n",
      "Epoch 00062: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 22s 253us/sample - loss: 0.3443 - acc: 0.8466 - val_loss: 0.3725 - val_acc: 0.8344\n",
      "Epoch 63/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8454\n",
      "Epoch 00063: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 19s 218us/sample - loss: 0.3465 - acc: 0.8454 - val_loss: 0.3743 - val_acc: 0.8296\n",
      "Epoch 64/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8459\n",
      "Epoch 00064: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3831 - val_acc: 0.8268\n",
      "Epoch 65/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8477\n",
      "Epoch 00065: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.3421 - acc: 0.8476 - val_loss: 0.3684 - val_acc: 0.8330\n",
      "Epoch 66/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8479- ETA: 6s -\n",
      "Epoch 00066: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3431 - acc: 0.8479 - val_loss: 0.3728 - val_acc: 0.8332\n",
      "Epoch 67/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8484\n",
      "Epoch 00067: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3420 - acc: 0.8484 - val_loss: 0.3723 - val_acc: 0.8329\n",
      "Epoch 68/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8479- ETA: 2 - ETA: 0s - loss: 0.3405 - acc\n",
      "Epoch 00068: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3409 - acc: 0.8479 - val_loss: 0.3763 - val_acc: 0.8318\n",
      "Epoch 69/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8468- ETA: 0s - loss: 0.3435 - acc:\n",
      "Epoch 00069: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3436 - acc: 0.8467 - val_loss: 0.3845 - val_acc: 0.8258\n",
      "Epoch 70/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.8489\n",
      "Epoch 00070: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3405 - acc: 0.8488 - val_loss: 0.3739 - val_acc: 0.8324\n",
      "Epoch 71/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8491- ETA: 3s - ETA: 1s - loss: 0.3406 - acc:  - ETA: 1s - loss: 0.34\n",
      "Epoch 00071: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3410 - acc: 0.8491 - val_loss: 0.3724 - val_acc: 0.8317\n",
      "Epoch 72/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8487\n",
      "Epoch 00072: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3406 - acc: 0.8488 - val_loss: 0.3695 - val_acc: 0.8321\n",
      "Epoch 73/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.8486\n",
      "Epoch 00073: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3396 - acc: 0.8488 - val_loss: 0.3690 - val_acc: 0.8332\n",
      "Epoch 74/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8495\n",
      "Epoch 00074: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3381 - acc: 0.8495 - val_loss: 0.3753 - val_acc: 0.8273\n",
      "Epoch 75/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8494\n",
      "Epoch 00075: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3391 - acc: 0.8495 - val_loss: 0.3722 - val_acc: 0.8352\n",
      "Epoch 76/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8496\n",
      "Epoch 00076: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3402 - acc: 0.8495 - val_loss: 0.3753 - val_acc: 0.8285\n",
      "Epoch 77/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8503\n",
      "Epoch 00077: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3383 - acc: 0.8503 - val_loss: 0.3726 - val_acc: 0.8321\n",
      "Epoch 78/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8502\n",
      "Epoch 00078: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3385 - acc: 0.8501 - val_loss: 0.3700 - val_acc: 0.8335\n",
      "Epoch 79/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8508\n",
      "Epoch 00079: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3372 - acc: 0.8506 - val_loss: 0.3792 - val_acc: 0.8289\n",
      "Epoch 80/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8505\n",
      "Epoch 00080: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3385 - acc: 0.8503 - val_loss: 0.3706 - val_acc: 0.8344\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "dense_layers = [1]\n",
    "layer_sizes = [150]\n",
    "conv_layers = [2]\n",
    "kernal_size = [(2,2)]\n",
    "for filter_size in kernal_size:\n",
    "    for dense_layer in dense_layers:\n",
    "        for layer_size in layer_sizes:\n",
    "            for conv_layer in conv_layers:\n",
    "\n",
    "                NAME =\"PMT-MuEl-{}-filter_size-{}-conv-{}-nodes-{}-dense\".format(filter_size,conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "                tensorboard = TensorBoard(log_dir = 'logs\\PMTsmall\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(layer_size,filter_size,strides=1, input_shape= XL.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                for l in range(conv_layer-1):                   \n",
    "                    model.add(Conv2D(layer_size,filter_size,padding='same',activation=\"relu\"))              \n",
    "                    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))            \n",
    "                #model.add(GlobalAveragePooling2D())\n",
    "                model.add(Flatten())\n",
    "                for l in range(dense_layer-1):\n",
    "                    model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))\n",
    "                model.add(Dense(32,activation=\"relu\"))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(2))\n",
    "                model.add(Activation('softmax'))\n",
    "                #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "                model.compile(loss=\"binary_crossentropy\",\n",
    "                             optimizer=\"adam\",\n",
    "                              metrics=['accuracy']\n",
    "                             )   \n",
    "                filepath=\"PMT_24_PID_120k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "                checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "                #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "                model.summary()\n",
    "                history=model.fit(XTraining,YTraining,\n",
    "              validation_data=(XVal,Yval)\n",
    "              ,batch_size=100,\n",
    "                shuffle=True,\n",
    "                class_weight='balanced',\n",
    "                callbacks=[\n",
    "                            #monitor,\n",
    "                            checkpoint,\n",
    "                            #tensorboard \n",
    "                ],\n",
    "              epochs= 80)\n",
    "                print(history.history.keys())\n",
    "                # summarize history for accuracy\n",
    "                plt.plot(history.history['acc'])\n",
    "                plt.plot(history.history['val_acc'])\n",
    "                plt.title('model accuracy')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'test'], loc='upper left')\n",
    "                plt.show()\n",
    "                # summarize history for loss\n",
    "                plt.plot(history.history['loss'])\n",
    "                plt.plot(history.history['val_loss'])\n",
    "                plt.title('model loss')\n",
    "                plt.ylabel('loss')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'test'], loc='upper left')\n",
    "                plt.show()\n",
    "\n",
    "                print(\"Ende des Versuchs: \")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.3565329247734937\n",
      "Test accuracy:  0.83852047\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "\n",
    "cd \"Documents\\Python\\CNN_Masterarbeit\"\n",
    "\n",
    "tensorboard --logdir=logs/ --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/Overfitting_Studie --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/Modell_Studie --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/MuonElectron --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=BeamlikePI/logs/Time --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15005, 10, 16, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Test_LAPPD(1x1)_120k.pickle\",\"wb\")\n",
    "pickle.dump(XTest,pickle_out,protocol=4)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Test_LAPPD(1x1)_120k.pickle\",\"wb\")\n",
    "pickle.dump(YTest,pickle_out,protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0521 13:34:22.750493  8792 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0521 13:34:22.766009  8792 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0521 13:34:22.781656  8792 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"PMT_24_PID_120k-improvement-val-acc_0.84.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAPPD\n",
    "model = tf.keras.models.load_model(\"LAPPD(1x1)_PID_120k-improvement-val-acc_0.83.model\")\n",
    "\n",
    "#Combined\n",
    "#model = tf.keras.models.load_model(\"PMTOnly_Combined_PI_120k-60epoch_-improvement-val-acc_0.93.model\")\n",
    "#model = tf.keras.models.load_model(\"PMTOnly_Combined_PI_22k-80epoch-improvement-val-acc_0.92.model\")\n",
    "#Time\n",
    "#model = tf.keras.models.load_model(\"PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.81.model\")\n",
    "#Charge\n",
    "#model = tf.keras.models.load_model(\"PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.93.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120005"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTestC = X[:,:,:,0].reshape(120005,10,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120005, 10, 16, 1) (120005, 2)\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(XTestC.shape,Y.shape)\n",
    "for sample in Y[:20]:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cbf7094208>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XTest[7,:,:,0], cmap='binary', interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.36467623370204594\n",
      "Test accuracy:  0.8353882\n"
     ]
    }
   ],
   "source": [
    "### LAPPD\n",
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.35680383114329917\n",
      "Test accuracy:  0.90992105\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.23812034977390828\n",
      "Test accuracy:  0.90469563\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestC, Y, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.19774321766031458\n",
      "Test accuracy:  0.92719644\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestC, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.4832646434304745\n",
      "Test accuracy:  0.8005923\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestT, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5818 1629]\n",
      " [ 841 6717]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#prediction = model.predict(XTestC)\n",
    "#print(prediction.shape,YTest.shape)\n",
    "rounded_labels =np.argmax(YTest, axis=1)\n",
    "y_prob = np.array(model.predict(XTest, batch_size=128, verbose=0))\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "cm = confusion_matrix(rounded_labels, y_classes)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    " \n",
    "    print(cm)\n",
    " \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAPPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7812542  0.2187458 ]\n",
      " [0.11127282 0.88872718]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "cm = cm.reshape((2,2))\n",
    " \n",
    "class_names = [r\"$e^{-}$\", \"muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n CNN-model with 84% accuracy \\n Pure LAPPD')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL PMTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90041109 0.09958891]\n",
      " [0.03416399 0.96583601]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "cm = cm.reshape((2,2))\n",
    " \n",
    "class_names = [r\"$e^{-}$\", \"muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n CNN-model with 93% accuracy \\n Pure PMT')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 PMTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2=[[0.79498861,0.20501139],\n",
    " [0.11230443,0.88769557]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79498861 0.20501139]\n",
      " [0.11230443 0.88769557]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "#cm = cm.reshape((2,2))\n",
    "cm2=np.array(cm2)\n",
    "class_names = [r\"$e^{-}$\", \"$muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm2, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n CNN-model with 84% accuracy \\n 24 PMTs')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import pydot_ng as pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, 'my_first_model_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_12 (5, 5, 2, 130)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 130 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summarize filter shapes\n",
    "pyplot.subplots_adjust(wspace = 0.05 ,hspace = 0.05)\n",
    "for layer in model.layers:\n",
    "    #check for conv. layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    #get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)\n",
    "    f_min, f_max= filters.min(),filters.max()\n",
    "    filters = (filters-f_min)/(f_max-f_min)\n",
    "    # plot first few filters\n",
    "    \n",
    "\n",
    "    n_filters, ix = 130, 1\n",
    "\n",
    "\n",
    "    for i in range(n_filters):\n",
    "        #get the filter\n",
    "        f = filters[:, :, :, i]\n",
    "        # plot each channel separately\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        ax = pyplot.subplot(n_filters, 4, ix)\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:,:, 1], cmap='viridis')\n",
    "        ix += 1\n",
    "    # show the figure\n",
    "    #pyplot.savefig(\"PMT Model 85% layer0 ALL ConvFilters-Time.jpg\",format =\"jpg\", bbox_inches='tight')\n",
    "    pyplot.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6.4, 30*4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv2d_12 (?, 10, 16, 130)\n",
      "3 conv2d_13 (?, 5, 8, 130)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    print(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c9023abe10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[9,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6.4, 10*4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14_input (InputLayer) [(None, 10, 16, 2)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 10, 16, 130)       6630      \n",
      "=================================================================\n",
      "Total params: 6,630\n",
      "Trainable params: 6,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 10, 16, 130)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 52, not 53",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-f9c6a5616c25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# specify subplot and turn of axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     raise ValueError(\n\u001b[1;32m---> 59\u001b[1;33m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[0;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[0;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[1;31mValueError\u001b[0m: num must be 1 <= num <= 52, not 53"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x3456 with 52 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"PMTOnly_PI_22k_RANDOM-improvement-val-acc_0.93.model\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "\n",
    "\n",
    "\n",
    "ixs = [0]\n",
    "\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(XTest[9:10])\n",
    "print(feature_maps.shape)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "\n",
    "#for fmap in feature_maps:\n",
    "ix = 1\n",
    "a=130\n",
    "for _ in range(a):\n",
    "    for _ in range(a):\n",
    "        if ix==a+1:\n",
    "            break\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(26, 2, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='viridis')\n",
    "        ix += 1\n",
    "    # show the figure\n",
    "#pyplot.savefig(\"PMT layer0 ALL Conv ElectronEvent9.jpg\",format =\"jpg\", bbox_inches='tight')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
