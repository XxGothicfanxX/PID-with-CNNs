{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  CSV erkennen by David Maksimovic 24.06.2019\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "#import keras\n",
    "\n",
    "#from keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,LeakyReLU, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot_ng as pydot\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "########### Normalisieren ###########\n",
    "\n",
    "#Ist schon normalisiert\n",
    "########### Normalisieren ###########\n",
    "\n",
    "#Ist schon normalisiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "XL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_25_PMTS_Beamlike_PI_120k_Files_mitTopBottom.pickle\",\"rb\"))\n",
    "YL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_25_PMTS_Beamlike_PI_120k_Files_mitTopBottom.pickle\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLA=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(15x40)_120k_Files.pickle\",\"rb\"))\n",
    "YLA=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(15x40)_120k_Files.pickle\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X,Y\n",
    "########### Daten laden ###########\n",
    "#X= pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_10x10_PMT_160_Only_20k_Files_mitTopBottom_mitFill.pickle\",\"rb\"))\n",
    "#Y= pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_10x10_PMT_160_Only_20k_Files_mitTopBottom_mitFill.pickle\",\"rb\"))\n",
    "#X= pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_3x3_PMT_160_Only_20k_Files_mitTopBottom_mitFill.pickle\",\"rb\"))\n",
    "#Y= pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_3x3_PMT_160_Only_20k_Files_mitTopBottom_mitFill.pickle\",\"rb\"))\n",
    "#X=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_3x3_PMT_160_Only_20k_Files_mitTopBottom_mitSpalten.pickle\",\"rb\"))\n",
    "#Y=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_3x3_PMT_160_Only_20k_Files_mitTopBottom_mitSpalten.pickle\",\"rb\"))\n",
    "X=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_1x1_PMT_160_Only_20k_Files_mitTopBottom.pickle\",\"rb\"))\n",
    "Y=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_1x1_PMT_160_Only_20k_Files_mitTopBottom.pickle\",\"rb\"))\n",
    "\n",
    "\n",
    "#X=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_1x1_PMT_160_120k_Files_mitTopBottom.pickle\",\"rb\"))\n",
    "#Y=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_1x1_PMT_160_120k_Files_mitTopBottom.pickle\",\"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much from one kind, how much from the other: \n",
      " [59977 60028]\n",
      "How do they look like? \n",
      " [[0 1]\n",
      " [1 0]]\n",
      "Percentage of one kind: \n",
      " 50.021249114620225\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(YL, return_counts=True, axis=0)\n",
    "print(\"How much from one kind, how much from the other: \\n\",counts)\n",
    "print(\"How do they look like? \\n\",unique)\n",
    "print(\"Percentage of one kind: \\n\", 100/(counts[0]+counts[1])*counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e697d53940>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XLA[40,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e697cf9198>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XL[40,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e696520a20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XLA[10,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e69646d898>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XL[10,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20fcfff0f60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[10,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "(17000, 10, 16, 2) (2500, 10, 16, 2) (4052, 10, 16, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(X, Y))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:20]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:17000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[17000:19500]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[19500:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X,Y,X1,X2,X3,Y1,Y2,Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testen der besten Methode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umsetzung\n",
    "\n",
    "20 x 16 px und 2 Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 Zeilen 16 Spalten und 2 Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,10,16,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,10,16,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,10,16,1)\n",
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,10,16,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,10,16,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,10,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.2766162261223422\n",
      "Test accuracy:  0.91880554\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "model = tf.keras.models.load_model(\"CNN_MutiNetwork_PI_PMT_TandC-improvement-val-acc_0.92.model\")\n",
    "score = model.evaluate([XTestC,XTestT], YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Charge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen der besten Methode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6820 - acc: 0.5536 - val_loss: 0.6740 - val_acc: 0.5852\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6680 - acc: 0.5840 - val_loss: 0.6664 - val_acc: 0.5932\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6624 - acc: 0.5964 - val_loss: 0.6623 - val_acc: 0.6020\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6599 - acc: 0.6037 - val_loss: 0.6613 - val_acc: 0.6008\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6584 - acc: 0.6044 - val_loss: 0.6616 - val_acc: 0.6056\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6578 - acc: 0.6061 - val_loss: 0.6593 - val_acc: 0.6072\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6570 - acc: 0.6074 - val_loss: 0.6599 - val_acc: 0.6068\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6566 - acc: 0.6069 - val_loss: 0.6590 - val_acc: 0.6060\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6564 - acc: 0.6069 - val_loss: 0.6596 - val_acc: 0.6112\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6562 - acc: 0.6102 - val_loss: 0.6584 - val_acc: 0.6096\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6558 - acc: 0.6099 - val_loss: 0.6586 - val_acc: 0.6072\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6557 - acc: 0.6079 - val_loss: 0.6585 - val_acc: 0.6072\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6556 - acc: 0.6082 - val_loss: 0.6587 - val_acc: 0.6036\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6554 - acc: 0.6107 - val_loss: 0.6587 - val_acc: 0.6072\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6555 - acc: 0.6099 - val_loss: 0.6588 - val_acc: 0.6064\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6552 - acc: 0.6102 - val_loss: 0.6587 - val_acc: 0.6080\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6552 - acc: 0.6106 - val_loss: 0.6590 - val_acc: 0.6076\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6550 - acc: 0.6094 - val_loss: 0.6594 - val_acc: 0.6076\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6551 - acc: 0.6091 - val_loss: 0.6588 - val_acc: 0.6072\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6551 - acc: 0.6076 - val_loss: 0.6586 - val_acc: 0.6052\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6550 - acc: 0.6089 - val_loss: 0.6596 - val_acc: 0.6048\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6551 - acc: 0.6090 - val_loss: 0.6589 - val_acc: 0.6068\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6551 - acc: 0.6112 - val_loss: 0.6593 - val_acc: 0.6028\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6548 - acc: 0.6091 - val_loss: 0.6592 - val_acc: 0.6060\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6548 - acc: 0.6082 - val_loss: 0.6602 - val_acc: 0.6092\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6546 - acc: 0.6086 - val_loss: 0.6607 - val_acc: 0.6080\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6549 - acc: 0.6107 - val_loss: 0.6600 - val_acc: 0.6032\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6549 - acc: 0.6076 - val_loss: 0.6606 - val_acc: 0.6104\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6547 - acc: 0.6112 - val_loss: 0.6598 - val_acc: 0.6068\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6547 - acc: 0.6097 - val_loss: 0.6626 - val_acc: 0.6124\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.6835 - acc: 0.5515 - val_loss: 0.6764 - val_acc: 0.5652\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6683 - acc: 0.5826 - val_loss: 0.6686 - val_acc: 0.5956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6624 - acc: 0.5965 - val_loss: 0.6638 - val_acc: 0.5912\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6595 - acc: 0.6000 - val_loss: 0.6623 - val_acc: 0.6028\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6581 - acc: 0.6042 - val_loss: 0.6606 - val_acc: 0.6044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6575 - acc: 0.6072 - val_loss: 0.6606 - val_acc: 0.6020\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6568 - acc: 0.6048 - val_loss: 0.6604 - val_acc: 0.6064\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6564 - acc: 0.6076 - val_loss: 0.6591 - val_acc: 0.6060\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6563 - acc: 0.6096 - val_loss: 0.6596 - val_acc: 0.6072\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6561 - acc: 0.6079 - val_loss: 0.6595 - val_acc: 0.6080\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6560 - acc: 0.6084 - val_loss: 0.6596 - val_acc: 0.6100\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6556 - acc: 0.6105 - val_loss: 0.6589 - val_acc: 0.6060\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6556 - acc: 0.6091 - val_loss: 0.6590 - val_acc: 0.6080\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6556 - acc: 0.6102 - val_loss: 0.6600 - val_acc: 0.6060\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6554 - acc: 0.6095 - val_loss: 0.6595 - val_acc: 0.6056\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6551 - acc: 0.6085 - val_loss: 0.6594 - val_acc: 0.6032\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6553 - acc: 0.6090 - val_loss: 0.6601 - val_acc: 0.6084\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6553 - acc: 0.6083 - val_loss: 0.6590 - val_acc: 0.6068\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6551 - acc: 0.6070 - val_loss: 0.6588 - val_acc: 0.6060\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6552 - acc: 0.6084 - val_loss: 0.6593 - val_acc: 0.6040\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6550 - acc: 0.6085 - val_loss: 0.6592 - val_acc: 0.6100\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6548 - acc: 0.6082 - val_loss: 0.6594 - val_acc: 0.6056\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6550 - acc: 0.6105 - val_loss: 0.6596 - val_acc: 0.6060\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6551 - acc: 0.6083 - val_loss: 0.6595 - val_acc: 0.6064\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6547 - acc: 0.6102 - val_loss: 0.6607 - val_acc: 0.6052\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6551 - acc: 0.6086 - val_loss: 0.6592 - val_acc: 0.6084\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6547 - acc: 0.6111 - val_loss: 0.6601 - val_acc: 0.6088\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6547 - acc: 0.6080 - val_loss: 0.6592 - val_acc: 0.6064\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6545 - acc: 0.6088 - val_loss: 0.6625 - val_acc: 0.6088\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6549 - acc: 0.6111 - val_loss: 0.6596 - val_acc: 0.6028\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6806 - acc: 0.5547 - val_loss: 0.6744 - val_acc: 0.5796\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6673 - acc: 0.5879 - val_loss: 0.6664 - val_acc: 0.5964\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6623 - acc: 0.5972 - val_loss: 0.6630 - val_acc: 0.6020\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6596 - acc: 0.6011 - val_loss: 0.6615 - val_acc: 0.6004\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6581 - acc: 0.6031 - val_loss: 0.6598 - val_acc: 0.6080\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6573 - acc: 0.6065 - val_loss: 0.6597 - val_acc: 0.5996\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6569 - acc: 0.6050 - val_loss: 0.6590 - val_acc: 0.6068\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6566 - acc: 0.6089 - val_loss: 0.6592 - val_acc: 0.6060\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6564 - acc: 0.6078 - val_loss: 0.6582 - val_acc: 0.6060\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6561 - acc: 0.6087 - val_loss: 0.6584 - val_acc: 0.6084\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6559 - acc: 0.6071 - val_loss: 0.6582 - val_acc: 0.6108\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6557 - acc: 0.6086 - val_loss: 0.6590 - val_acc: 0.6060\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6556 - acc: 0.6076 - val_loss: 0.6587 - val_acc: 0.6076\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6553 - acc: 0.6049 - val_loss: 0.6582 - val_acc: 0.6112\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6553 - acc: 0.6079 - val_loss: 0.6587 - val_acc: 0.6088\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6553 - acc: 0.6078 - val_loss: 0.6593 - val_acc: 0.6076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6552 - acc: 0.6110 - val_loss: 0.6598 - val_acc: 0.6076\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6552 - acc: 0.6092 - val_loss: 0.6597 - val_acc: 0.6084\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6551 - acc: 0.6078 - val_loss: 0.6587 - val_acc: 0.6068\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6550 - acc: 0.6074 - val_loss: 0.6588 - val_acc: 0.6080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6550 - acc: 0.6095 - val_loss: 0.6594 - val_acc: 0.6088\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6550 - acc: 0.6095 - val_loss: 0.6589 - val_acc: 0.6068\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6549 - acc: 0.6068 - val_loss: 0.6587 - val_acc: 0.6068\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6550 - acc: 0.6112 - val_loss: 0.6595 - val_acc: 0.6088\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6551 - acc: 0.6098 - val_loss: 0.6589 - val_acc: 0.6068\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6550 - acc: 0.6089 - val_loss: 0.6596 - val_acc: 0.6072\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6549 - acc: 0.6085 - val_loss: 0.6592 - val_acc: 0.6076\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6549 - acc: 0.6104 - val_loss: 0.6606 - val_acc: 0.6068\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6549 - acc: 0.6101 - val_loss: 0.6596 - val_acc: 0.6088\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6548 - acc: 0.6086 - val_loss: 0.6604 - val_acc: 0.6080\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6821 - acc: 0.5537 - val_loss: 0.6749 - val_acc: 0.5760\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6688 - acc: 0.5844 - val_loss: 0.6670 - val_acc: 0.5852\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6630 - acc: 0.5952 - val_loss: 0.6626 - val_acc: 0.5964\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6600 - acc: 0.6006 - val_loss: 0.6607 - val_acc: 0.6024\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6585 - acc: 0.6053 - val_loss: 0.6601 - val_acc: 0.6008\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6577 - acc: 0.6035 - val_loss: 0.6595 - val_acc: 0.6000\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6568 - acc: 0.6066 - val_loss: 0.6590 - val_acc: 0.6016\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6566 - acc: 0.6074 - val_loss: 0.6605 - val_acc: 0.6080\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6564 - acc: 0.6073 - val_loss: 0.6591 - val_acc: 0.6064\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6561 - acc: 0.6109 - val_loss: 0.6589 - val_acc: 0.6104\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6560 - acc: 0.6083 - val_loss: 0.6595 - val_acc: 0.6128\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6557 - acc: 0.6103 - val_loss: 0.6583 - val_acc: 0.6052\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6554 - acc: 0.6076 - val_loss: 0.6589 - val_acc: 0.6048\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6554 - acc: 0.6106 - val_loss: 0.6586 - val_acc: 0.6016\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6553 - acc: 0.6080 - val_loss: 0.6597 - val_acc: 0.6072\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6552 - acc: 0.6090 - val_loss: 0.6607 - val_acc: 0.6076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6552 - acc: 0.6095 - val_loss: 0.6600 - val_acc: 0.6084\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6551 - acc: 0.6081 - val_loss: 0.6603 - val_acc: 0.6092\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6549 - acc: 0.6090 - val_loss: 0.6596 - val_acc: 0.6060\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6551 - acc: 0.6099 - val_loss: 0.6599 - val_acc: 0.6072\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6550 - acc: 0.6105 - val_loss: 0.6591 - val_acc: 0.6092\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6547 - acc: 0.6085 - val_loss: 0.6605 - val_acc: 0.6032\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6549 - acc: 0.6087 - val_loss: 0.6599 - val_acc: 0.6052\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6550 - acc: 0.6109 - val_loss: 0.6593 - val_acc: 0.6080\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6549 - acc: 0.6101 - val_loss: 0.6587 - val_acc: 0.6048\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6550 - acc: 0.6096 - val_loss: 0.6594 - val_acc: 0.6060\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6549 - acc: 0.6074 - val_loss: 0.6601 - val_acc: 0.6048\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6548 - acc: 0.6113 - val_loss: 0.6618 - val_acc: 0.6084\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6551 - acc: 0.6107 - val_loss: 0.6599 - val_acc: 0.6076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6545 - acc: 0.6104 - val_loss: 0.6590 - val_acc: 0.6080\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6836 - acc: 0.5474 - val_loss: 0.6777 - val_acc: 0.5680\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6686 - acc: 0.5830 - val_loss: 0.6682 - val_acc: 0.5920\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6627 - acc: 0.5955 - val_loss: 0.6647 - val_acc: 0.5908\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6600 - acc: 0.5998 - val_loss: 0.6613 - val_acc: 0.6012\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6585 - acc: 0.6031 - val_loss: 0.6603 - val_acc: 0.6020\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6577 - acc: 0.6048 - val_loss: 0.6601 - val_acc: 0.6072\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6571 - acc: 0.6058 - val_loss: 0.6597 - val_acc: 0.6028\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6568 - acc: 0.6067 - val_loss: 0.6594 - val_acc: 0.6148\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6563 - acc: 0.6067 - val_loss: 0.6599 - val_acc: 0.6012\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6561 - acc: 0.6081 - val_loss: 0.6592 - val_acc: 0.6068\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6560 - acc: 0.6078 - val_loss: 0.6588 - val_acc: 0.6088\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6558 - acc: 0.6101 - val_loss: 0.6591 - val_acc: 0.6012\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6555 - acc: 0.6085 - val_loss: 0.6597 - val_acc: 0.6032\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6556 - acc: 0.6082 - val_loss: 0.6588 - val_acc: 0.6052\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6556 - acc: 0.6098 - val_loss: 0.6592 - val_acc: 0.6040\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6553 - acc: 0.6071 - val_loss: 0.6589 - val_acc: 0.6104\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6553 - acc: 0.6069 - val_loss: 0.6591 - val_acc: 0.6076\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6552 - acc: 0.6105 - val_loss: 0.6598 - val_acc: 0.6068\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6551 - acc: 0.6085 - val_loss: 0.6588 - val_acc: 0.6072\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6551 - acc: 0.6083 - val_loss: 0.6602 - val_acc: 0.6080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6550 - acc: 0.6094 - val_loss: 0.6605 - val_acc: 0.6100\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6551 - acc: 0.6083 - val_loss: 0.6595 - val_acc: 0.6072\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6549 - acc: 0.6092 - val_loss: 0.6606 - val_acc: 0.6004\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6549 - acc: 0.6104 - val_loss: 0.6592 - val_acc: 0.6084\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6547 - acc: 0.6101 - val_loss: 0.6598 - val_acc: 0.6080\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6548 - acc: 0.6108 - val_loss: 0.6591 - val_acc: 0.6100\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6550 - acc: 0.6097 - val_loss: 0.6588 - val_acc: 0.6068\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6547 - acc: 0.6101 - val_loss: 0.6595 - val_acc: 0.6080\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6546 - acc: 0.6100 - val_loss: 0.6598 - val_acc: 0.6056\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6548 - acc: 0.6114 - val_loss: 0.6599 - val_acc: 0.6084\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6853 - acc: 0.5431 - val_loss: 0.6793 - val_acc: 0.5700\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6713 - acc: 0.5829 - val_loss: 0.6733 - val_acc: 0.5860\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6659 - acc: 0.5904 - val_loss: 0.6680 - val_acc: 0.5992\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6624 - acc: 0.5995 - val_loss: 0.6651 - val_acc: 0.6000\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6603 - acc: 0.6032 - val_loss: 0.6633 - val_acc: 0.5980\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6588 - acc: 0.6046 - val_loss: 0.6625 - val_acc: 0.5996\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6580 - acc: 0.6056 - val_loss: 0.6611 - val_acc: 0.6044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6573 - acc: 0.6066 - val_loss: 0.6617 - val_acc: 0.6012\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6569 - acc: 0.6061 - val_loss: 0.6610 - val_acc: 0.6044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6567 - acc: 0.6080 - val_loss: 0.6614 - val_acc: 0.6048\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6563 - acc: 0.6066 - val_loss: 0.6603 - val_acc: 0.6036\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6561 - acc: 0.6079 - val_loss: 0.6605 - val_acc: 0.6044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6560 - acc: 0.6084 - val_loss: 0.6599 - val_acc: 0.6032\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6556 - acc: 0.6092 - val_loss: 0.6597 - val_acc: 0.6040\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6556 - acc: 0.6086 - val_loss: 0.6600 - val_acc: 0.6048\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6553 - acc: 0.6079 - val_loss: 0.6606 - val_acc: 0.6072\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6555 - acc: 0.6068 - val_loss: 0.6595 - val_acc: 0.6068\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6553 - acc: 0.6095 - val_loss: 0.6595 - val_acc: 0.6036\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6552 - acc: 0.6108 - val_loss: 0.6593 - val_acc: 0.6044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6550 - acc: 0.6092 - val_loss: 0.6597 - val_acc: 0.6048\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6551 - acc: 0.6089 - val_loss: 0.6599 - val_acc: 0.6068\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6550 - acc: 0.6098 - val_loss: 0.6599 - val_acc: 0.6056\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6550 - acc: 0.6107 - val_loss: 0.6597 - val_acc: 0.6068\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6549 - acc: 0.6094 - val_loss: 0.6599 - val_acc: 0.6064\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6548 - acc: 0.6102 - val_loss: 0.6606 - val_acc: 0.6068\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6549 - acc: 0.6108 - val_loss: 0.6608 - val_acc: 0.6088\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6548 - acc: 0.6101 - val_loss: 0.6597 - val_acc: 0.6048\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6547 - acc: 0.6075 - val_loss: 0.6602 - val_acc: 0.6072\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6546 - acc: 0.6081 - val_loss: 0.6600 - val_acc: 0.6068\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6544 - acc: 0.6084 - val_loss: 0.6616 - val_acc: 0.6112\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6853 - acc: 0.5506 - val_loss: 0.6822 - val_acc: 0.5572\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6712 - acc: 0.5849 - val_loss: 0.6735 - val_acc: 0.5852\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6655 - acc: 0.5903 - val_loss: 0.6686 - val_acc: 0.5832\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6621 - acc: 0.5972 - val_loss: 0.6668 - val_acc: 0.5996\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6603 - acc: 0.6037 - val_loss: 0.6642 - val_acc: 0.5916\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6590 - acc: 0.6057 - val_loss: 0.6623 - val_acc: 0.5988\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.6581 - acc: 0.6062 - val_loss: 0.6616 - val_acc: 0.6028\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6572 - acc: 0.6072 - val_loss: 0.6617 - val_acc: 0.5976\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6568 - acc: 0.6079 - val_loss: 0.6601 - val_acc: 0.6088\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6567 - acc: 0.6060 - val_loss: 0.6601 - val_acc: 0.6068\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6563 - acc: 0.6073 - val_loss: 0.6601 - val_acc: 0.6108\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6560 - acc: 0.6094 - val_loss: 0.6605 - val_acc: 0.6068\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6558 - acc: 0.6094 - val_loss: 0.6598 - val_acc: 0.6048\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6558 - acc: 0.6081 - val_loss: 0.6594 - val_acc: 0.6092\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.6556 - acc: 0.6097 - val_loss: 0.6599 - val_acc: 0.6072\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6555 - acc: 0.6108 - val_loss: 0.6597 - val_acc: 0.6088\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6555 - acc: 0.6092 - val_loss: 0.6597 - val_acc: 0.6088\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6553 - acc: 0.6088 - val_loss: 0.6595 - val_acc: 0.6056\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6552 - acc: 0.6095 - val_loss: 0.6596 - val_acc: 0.6072\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6549 - acc: 0.6111 - val_loss: 0.6608 - val_acc: 0.6056\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6551 - acc: 0.6099 - val_loss: 0.6591 - val_acc: 0.6080\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6550 - acc: 0.6092 - val_loss: 0.6592 - val_acc: 0.6080\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6550 - acc: 0.6079 - val_loss: 0.6597 - val_acc: 0.6084\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6549 - acc: 0.6096 - val_loss: 0.6591 - val_acc: 0.6080\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6549 - acc: 0.6093 - val_loss: 0.6595 - val_acc: 0.6080\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6549 - acc: 0.6089 - val_loss: 0.6597 - val_acc: 0.6060\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6548 - acc: 0.6089 - val_loss: 0.6592 - val_acc: 0.6076\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6549 - acc: 0.6088 - val_loss: 0.6597 - val_acc: 0.6060\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6548 - acc: 0.6094 - val_loss: 0.6590 - val_acc: 0.6072\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6547 - acc: 0.6088 - val_loss: 0.6594 - val_acc: 0.6084\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6815 - acc: 0.5544 - val_loss: 0.6770 - val_acc: 0.5668\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6703 - acc: 0.5799 - val_loss: 0.6705 - val_acc: 0.5824\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6652 - acc: 0.5935 - val_loss: 0.6659 - val_acc: 0.5820\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6619 - acc: 0.5959 - val_loss: 0.6642 - val_acc: 0.5932\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6602 - acc: 0.6020 - val_loss: 0.6618 - val_acc: 0.6008\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6589 - acc: 0.6051 - val_loss: 0.6603 - val_acc: 0.6068\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6579 - acc: 0.6035 - val_loss: 0.6602 - val_acc: 0.6092\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6575 - acc: 0.6065 - val_loss: 0.6588 - val_acc: 0.6072\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6570 - acc: 0.6072 - val_loss: 0.6591 - val_acc: 0.6080\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6568 - acc: 0.6064 - val_loss: 0.6589 - val_acc: 0.6080\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6561 - acc: 0.6085 - val_loss: 0.6595 - val_acc: 0.6040\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6560 - acc: 0.6083 - val_loss: 0.6593 - val_acc: 0.6096\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6558 - acc: 0.6098 - val_loss: 0.6579 - val_acc: 0.6052\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6558 - acc: 0.6101 - val_loss: 0.6587 - val_acc: 0.6052\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6557 - acc: 0.6094 - val_loss: 0.6585 - val_acc: 0.6072\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6553 - acc: 0.6088 - val_loss: 0.6590 - val_acc: 0.6084\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6555 - acc: 0.6105 - val_loss: 0.6589 - val_acc: 0.6060\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6552 - acc: 0.6102 - val_loss: 0.6584 - val_acc: 0.6040\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6553 - acc: 0.6104 - val_loss: 0.6588 - val_acc: 0.6056\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6552 - acc: 0.6105 - val_loss: 0.6588 - val_acc: 0.6088\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6551 - acc: 0.6091 - val_loss: 0.6586 - val_acc: 0.6080\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6550 - acc: 0.6105 - val_loss: 0.6589 - val_acc: 0.6068\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6548 - acc: 0.6108 - val_loss: 0.6589 - val_acc: 0.6096\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6549 - acc: 0.6101 - val_loss: 0.6588 - val_acc: 0.6060\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6548 - acc: 0.6101 - val_loss: 0.6583 - val_acc: 0.6072\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6549 - acc: 0.6100 - val_loss: 0.6586 - val_acc: 0.6064\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6549 - acc: 0.6106 - val_loss: 0.6591 - val_acc: 0.6080\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6549 - acc: 0.6084 - val_loss: 0.6591 - val_acc: 0.6104\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6546 - acc: 0.6087 - val_loss: 0.6593 - val_acc: 0.6080\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6547 - acc: 0.6100 - val_loss: 0.6590 - val_acc: 0.6084\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6848 - acc: 0.5401 - val_loss: 0.6786 - val_acc: 0.5652\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.6719 - acc: 0.5804 - val_loss: 0.6715 - val_acc: 0.5788\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.6661 - acc: 0.5878 - val_loss: 0.6666 - val_acc: 0.5888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6625 - acc: 0.5964 - val_loss: 0.6634 - val_acc: 0.5968\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.6603 - acc: 0.6003 - val_loss: 0.6619 - val_acc: 0.5984\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6590 - acc: 0.6019 - val_loss: 0.6612 - val_acc: 0.6044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6581 - acc: 0.6055 - val_loss: 0.6601 - val_acc: 0.6040\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6574 - acc: 0.6059 - val_loss: 0.6597 - val_acc: 0.6104\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6570 - acc: 0.6058 - val_loss: 0.6594 - val_acc: 0.6084\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6565 - acc: 0.6062 - val_loss: 0.6592 - val_acc: 0.6064\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6562 - acc: 0.6061 - val_loss: 0.6585 - val_acc: 0.6116\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6562 - acc: 0.6063 - val_loss: 0.6587 - val_acc: 0.6092\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6559 - acc: 0.6066 - val_loss: 0.6586 - val_acc: 0.6108\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6558 - acc: 0.6074 - val_loss: 0.6589 - val_acc: 0.6076\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6555 - acc: 0.6090 - val_loss: 0.6591 - val_acc: 0.6104\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6555 - acc: 0.6066 - val_loss: 0.6589 - val_acc: 0.6088\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6553 - acc: 0.6082 - val_loss: 0.6589 - val_acc: 0.6060\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6551 - acc: 0.6076 - val_loss: 0.6583 - val_acc: 0.6068\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6552 - acc: 0.6088 - val_loss: 0.6597 - val_acc: 0.6136\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6553 - acc: 0.6095 - val_loss: 0.6585 - val_acc: 0.6068\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6549 - acc: 0.6089 - val_loss: 0.6592 - val_acc: 0.6056\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6551 - acc: 0.6099 - val_loss: 0.6594 - val_acc: 0.6084\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6548 - acc: 0.6094 - val_loss: 0.6590 - val_acc: 0.6088\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6550 - acc: 0.6095 - val_loss: 0.6585 - val_acc: 0.6064\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6549 - acc: 0.6091 - val_loss: 0.6596 - val_acc: 0.6076\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6549 - acc: 0.6101 - val_loss: 0.6590 - val_acc: 0.6072\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6547 - acc: 0.6088 - val_loss: 0.6595 - val_acc: 0.6076\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6548 - acc: 0.6086 - val_loss: 0.6590 - val_acc: 0.6076\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6546 - acc: 0.6092 - val_loss: 0.6587 - val_acc: 0.6036\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6548 - acc: 0.6079 - val_loss: 0.6593 - val_acc: 0.6080\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6801 - acc: 0.5645 - val_loss: 0.6773 - val_acc: 0.5732\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6696 - acc: 0.5798 - val_loss: 0.6704 - val_acc: 0.5872\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6643 - acc: 0.5921 - val_loss: 0.6662 - val_acc: 0.5900\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6615 - acc: 0.5986 - val_loss: 0.6631 - val_acc: 0.6032\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6592 - acc: 0.6025 - val_loss: 0.6618 - val_acc: 0.6008\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6581 - acc: 0.6032 - val_loss: 0.6606 - val_acc: 0.6024\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6574 - acc: 0.6055 - val_loss: 0.6601 - val_acc: 0.6012\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6570 - acc: 0.6064 - val_loss: 0.6594 - val_acc: 0.6040\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6566 - acc: 0.6044 - val_loss: 0.6593 - val_acc: 0.6024\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6562 - acc: 0.6071 - val_loss: 0.6595 - val_acc: 0.6024\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6562 - acc: 0.6066 - val_loss: 0.6599 - val_acc: 0.6040\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6557 - acc: 0.6081 - val_loss: 0.6588 - val_acc: 0.6012\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6558 - acc: 0.6082 - val_loss: 0.6590 - val_acc: 0.6036\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6555 - acc: 0.6085 - val_loss: 0.6594 - val_acc: 0.6056\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6555 - acc: 0.6076 - val_loss: 0.6599 - val_acc: 0.6052\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6554 - acc: 0.6085 - val_loss: 0.6593 - val_acc: 0.6060\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6553 - acc: 0.6091 - val_loss: 0.6591 - val_acc: 0.6068\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6552 - acc: 0.6101 - val_loss: 0.6587 - val_acc: 0.6036\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6551 - acc: 0.6104 - val_loss: 0.6589 - val_acc: 0.6048\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6551 - acc: 0.6080 - val_loss: 0.6593 - val_acc: 0.6060\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6548 - acc: 0.6074 - val_loss: 0.6593 - val_acc: 0.6044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6549 - acc: 0.6100 - val_loss: 0.6593 - val_acc: 0.6040\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6550 - acc: 0.6086 - val_loss: 0.6593 - val_acc: 0.6068\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6550 - acc: 0.6097 - val_loss: 0.6595 - val_acc: 0.6060\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.6546 - acc: 0.6091 - val_loss: 0.6591 - val_acc: 0.6064\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.6548 - acc: 0.6110 - val_loss: 0.6590 - val_acc: 0.6052\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6546 - acc: 0.6088 - val_loss: 0.6593 - val_acc: 0.6040\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6547 - acc: 0.6105 - val_loss: 0.6593 - val_acc: 0.6048\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.6549 - acc: 0.6102 - val_loss: 0.6590 - val_acc: 0.6060\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6547 - acc: 0.6092 - val_loss: 0.6593 - val_acc: 0.6056\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6900 - acc: 0.5323 - val_loss: 0.6835 - val_acc: 0.5564\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6780 - acc: 0.5682 - val_loss: 0.6769 - val_acc: 0.5708\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6717 - acc: 0.5748 - val_loss: 0.6725 - val_acc: 0.5768\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6676 - acc: 0.5844 - val_loss: 0.6685 - val_acc: 0.5884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6644 - acc: 0.5919 - val_loss: 0.6655 - val_acc: 0.5948\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6622 - acc: 0.5945 - val_loss: 0.6636 - val_acc: 0.6004\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6606 - acc: 0.5987 - val_loss: 0.6619 - val_acc: 0.6056\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6594 - acc: 0.6023 - val_loss: 0.6608 - val_acc: 0.6040\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6585 - acc: 0.6029 - val_loss: 0.6602 - val_acc: 0.6068\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6577 - acc: 0.6055 - val_loss: 0.6600 - val_acc: 0.6028\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6572 - acc: 0.6054 - val_loss: 0.6593 - val_acc: 0.6060\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6570 - acc: 0.6044 - val_loss: 0.6589 - val_acc: 0.6028\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6565 - acc: 0.6059 - val_loss: 0.6591 - val_acc: 0.6060\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6563 - acc: 0.6072 - val_loss: 0.6585 - val_acc: 0.6052\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6560 - acc: 0.6076 - val_loss: 0.6585 - val_acc: 0.6052\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6558 - acc: 0.6078 - val_loss: 0.6585 - val_acc: 0.6080\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6557 - acc: 0.6075 - val_loss: 0.6586 - val_acc: 0.6096\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6555 - acc: 0.6086 - val_loss: 0.6586 - val_acc: 0.6104\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6555 - acc: 0.6082 - val_loss: 0.6584 - val_acc: 0.6096\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6553 - acc: 0.6104 - val_loss: 0.6585 - val_acc: 0.6080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6553 - acc: 0.6082 - val_loss: 0.6581 - val_acc: 0.6068\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6552 - acc: 0.6097 - val_loss: 0.6594 - val_acc: 0.6084\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6552 - acc: 0.6088 - val_loss: 0.6585 - val_acc: 0.6108\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6550 - acc: 0.6092 - val_loss: 0.6590 - val_acc: 0.6100\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6551 - acc: 0.6101 - val_loss: 0.6585 - val_acc: 0.6104\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6550 - acc: 0.6106 - val_loss: 0.6588 - val_acc: 0.6104\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6549 - acc: 0.6101 - val_loss: 0.6587 - val_acc: 0.6096\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6548 - acc: 0.6079 - val_loss: 0.6586 - val_acc: 0.6072\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6548 - acc: 0.6095 - val_loss: 0.6595 - val_acc: 0.6100\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6546 - acc: 0.6118 - val_loss: 0.6585 - val_acc: 0.6092\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6853 - acc: 0.5460 - val_loss: 0.6815 - val_acc: 0.5652\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6744 - acc: 0.5798 - val_loss: 0.6759 - val_acc: 0.5748\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6689 - acc: 0.5902 - val_loss: 0.6714 - val_acc: 0.5796\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6651 - acc: 0.5952 - val_loss: 0.6689 - val_acc: 0.5876\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6627 - acc: 0.6019 - val_loss: 0.6659 - val_acc: 0.5876\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6608 - acc: 0.6038 - val_loss: 0.6644 - val_acc: 0.5908\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6595 - acc: 0.6014 - val_loss: 0.6631 - val_acc: 0.5916\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6585 - acc: 0.6069 - val_loss: 0.6624 - val_acc: 0.5916\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6578 - acc: 0.6052 - val_loss: 0.6615 - val_acc: 0.5956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6572 - acc: 0.6079 - val_loss: 0.6611 - val_acc: 0.6016\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6568 - acc: 0.6081 - val_loss: 0.6606 - val_acc: 0.6044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6565 - acc: 0.6089 - val_loss: 0.6602 - val_acc: 0.6000\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6562 - acc: 0.6079 - val_loss: 0.6602 - val_acc: 0.6072\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6560 - acc: 0.6074 - val_loss: 0.6606 - val_acc: 0.6060\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6559 - acc: 0.6101 - val_loss: 0.6598 - val_acc: 0.6072\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6556 - acc: 0.6115 - val_loss: 0.6595 - val_acc: 0.6100\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6556 - acc: 0.6088 - val_loss: 0.6597 - val_acc: 0.6092\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6554 - acc: 0.6108 - val_loss: 0.6594 - val_acc: 0.6032\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6553 - acc: 0.6093 - val_loss: 0.6595 - val_acc: 0.6072\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6553 - acc: 0.6098 - val_loss: 0.6597 - val_acc: 0.6072\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6553 - acc: 0.6095 - val_loss: 0.6591 - val_acc: 0.6092\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6550 - acc: 0.6105 - val_loss: 0.6593 - val_acc: 0.6056\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6552 - acc: 0.6097 - val_loss: 0.6594 - val_acc: 0.6096\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6549 - acc: 0.6100 - val_loss: 0.6591 - val_acc: 0.6072\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6549 - acc: 0.6121 - val_loss: 0.6593 - val_acc: 0.6028\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6550 - acc: 0.6105 - val_loss: 0.6592 - val_acc: 0.6044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6548 - acc: 0.6102 - val_loss: 0.6596 - val_acc: 0.6072\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6547 - acc: 0.6101 - val_loss: 0.6594 - val_acc: 0.6060\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6547 - acc: 0.6122 - val_loss: 0.6592 - val_acc: 0.6048\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6546 - acc: 0.6091 - val_loss: 0.6594 - val_acc: 0.6052\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6828 - acc: 0.5520 - val_loss: 0.6826 - val_acc: 0.5612\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6740 - acc: 0.5755 - val_loss: 0.6767 - val_acc: 0.5632\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6687 - acc: 0.5849 - val_loss: 0.6723 - val_acc: 0.5760\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6651 - acc: 0.5922 - val_loss: 0.6694 - val_acc: 0.5844\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6627 - acc: 0.5956 - val_loss: 0.6663 - val_acc: 0.5936\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6607 - acc: 0.6012 - val_loss: 0.6644 - val_acc: 0.6004\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6596 - acc: 0.6004 - val_loss: 0.6639 - val_acc: 0.5960\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6587 - acc: 0.6044 - val_loss: 0.6621 - val_acc: 0.6032\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6578 - acc: 0.6065 - val_loss: 0.6614 - val_acc: 0.6032\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6574 - acc: 0.6056 - val_loss: 0.6612 - val_acc: 0.6008\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6569 - acc: 0.6071 - val_loss: 0.6609 - val_acc: 0.6060\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6567 - acc: 0.6069 - val_loss: 0.6601 - val_acc: 0.6036\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6564 - acc: 0.6060 - val_loss: 0.6601 - val_acc: 0.6036\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6562 - acc: 0.6075 - val_loss: 0.6599 - val_acc: 0.6064\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6558 - acc: 0.6072 - val_loss: 0.6599 - val_acc: 0.6036\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6560 - acc: 0.6086 - val_loss: 0.6600 - val_acc: 0.6056\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6558 - acc: 0.6085 - val_loss: 0.6596 - val_acc: 0.6104\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6557 - acc: 0.6094 - val_loss: 0.6593 - val_acc: 0.6072\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6555 - acc: 0.6085 - val_loss: 0.6593 - val_acc: 0.6060\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6553 - acc: 0.6091 - val_loss: 0.6600 - val_acc: 0.6080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6553 - acc: 0.6091 - val_loss: 0.6595 - val_acc: 0.6064\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6552 - acc: 0.6094 - val_loss: 0.6595 - val_acc: 0.6080\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6551 - acc: 0.6099 - val_loss: 0.6593 - val_acc: 0.6060\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6551 - acc: 0.6098 - val_loss: 0.6595 - val_acc: 0.6076\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6550 - acc: 0.6100 - val_loss: 0.6594 - val_acc: 0.6044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6550 - acc: 0.6106 - val_loss: 0.6597 - val_acc: 0.6092\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6549 - acc: 0.6101 - val_loss: 0.6593 - val_acc: 0.6072\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6548 - acc: 0.6113 - val_loss: 0.6592 - val_acc: 0.6032\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6548 - acc: 0.6103 - val_loss: 0.6591 - val_acc: 0.6076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6547 - acc: 0.6100 - val_loss: 0.6595 - val_acc: 0.6084\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6851 - acc: 0.5483 - val_loss: 0.6829 - val_acc: 0.5572\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6745 - acc: 0.5739 - val_loss: 0.6764 - val_acc: 0.5708\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6693 - acc: 0.5855 - val_loss: 0.6719 - val_acc: 0.5840\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6657 - acc: 0.5911 - val_loss: 0.6690 - val_acc: 0.5884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6632 - acc: 0.5940 - val_loss: 0.6668 - val_acc: 0.5940\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6613 - acc: 0.5997 - val_loss: 0.6647 - val_acc: 0.5968\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6599 - acc: 0.6011 - val_loss: 0.6635 - val_acc: 0.6004\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6590 - acc: 0.6034 - val_loss: 0.6624 - val_acc: 0.5980\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6581 - acc: 0.6038 - val_loss: 0.6617 - val_acc: 0.6028\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6576 - acc: 0.6050 - val_loss: 0.6607 - val_acc: 0.5980\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6572 - acc: 0.6060 - val_loss: 0.6606 - val_acc: 0.6000\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6567 - acc: 0.6084 - val_loss: 0.6603 - val_acc: 0.5976\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6565 - acc: 0.6060 - val_loss: 0.6596 - val_acc: 0.6032\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6562 - acc: 0.6082 - val_loss: 0.6600 - val_acc: 0.6076\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6560 - acc: 0.6087 - val_loss: 0.6596 - val_acc: 0.6032\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6559 - acc: 0.6084 - val_loss: 0.6597 - val_acc: 0.6096\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 21us/sample - loss: 0.6557 - acc: 0.6091 - val_loss: 0.6593 - val_acc: 0.6048\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6556 - acc: 0.6084 - val_loss: 0.6594 - val_acc: 0.6076\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6555 - acc: 0.6104 - val_loss: 0.6592 - val_acc: 0.6088\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6554 - acc: 0.6078 - val_loss: 0.6599 - val_acc: 0.6056\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6553 - acc: 0.6106 - val_loss: 0.6593 - val_acc: 0.6064\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6552 - acc: 0.6091 - val_loss: 0.6595 - val_acc: 0.6040\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6551 - acc: 0.6103 - val_loss: 0.6596 - val_acc: 0.6072\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6551 - acc: 0.6099 - val_loss: 0.6592 - val_acc: 0.6084\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6551 - acc: 0.6112 - val_loss: 0.6592 - val_acc: 0.6100\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6551 - acc: 0.6106 - val_loss: 0.6591 - val_acc: 0.6080\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6550 - acc: 0.6091 - val_loss: 0.6592 - val_acc: 0.6080\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6548 - acc: 0.6099 - val_loss: 0.6593 - val_acc: 0.6068\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6550 - acc: 0.6097 - val_loss: 0.6591 - val_acc: 0.6076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6547 - acc: 0.6100 - val_loss: 0.6592 - val_acc: 0.6036\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6856 - acc: 0.5389 - val_loss: 0.6826 - val_acc: 0.5556\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6753 - acc: 0.5703 - val_loss: 0.6764 - val_acc: 0.5688\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6697 - acc: 0.5833 - val_loss: 0.6719 - val_acc: 0.5832\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6659 - acc: 0.5891 - val_loss: 0.6686 - val_acc: 0.5888\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6633 - acc: 0.5970 - val_loss: 0.6665 - val_acc: 0.5908\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6614 - acc: 0.6004 - val_loss: 0.6646 - val_acc: 0.5940\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6601 - acc: 0.6015 - val_loss: 0.6631 - val_acc: 0.5952\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6589 - acc: 0.6034 - val_loss: 0.6625 - val_acc: 0.6028\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6582 - acc: 0.6065 - val_loss: 0.6618 - val_acc: 0.6028\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6576 - acc: 0.6088 - val_loss: 0.6613 - val_acc: 0.6068\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6571 - acc: 0.6077 - val_loss: 0.6611 - val_acc: 0.6076\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6568 - acc: 0.6085 - val_loss: 0.6606 - val_acc: 0.6072\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6567 - acc: 0.6069 - val_loss: 0.6599 - val_acc: 0.6008\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6564 - acc: 0.6106 - val_loss: 0.6599 - val_acc: 0.6072\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6561 - acc: 0.6085 - val_loss: 0.6601 - val_acc: 0.6092\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6559 - acc: 0.6092 - val_loss: 0.6597 - val_acc: 0.6076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6558 - acc: 0.6085 - val_loss: 0.6595 - val_acc: 0.6088\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6556 - acc: 0.6099 - val_loss: 0.6596 - val_acc: 0.6052\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6556 - acc: 0.6111 - val_loss: 0.6593 - val_acc: 0.6072\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6554 - acc: 0.6097 - val_loss: 0.6595 - val_acc: 0.6072\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6552 - acc: 0.6091 - val_loss: 0.6595 - val_acc: 0.6044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6553 - acc: 0.6113 - val_loss: 0.6594 - val_acc: 0.6096\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6552 - acc: 0.6105 - val_loss: 0.6592 - val_acc: 0.6088\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6551 - acc: 0.6109 - val_loss: 0.6590 - val_acc: 0.6084\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6550 - acc: 0.6103 - val_loss: 0.6600 - val_acc: 0.6112\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6551 - acc: 0.6119 - val_loss: 0.6590 - val_acc: 0.6072\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6550 - acc: 0.6094 - val_loss: 0.6596 - val_acc: 0.6084\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6549 - acc: 0.6101 - val_loss: 0.6595 - val_acc: 0.6100\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6549 - acc: 0.6095 - val_loss: 0.6591 - val_acc: 0.6060\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 22us/sample - loss: 0.6550 - acc: 0.6106 - val_loss: 0.6594 - val_acc: 0.6108\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6928 - acc: 0.5121 - val_loss: 0.6830 - val_acc: 0.5612\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6809 - acc: 0.5636 - val_loss: 0.6781 - val_acc: 0.5840\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6751 - acc: 0.5715 - val_loss: 0.6748 - val_acc: 0.5864\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6713 - acc: 0.5758 - val_loss: 0.6714 - val_acc: 0.5868\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6681 - acc: 0.5794 - val_loss: 0.6691 - val_acc: 0.5856\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6658 - acc: 0.5866 - val_loss: 0.6671 - val_acc: 0.5888\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6638 - acc: 0.5901 - val_loss: 0.6654 - val_acc: 0.5964\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6623 - acc: 0.5949 - val_loss: 0.6642 - val_acc: 0.5976\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6610 - acc: 0.5975 - val_loss: 0.6631 - val_acc: 0.6000\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6601 - acc: 0.5996 - val_loss: 0.6622 - val_acc: 0.6012\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6593 - acc: 0.6019 - val_loss: 0.6614 - val_acc: 0.6036\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6586 - acc: 0.6016 - val_loss: 0.6610 - val_acc: 0.6040\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6581 - acc: 0.6034 - val_loss: 0.6602 - val_acc: 0.6016\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6576 - acc: 0.6033 - val_loss: 0.6602 - val_acc: 0.6032\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6572 - acc: 0.6038 - val_loss: 0.6599 - val_acc: 0.6036\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6569 - acc: 0.6058 - val_loss: 0.6595 - val_acc: 0.6040\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6568 - acc: 0.6058 - val_loss: 0.6592 - val_acc: 0.6048\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6565 - acc: 0.6060 - val_loss: 0.6589 - val_acc: 0.6008\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6563 - acc: 0.6064 - val_loss: 0.6589 - val_acc: 0.6032\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6560 - acc: 0.6063 - val_loss: 0.6594 - val_acc: 0.6048\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6560 - acc: 0.6069 - val_loss: 0.6588 - val_acc: 0.6028\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6557 - acc: 0.6095 - val_loss: 0.6589 - val_acc: 0.6048\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 15us/sample - loss: 0.6556 - acc: 0.6065 - val_loss: 0.6588 - val_acc: 0.6052\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6558 - acc: 0.6076 - val_loss: 0.6588 - val_acc: 0.6052\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6555 - acc: 0.6087 - val_loss: 0.6585 - val_acc: 0.6028\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6554 - acc: 0.6087 - val_loss: 0.6589 - val_acc: 0.6076\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6553 - acc: 0.6085 - val_loss: 0.6586 - val_acc: 0.6016\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6553 - acc: 0.6075 - val_loss: 0.6586 - val_acc: 0.6084\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6552 - acc: 0.6096 - val_loss: 0.6588 - val_acc: 0.6048\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6553 - acc: 0.6087 - val_loss: 0.6586 - val_acc: 0.6060\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6913 - acc: 0.5249 - val_loss: 0.6853 - val_acc: 0.5548\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6805 - acc: 0.5624 - val_loss: 0.6798 - val_acc: 0.5672\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6751 - acc: 0.5679 - val_loss: 0.6765 - val_acc: 0.5768\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6714 - acc: 0.5749 - val_loss: 0.6733 - val_acc: 0.5776\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6686 - acc: 0.5818 - val_loss: 0.6710 - val_acc: 0.5832\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6661 - acc: 0.5849 - val_loss: 0.6688 - val_acc: 0.5892\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6643 - acc: 0.5912 - val_loss: 0.6672 - val_acc: 0.5928\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6628 - acc: 0.5954 - val_loss: 0.6656 - val_acc: 0.5964\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6615 - acc: 0.5982 - val_loss: 0.6643 - val_acc: 0.5972\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6605 - acc: 0.6005 - val_loss: 0.6634 - val_acc: 0.5980\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6598 - acc: 0.6000 - val_loss: 0.6626 - val_acc: 0.5980\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6589 - acc: 0.6043 - val_loss: 0.6617 - val_acc: 0.6044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6584 - acc: 0.6036 - val_loss: 0.6614 - val_acc: 0.6020\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6579 - acc: 0.6069 - val_loss: 0.6610 - val_acc: 0.6004\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6575 - acc: 0.6057 - val_loss: 0.6604 - val_acc: 0.6032\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6571 - acc: 0.6062 - val_loss: 0.6604 - val_acc: 0.6044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6568 - acc: 0.6073 - val_loss: 0.6599 - val_acc: 0.6076\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6567 - acc: 0.6071 - val_loss: 0.6598 - val_acc: 0.6088\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6564 - acc: 0.6086 - val_loss: 0.6596 - val_acc: 0.6084\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6563 - acc: 0.6066 - val_loss: 0.6597 - val_acc: 0.6080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6562 - acc: 0.6083 - val_loss: 0.6592 - val_acc: 0.6068\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6558 - acc: 0.6091 - val_loss: 0.6592 - val_acc: 0.6068\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6557 - acc: 0.6088 - val_loss: 0.6594 - val_acc: 0.6068\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6556 - acc: 0.6089 - val_loss: 0.6592 - val_acc: 0.6104\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6555 - acc: 0.6088 - val_loss: 0.6591 - val_acc: 0.6084\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6555 - acc: 0.6086 - val_loss: 0.6590 - val_acc: 0.6088\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6554 - acc: 0.6094 - val_loss: 0.6590 - val_acc: 0.6096\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6555 - acc: 0.6081 - val_loss: 0.6590 - val_acc: 0.6092\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6553 - acc: 0.6091 - val_loss: 0.6589 - val_acc: 0.6084\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6551 - acc: 0.6099 - val_loss: 0.6589 - val_acc: 0.6092\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6922 - acc: 0.5315 - val_loss: 0.6892 - val_acc: 0.5524\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6813 - acc: 0.5638 - val_loss: 0.6837 - val_acc: 0.5636\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6761 - acc: 0.5737 - val_loss: 0.6792 - val_acc: 0.5752\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6722 - acc: 0.5837 - val_loss: 0.6758 - val_acc: 0.5832\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6693 - acc: 0.5861 - val_loss: 0.6730 - val_acc: 0.5840\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6668 - acc: 0.5885 - val_loss: 0.6704 - val_acc: 0.5860\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6648 - acc: 0.5901 - val_loss: 0.6687 - val_acc: 0.5912\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6633 - acc: 0.5968 - val_loss: 0.6668 - val_acc: 0.5900\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6620 - acc: 0.5979 - val_loss: 0.6654 - val_acc: 0.5948\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6609 - acc: 0.6004 - val_loss: 0.6647 - val_acc: 0.5996\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6600 - acc: 0.6012 - val_loss: 0.6634 - val_acc: 0.5996\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6591 - acc: 0.6048 - val_loss: 0.6624 - val_acc: 0.5952\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6589 - acc: 0.6028 - val_loss: 0.6618 - val_acc: 0.5984\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6580 - acc: 0.6049 - val_loss: 0.6619 - val_acc: 0.6004\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6576 - acc: 0.6058 - val_loss: 0.6609 - val_acc: 0.6032\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6573 - acc: 0.6053 - val_loss: 0.6606 - val_acc: 0.6020\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6570 - acc: 0.6048 - val_loss: 0.6603 - val_acc: 0.6064\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6568 - acc: 0.6066 - val_loss: 0.6601 - val_acc: 0.6052\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6565 - acc: 0.6076 - val_loss: 0.6598 - val_acc: 0.6036\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6562 - acc: 0.6079 - val_loss: 0.6599 - val_acc: 0.6104\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6561 - acc: 0.6068 - val_loss: 0.6596 - val_acc: 0.6068\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6559 - acc: 0.6076 - val_loss: 0.6602 - val_acc: 0.6044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6558 - acc: 0.6078 - val_loss: 0.6591 - val_acc: 0.6072\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6557 - acc: 0.6086 - val_loss: 0.6591 - val_acc: 0.6060\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6556 - acc: 0.6074 - val_loss: 0.6596 - val_acc: 0.6056\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6555 - acc: 0.6094 - val_loss: 0.6589 - val_acc: 0.6108\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6553 - acc: 0.6088 - val_loss: 0.6587 - val_acc: 0.6092\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6553 - acc: 0.6094 - val_loss: 0.6590 - val_acc: 0.6072\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6553 - acc: 0.6093 - val_loss: 0.6586 - val_acc: 0.6084\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6552 - acc: 0.6099 - val_loss: 0.6587 - val_acc: 0.6104\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6895 - acc: 0.5321 - val_loss: 0.6865 - val_acc: 0.5376\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6797 - acc: 0.5681 - val_loss: 0.6819 - val_acc: 0.5660\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6747 - acc: 0.5750 - val_loss: 0.6775 - val_acc: 0.5672\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6709 - acc: 0.5815 - val_loss: 0.6741 - val_acc: 0.5736\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6681 - acc: 0.5871 - val_loss: 0.6713 - val_acc: 0.5800\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6658 - acc: 0.5878 - val_loss: 0.6692 - val_acc: 0.5876\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6639 - acc: 0.5941 - val_loss: 0.6675 - val_acc: 0.5912\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6624 - acc: 0.5922 - val_loss: 0.6662 - val_acc: 0.5972\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6613 - acc: 0.5950 - val_loss: 0.6648 - val_acc: 0.5988\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6602 - acc: 0.6002 - val_loss: 0.6638 - val_acc: 0.6040\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6595 - acc: 0.6002 - val_loss: 0.6631 - val_acc: 0.6036\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6587 - acc: 0.6022 - val_loss: 0.6621 - val_acc: 0.6076\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6580 - acc: 0.6038 - val_loss: 0.6622 - val_acc: 0.6052\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6576 - acc: 0.6071 - val_loss: 0.6611 - val_acc: 0.6036\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6574 - acc: 0.6038 - val_loss: 0.6607 - val_acc: 0.6076\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6568 - acc: 0.6072 - val_loss: 0.6607 - val_acc: 0.6048\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6568 - acc: 0.6054 - val_loss: 0.6604 - val_acc: 0.6080\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6565 - acc: 0.6057 - val_loss: 0.6598 - val_acc: 0.6064\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6563 - acc: 0.6080 - val_loss: 0.6598 - val_acc: 0.6076\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6560 - acc: 0.6066 - val_loss: 0.6601 - val_acc: 0.6072\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6559 - acc: 0.6099 - val_loss: 0.6595 - val_acc: 0.6080\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6557 - acc: 0.6079 - val_loss: 0.6594 - val_acc: 0.6036\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6557 - acc: 0.6061 - val_loss: 0.6590 - val_acc: 0.6060\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6556 - acc: 0.6065 - val_loss: 0.6594 - val_acc: 0.6072\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6555 - acc: 0.6071 - val_loss: 0.6594 - val_acc: 0.6048\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6554 - acc: 0.6074 - val_loss: 0.6591 - val_acc: 0.6056\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6553 - acc: 0.6096 - val_loss: 0.6593 - val_acc: 0.6052\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6552 - acc: 0.6097 - val_loss: 0.6588 - val_acc: 0.6092\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6551 - acc: 0.6096 - val_loss: 0.6593 - val_acc: 0.6068\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6551 - acc: 0.6101 - val_loss: 0.6590 - val_acc: 0.6076\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6942 - acc: 0.5217 - val_loss: 0.6827 - val_acc: 0.5520\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6808 - acc: 0.5568 - val_loss: 0.6778 - val_acc: 0.5656\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6751 - acc: 0.5762 - val_loss: 0.6746 - val_acc: 0.5768\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6714 - acc: 0.5811 - val_loss: 0.6722 - val_acc: 0.5744\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6687 - acc: 0.5854 - val_loss: 0.6700 - val_acc: 0.5828\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 15us/sample - loss: 0.6663 - acc: 0.5892 - val_loss: 0.6678 - val_acc: 0.5892\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6646 - acc: 0.5913 - val_loss: 0.6663 - val_acc: 0.5912\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6628 - acc: 0.5948 - val_loss: 0.6649 - val_acc: 0.5892\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6616 - acc: 0.5968 - val_loss: 0.6639 - val_acc: 0.5924\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6607 - acc: 0.5989 - val_loss: 0.6630 - val_acc: 0.5964\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6598 - acc: 0.6007 - val_loss: 0.6622 - val_acc: 0.5976\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6590 - acc: 0.6012 - val_loss: 0.6615 - val_acc: 0.6024\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6585 - acc: 0.6035 - val_loss: 0.6610 - val_acc: 0.5992\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6580 - acc: 0.6036 - val_loss: 0.6603 - val_acc: 0.6008\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6575 - acc: 0.6039 - val_loss: 0.6601 - val_acc: 0.6056\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6573 - acc: 0.6051 - val_loss: 0.6601 - val_acc: 0.6076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6568 - acc: 0.6054 - val_loss: 0.6598 - val_acc: 0.6084\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6567 - acc: 0.6060 - val_loss: 0.6593 - val_acc: 0.6104\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6564 - acc: 0.6069 - val_loss: 0.6591 - val_acc: 0.6080\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6562 - acc: 0.6062 - val_loss: 0.6592 - val_acc: 0.6060\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6560 - acc: 0.6073 - val_loss: 0.6589 - val_acc: 0.6056\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6559 - acc: 0.6081 - val_loss: 0.6588 - val_acc: 0.6056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6558 - acc: 0.6069 - val_loss: 0.6587 - val_acc: 0.6076\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6555 - acc: 0.6086 - val_loss: 0.6585 - val_acc: 0.6072\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6556 - acc: 0.6081 - val_loss: 0.6586 - val_acc: 0.6088\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 14us/sample - loss: 0.6554 - acc: 0.6077 - val_loss: 0.6589 - val_acc: 0.6100\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6553 - acc: 0.6084 - val_loss: 0.6585 - val_acc: 0.6084\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6552 - acc: 0.6082 - val_loss: 0.6586 - val_acc: 0.6088\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 12us/sample - loss: 0.6551 - acc: 0.6092 - val_loss: 0.6587 - val_acc: 0.6100\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 13us/sample - loss: 0.6551 - acc: 0.6085 - val_loss: 0.6586 - val_acc: 0.6088\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,312\n",
      "Trainable params: 1,972\n",
      "Non-trainable params: 340\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.7212 - acc: 0.5672 - val_loss: 0.6669 - val_acc: 0.5876\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6625 - acc: 0.6015 - val_loss: 0.6583 - val_acc: 0.6048\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6374 - acc: 0.6297 - val_loss: 0.6352 - val_acc: 0.6524\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6225 - acc: 0.6445 - val_loss: 0.6248 - val_acc: 0.6556\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6056 - acc: 0.6564 - val_loss: 0.6210 - val_acc: 0.6648\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5968 - acc: 0.6671 - val_loss: 0.6063 - val_acc: 0.6832\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5909 - acc: 0.6731 - val_loss: 0.6063 - val_acc: 0.6812\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5829 - acc: 0.6807 - val_loss: 0.5951 - val_acc: 0.6936\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5731 - acc: 0.6889 - val_loss: 0.6046 - val_acc: 0.6564\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5711 - acc: 0.6955 - val_loss: 0.5897 - val_acc: 0.6988\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5754 - acc: 0.6848 - val_loss: 0.5765 - val_acc: 0.7156\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5668 - acc: 0.6944 - val_loss: 0.5804 - val_acc: 0.7120\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5594 - acc: 0.7001 - val_loss: 0.5698 - val_acc: 0.7100\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5620 - acc: 0.6987 - val_loss: 0.5647 - val_acc: 0.7204\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5552 - acc: 0.7017 - val_loss: 0.5952 - val_acc: 0.6428\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5568 - acc: 0.7004 - val_loss: 0.5730 - val_acc: 0.7052\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5544 - acc: 0.7065 - val_loss: 0.5922 - val_acc: 0.6420\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5514 - acc: 0.7049 - val_loss: 0.5885 - val_acc: 0.6536\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5541 - acc: 0.7040 - val_loss: 0.5470 - val_acc: 0.7308\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5482 - acc: 0.7109 - val_loss: 0.5653 - val_acc: 0.7128\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5466 - acc: 0.7109 - val_loss: 0.5938 - val_acc: 0.6368\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5475 - acc: 0.7081 - val_loss: 0.5636 - val_acc: 0.7024\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5447 - acc: 0.7135 - val_loss: 0.5493 - val_acc: 0.7224\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5476 - acc: 0.7096 - val_loss: 0.5854 - val_acc: 0.6408\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5437 - acc: 0.7166 - val_loss: 0.5816 - val_acc: 0.6508\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5432 - acc: 0.7128 - val_loss: 0.5919 - val_acc: 0.6400\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5414 - acc: 0.7152 - val_loss: 0.5662 - val_acc: 0.6840\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.5413 - acc: 0.7166 - val_loss: 0.5800 - val_acc: 0.6568\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5445 - acc: 0.7118 - val_loss: 0.5697 - val_acc: 0.6920\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5427 - acc: 0.7140 - val_loss: 0.5558 - val_acc: 0.7084\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 8,992\n",
      "Trainable params: 8,572\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.7165 - acc: 0.5775 - val_loss: 0.7507 - val_acc: 0.4912\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6346 - acc: 0.6329 - val_loss: 0.6566 - val_acc: 0.5748\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6063 - acc: 0.6592 - val_loss: 0.6453 - val_acc: 0.6136\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.5851 - acc: 0.682 - 2s 133us/sample - loss: 0.5852 - acc: 0.6823 - val_loss: 0.6246 - val_acc: 0.6744\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5718 - acc: 0.6905 - val_loss: 0.6033 - val_acc: 0.7068\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5629 - acc: 0.6956 - val_loss: 0.6077 - val_acc: 0.7124\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5552 - acc: 0.7055 - val_loss: 0.6127 - val_acc: 0.6928\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5480 - acc: 0.7096 - val_loss: 0.6115 - val_acc: 0.6684\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5400 - acc: 0.7172 - val_loss: 0.6494 - val_acc: 0.6168\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5342 - acc: 0.7214 - val_loss: 0.6104 - val_acc: 0.7148\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5327 - acc: 0.7232 - val_loss: 0.6039 - val_acc: 0.6744\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5266 - acc: 0.7231 - val_loss: 0.6045 - val_acc: 0.6716\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5212 - acc: 0.7309 - val_loss: 0.6109 - val_acc: 0.7208\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5178 - acc: 0.7342 - val_loss: 0.6109 - val_acc: 0.6800\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5178 - acc: 0.7308 - val_loss: 0.6139 - val_acc: 0.6568\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5110 - acc: 0.7398 - val_loss: 0.6319 - val_acc: 0.6548\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5111 - acc: 0.7388 - val_loss: 0.6065 - val_acc: 0.7260\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5062 - acc: 0.7434 - val_loss: 0.6692 - val_acc: 0.6300\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5040 - acc: 0.7424 - val_loss: 0.7078 - val_acc: 0.6008\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5014 - acc: 0.7447 - val_loss: 0.6110 - val_acc: 0.7424\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5000 - acc: 0.7480 - val_loss: 0.7183 - val_acc: 0.6052\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4947 - acc: 0.7495 - val_loss: 0.6815 - val_acc: 0.6268\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4948 - acc: 0.7520 - val_loss: 0.6145 - val_acc: 0.6944\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4923 - acc: 0.7531 - val_loss: 0.6573 - val_acc: 0.6560\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4893 - acc: 0.7508 - val_loss: 0.6795 - val_acc: 0.6452\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4898 - acc: 0.7529 - val_loss: 0.6192 - val_acc: 0.6824\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4831 - acc: 0.7564 - val_loss: 0.7135 - val_acc: 0.6164\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4835 - acc: 0.7568 - val_loss: 0.6099 - val_acc: 0.7008\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4826 - acc: 0.7614 - val_loss: 0.6645 - val_acc: 0.6684\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4807 - acc: 0.7611 - val_loss: 0.6672 - val_acc: 0.6560\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 17,342\n",
      "Trainable params: 16,822\n",
      "Non-trainable params: 520\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 214us/sample - loss: 0.7124 - acc: 0.5948 - val_loss: 0.9272 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6337 - acc: 0.6354 - val_loss: 0.6471 - val_acc: 0.6024\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6044 - acc: 0.6614 - val_loss: 0.6240 - val_acc: 0.6712\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5877 - acc: 0.6789 - val_loss: 0.6551 - val_acc: 0.5884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5751 - acc: 0.6881 - val_loss: 0.6595 - val_acc: 0.6088\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5576 - acc: 0.7016 - val_loss: 0.6417 - val_acc: 0.6556\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5544 - acc: 0.7032 - val_loss: 0.6380 - val_acc: 0.6556\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5482 - acc: 0.7103 - val_loss: 0.6606 - val_acc: 0.6708\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5340 - acc: 0.7168 - val_loss: 0.6086 - val_acc: 0.7100\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5299 - acc: 0.7265 - val_loss: 0.6266 - val_acc: 0.7000\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5181 - acc: 0.7309 - val_loss: 0.6678 - val_acc: 0.6940\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5209 - acc: 0.7343 - val_loss: 0.6669 - val_acc: 0.6408\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5136 - acc: 0.7349 - val_loss: 0.6348 - val_acc: 0.6624\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5098 - acc: 0.7391 - val_loss: 0.6316 - val_acc: 0.6948\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5033 - acc: 0.7462 - val_loss: 0.6619 - val_acc: 0.6756\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4992 - acc: 0.7452 - val_loss: 0.6146 - val_acc: 0.7220\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5029 - acc: 0.7414 - val_loss: 0.6331 - val_acc: 0.7000\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4908 - acc: 0.7521 - val_loss: 0.6812 - val_acc: 0.6528\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4932 - acc: 0.7521 - val_loss: 0.7449 - val_acc: 0.6280\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4874 - acc: 0.7537 - val_loss: 0.7558 - val_acc: 0.6024\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4864 - acc: 0.7566 - val_loss: 0.6847 - val_acc: 0.6900\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4855 - acc: 0.7575 - val_loss: 0.7091 - val_acc: 0.6672\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4801 - acc: 0.7598 - val_loss: 0.7511 - val_acc: 0.6088\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4722 - acc: 0.7676 - val_loss: 0.7186 - val_acc: 0.6708\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4729 - acc: 0.7655 - val_loss: 0.7501 - val_acc: 0.6376\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4711 - acc: 0.7664 - val_loss: 0.7373 - val_acc: 0.6392\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4727 - acc: 0.7704 - val_loss: 0.7577 - val_acc: 0.6328\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4645 - acc: 0.7701 - val_loss: 0.7352 - val_acc: 0.6524\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4636 - acc: 0.7720 - val_loss: 0.7553 - val_acc: 0.6628\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4626 - acc: 0.7734 - val_loss: 0.7167 - val_acc: 0.6772\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 27,362\n",
      "Trainable params: 26,722\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 217us/sample - loss: 0.7225 - acc: 0.5815 - val_loss: 1.0056 - val_acc: 0.4892\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6314 - acc: 0.6341 - val_loss: 0.6746 - val_acc: 0.5740\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6049 - acc: 0.6619 - val_loss: 0.7445 - val_acc: 0.5420\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5871 - acc: 0.6764 - val_loss: 0.6351 - val_acc: 0.6740\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5743 - acc: 0.6883 - val_loss: 0.6176 - val_acc: 0.6972\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5609 - acc: 0.7002 - val_loss: 0.6334 - val_acc: 0.6620\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5530 - acc: 0.7042 - val_loss: 0.6513 - val_acc: 0.6276\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5449 - acc: 0.7136 - val_loss: 0.6361 - val_acc: 0.6692\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5382 - acc: 0.7210 - val_loss: 0.6842 - val_acc: 0.6356\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5328 - acc: 0.7177 - val_loss: 0.6497 - val_acc: 0.6356\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5204 - acc: 0.7293 - val_loss: 0.6077 - val_acc: 0.7184\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5141 - acc: 0.7344 - val_loss: 0.6347 - val_acc: 0.6900\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5083 - acc: 0.7427 - val_loss: 0.6568 - val_acc: 0.6552\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5006 - acc: 0.7456 - val_loss: 0.6935 - val_acc: 0.6408\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4974 - acc: 0.7493 - val_loss: 0.7040 - val_acc: 0.6664\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.4930 - acc: 0.7533 - val_loss: 0.7520 - val_acc: 0.6480\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4908 - acc: 0.7519 - val_loss: 0.6948 - val_acc: 0.6392\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4809 - acc: 0.7584 - val_loss: 0.7727 - val_acc: 0.6136\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4797 - acc: 0.7615 - val_loss: 0.8006 - val_acc: 0.5988\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.4746 - acc: 0.7633 - val_loss: 0.7349 - val_acc: 0.6624\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.4722 - acc: 0.7668 - val_loss: 0.6861 - val_acc: 0.6716\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4693 - acc: 0.7698 - val_loss: 0.7586 - val_acc: 0.6508\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4620 - acc: 0.7737 - val_loss: 0.8097 - val_acc: 0.6724\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4545 - acc: 0.7794 - val_loss: 0.7894 - val_acc: 0.6648\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4582 - acc: 0.7733 - val_loss: 0.7821 - val_acc: 0.6512\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4553 - acc: 0.7776 - val_loss: 0.8009 - val_acc: 0.6536\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4501 - acc: 0.7810 - val_loss: 0.7538 - val_acc: 0.6816\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4478 - acc: 0.7817 - val_loss: 0.8124 - val_acc: 0.6464\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4382 - acc: 0.7878 - val_loss: 0.7887 - val_acc: 0.6588\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4429 - acc: 0.7850 - val_loss: 0.8389 - val_acc: 0.6504\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 100,842\n",
      "Trainable params: 99,322\n",
      "Non-trainable params: 1,520\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 218us/sample - loss: 0.7636 - acc: 0.5858 - val_loss: 1.5850 - val_acc: 0.4888\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6442 - acc: 0.6363 - val_loss: 0.8976 - val_acc: 0.5008\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6195 - acc: 0.6508 - val_loss: 0.7146 - val_acc: 0.5636\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5985 - acc: 0.6725 - val_loss: 0.6331 - val_acc: 0.6652\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5908 - acc: 0.6759 - val_loss: 0.5989 - val_acc: 0.6888\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5743 - acc: 0.6919 - val_loss: 0.6140 - val_acc: 0.6964\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5610 - acc: 0.7036 - val_loss: 0.7393 - val_acc: 0.6100\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5512 - acc: 0.7101 - val_loss: 0.6259 - val_acc: 0.7168\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5388 - acc: 0.7187 - val_loss: 0.6958 - val_acc: 0.6876\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5340 - acc: 0.7199 - val_loss: 0.6776 - val_acc: 0.6652\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5221 - acc: 0.7319 - val_loss: 0.6446 - val_acc: 0.7320\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5156 - acc: 0.7385 - val_loss: 0.6440 - val_acc: 0.7472\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.5021 - acc: 0.7436 - val_loss: 0.7338 - val_acc: 0.6556\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4972 - acc: 0.7493 - val_loss: 0.6836 - val_acc: 0.6772\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.4911 - acc: 0.7534 - val_loss: 0.7209 - val_acc: 0.7316\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4836 - acc: 0.7622 - val_loss: 0.6719 - val_acc: 0.7084\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4786 - acc: 0.7622 - val_loss: 0.6970 - val_acc: 0.6912\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4679 - acc: 0.7733 - val_loss: 0.8104 - val_acc: 0.6180\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4669 - acc: 0.7740 - val_loss: 0.7734 - val_acc: 0.6576\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4642 - acc: 0.7763 - val_loss: 0.7588 - val_acc: 0.7048\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4481 - acc: 0.7818 - val_loss: 0.8188 - val_acc: 0.6808\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4526 - acc: 0.7804 - val_loss: 0.8825 - val_acc: 0.6504\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4483 - acc: 0.7811 - val_loss: 0.7909 - val_acc: 0.6556\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4424 - acc: 0.7878 - val_loss: 0.8830 - val_acc: 0.6248\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4365 - acc: 0.7928 - val_loss: 0.7684 - val_acc: 0.7356\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4363 - acc: 0.7894 - val_loss: 0.9552 - val_acc: 0.6256\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4302 - acc: 0.7938 - val_loss: 0.8515 - val_acc: 0.7272\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4279 - acc: 0.7972 - val_loss: 0.9232 - val_acc: 0.6684\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4243 - acc: 0.7978 - val_loss: 0.8963 - val_acc: 0.6636\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4173 - acc: 0.8014 - val_loss: 0.8646 - val_acc: 0.7056\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,312\n",
      "Trainable params: 1,972\n",
      "Non-trainable params: 340\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.7462 - acc: 0.5656 - val_loss: 0.6767 - val_acc: 0.5544\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6686 - acc: 0.5954 - val_loss: 0.6765 - val_acc: 0.5436\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6530 - acc: 0.6093 - val_loss: 0.6446 - val_acc: 0.6276\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6405 - acc: 0.6262 - val_loss: 0.6421 - val_acc: 0.6368\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6299 - acc: 0.6362 - val_loss: 0.6387 - val_acc: 0.6348\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6220 - acc: 0.6446 - val_loss: 0.6244 - val_acc: 0.6584\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6138 - acc: 0.6552 - val_loss: 0.6154 - val_acc: 0.6668\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6078 - acc: 0.6608 - val_loss: 0.6047 - val_acc: 0.6692\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5971 - acc: 0.6683 - val_loss: 0.6027 - val_acc: 0.6808\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5914 - acc: 0.6741 - val_loss: 0.5881 - val_acc: 0.6996\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5840 - acc: 0.6794 - val_loss: 0.5891 - val_acc: 0.6920\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5812 - acc: 0.6831 - val_loss: 0.5804 - val_acc: 0.7100\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5721 - acc: 0.6898 - val_loss: 0.5790 - val_acc: 0.7096\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5658 - acc: 0.6971 - val_loss: 0.5625 - val_acc: 0.7228\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5645 - acc: 0.6956 - val_loss: 0.5650 - val_acc: 0.7152\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5596 - acc: 0.6979 - val_loss: 0.5626 - val_acc: 0.7188\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5597 - acc: 0.7023 - val_loss: 0.5540 - val_acc: 0.7252\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5485 - acc: 0.7106 - val_loss: 0.5622 - val_acc: 0.7220\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5478 - acc: 0.7108 - val_loss: 0.5635 - val_acc: 0.7108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5460 - acc: 0.7110 - val_loss: 0.5445 - val_acc: 0.7248\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5466 - acc: 0.7118 - val_loss: 0.5407 - val_acc: 0.7308\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5411 - acc: 0.7164 - val_loss: 0.5615 - val_acc: 0.7092\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5412 - acc: 0.7132 - val_loss: 0.5720 - val_acc: 0.7080\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5427 - acc: 0.7141 - val_loss: 0.5524 - val_acc: 0.7292\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5347 - acc: 0.7228 - val_loss: 0.5771 - val_acc: 0.6672\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5379 - acc: 0.7219 - val_loss: 0.5405 - val_acc: 0.7408\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5345 - acc: 0.7200 - val_loss: 0.5489 - val_acc: 0.7364\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5325 - acc: 0.7240 - val_loss: 0.5459 - val_acc: 0.7244\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5329 - acc: 0.7196 - val_loss: 0.5508 - val_acc: 0.7116\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5282 - acc: 0.7254 - val_loss: 0.5401 - val_acc: 0.7360\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 8,992\n",
      "Trainable params: 8,572\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.7565 - acc: 0.5654 - val_loss: 0.8147 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6616 - acc: 0.6115 - val_loss: 0.8508 - val_acc: 0.4888\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6288 - acc: 0.6386 - val_loss: 0.7049 - val_acc: 0.5236\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6062 - acc: 0.6586 - val_loss: 0.6397 - val_acc: 0.6100\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5951 - acc: 0.6724 - val_loss: 0.6078 - val_acc: 0.6620\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5792 - acc: 0.6851 - val_loss: 0.6101 - val_acc: 0.6568\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5717 - acc: 0.6895 - val_loss: 0.6127 - val_acc: 0.6548\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5628 - acc: 0.6971 - val_loss: 0.5841 - val_acc: 0.7040\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5559 - acc: 0.7037 - val_loss: 0.5772 - val_acc: 0.7140\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5588 - acc: 0.7005 - val_loss: 0.5768 - val_acc: 0.7064\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5444 - acc: 0.7096 - val_loss: 0.5792 - val_acc: 0.6988\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5407 - acc: 0.7158 - val_loss: 0.5701 - val_acc: 0.7200\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.5357 - acc: 0.7130 - val_loss: 0.5720 - val_acc: 0.7056\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5290 - acc: 0.7261 - val_loss: 0.5521 - val_acc: 0.7340\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5226 - acc: 0.7285 - val_loss: 0.5811 - val_acc: 0.6908\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.5234 - acc: 0.7266 - val_loss: 0.5747 - val_acc: 0.7068\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5170 - acc: 0.7337 - val_loss: 0.5550 - val_acc: 0.7336\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5140 - acc: 0.7378 - val_loss: 0.5669 - val_acc: 0.7244\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5068 - acc: 0.7455 - val_loss: 0.5520 - val_acc: 0.7468\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5048 - acc: 0.7426 - val_loss: 0.5732 - val_acc: 0.7032\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5026 - acc: 0.7446 - val_loss: 0.5973 - val_acc: 0.6672\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5022 - acc: 0.7462 - val_loss: 0.5943 - val_acc: 0.6632\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4932 - acc: 0.7529 - val_loss: 0.5527 - val_acc: 0.7292\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4909 - acc: 0.7512 - val_loss: 0.5766 - val_acc: 0.6920\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4865 - acc: 0.7571 - val_loss: 0.5474 - val_acc: 0.7276\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4847 - acc: 0.7573 - val_loss: 0.5723 - val_acc: 0.6920\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4839 - acc: 0.7616 - val_loss: 0.5550 - val_acc: 0.7160\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4808 - acc: 0.7601 - val_loss: 0.5399 - val_acc: 0.7412\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4728 - acc: 0.7651 - val_loss: 0.6266 - val_acc: 0.6396\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4754 - acc: 0.7637 - val_loss: 0.5862 - val_acc: 0.6884\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 17,342\n",
      "Trainable params: 16,822\n",
      "Non-trainable params: 520\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.7614 - acc: 0.5708 - val_loss: 1.0370 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6604 - acc: 0.6185 - val_loss: 0.9416 - val_acc: 0.4896\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6248 - acc: 0.6485 - val_loss: 0.7156 - val_acc: 0.5248\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6067 - acc: 0.6566 - val_loss: 0.7076 - val_acc: 0.5312\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5932 - acc: 0.6706 - val_loss: 0.6358 - val_acc: 0.6052\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5776 - acc: 0.6822 - val_loss: 0.6109 - val_acc: 0.6528\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5651 - acc: 0.6949 - val_loss: 0.6468 - val_acc: 0.6032\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.5572 - acc: 0.6990 - val_loss: 0.6005 - val_acc: 0.6744\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5466 - acc: 0.7116 - val_loss: 0.5761 - val_acc: 0.7132\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.5411 - acc: 0.7130 - val_loss: 0.6072 - val_acc: 0.6460\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5347 - acc: 0.7218 - val_loss: 0.5649 - val_acc: 0.7148\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5277 - acc: 0.7251 - val_loss: 0.5696 - val_acc: 0.6980\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5222 - acc: 0.7294 - val_loss: 0.5853 - val_acc: 0.6664\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5208 - acc: 0.7281 - val_loss: 0.5609 - val_acc: 0.7140\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.5128 - acc: 0.7374 - val_loss: 0.5540 - val_acc: 0.7256\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5123 - acc: 0.7354 - val_loss: 0.5572 - val_acc: 0.7188\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5056 - acc: 0.7391 - val_loss: 0.5620 - val_acc: 0.7192\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5011 - acc: 0.7461 - val_loss: 0.5588 - val_acc: 0.7060\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4937 - acc: 0.7505 - val_loss: 0.6491 - val_acc: 0.6304\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4929 - acc: 0.7512 - val_loss: 0.5933 - val_acc: 0.6640\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4845 - acc: 0.7591 - val_loss: 0.5779 - val_acc: 0.6820\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4834 - acc: 0.7580 - val_loss: 0.5673 - val_acc: 0.6984\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4831 - acc: 0.7564 - val_loss: 0.5513 - val_acc: 0.7268\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4779 - acc: 0.7619 - val_loss: 0.5960 - val_acc: 0.6660\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4704 - acc: 0.7665 - val_loss: 0.5645 - val_acc: 0.7176\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4683 - acc: 0.7706 - val_loss: 0.5727 - val_acc: 0.6984\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4672 - acc: 0.7678 - val_loss: 0.5622 - val_acc: 0.7276\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4655 - acc: 0.7707 - val_loss: 0.5376 - val_acc: 0.7484\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4596 - acc: 0.7712 - val_loss: 0.6009 - val_acc: 0.6756\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4572 - acc: 0.7750 - val_loss: 0.5854 - val_acc: 0.7152\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 27,362\n",
      "Trainable params: 26,722\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.7346 - acc: 0.5834 - val_loss: 1.1494 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6485 - acc: 0.6235 - val_loss: 1.0876 - val_acc: 0.4888\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6181 - acc: 0.6525 - val_loss: 0.7399 - val_acc: 0.5212\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6022 - acc: 0.6638 - val_loss: 0.6909 - val_acc: 0.5508\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5868 - acc: 0.6734 - val_loss: 0.6441 - val_acc: 0.5872\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.5777 - acc: 0.6844 - val_loss: 0.6120 - val_acc: 0.6676\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5685 - acc: 0.6896 - val_loss: 0.6078 - val_acc: 0.6852\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5600 - acc: 0.6983 - val_loss: 0.5773 - val_acc: 0.7108\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5492 - acc: 0.7125 - val_loss: 0.5698 - val_acc: 0.7172\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5396 - acc: 0.7162 - val_loss: 0.5822 - val_acc: 0.6944\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5322 - acc: 0.7245 - val_loss: 0.5633 - val_acc: 0.7340\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5268 - acc: 0.7254 - val_loss: 0.5782 - val_acc: 0.6868\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5202 - acc: 0.7302 - val_loss: 0.5654 - val_acc: 0.7268\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5133 - acc: 0.7363 - val_loss: 0.5541 - val_acc: 0.7404\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.5123 - acc: 0.7374 - val_loss: 0.5563 - val_acc: 0.7252\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.5031 - acc: 0.7444 - val_loss: 0.5563 - val_acc: 0.7276\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4969 - acc: 0.7477 - val_loss: 0.6026 - val_acc: 0.6608\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4914 - acc: 0.7507 - val_loss: 0.6181 - val_acc: 0.6680\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4907 - acc: 0.7539 - val_loss: 0.5681 - val_acc: 0.7100\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4836 - acc: 0.7568 - val_loss: 0.5415 - val_acc: 0.7328\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4819 - acc: 0.7589 - val_loss: 0.5402 - val_acc: 0.7472\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4764 - acc: 0.7622 - val_loss: 0.5843 - val_acc: 0.7036\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4695 - acc: 0.7667 - val_loss: 0.5869 - val_acc: 0.6908\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4650 - acc: 0.7691 - val_loss: 0.5302 - val_acc: 0.7748\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4682 - acc: 0.7652 - val_loss: 0.5636 - val_acc: 0.7332\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.4579 - acc: 0.7768 - val_loss: 0.5661 - val_acc: 0.7220\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4586 - acc: 0.7729 - val_loss: 0.5783 - val_acc: 0.7180\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4496 - acc: 0.7788 - val_loss: 0.5509 - val_acc: 0.7528\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4473 - acc: 0.7797 - val_loss: 0.6123 - val_acc: 0.6876\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4461 - acc: 0.7825 - val_loss: 0.5673 - val_acc: 0.7284\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 100,842\n",
      "Trainable params: 99,322\n",
      "Non-trainable params: 1,520\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.7720 - acc: 0.5769 - val_loss: 1.5025 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6634 - acc: 0.6175 - val_loss: 1.5727 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6346 - acc: 0.6379 - val_loss: 1.0560 - val_acc: 0.4944\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6114 - acc: 0.6545 - val_loss: 0.7203 - val_acc: 0.5512\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6018 - acc: 0.6672 - val_loss: 0.6633 - val_acc: 0.5764\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.5827 - acc: 0.6841 - val_loss: 0.6450 - val_acc: 0.6172\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.5775 - acc: 0.6881 - val_loss: 0.6107 - val_acc: 0.6684\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5650 - acc: 0.6991 - val_loss: 0.5941 - val_acc: 0.7132\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5534 - acc: 0.7067 - val_loss: 0.5924 - val_acc: 0.7088\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.5460 - acc: 0.7115 - val_loss: 0.6106 - val_acc: 0.6812\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.5349 - acc: 0.7221 - val_loss: 0.6210 - val_acc: 0.6588\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.5297 - acc: 0.7252 - val_loss: 0.6773 - val_acc: 0.6108\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.5228 - acc: 0.7298 - val_loss: 0.6087 - val_acc: 0.6968\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.5178 - acc: 0.7329 - val_loss: 0.6337 - val_acc: 0.6700\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.5068 - acc: 0.7436 - val_loss: 0.5544 - val_acc: 0.7516\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4994 - acc: 0.7505 - val_loss: 0.5554 - val_acc: 0.7572\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4875 - acc: 0.7575 - val_loss: 0.5722 - val_acc: 0.7448\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4866 - acc: 0.7559 - val_loss: 0.6087 - val_acc: 0.7268\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4811 - acc: 0.7591 - val_loss: 0.6034 - val_acc: 0.6968\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4784 - acc: 0.7634 - val_loss: 0.5477 - val_acc: 0.7492\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4624 - acc: 0.7705 - val_loss: 0.5682 - val_acc: 0.7640\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.4637 - acc: 0.7697 - val_loss: 0.6198 - val_acc: 0.6680\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.4563 - acc: 0.7758 - val_loss: 0.5874 - val_acc: 0.7452\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.4462 - acc: 0.7808 - val_loss: 0.5957 - val_acc: 0.7012\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4390 - acc: 0.7861 - val_loss: 0.5521 - val_acc: 0.7376\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4393 - acc: 0.7832 - val_loss: 0.5589 - val_acc: 0.7708\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4284 - acc: 0.7968 - val_loss: 0.5646 - val_acc: 0.7532\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.4334 - acc: 0.7902 - val_loss: 0.6364 - val_acc: 0.7032\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.4284 - acc: 0.7920 - val_loss: 0.6016 - val_acc: 0.7152\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.4151 - acc: 0.7994 - val_loss: 0.6014 - val_acc: 0.7188\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,312\n",
      "Trainable params: 1,972\n",
      "Non-trainable params: 340\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.8851 - acc: 0.5478 - val_loss: 0.6946 - val_acc: 0.4896\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.7304 - acc: 0.5781 - val_loss: 0.7056 - val_acc: 0.4880\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.6906 - acc: 0.5920 - val_loss: 0.6901 - val_acc: 0.5156\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6657 - acc: 0.6059 - val_loss: 0.6623 - val_acc: 0.5876\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6543 - acc: 0.6099 - val_loss: 0.6493 - val_acc: 0.6140\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6463 - acc: 0.6200 - val_loss: 0.6429 - val_acc: 0.6216\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6429 - acc: 0.6218 - val_loss: 0.6327 - val_acc: 0.6424\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6346 - acc: 0.6311 - val_loss: 0.6321 - val_acc: 0.6304\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6290 - acc: 0.6358 - val_loss: 0.6203 - val_acc: 0.6548\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6221 - acc: 0.6453 - val_loss: 0.6163 - val_acc: 0.6620\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6153 - acc: 0.6451 - val_loss: 0.6113 - val_acc: 0.6736\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6078 - acc: 0.6542 - val_loss: 0.6037 - val_acc: 0.6692\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6066 - acc: 0.6590 - val_loss: 0.6034 - val_acc: 0.6776\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6028 - acc: 0.6588 - val_loss: 0.6136 - val_acc: 0.6512\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5962 - acc: 0.6643 - val_loss: 0.6019 - val_acc: 0.6728\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5948 - acc: 0.6693 - val_loss: 0.5940 - val_acc: 0.6936\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5935 - acc: 0.6713 - val_loss: 0.5862 - val_acc: 0.6920\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5885 - acc: 0.6692 - val_loss: 0.5794 - val_acc: 0.6944\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5862 - acc: 0.6768 - val_loss: 0.5822 - val_acc: 0.7032\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5791 - acc: 0.6794 - val_loss: 0.5846 - val_acc: 0.6976\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5794 - acc: 0.6856 - val_loss: 0.5788 - val_acc: 0.7036\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5753 - acc: 0.6898 - val_loss: 0.5790 - val_acc: 0.6996\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5645 - acc: 0.6948 - val_loss: 0.5783 - val_acc: 0.6964\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5689 - acc: 0.6933 - val_loss: 0.5649 - val_acc: 0.7108\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5602 - acc: 0.6961 - val_loss: 0.5552 - val_acc: 0.7140\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5627 - acc: 0.6979 - val_loss: 0.5672 - val_acc: 0.7116\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5581 - acc: 0.7019 - val_loss: 0.5518 - val_acc: 0.7184\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5542 - acc: 0.7033 - val_loss: 0.5650 - val_acc: 0.7092\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5544 - acc: 0.7007 - val_loss: 0.5599 - val_acc: 0.7192\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5497 - acc: 0.7065 - val_loss: 0.5602 - val_acc: 0.7176\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 8,992\n",
      "Trainable params: 8,572\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.7782 - acc: 0.5567 - val_loss: 0.7979 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.6939 - acc: 0.5861 - val_loss: 0.8974 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.6604 - acc: 0.6104 - val_loss: 0.9760 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6399 - acc: 0.6262 - val_loss: 0.8349 - val_acc: 0.4896\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6248 - acc: 0.6453 - val_loss: 0.7463 - val_acc: 0.5048\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.6162 - acc: 0.6516 - val_loss: 0.6690 - val_acc: 0.5536\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6047 - acc: 0.6625 - val_loss: 0.6414 - val_acc: 0.5724\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5957 - acc: 0.6688 - val_loss: 0.6260 - val_acc: 0.6188\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5855 - acc: 0.6764 - val_loss: 0.6012 - val_acc: 0.6648\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5785 - acc: 0.6834 - val_loss: 0.5950 - val_acc: 0.6784\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5675 - acc: 0.6946 - val_loss: 0.6071 - val_acc: 0.6456\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5602 - acc: 0.6969 - val_loss: 0.5868 - val_acc: 0.6916\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5560 - acc: 0.7014 - val_loss: 0.5596 - val_acc: 0.7160\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5464 - acc: 0.7092 - val_loss: 0.5557 - val_acc: 0.7104\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5439 - acc: 0.7111 - val_loss: 0.5505 - val_acc: 0.7196\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5339 - acc: 0.7193 - val_loss: 0.5476 - val_acc: 0.7268\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5315 - acc: 0.7221 - val_loss: 0.5429 - val_acc: 0.7228\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5275 - acc: 0.7239 - val_loss: 0.5432 - val_acc: 0.7292\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5250 - acc: 0.7265 - val_loss: 0.5515 - val_acc: 0.7104\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5168 - acc: 0.7339 - val_loss: 0.5305 - val_acc: 0.7324\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5170 - acc: 0.7309 - val_loss: 0.5386 - val_acc: 0.7332\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5155 - acc: 0.7361 - val_loss: 0.5340 - val_acc: 0.7380\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5096 - acc: 0.7372 - val_loss: 0.5351 - val_acc: 0.7356\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5049 - acc: 0.7416 - val_loss: 0.5289 - val_acc: 0.7532\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5033 - acc: 0.7417 - val_loss: 0.5219 - val_acc: 0.7568\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5021 - acc: 0.7425 - val_loss: 0.5130 - val_acc: 0.7524\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4964 - acc: 0.7458 - val_loss: 0.5144 - val_acc: 0.7560\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4962 - acc: 0.7468 - val_loss: 0.5435 - val_acc: 0.7192\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4961 - acc: 0.7480 - val_loss: 0.5031 - val_acc: 0.7616\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4912 - acc: 0.7517 - val_loss: 0.5150 - val_acc: 0.7608\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 17,342\n",
      "Trainable params: 16,822\n",
      "Non-trainable params: 520\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.7661 - acc: 0.5699 - val_loss: 0.7228 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6758 - acc: 0.6019 - val_loss: 1.0803 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6467 - acc: 0.6285 - val_loss: 1.1763 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6240 - acc: 0.6442 - val_loss: 0.9073 - val_acc: 0.4880\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6119 - acc: 0.6535 - val_loss: 0.7723 - val_acc: 0.5088\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5995 - acc: 0.6641 - val_loss: 0.6998 - val_acc: 0.5428\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5889 - acc: 0.6746 - val_loss: 0.6304 - val_acc: 0.6336\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5809 - acc: 0.6825 - val_loss: 0.6212 - val_acc: 0.6332\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5715 - acc: 0.6874 - val_loss: 0.6185 - val_acc: 0.6256\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5595 - acc: 0.6986 - val_loss: 0.5873 - val_acc: 0.6868\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5561 - acc: 0.6987 - val_loss: 0.5874 - val_acc: 0.6980\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5499 - acc: 0.7080 - val_loss: 0.5586 - val_acc: 0.7112\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5395 - acc: 0.7097 - val_loss: 0.5606 - val_acc: 0.7176\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5327 - acc: 0.7206 - val_loss: 0.5693 - val_acc: 0.7024\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5293 - acc: 0.7230 - val_loss: 0.5669 - val_acc: 0.6936\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5202 - acc: 0.7299 - val_loss: 0.5718 - val_acc: 0.6876\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5182 - acc: 0.7315 - val_loss: 0.5464 - val_acc: 0.7324\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5113 - acc: 0.7366 - val_loss: 0.5534 - val_acc: 0.7184\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5080 - acc: 0.7406 - val_loss: 0.5371 - val_acc: 0.7392\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.5049 - acc: 0.7366 - val_loss: 0.5311 - val_acc: 0.7520\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4988 - acc: 0.7494 - val_loss: 0.5569 - val_acc: 0.7168\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4948 - acc: 0.7495 - val_loss: 0.5702 - val_acc: 0.6908\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4975 - acc: 0.7474 - val_loss: 0.5462 - val_acc: 0.7248\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4860 - acc: 0.7541 - val_loss: 0.5646 - val_acc: 0.6912\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4817 - acc: 0.7588 - val_loss: 0.5183 - val_acc: 0.7500\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4807 - acc: 0.7582 - val_loss: 0.5351 - val_acc: 0.7372\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.4753 - acc: 0.7649 - val_loss: 0.5344 - val_acc: 0.7484\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.4732 - acc: 0.7633 - val_loss: 0.5288 - val_acc: 0.7480\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4701 - acc: 0.7671 - val_loss: 0.5701 - val_acc: 0.6996\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4669 - acc: 0.7726 - val_loss: 0.5409 - val_acc: 0.7236\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 27,362\n",
      "Trainable params: 26,722\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.7586 - acc: 0.5634 - val_loss: 0.7109 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.6781 - acc: 0.6045 - val_loss: 1.1468 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6413 - acc: 0.6308 - val_loss: 1.3640 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.6213 - acc: 0.6432 - val_loss: 1.0272 - val_acc: 0.4880\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.6065 - acc: 0.6592 - val_loss: 0.8238 - val_acc: 0.4988\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5975 - acc: 0.6662 - val_loss: 0.7400 - val_acc: 0.5200\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5907 - acc: 0.6718 - val_loss: 0.6431 - val_acc: 0.6128\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5758 - acc: 0.6854 - val_loss: 0.6017 - val_acc: 0.6816\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5623 - acc: 0.6977 - val_loss: 0.6088 - val_acc: 0.6736\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5556 - acc: 0.7020 - val_loss: 0.6022 - val_acc: 0.6796\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5468 - acc: 0.7106 - val_loss: 0.5842 - val_acc: 0.6860\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5404 - acc: 0.7156 - val_loss: 0.5816 - val_acc: 0.7088\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5331 - acc: 0.7209 - val_loss: 0.5688 - val_acc: 0.7168\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.5298 - acc: 0.723 - 1s 42us/sample - loss: 0.5298 - acc: 0.7229 - val_loss: 0.5730 - val_acc: 0.7128\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5257 - acc: 0.7251 - val_loss: 0.5822 - val_acc: 0.7068\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5244 - acc: 0.7289 - val_loss: 0.5489 - val_acc: 0.7260\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5159 - acc: 0.7307 - val_loss: 0.5520 - val_acc: 0.7292\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5078 - acc: 0.7421 - val_loss: 0.5429 - val_acc: 0.7416\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5035 - acc: 0.7418 - val_loss: 0.5426 - val_acc: 0.7428\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4977 - acc: 0.7469 - val_loss: 0.5553 - val_acc: 0.7224\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4965 - acc: 0.7434 - val_loss: 0.5540 - val_acc: 0.7204\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4830 - acc: 0.7560 - val_loss: 0.5337 - val_acc: 0.7528\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4816 - acc: 0.7559 - val_loss: 0.5660 - val_acc: 0.6896\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.4844 - acc: 0.7568 - val_loss: 0.5743 - val_acc: 0.6864\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.4755 - acc: 0.7611 - val_loss: 0.5344 - val_acc: 0.7600\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4729 - acc: 0.7652 - val_loss: 0.5410 - val_acc: 0.7476\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4622 - acc: 0.7688 - val_loss: 0.5340 - val_acc: 0.7596\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4648 - acc: 0.7749 - val_loss: 0.5349 - val_acc: 0.7600\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4637 - acc: 0.7655 - val_loss: 0.5486 - val_acc: 0.7196\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.4567 - acc: 0.7782 - val_loss: 0.5174 - val_acc: 0.7676\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 100,842\n",
      "Trainable params: 99,322\n",
      "Non-trainable params: 1,520\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.7895 - acc: 0.5692 - val_loss: 0.9772 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6910 - acc: 0.6071 - val_loss: 1.8037 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6512 - acc: 0.6278 - val_loss: 1.8590 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6382 - acc: 0.6371 - val_loss: 1.5969 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6131 - acc: 0.6553 - val_loss: 1.2778 - val_acc: 0.4892\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6077 - acc: 0.6588 - val_loss: 0.9666 - val_acc: 0.5008\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5942 - acc: 0.6713 - val_loss: 0.7324 - val_acc: 0.5384\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5863 - acc: 0.6791 - val_loss: 0.7080 - val_acc: 0.5768\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5754 - acc: 0.6818 - val_loss: 0.6436 - val_acc: 0.6016\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5653 - acc: 0.6942 - val_loss: 0.6059 - val_acc: 0.6624\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5567 - acc: 0.7032 - val_loss: 0.5858 - val_acc: 0.6944\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5461 - acc: 0.7112 - val_loss: 0.5771 - val_acc: 0.7052\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5372 - acc: 0.7169 - val_loss: 0.5651 - val_acc: 0.7356\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5322 - acc: 0.7228 - val_loss: 0.5823 - val_acc: 0.7012\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5244 - acc: 0.7272 - val_loss: 0.5552 - val_acc: 0.7288\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5142 - acc: 0.7353 - val_loss: 0.5642 - val_acc: 0.7172\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5136 - acc: 0.7349 - val_loss: 0.5446 - val_acc: 0.7372\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5051 - acc: 0.7403 - val_loss: 0.5415 - val_acc: 0.7412\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5032 - acc: 0.7446 - val_loss: 0.5415 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4960 - acc: 0.7494 - val_loss: 0.5677 - val_acc: 0.7180\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4845 - acc: 0.7572 - val_loss: 0.5361 - val_acc: 0.7372\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4833 - acc: 0.7605 - val_loss: 0.5404 - val_acc: 0.7624\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4765 - acc: 0.7632 - val_loss: 0.5258 - val_acc: 0.7560\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4708 - acc: 0.7696 - val_loss: 0.5142 - val_acc: 0.7620\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4643 - acc: 0.7696 - val_loss: 0.5328 - val_acc: 0.7532\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4568 - acc: 0.7732 - val_loss: 0.5250 - val_acc: 0.7600\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4574 - acc: 0.7762 - val_loss: 0.5293 - val_acc: 0.7676\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4525 - acc: 0.7796 - val_loss: 0.5288 - val_acc: 0.7472\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.4401 - acc: 0.786 - 1s 43us/sample - loss: 0.4413 - acc: 0.7856 - val_loss: 0.5336 - val_acc: 0.7472\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4414 - acc: 0.7852 - val_loss: 0.5016 - val_acc: 0.7884\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,312\n",
      "Trainable params: 1,972\n",
      "Non-trainable params: 340\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.7776 - acc: 0.5522 - val_loss: 0.7033 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.7086 - acc: 0.5769 - val_loss: 0.6822 - val_acc: 0.5616\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6901 - acc: 0.5831 - val_loss: 0.6746 - val_acc: 0.5940\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6753 - acc: 0.5874 - val_loss: 0.6790 - val_acc: 0.5356\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6644 - acc: 0.5975 - val_loss: 0.6978 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6570 - acc: 0.6052 - val_loss: 0.7154 - val_acc: 0.4912\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6486 - acc: 0.6147 - val_loss: 0.6975 - val_acc: 0.5068\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6426 - acc: 0.6226 - val_loss: 0.6959 - val_acc: 0.5108\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6345 - acc: 0.6317 - val_loss: 0.6863 - val_acc: 0.5236\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6247 - acc: 0.6398 - val_loss: 0.6494 - val_acc: 0.5940\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6192 - acc: 0.6432 - val_loss: 0.6344 - val_acc: 0.6388\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6170 - acc: 0.6517 - val_loss: 0.6206 - val_acc: 0.6528\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6097 - acc: 0.6568 - val_loss: 0.6202 - val_acc: 0.6604\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6037 - acc: 0.6588 - val_loss: 0.6061 - val_acc: 0.6676\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5972 - acc: 0.6674 - val_loss: 0.6022 - val_acc: 0.6708\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5930 - acc: 0.6715 - val_loss: 0.6171 - val_acc: 0.6452\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5866 - acc: 0.6786 - val_loss: 0.5876 - val_acc: 0.6936\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5850 - acc: 0.6742 - val_loss: 0.5966 - val_acc: 0.6796\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5823 - acc: 0.6829 - val_loss: 0.5803 - val_acc: 0.6952\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5774 - acc: 0.6871 - val_loss: 0.5776 - val_acc: 0.6980\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5707 - acc: 0.6935 - val_loss: 0.5727 - val_acc: 0.7060\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5650 - acc: 0.6975 - val_loss: 0.5735 - val_acc: 0.7048\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5641 - acc: 0.6987 - val_loss: 0.5707 - val_acc: 0.7092\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5626 - acc: 0.6986 - val_loss: 0.5743 - val_acc: 0.7072\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5589 - acc: 0.7019 - val_loss: 0.5590 - val_acc: 0.7132\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5584 - acc: 0.7029 - val_loss: 0.5572 - val_acc: 0.7124\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5531 - acc: 0.7109 - val_loss: 0.5541 - val_acc: 0.7112\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5507 - acc: 0.7087 - val_loss: 0.5505 - val_acc: 0.7184\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5489 - acc: 0.7109 - val_loss: 0.5602 - val_acc: 0.7168\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5486 - acc: 0.7129 - val_loss: 0.5536 - val_acc: 0.7168\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 8,992\n",
      "Trainable params: 8,572\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.7695 - acc: 0.5593 - val_loss: 0.7315 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6908 - acc: 0.5926 - val_loss: 0.7614 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6680 - acc: 0.6038 - val_loss: 0.8481 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6553 - acc: 0.6142 - val_loss: 0.9527 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6395 - acc: 0.6300 - val_loss: 0.9949 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6287 - acc: 0.6335 - val_loss: 1.0064 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6187 - acc: 0.6479 - val_loss: 0.9445 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6110 - acc: 0.6564 - val_loss: 0.8281 - val_acc: 0.4916\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.6021 - acc: 0.6670 - val_loss: 0.8122 - val_acc: 0.4940\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5951 - acc: 0.6692 - val_loss: 0.7177 - val_acc: 0.5172\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5836 - acc: 0.6779 - val_loss: 0.7301 - val_acc: 0.5188\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5786 - acc: 0.6829 - val_loss: 0.6285 - val_acc: 0.6132\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5729 - acc: 0.6889 - val_loss: 0.6302 - val_acc: 0.5928\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5686 - acc: 0.6914 - val_loss: 0.5950 - val_acc: 0.6748\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5598 - acc: 0.7015 - val_loss: 0.5832 - val_acc: 0.6880\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5562 - acc: 0.6989 - val_loss: 0.5830 - val_acc: 0.6844\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5479 - acc: 0.7075 - val_loss: 0.5707 - val_acc: 0.6952\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5451 - acc: 0.7110 - val_loss: 0.5617 - val_acc: 0.7096\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5405 - acc: 0.7103 - val_loss: 0.5519 - val_acc: 0.7240\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5347 - acc: 0.7186 - val_loss: 0.5466 - val_acc: 0.7152\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5323 - acc: 0.7212 - val_loss: 0.5574 - val_acc: 0.7188\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5315 - acc: 0.7205 - val_loss: 0.5413 - val_acc: 0.7268\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5262 - acc: 0.7234 - val_loss: 0.5481 - val_acc: 0.7244\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5212 - acc: 0.7264 - val_loss: 0.5372 - val_acc: 0.7300\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5172 - acc: 0.7288 - val_loss: 0.5335 - val_acc: 0.7384\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5111 - acc: 0.7379 - val_loss: 0.5439 - val_acc: 0.7220\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5135 - acc: 0.7329 - val_loss: 0.5284 - val_acc: 0.7344\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5071 - acc: 0.7385 - val_loss: 0.5406 - val_acc: 0.7264\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5069 - acc: 0.7414 - val_loss: 0.5393 - val_acc: 0.7300\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5017 - acc: 0.7436 - val_loss: 0.5262 - val_acc: 0.7392\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 17,342\n",
      "Trainable params: 16,822\n",
      "Non-trainable params: 520\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.7653 - acc: 0.5675 - val_loss: 0.6870 - val_acc: 0.5892\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6940 - acc: 0.5914 - val_loss: 0.7535 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6640 - acc: 0.6080 - val_loss: 0.9948 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6437 - acc: 0.6270 - val_loss: 1.1622 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6323 - acc: 0.6358 - val_loss: 1.2223 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6194 - acc: 0.6510 - val_loss: 1.1767 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6080 - acc: 0.6610 - val_loss: 1.0945 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6040 - acc: 0.6626 - val_loss: 0.9121 - val_acc: 0.4900\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5932 - acc: 0.6729 - val_loss: 0.8205 - val_acc: 0.4976\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5858 - acc: 0.6761 - val_loss: 0.6958 - val_acc: 0.5424\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5806 - acc: 0.6817 - val_loss: 0.6610 - val_acc: 0.5660\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5732 - acc: 0.6868 - val_loss: 0.6682 - val_acc: 0.5680\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5691 - acc: 0.6919 - val_loss: 0.6090 - val_acc: 0.6456\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5637 - acc: 0.6973 - val_loss: 0.6251 - val_acc: 0.6128\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5563 - acc: 0.7022 - val_loss: 0.6038 - val_acc: 0.6564\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5526 - acc: 0.7048 - val_loss: 0.5920 - val_acc: 0.6504\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5468 - acc: 0.7111 - val_loss: 0.5666 - val_acc: 0.6992\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5403 - acc: 0.7132 - val_loss: 0.5701 - val_acc: 0.7024\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 23us/sample - loss: 0.5352 - acc: 0.7164 - val_loss: 0.5694 - val_acc: 0.7008\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5323 - acc: 0.7191 - val_loss: 0.5540 - val_acc: 0.7248\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5268 - acc: 0.7235 - val_loss: 0.5495 - val_acc: 0.7228\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5212 - acc: 0.7258 - val_loss: 0.5496 - val_acc: 0.7228\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5211 - acc: 0.7304 - val_loss: 0.5554 - val_acc: 0.7200\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5133 - acc: 0.7342 - val_loss: 0.5368 - val_acc: 0.7368\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5121 - acc: 0.7329 - val_loss: 0.5381 - val_acc: 0.7336\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5089 - acc: 0.7369 - val_loss: 0.5352 - val_acc: 0.7448\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5052 - acc: 0.7379 - val_loss: 0.5244 - val_acc: 0.7452\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.4983 - acc: 0.7411 - val_loss: 0.5254 - val_acc: 0.7432\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.4978 - acc: 0.7444 - val_loss: 0.5279 - val_acc: 0.7404\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.4935 - acc: 0.7492 - val_loss: 0.5168 - val_acc: 0.7440\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 27,362\n",
      "Trainable params: 26,722\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.7900 - acc: 0.5665 - val_loss: 0.6901 - val_acc: 0.5020\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.7062 - acc: 0.5899 - val_loss: 0.8310 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6772 - acc: 0.6022 - val_loss: 1.1800 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.6521 - acc: 0.6230 - val_loss: 1.3957 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.6384 - acc: 0.6331 - val_loss: 1.4532 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6261 - acc: 0.6435 - val_loss: 1.4023 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.6156 - acc: 0.6495 - val_loss: 1.2750 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6083 - acc: 0.6598 - val_loss: 1.2062 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6009 - acc: 0.6636 - val_loss: 1.0114 - val_acc: 0.4904\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5895 - acc: 0.6738 - val_loss: 0.8558 - val_acc: 0.4976\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.5821 - acc: 0.6786 - val_loss: 0.7818 - val_acc: 0.5196\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.5742 - acc: 0.6856 - val_loss: 0.7377 - val_acc: 0.5384\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5700 - acc: 0.6864 - val_loss: 0.6507 - val_acc: 0.5772\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5624 - acc: 0.6948 - val_loss: 0.6764 - val_acc: 0.5740\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5554 - acc: 0.6989 - val_loss: 0.6462 - val_acc: 0.6064\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.5517 - acc: 0.7028 - val_loss: 0.6059 - val_acc: 0.6708\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5467 - acc: 0.7075 - val_loss: 0.5918 - val_acc: 0.6836\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5380 - acc: 0.7138 - val_loss: 0.5780 - val_acc: 0.6952\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5341 - acc: 0.7175 - val_loss: 0.5669 - val_acc: 0.7096\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5292 - acc: 0.7229 - val_loss: 0.5632 - val_acc: 0.7108\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5257 - acc: 0.7230 - val_loss: 0.5531 - val_acc: 0.7112\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5207 - acc: 0.7304 - val_loss: 0.5649 - val_acc: 0.7176\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5146 - acc: 0.7334 - val_loss: 0.5468 - val_acc: 0.7280\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5087 - acc: 0.7336 - val_loss: 0.5521 - val_acc: 0.7328\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5058 - acc: 0.7372 - val_loss: 0.5664 - val_acc: 0.7020\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.4992 - acc: 0.7468 - val_loss: 0.5353 - val_acc: 0.7464\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.4994 - acc: 0.7442 - val_loss: 0.5320 - val_acc: 0.7500\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 28us/sample - loss: 0.4919 - acc: 0.7476 - val_loss: 0.5311 - val_acc: 0.7444\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.4911 - acc: 0.7550 - val_loss: 0.5354 - val_acc: 0.7392\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.4815 - acc: 0.7552 - val_loss: 0.5416 - val_acc: 0.7356\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 100,842\n",
      "Trainable params: 99,322\n",
      "Non-trainable params: 1,520\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.7988 - acc: 0.5605 - val_loss: 0.6937 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.7110 - acc: 0.5888 - val_loss: 1.0608 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6816 - acc: 0.6074 - val_loss: 1.4864 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.6581 - acc: 0.6186 - val_loss: 1.8092 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6394 - acc: 0.6282 - val_loss: 1.9496 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6282 - acc: 0.6431 - val_loss: 1.8837 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.6145 - acc: 0.6552 - val_loss: 1.7769 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.6068 - acc: 0.6576 - val_loss: 1.5814 - val_acc: 0.4888\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.6001 - acc: 0.6648 - val_loss: 1.3386 - val_acc: 0.4896\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5919 - acc: 0.6714 - val_loss: 1.1905 - val_acc: 0.4916\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5846 - acc: 0.6772 - val_loss: 0.9618 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5776 - acc: 0.6873 - val_loss: 0.8482 - val_acc: 0.5160\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5754 - acc: 0.6851 - val_loss: 0.7371 - val_acc: 0.5412\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.5631 - acc: 0.6979 - val_loss: 0.7215 - val_acc: 0.5648\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5565 - acc: 0.7033 - val_loss: 0.6490 - val_acc: 0.5984\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.5514 - acc: 0.7028 - val_loss: 0.6449 - val_acc: 0.6040\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.5446 - acc: 0.7125 - val_loss: 0.5814 - val_acc: 0.6916\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5392 - acc: 0.7127 - val_loss: 0.5798 - val_acc: 0.6892\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5349 - acc: 0.7203 - val_loss: 0.5746 - val_acc: 0.6956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5320 - acc: 0.7214 - val_loss: 0.5936 - val_acc: 0.6912\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5269 - acc: 0.7250 - val_loss: 0.5534 - val_acc: 0.7216\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 0s 24us/sample - loss: 0.5162 - acc: 0.7299 - val_loss: 0.5433 - val_acc: 0.7208\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.5178 - acc: 0.7331 - val_loss: 0.5409 - val_acc: 0.7392\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.5055 - acc: 0.7362 - val_loss: 0.5504 - val_acc: 0.7144\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.5035 - acc: 0.7439 - val_loss: 0.5205 - val_acc: 0.7436\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.4931 - acc: 0.7475 - val_loss: 0.5458 - val_acc: 0.7284\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.4940 - acc: 0.7496 - val_loss: 0.5259 - val_acc: 0.7452\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 0s 25us/sample - loss: 0.4926 - acc: 0.7532 - val_loss: 0.5225 - val_acc: 0.7556\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 0s 26us/sample - loss: 0.4812 - acc: 0.7583 - val_loss: 0.5171 - val_acc: 0.7492\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 0s 27us/sample - loss: 0.4712 - acc: 0.7666 - val_loss: 0.5127 - val_acc: 0.7664\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,502\n",
      "Trainable params: 2,122\n",
      "Non-trainable params: 380\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 382us/sample - loss: 0.7898 - acc: 0.5458 - val_loss: 0.6649 - val_acc: 0.6004\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.6839 - acc: 0.5779 - val_loss: 0.6581 - val_acc: 0.6120\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 4s 240us/sample - loss: 0.6631 - acc: 0.5979 - val_loss: 0.6531 - val_acc: 0.6160\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.6479 - acc: 0.6148 - val_loss: 0.6436 - val_acc: 0.6252\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.6328 - acc: 0.6344 - val_loss: 0.6345 - val_acc: 0.6408\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 4s 239us/sample - loss: 0.6195 - acc: 0.6518 - val_loss: 0.6273 - val_acc: 0.6528\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 4s 239us/sample - loss: 0.6055 - acc: 0.6665 - val_loss: 0.6088 - val_acc: 0.6688\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5941 - acc: 0.6749 - val_loss: 0.6166 - val_acc: 0.6344\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5874 - acc: 0.6740 - val_loss: 0.5812 - val_acc: 0.7044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 4s 243us/sample - loss: 0.5804 - acc: 0.6838 - val_loss: 0.5853 - val_acc: 0.6960\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 4s 244us/sample - loss: 0.5759 - acc: 0.6900 - val_loss: 0.5862 - val_acc: 0.7048\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.5654 - acc: 0.7002 - val_loss: 0.6270 - val_acc: 0.5948\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.5665 - acc: 0.7002 - val_loss: 0.5676 - val_acc: 0.7280\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5632 - acc: 0.7020 - val_loss: 0.5847 - val_acc: 0.6716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5606 - acc: 0.7059 - val_loss: 0.6008 - val_acc: 0.6260\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5571 - acc: 0.7072 - val_loss: 0.5781 - val_acc: 0.6740\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 4s 240us/sample - loss: 0.5578 - acc: 0.7053 - val_loss: 0.5964 - val_acc: 0.6396\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5500 - acc: 0.7119 - val_loss: 0.5951 - val_acc: 0.6436\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 4s 239us/sample - loss: 0.5457 - acc: 0.7196 - val_loss: 0.5645 - val_acc: 0.7072\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5449 - acc: 0.7190 - val_loss: 0.5762 - val_acc: 0.6668\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5458 - acc: 0.7155 - val_loss: 0.6006 - val_acc: 0.6188\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 4s 240us/sample - loss: 0.5446 - acc: 0.7176 - val_loss: 0.5807 - val_acc: 0.6628\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 4s 238us/sample - loss: 0.5442 - acc: 0.7187 - val_loss: 0.5643 - val_acc: 0.7148\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 4s 239us/sample - loss: 0.5424 - acc: 0.7208 - val_loss: 0.5864 - val_acc: 0.6488\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 4s 240us/sample - loss: 0.5380 - acc: 0.7239 - val_loss: 0.5809 - val_acc: 0.6612\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5369 - acc: 0.7253 - val_loss: 0.6158 - val_acc: 0.6072\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 4s 240us/sample - loss: 0.5438 - acc: 0.7214 - val_loss: 0.5955 - val_acc: 0.6340\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5352 - acc: 0.7241 - val_loss: 0.6316 - val_acc: 0.5952\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 4s 240us/sample - loss: 0.5417 - acc: 0.7168 - val_loss: 0.6271 - val_acc: 0.5968\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 4s 240us/sample - loss: 0.5374 - acc: 0.7171 - val_loss: 0.5867 - val_acc: 0.6460\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 11,942\n",
      "Trainable params: 11,322\n",
      "Non-trainable params: 620\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 373us/sample - loss: 0.7254 - acc: 0.5713 - val_loss: 0.6649 - val_acc: 0.5728\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 4s 243us/sample - loss: 0.6582 - acc: 0.6076 - val_loss: 0.7228 - val_acc: 0.5108\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.6217 - acc: 0.6441 - val_loss: 0.6953 - val_acc: 0.5428\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5984 - acc: 0.6665 - val_loss: 0.6064 - val_acc: 0.6548\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5799 - acc: 0.6839 - val_loss: 0.6442 - val_acc: 0.5856\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5675 - acc: 0.6941 - val_loss: 0.5885 - val_acc: 0.6916\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5580 - acc: 0.7028 - val_loss: 0.6033 - val_acc: 0.6532\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5459 - acc: 0.7094 - val_loss: 0.5971 - val_acc: 0.6492\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5376 - acc: 0.7166 - val_loss: 0.5767 - val_acc: 0.6996\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5301 - acc: 0.7229 - val_loss: 0.7211 - val_acc: 0.5756\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5274 - acc: 0.7298 - val_loss: 0.5867 - val_acc: 0.6844\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5224 - acc: 0.7319 - val_loss: 0.6177 - val_acc: 0.6284\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5170 - acc: 0.7296 - val_loss: 0.6320 - val_acc: 0.6368\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5137 - acc: 0.7412 - val_loss: 0.5730 - val_acc: 0.7052\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5088 - acc: 0.7446 - val_loss: 0.6095 - val_acc: 0.6500\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.5083 - acc: 0.7414 - val_loss: 0.6224 - val_acc: 0.6284\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.5038 - acc: 0.7429 - val_loss: 0.6118 - val_acc: 0.6448\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.4988 - acc: 0.7496 - val_loss: 0.6378 - val_acc: 0.6280\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.4919 - acc: 0.7531 - val_loss: 0.6485 - val_acc: 0.6320\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 4s 248us/sample - loss: 0.4933 - acc: 0.7514 - val_loss: 0.7263 - val_acc: 0.5748\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 4s 246us/sample - loss: 0.4905 - acc: 0.7531 - val_loss: 0.7378 - val_acc: 0.5852\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 4s 254us/sample - loss: 0.4876 - acc: 0.7552 - val_loss: 0.7829 - val_acc: 0.5716\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 4s 258us/sample - loss: 0.4883 - acc: 0.7568 - val_loss: 0.6452 - val_acc: 0.6216\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.4877 - acc: 0.7593 - val_loss: 0.7043 - val_acc: 0.5984\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 4s 241us/sample - loss: 0.4800 - acc: 0.7619 - val_loss: 0.7349 - val_acc: 0.5836\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 4s 244us/sample - loss: 0.4777 - acc: 0.7616 - val_loss: 0.6538 - val_acc: 0.6408\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.4775 - acc: 0.7633 - val_loss: 0.6525 - val_acc: 0.6288\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 4s 243us/sample - loss: 0.4743 - acc: 0.7617 - val_loss: 0.6776 - val_acc: 0.6196\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 4s 244us/sample - loss: 0.4706 - acc: 0.7651 - val_loss: 0.7025 - val_acc: 0.5932\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 4s 244us/sample - loss: 0.4684 - acc: 0.7669 - val_loss: 0.6804 - val_acc: 0.6176\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 28,242\n",
      "Trainable params: 27,322\n",
      "Non-trainable params: 920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 391us/sample - loss: 0.7471 - acc: 0.5724 - val_loss: 0.6702 - val_acc: 0.5728\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 4s 253us/sample - loss: 0.6530 - acc: 0.6149 - val_loss: 0.7494 - val_acc: 0.5124\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.6199 - acc: 0.6462 - val_loss: 0.7370 - val_acc: 0.5324\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 4s 254us/sample - loss: 0.5936 - acc: 0.6703 - val_loss: 0.6629 - val_acc: 0.5888\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 4s 264us/sample - loss: 0.5749 - acc: 0.6894 - val_loss: 0.6877 - val_acc: 0.5920\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 4s 248us/sample - loss: 0.5595 - acc: 0.7018 - val_loss: 0.6216 - val_acc: 0.6456\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.5440 - acc: 0.7129 - val_loss: 0.6044 - val_acc: 0.6456\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.5378 - acc: 0.7194 - val_loss: 0.6591 - val_acc: 0.6032\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 4s 249us/sample - loss: 0.5300 - acc: 0.7231 - val_loss: 0.6608 - val_acc: 0.6052\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 4s 245us/sample - loss: 0.5206 - acc: 0.7296 - val_loss: 0.6479 - val_acc: 0.6148\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 4s 245us/sample - loss: 0.5178 - acc: 0.7312 - val_loss: 0.6036 - val_acc: 0.6780\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 4s 246us/sample - loss: 0.5134 - acc: 0.7369 - val_loss: 0.6877 - val_acc: 0.5936\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 4s 246us/sample - loss: 0.5065 - acc: 0.7429 - val_loss: 0.5837 - val_acc: 0.6812\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.5034 - acc: 0.7447 - val_loss: 0.6826 - val_acc: 0.6060\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 4s 244us/sample - loss: 0.4996 - acc: 0.7469 - val_loss: 0.7082 - val_acc: 0.5832\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 4s 246us/sample - loss: 0.4893 - acc: 0.7514 - val_loss: 0.6377 - val_acc: 0.6404\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.4871 - acc: 0.7548 - val_loss: 0.7707 - val_acc: 0.5876\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.4819 - acc: 0.7614 - val_loss: 0.6957 - val_acc: 0.5940\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.4847 - acc: 0.7551 - val_loss: 0.6534 - val_acc: 0.6296\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.4763 - acc: 0.7630 - val_loss: 0.8041 - val_acc: 0.5748\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.4720 - acc: 0.7716 - val_loss: 0.7847 - val_acc: 0.5856\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 4s 248us/sample - loss: 0.4624 - acc: 0.7722 - val_loss: 0.8204 - val_acc: 0.5816\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 4s 253us/sample - loss: 0.4602 - acc: 0.7765 - val_loss: 0.7709 - val_acc: 0.5884\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.4629 - acc: 0.7701 - val_loss: 0.7695 - val_acc: 0.5872\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 4s 248us/sample - loss: 0.4470 - acc: 0.7803 - val_loss: 0.7519 - val_acc: 0.6020\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 4s 248us/sample - loss: 0.4533 - acc: 0.7771 - val_loss: 0.7664 - val_acc: 0.5896\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.4474 - acc: 0.7843 - val_loss: 0.7009 - val_acc: 0.6268\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 4s 247us/sample - loss: 0.4469 - acc: 0.7832 - val_loss: 0.7311 - val_acc: 0.5988\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 4s 249us/sample - loss: 0.4407 - acc: 0.7854 - val_loss: 0.9017 - val_acc: 0.5640\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 4s 248us/sample - loss: 0.4374 - acc: 0.7875 - val_loss: 0.7279 - val_acc: 0.6244\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 54,402\n",
      "Trainable params: 53,122\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 393us/sample - loss: 0.7266 - acc: 0.5782 - val_loss: 0.7280 - val_acc: 0.4988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 4s 256us/sample - loss: 0.6339 - acc: 0.6348 - val_loss: 0.8448 - val_acc: 0.5080\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 4s 255us/sample - loss: 0.6043 - acc: 0.6641 - val_loss: 0.6586 - val_acc: 0.5804\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.5814 - acc: 0.6839 - val_loss: 0.6601 - val_acc: 0.6008\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 4s 262us/sample - loss: 0.5591 - acc: 0.7008 - val_loss: 0.6690 - val_acc: 0.5880\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 4s 253us/sample - loss: 0.5504 - acc: 0.7062 - val_loss: 0.6524 - val_acc: 0.6136\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.5369 - acc: 0.7184 - val_loss: 0.6008 - val_acc: 0.6616\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 4s 263us/sample - loss: 0.5276 - acc: 0.7238 - val_loss: 0.7482 - val_acc: 0.5756\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.5208 - acc: 0.7294 - val_loss: 0.5764 - val_acc: 0.7064\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.5112 - acc: 0.7428 - val_loss: 0.6715 - val_acc: 0.6244\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 4s 254us/sample - loss: 0.5085 - acc: 0.7410 - val_loss: 0.6261 - val_acc: 0.6628\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 4s 257us/sample - loss: 0.5053 - acc: 0.7405 - val_loss: 0.7627 - val_acc: 0.5752\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 4s 261us/sample - loss: 0.4973 - acc: 0.7493 - val_loss: 0.7635 - val_acc: 0.5888\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.4883 - acc: 0.7549 - val_loss: 0.6931 - val_acc: 0.6240\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 4s 250us/sample - loss: 0.4780 - acc: 0.7605 - val_loss: 0.7210 - val_acc: 0.6056\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 4s 249us/sample - loss: 0.4765 - acc: 0.7603 - val_loss: 0.7997 - val_acc: 0.5820\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 4s 249us/sample - loss: 0.4743 - acc: 0.7613 - val_loss: 0.6663 - val_acc: 0.6436\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 4s 249us/sample - loss: 0.4698 - acc: 0.7706 - val_loss: 0.6610 - val_acc: 0.6360\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 4s 248us/sample - loss: 0.4573 - acc: 0.7739 - val_loss: 0.7017 - val_acc: 0.6204\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 4s 250us/sample - loss: 0.4547 - acc: 0.7780 - val_loss: 0.6697 - val_acc: 0.6496\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 4s 256us/sample - loss: 0.4478 - acc: 0.7780 - val_loss: 0.7868 - val_acc: 0.5964\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 4s 261us/sample - loss: 0.4401 - acc: 0.7869 - val_loss: 0.7147 - val_acc: 0.6272\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 5s 273us/sample - loss: 0.4432 - acc: 0.7854 - val_loss: 0.7218 - val_acc: 0.6148\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 5s 269us/sample - loss: 0.4431 - acc: 0.7851 - val_loss: 0.7132 - val_acc: 0.6344\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 5s 271us/sample - loss: 0.4362 - acc: 0.7888 - val_loss: 0.7625 - val_acc: 0.6052\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 5s 268us/sample - loss: 0.4312 - acc: 0.7906 - val_loss: 0.8815 - val_acc: 0.5924\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 4s 257us/sample - loss: 0.4325 - acc: 0.7914 - val_loss: 0.6843 - val_acc: 0.6420\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 4s 255us/sample - loss: 0.4245 - acc: 0.7956 - val_loss: 0.7511 - val_acc: 0.6056\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 5s 267us/sample - loss: 0.4197 - acc: 0.7982 - val_loss: 0.7890 - val_acc: 0.6132\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 4s 265us/sample - loss: 0.4177 - acc: 0.8001 - val_loss: 0.7832 - val_acc: 0.6176\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 466,242\n",
      "Trainable params: 462,322\n",
      "Non-trainable params: 3,920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 398us/sample - loss: 0.7768 - acc: 0.5932 - val_loss: 1.3147 - val_acc: 0.4936\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 4s 254us/sample - loss: 0.6350 - acc: 0.6382 - val_loss: 1.1576 - val_acc: 0.5024\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 5s 267us/sample - loss: 0.6042 - acc: 0.6674 - val_loss: 0.9089 - val_acc: 0.5388\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 4s 255us/sample - loss: 0.5824 - acc: 0.6832 - val_loss: 0.7900 - val_acc: 0.5672\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.5657 - acc: 0.7021 - val_loss: 0.6467 - val_acc: 0.6256\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 4s 255us/sample - loss: 0.5495 - acc: 0.7107 - val_loss: 0.6822 - val_acc: 0.6160\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 4s 254us/sample - loss: 0.5361 - acc: 0.7259 - val_loss: 0.6155 - val_acc: 0.6800\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 4s 256us/sample - loss: 0.5273 - acc: 0.7307 - val_loss: 0.6313 - val_acc: 0.6784\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 4s 256us/sample - loss: 0.5145 - acc: 0.7345 - val_loss: 0.6358 - val_acc: 0.6448\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 4s 254us/sample - loss: 0.5116 - acc: 0.7398 - val_loss: 0.8259 - val_acc: 0.5936\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 4s 253us/sample - loss: 0.5015 - acc: 0.7515 - val_loss: 0.7002 - val_acc: 0.6312\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 4s 254us/sample - loss: 0.4884 - acc: 0.7539 - val_loss: 0.5787 - val_acc: 0.7152\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 4s 254us/sample - loss: 0.4761 - acc: 0.7623 - val_loss: 0.7763 - val_acc: 0.6156\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 4s 255us/sample - loss: 0.4669 - acc: 0.7709 - val_loss: 0.7356 - val_acc: 0.6340\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.4654 - acc: 0.7678 - val_loss: 0.8401 - val_acc: 0.6028\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 4s 253us/sample - loss: 0.4494 - acc: 0.7824 - val_loss: 0.7223 - val_acc: 0.6416\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.4462 - acc: 0.7817 - val_loss: 0.6805 - val_acc: 0.6588\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 4s 250us/sample - loss: 0.4373 - acc: 0.7915 - val_loss: 0.8442 - val_acc: 0.6000\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 4s 253us/sample - loss: 0.4364 - acc: 0.7909 - val_loss: 0.6610 - val_acc: 0.6748\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.4225 - acc: 0.7982 - val_loss: 0.7023 - val_acc: 0.6500\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.4206 - acc: 0.7951 - val_loss: 0.7458 - val_acc: 0.6288\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.4159 - acc: 0.8006 - val_loss: 0.6826 - val_acc: 0.6736\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 4s 253us/sample - loss: 0.4123 - acc: 0.8005 - val_loss: 0.7135 - val_acc: 0.6624\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.3949 - acc: 0.8158 - val_loss: 0.8160 - val_acc: 0.6424\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.3996 - acc: 0.8108 - val_loss: 0.8945 - val_acc: 0.6060\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.3946 - acc: 0.8108 - val_loss: 0.8958 - val_acc: 0.6168\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.3907 - acc: 0.8171 - val_loss: 0.7673 - val_acc: 0.6640\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.3786 - acc: 0.8242 - val_loss: 0.9733 - val_acc: 0.6136\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.3760 - acc: 0.8258 - val_loss: 0.9109 - val_acc: 0.6356\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 4s 252us/sample - loss: 0.3781 - acc: 0.8269 - val_loss: 0.9257 - val_acc: 0.6012\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_100 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,502\n",
      "Trainable params: 2,122\n",
      "Non-trainable params: 380\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 274us/sample - loss: 0.8063 - acc: 0.5478 - val_loss: 0.6777 - val_acc: 0.5828\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.7055 - acc: 0.5686 - val_loss: 0.6637 - val_acc: 0.6000\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6798 - acc: 0.5813 - val_loss: 0.6576 - val_acc: 0.6068\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6677 - acc: 0.5944 - val_loss: 0.6546 - val_acc: 0.6140\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6592 - acc: 0.5991 - val_loss: 0.6542 - val_acc: 0.6092\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6529 - acc: 0.6100 - val_loss: 0.6453 - val_acc: 0.6192\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6460 - acc: 0.6214 - val_loss: 0.6360 - val_acc: 0.6360\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6382 - acc: 0.6244 - val_loss: 0.6373 - val_acc: 0.6312\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.6310 - acc: 0.635 - 2s 127us/sample - loss: 0.6313 - acc: 0.6344 - val_loss: 0.6254 - val_acc: 0.6608\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6227 - acc: 0.6480 - val_loss: 0.6230 - val_acc: 0.6628\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6176 - acc: 0.6506 - val_loss: 0.6169 - val_acc: 0.6660\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6071 - acc: 0.6621 - val_loss: 0.6114 - val_acc: 0.6720\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6064 - acc: 0.6637 - val_loss: 0.6045 - val_acc: 0.6788\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.5960 - acc: 0.6699 - val_loss: 0.6338 - val_acc: 0.5964\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.5947 - acc: 0.6728 - val_loss: 0.5992 - val_acc: 0.6892\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.5861 - acc: 0.6839 - val_loss: 0.5850 - val_acc: 0.7032\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.5837 - acc: 0.6842 - val_loss: 0.5914 - val_acc: 0.6896\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.5786 - acc: 0.6871 - val_loss: 0.6045 - val_acc: 0.6468\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.5776 - acc: 0.6903 - val_loss: 0.5810 - val_acc: 0.6984\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.5710 - acc: 0.6919 - val_loss: 0.5823 - val_acc: 0.6944\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.5718 - acc: 0.6977 - val_loss: 0.5694 - val_acc: 0.7136\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.5669 - acc: 0.6976 - val_loss: 0.5743 - val_acc: 0.7116\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.5682 - acc: 0.7015 - val_loss: 0.5734 - val_acc: 0.7028\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.5629 - acc: 0.7038 - val_loss: 0.5572 - val_acc: 0.7264\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.5598 - acc: 0.7040 - val_loss: 0.5715 - val_acc: 0.7076\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.5561 - acc: 0.7071 - val_loss: 0.5647 - val_acc: 0.7080\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.5544 - acc: 0.7106 - val_loss: 0.5747 - val_acc: 0.6872\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.5533 - acc: 0.7116 - val_loss: 0.5765 - val_acc: 0.6784\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.5476 - acc: 0.7182 - val_loss: 0.5389 - val_acc: 0.7376\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.5481 - acc: 0.7155 - val_loss: 0.5958 - val_acc: 0.6376\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_54 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_102 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_103 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_104 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_105 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 11,942\n",
      "Trainable params: 11,322\n",
      "Non-trainable params: 620\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 281us/sample - loss: 0.7503 - acc: 0.5691 - val_loss: 0.6938 - val_acc: 0.4924\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6829 - acc: 0.5898 - val_loss: 0.6729 - val_acc: 0.5496\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6544 - acc: 0.6105 - val_loss: 0.6968 - val_acc: 0.5056\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6325 - acc: 0.6344 - val_loss: 0.7241 - val_acc: 0.5216\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6158 - acc: 0.6451 - val_loss: 0.6756 - val_acc: 0.5468\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6007 - acc: 0.6620 - val_loss: 0.6848 - val_acc: 0.5548\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5853 - acc: 0.6771 - val_loss: 0.6366 - val_acc: 0.5936\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5714 - acc: 0.6895 - val_loss: 0.6339 - val_acc: 0.5920\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5591 - acc: 0.6989 - val_loss: 0.5838 - val_acc: 0.6860\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.5510 - acc: 0.706 - 2s 132us/sample - loss: 0.5514 - acc: 0.7059 - val_loss: 0.5910 - val_acc: 0.6724\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5425 - acc: 0.7183 - val_loss: 0.5718 - val_acc: 0.6904\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5284 - acc: 0.7266 - val_loss: 0.5525 - val_acc: 0.7220\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5266 - acc: 0.7249 - val_loss: 0.5505 - val_acc: 0.7200\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.5180 - acc: 0.7326 - val_loss: 0.5589 - val_acc: 0.7048\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5179 - acc: 0.7313 - val_loss: 0.5866 - val_acc: 0.6720\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5073 - acc: 0.7388 - val_loss: 0.5403 - val_acc: 0.7324\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5047 - acc: 0.7447 - val_loss: 0.5750 - val_acc: 0.6776\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5030 - acc: 0.7428 - val_loss: 0.5761 - val_acc: 0.6808\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5002 - acc: 0.7470 - val_loss: 0.5660 - val_acc: 0.6892\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4927 - acc: 0.7499 - val_loss: 0.5554 - val_acc: 0.7072\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4920 - acc: 0.7524 - val_loss: 0.6674 - val_acc: 0.6032\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4843 - acc: 0.7578 - val_loss: 0.5471 - val_acc: 0.7260\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4840 - acc: 0.7567 - val_loss: 0.5914 - val_acc: 0.6716\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4822 - acc: 0.7644 - val_loss: 0.5318 - val_acc: 0.7328\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4834 - acc: 0.7579 - val_loss: 0.6164 - val_acc: 0.6348\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4765 - acc: 0.7649 - val_loss: 0.6256 - val_acc: 0.6412\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4751 - acc: 0.7648 - val_loss: 0.5824 - val_acc: 0.6752\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4696 - acc: 0.7701 - val_loss: 0.6234 - val_acc: 0.6376\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4699 - acc: 0.7668 - val_loss: 0.5290 - val_acc: 0.7396\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4683 - acc: 0.7657 - val_loss: 0.6469 - val_acc: 0.6276\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_106 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_107 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_108 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_109 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 28,242\n",
      "Trainable params: 27,322\n",
      "Non-trainable params: 920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 289us/sample - loss: 0.7429 - acc: 0.5634 - val_loss: 0.7016 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6680 - acc: 0.6025 - val_loss: 0.7480 - val_acc: 0.4920\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6379 - acc: 0.6273 - val_loss: 0.8335 - val_acc: 0.4996\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6113 - acc: 0.6572 - val_loss: 0.7319 - val_acc: 0.5348\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5921 - acc: 0.6776 - val_loss: 0.7114 - val_acc: 0.5488\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5755 - acc: 0.6872 - val_loss: 0.6145 - val_acc: 0.6332\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5636 - acc: 0.6960 - val_loss: 0.5935 - val_acc: 0.6752\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5509 - acc: 0.7035 - val_loss: 0.5780 - val_acc: 0.6868\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5391 - acc: 0.7131 - val_loss: 0.6118 - val_acc: 0.6464\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5281 - acc: 0.7255 - val_loss: 0.5900 - val_acc: 0.6536\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5255 - acc: 0.7252 - val_loss: 0.6375 - val_acc: 0.6184\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.5100 - acc: 0.7412 - val_loss: 0.5351 - val_acc: 0.7444\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5109 - acc: 0.7352 - val_loss: 0.6287 - val_acc: 0.6220\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5022 - acc: 0.7463 - val_loss: 0.6396 - val_acc: 0.6324\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.4969 - acc: 0.7521 - val_loss: 0.6295 - val_acc: 0.6424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4930 - acc: 0.7536 - val_loss: 0.5778 - val_acc: 0.6868\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4868 - acc: 0.7580 - val_loss: 0.5662 - val_acc: 0.7000\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.4822 - acc: 0.7628 - val_loss: 0.5832 - val_acc: 0.6820\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.4796 - acc: 0.7617 - val_loss: 0.5930 - val_acc: 0.6708\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4742 - acc: 0.7651 - val_loss: 0.5681 - val_acc: 0.6976\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4712 - acc: 0.7669 - val_loss: 0.5904 - val_acc: 0.6712\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4660 - acc: 0.7738 - val_loss: 0.5628 - val_acc: 0.7016\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4620 - acc: 0.7735 - val_loss: 0.5587 - val_acc: 0.7124\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4549 - acc: 0.7779 - val_loss: 0.6505 - val_acc: 0.6412\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4560 - acc: 0.7739 - val_loss: 0.6072 - val_acc: 0.6628\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4499 - acc: 0.7802 - val_loss: 0.6015 - val_acc: 0.6764\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4471 - acc: 0.7792 - val_loss: 0.6810 - val_acc: 0.6384\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4406 - acc: 0.7861 - val_loss: 0.6054 - val_acc: 0.6692\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4366 - acc: 0.7863 - val_loss: 0.6282 - val_acc: 0.6500\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4342 - acc: 0.7914 - val_loss: 0.6702 - val_acc: 0.6392\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_56 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_112 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_113 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 54,402\n",
      "Trainable params: 53,122\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 295us/sample - loss: 0.7475 - acc: 0.5701 - val_loss: 0.7231 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.6639 - acc: 0.6141 - val_loss: 0.9609 - val_acc: 0.4876\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.6253 - acc: 0.6404 - val_loss: 1.0375 - val_acc: 0.4924\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6043 - acc: 0.6579 - val_loss: 0.7739 - val_acc: 0.5236\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.5824 - acc: 0.6814 - val_loss: 0.6706 - val_acc: 0.5800\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.5655 - acc: 0.6974 - val_loss: 0.7059 - val_acc: 0.5736\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.5530 - acc: 0.7041 - val_loss: 0.6875 - val_acc: 0.5792\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.5435 - acc: 0.7164 - val_loss: 0.5710 - val_acc: 0.6960\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.5292 - acc: 0.7258 - val_loss: 0.6066 - val_acc: 0.6504\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.5193 - acc: 0.7316 - val_loss: 0.6449 - val_acc: 0.6236\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.5170 - acc: 0.7332 - val_loss: 0.5996 - val_acc: 0.6500\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.5020 - acc: 0.7444 - val_loss: 0.5840 - val_acc: 0.6736\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.4999 - acc: 0.7439 - val_loss: 0.5440 - val_acc: 0.7232\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.4949 - acc: 0.7502 - val_loss: 0.5923 - val_acc: 0.6792\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.4860 - acc: 0.7546 - val_loss: 0.6274 - val_acc: 0.6516\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.4798 - acc: 0.7580 - val_loss: 0.5781 - val_acc: 0.6988\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.4724 - acc: 0.7679 - val_loss: 0.6006 - val_acc: 0.6924\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.4668 - acc: 0.7696 - val_loss: 0.5479 - val_acc: 0.7208\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.4625 - acc: 0.7682 - val_loss: 0.5132 - val_acc: 0.7604\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.4616 - acc: 0.7721 - val_loss: 0.5110 - val_acc: 0.7660\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.4531 - acc: 0.7784 - val_loss: 0.5845 - val_acc: 0.6956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.4467 - acc: 0.7844 - val_loss: 0.6357 - val_acc: 0.6604\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.4414 - acc: 0.7836 - val_loss: 0.6141 - val_acc: 0.6800\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.4361 - acc: 0.7911 - val_loss: 0.5058 - val_acc: 0.7620\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.4281 - acc: 0.7922 - val_loss: 0.6133 - val_acc: 0.6828\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.4209 - acc: 0.7975 - val_loss: 0.6352 - val_acc: 0.6672\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.4198 - acc: 0.8005 - val_loss: 0.5281 - val_acc: 0.7392\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.4175 - acc: 0.7995 - val_loss: 0.6909 - val_acc: 0.6540\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4128 - acc: 0.8032 - val_loss: 0.4992 - val_acc: 0.7736\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4069 - acc: 0.8058 - val_loss: 0.6797 - val_acc: 0.6592\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_57 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_114 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 466,242\n",
      "Trainable params: 462,322\n",
      "Non-trainable params: 3,920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 299us/sample - loss: 0.7919 - acc: 0.5858 - val_loss: 0.9469 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6470 - acc: 0.6330 - val_loss: 1.7286 - val_acc: 0.4876\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6102 - acc: 0.6618 - val_loss: 1.7447 - val_acc: 0.4868\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5906 - acc: 0.6757 - val_loss: 1.4447 - val_acc: 0.5020\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.5724 - acc: 0.6915 - val_loss: 1.0813 - val_acc: 0.5268\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.5496 - acc: 0.7076 - val_loss: 0.7466 - val_acc: 0.5816\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.5392 - acc: 0.7155 - val_loss: 0.6597 - val_acc: 0.6520\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5229 - acc: 0.7254 - val_loss: 0.7454 - val_acc: 0.5920\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5157 - acc: 0.7353 - val_loss: 0.6517 - val_acc: 0.6488\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.4978 - acc: 0.7494 - val_loss: 0.5989 - val_acc: 0.6808\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.4847 - acc: 0.7578 - val_loss: 0.5462 - val_acc: 0.7288\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4755 - acc: 0.7641 - val_loss: 0.6790 - val_acc: 0.6336\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4653 - acc: 0.7715 - val_loss: 0.5706 - val_acc: 0.7036\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4594 - acc: 0.7712 - val_loss: 0.5786 - val_acc: 0.7012\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.4493 - acc: 0.7776 - val_loss: 0.5043 - val_acc: 0.7652\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4361 - acc: 0.7881 - val_loss: 0.5536 - val_acc: 0.7196\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4289 - acc: 0.7914 - val_loss: 0.5300 - val_acc: 0.7532\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4252 - acc: 0.7936 - val_loss: 0.5426 - val_acc: 0.7328\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4140 - acc: 0.8016 - val_loss: 0.5824 - val_acc: 0.7096\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4093 - acc: 0.8073 - val_loss: 0.6735 - val_acc: 0.6612\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.3993 - acc: 0.8081 - val_loss: 0.6120 - val_acc: 0.6960\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3937 - acc: 0.8153 - val_loss: 0.5038 - val_acc: 0.7668\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3814 - acc: 0.8215 - val_loss: 0.6383 - val_acc: 0.7060\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3859 - acc: 0.8203 - val_loss: 0.7294 - val_acc: 0.6528\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3680 - acc: 0.8319 - val_loss: 0.5764 - val_acc: 0.7384\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3703 - acc: 0.8280 - val_loss: 0.6254 - val_acc: 0.7132\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3565 - acc: 0.8318 - val_loss: 0.5694 - val_acc: 0.7360\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3538 - acc: 0.8365 - val_loss: 0.6646 - val_acc: 0.6960\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3539 - acc: 0.8346 - val_loss: 0.5975 - val_acc: 0.7328\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3480 - acc: 0.8406 - val_loss: 0.6128 - val_acc: 0.7320\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_58 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_118 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_120 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,502\n",
      "Trainable params: 2,122\n",
      "Non-trainable params: 380\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 237us/sample - loss: 0.7605 - acc: 0.5531 - val_loss: 0.7131 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6935 - acc: 0.5696 - val_loss: 0.6778 - val_acc: 0.6080\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6742 - acc: 0.5840 - val_loss: 0.6735 - val_acc: 0.5700\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6645 - acc: 0.5976 - val_loss: 0.6733 - val_acc: 0.5592\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6602 - acc: 0.6014 - val_loss: 0.6633 - val_acc: 0.5808\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6556 - acc: 0.6084 - val_loss: 0.6646 - val_acc: 0.5700\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6469 - acc: 0.6159 - val_loss: 0.6688 - val_acc: 0.5480\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6420 - acc: 0.6224 - val_loss: 0.6382 - val_acc: 0.6300\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6327 - acc: 0.6327 - val_loss: 0.6364 - val_acc: 0.6232\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6216 - acc: 0.6446 - val_loss: 0.6421 - val_acc: 0.6008\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6120 - acc: 0.6561 - val_loss: 0.6192 - val_acc: 0.6556\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6066 - acc: 0.6604 - val_loss: 0.6034 - val_acc: 0.6812\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5998 - acc: 0.6658 - val_loss: 0.6097 - val_acc: 0.6640\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5905 - acc: 0.6749 - val_loss: 0.5946 - val_acc: 0.6824\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5848 - acc: 0.6824 - val_loss: 0.5872 - val_acc: 0.6972\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5775 - acc: 0.6912 - val_loss: 0.5769 - val_acc: 0.6960\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5752 - acc: 0.6945 - val_loss: 0.5680 - val_acc: 0.7052\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5718 - acc: 0.6935 - val_loss: 0.5628 - val_acc: 0.7112\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5672 - acc: 0.6948 - val_loss: 0.6479 - val_acc: 0.5768\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5619 - acc: 0.7021 - val_loss: 0.5595 - val_acc: 0.7212\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5548 - acc: 0.7086 - val_loss: 0.5587 - val_acc: 0.7244\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5543 - acc: 0.7082 - val_loss: 0.5821 - val_acc: 0.6768\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5495 - acc: 0.7158 - val_loss: 0.5453 - val_acc: 0.7304\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5504 - acc: 0.7121 - val_loss: 0.5399 - val_acc: 0.7328\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5460 - acc: 0.7126 - val_loss: 0.5410 - val_acc: 0.7300\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5401 - acc: 0.7197 - val_loss: 0.5339 - val_acc: 0.7388\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5401 - acc: 0.7212 - val_loss: 0.5308 - val_acc: 0.7380\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5422 - acc: 0.7207 - val_loss: 0.5501 - val_acc: 0.7192\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5416 - acc: 0.7211 - val_loss: 0.5407 - val_acc: 0.7344\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5378 - acc: 0.7194 - val_loss: 0.5342 - val_acc: 0.7488\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_59 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_125 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 11,942\n",
      "Trainable params: 11,322\n",
      "Non-trainable params: 620\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 242us/sample - loss: 0.7803 - acc: 0.5526 - val_loss: 0.7743 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.7045 - acc: 0.5752 - val_loss: 0.7186 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6739 - acc: 0.5932 - val_loss: 0.7380 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6529 - acc: 0.6149 - val_loss: 0.8242 - val_acc: 0.4880\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6373 - acc: 0.6288 - val_loss: 0.8240 - val_acc: 0.4904\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6186 - acc: 0.6486 - val_loss: 0.7973 - val_acc: 0.4980\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6095 - acc: 0.6517 - val_loss: 0.7396 - val_acc: 0.5244\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5947 - acc: 0.6686 - val_loss: 0.7284 - val_acc: 0.5384\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5852 - acc: 0.6795 - val_loss: 0.6358 - val_acc: 0.5964\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5732 - acc: 0.6888 - val_loss: 0.6544 - val_acc: 0.5836\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5634 - acc: 0.6964 - val_loss: 0.6690 - val_acc: 0.5796\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5582 - acc: 0.7032 - val_loss: 0.5716 - val_acc: 0.6984\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5523 - acc: 0.7033 - val_loss: 0.5947 - val_acc: 0.6532\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5427 - acc: 0.7134 - val_loss: 0.6032 - val_acc: 0.6380\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5351 - acc: 0.7225 - val_loss: 0.5592 - val_acc: 0.7100\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5329 - acc: 0.7219 - val_loss: 0.5870 - val_acc: 0.6728\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5240 - acc: 0.7252 - val_loss: 0.5629 - val_acc: 0.6948\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5217 - acc: 0.7316 - val_loss: 0.5364 - val_acc: 0.7324\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5139 - acc: 0.7305 - val_loss: 0.5422 - val_acc: 0.7388\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5097 - acc: 0.7348 - val_loss: 0.5512 - val_acc: 0.7088\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5110 - acc: 0.7415 - val_loss: 0.5622 - val_acc: 0.6932\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5062 - acc: 0.7433 - val_loss: 0.5257 - val_acc: 0.7464\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4989 - acc: 0.7471 - val_loss: 0.5382 - val_acc: 0.7276\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4989 - acc: 0.7472 - val_loss: 0.5339 - val_acc: 0.7352\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4934 - acc: 0.7497 - val_loss: 0.5529 - val_acc: 0.6992\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4926 - acc: 0.7502 - val_loss: 0.5634 - val_acc: 0.6904\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4868 - acc: 0.7578 - val_loss: 0.5824 - val_acc: 0.6796\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4900 - acc: 0.7503 - val_loss: 0.5075 - val_acc: 0.7632\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4809 - acc: 0.7586 - val_loss: 0.5433 - val_acc: 0.7168\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4794 - acc: 0.7612 - val_loss: 0.5181 - val_acc: 0.7540\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_60 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_126 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_127 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_128 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_129 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 28,242\n",
      "Trainable params: 27,322\n",
      "Non-trainable params: 920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 244us/sample - loss: 0.7851 - acc: 0.5585 - val_loss: 0.7044 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.7007 - acc: 0.5846 - val_loss: 0.7537 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6659 - acc: 0.6074 - val_loss: 0.9538 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6379 - acc: 0.6352 - val_loss: 1.1091 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6171 - acc: 0.6495 - val_loss: 1.1124 - val_acc: 0.4896\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6061 - acc: 0.6630 - val_loss: 0.8819 - val_acc: 0.5028\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5866 - acc: 0.6776 - val_loss: 0.7911 - val_acc: 0.5284\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5753 - acc: 0.6883 - val_loss: 0.6854 - val_acc: 0.5636\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5687 - acc: 0.6903 - val_loss: 0.6057 - val_acc: 0.6340\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5588 - acc: 0.6991 - val_loss: 0.6075 - val_acc: 0.6360\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5511 - acc: 0.7066 - val_loss: 0.5590 - val_acc: 0.7148\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5360 - acc: 0.7165 - val_loss: 0.5997 - val_acc: 0.6420\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5326 - acc: 0.7211 - val_loss: 0.6283 - val_acc: 0.6132\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5290 - acc: 0.7234 - val_loss: 0.5953 - val_acc: 0.6568\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5197 - acc: 0.7302 - val_loss: 0.5884 - val_acc: 0.6628\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5109 - acc: 0.7364 - val_loss: 0.5481 - val_acc: 0.6976\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5088 - acc: 0.7382 - val_loss: 0.5214 - val_acc: 0.7480\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5079 - acc: 0.7392 - val_loss: 0.5140 - val_acc: 0.7540\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4981 - acc: 0.7499 - val_loss: 0.5098 - val_acc: 0.7540\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4943 - acc: 0.7488 - val_loss: 0.5647 - val_acc: 0.6804\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4857 - acc: 0.7578 - val_loss: 0.5714 - val_acc: 0.6824\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4853 - acc: 0.7534 - val_loss: 0.5369 - val_acc: 0.7300\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4813 - acc: 0.7578 - val_loss: 0.5064 - val_acc: 0.7608\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4740 - acc: 0.7627 - val_loss: 0.5451 - val_acc: 0.7060\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4699 - acc: 0.7638 - val_loss: 0.5040 - val_acc: 0.7568\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4739 - acc: 0.7635 - val_loss: 0.5068 - val_acc: 0.7620\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4606 - acc: 0.7697 - val_loss: 0.6168 - val_acc: 0.6528\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4640 - acc: 0.7715 - val_loss: 0.4954 - val_acc: 0.7684\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4567 - acc: 0.7764 - val_loss: 0.6051 - val_acc: 0.6660\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4509 - acc: 0.7775 - val_loss: 0.4919 - val_acc: 0.7732\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_61 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_130 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_131 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_132 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_133 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 54,402\n",
      "Trainable params: 53,122\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 248us/sample - loss: 0.7703 - acc: 0.5708 - val_loss: 0.7131 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6924 - acc: 0.5939 - val_loss: 0.7549 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6592 - acc: 0.6141 - val_loss: 1.0575 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6310 - acc: 0.6403 - val_loss: 1.3580 - val_acc: 0.4888\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6136 - acc: 0.6538 - val_loss: 1.1140 - val_acc: 0.4908\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6010 - acc: 0.6649 - val_loss: 1.0790 - val_acc: 0.4984\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5810 - acc: 0.6817 - val_loss: 0.9397 - val_acc: 0.5104\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5691 - acc: 0.6928 - val_loss: 0.8501 - val_acc: 0.5252\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5585 - acc: 0.7018 - val_loss: 0.6759 - val_acc: 0.5820\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5471 - acc: 0.7080 - val_loss: 0.6760 - val_acc: 0.5956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5390 - acc: 0.7178 - val_loss: 0.6274 - val_acc: 0.6344\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5341 - acc: 0.7204 - val_loss: 0.5972 - val_acc: 0.6596\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5241 - acc: 0.7288 - val_loss: 0.5734 - val_acc: 0.6880\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5159 - acc: 0.7348 - val_loss: 0.6321 - val_acc: 0.6196\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.5055 - acc: 0.7452 - val_loss: 0.5492 - val_acc: 0.6988\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5019 - acc: 0.7448 - val_loss: 0.5538 - val_acc: 0.7116\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4935 - acc: 0.7486 - val_loss: 0.5290 - val_acc: 0.7404\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4862 - acc: 0.7572 - val_loss: 0.5167 - val_acc: 0.7464\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4823 - acc: 0.7585 - val_loss: 0.6205 - val_acc: 0.6552\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4800 - acc: 0.7586 - val_loss: 0.5740 - val_acc: 0.6908\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4708 - acc: 0.7656 - val_loss: 0.5271 - val_acc: 0.7432\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4662 - acc: 0.7706 - val_loss: 0.4978 - val_acc: 0.7760\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4632 - acc: 0.7703 - val_loss: 0.5633 - val_acc: 0.6992\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4574 - acc: 0.7757 - val_loss: 0.5045 - val_acc: 0.7580\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4504 - acc: 0.7769 - val_loss: 0.5135 - val_acc: 0.7508\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4475 - acc: 0.7796 - val_loss: 0.5137 - val_acc: 0.7576\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4431 - acc: 0.7812 - val_loss: 0.5368 - val_acc: 0.7140\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4351 - acc: 0.7905 - val_loss: 0.4965 - val_acc: 0.7736\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4270 - acc: 0.7921 - val_loss: 0.5574 - val_acc: 0.7120\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4326 - acc: 0.7902 - val_loss: 0.4727 - val_acc: 0.7892\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_62 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_134 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_135 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 466,242\n",
      "Trainable params: 462,322\n",
      "Non-trainable params: 3,920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 251us/sample - loss: 0.8239 - acc: 0.5794 - val_loss: 0.7327 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6699 - acc: 0.6178 - val_loss: 1.0798 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6333 - acc: 0.6424 - val_loss: 2.0812 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6086 - acc: 0.6622 - val_loss: 2.5092 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5918 - acc: 0.6787 - val_loss: 2.3070 - val_acc: 0.4912\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5687 - acc: 0.6929 - val_loss: 1.9215 - val_acc: 0.4904\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5580 - acc: 0.7077 - val_loss: 1.4375 - val_acc: 0.5076\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5385 - acc: 0.7219 - val_loss: 1.5654 - val_acc: 0.5096\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5332 - acc: 0.7253 - val_loss: 1.0186 - val_acc: 0.5372\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5172 - acc: 0.7357 - val_loss: 0.7570 - val_acc: 0.5840\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5045 - acc: 0.7427 - val_loss: 0.7395 - val_acc: 0.5984\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4924 - acc: 0.7554 - val_loss: 0.7583 - val_acc: 0.5960\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4814 - acc: 0.7591 - val_loss: 0.6877 - val_acc: 0.6140\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4702 - acc: 0.7675 - val_loss: 0.6305 - val_acc: 0.6524\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4669 - acc: 0.7651 - val_loss: 0.8435 - val_acc: 0.5960\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4541 - acc: 0.7778 - val_loss: 0.5169 - val_acc: 0.7424\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4395 - acc: 0.7879 - val_loss: 0.5247 - val_acc: 0.7564\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4365 - acc: 0.7898 - val_loss: 0.5988 - val_acc: 0.6972\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4278 - acc: 0.7911 - val_loss: 0.5986 - val_acc: 0.6976\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4131 - acc: 0.8034 - val_loss: 0.5686 - val_acc: 0.7192\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4096 - acc: 0.8049 - val_loss: 0.5970 - val_acc: 0.7128\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4014 - acc: 0.8090 - val_loss: 0.5667 - val_acc: 0.7228\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3957 - acc: 0.8123 - val_loss: 0.4988 - val_acc: 0.7708\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3859 - acc: 0.8182 - val_loss: 0.5537 - val_acc: 0.7388\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3884 - acc: 0.8156 - val_loss: 0.5122 - val_acc: 0.7668\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3780 - acc: 0.8231 - val_loss: 0.5370 - val_acc: 0.7564\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3669 - acc: 0.8290 - val_loss: 0.6147 - val_acc: 0.7088\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3665 - acc: 0.8299 - val_loss: 0.6359 - val_acc: 0.7088\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3544 - acc: 0.8359 - val_loss: 0.4919 - val_acc: 0.7788\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3455 - acc: 0.8428 - val_loss: 0.5113 - val_acc: 0.7784\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_63 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_140 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_141 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,502\n",
      "Trainable params: 2,122\n",
      "Non-trainable params: 380\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.7849 - acc: 0.5516 - val_loss: 0.6991 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.7075 - acc: 0.5636 - val_loss: 0.6897 - val_acc: 0.5140\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6945 - acc: 0.5681 - val_loss: 0.6826 - val_acc: 0.5888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6821 - acc: 0.5780 - val_loss: 0.6840 - val_acc: 0.5148\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6761 - acc: 0.5791 - val_loss: 0.6865 - val_acc: 0.5028\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6650 - acc: 0.6014 - val_loss: 0.7028 - val_acc: 0.4912\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6587 - acc: 0.6045 - val_loss: 0.7103 - val_acc: 0.4948\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6513 - acc: 0.6145 - val_loss: 0.7113 - val_acc: 0.4968\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6478 - acc: 0.6185 - val_loss: 0.6811 - val_acc: 0.5196\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6400 - acc: 0.6266 - val_loss: 0.6698 - val_acc: 0.5508\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6339 - acc: 0.6308 - val_loss: 0.6626 - val_acc: 0.5576\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6293 - acc: 0.6369 - val_loss: 0.6484 - val_acc: 0.5920\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6230 - acc: 0.6454 - val_loss: 0.6528 - val_acc: 0.5856\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6210 - acc: 0.6490 - val_loss: 0.6396 - val_acc: 0.6028\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6129 - acc: 0.6563 - val_loss: 0.6294 - val_acc: 0.6320\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.6112 - acc: 0.6571 - val_loss: 0.6230 - val_acc: 0.6440\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6056 - acc: 0.6649 - val_loss: 0.6120 - val_acc: 0.6636\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5973 - acc: 0.6740 - val_loss: 0.6064 - val_acc: 0.6760\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5937 - acc: 0.6741 - val_loss: 0.6107 - val_acc: 0.6656\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5923 - acc: 0.6746 - val_loss: 0.6101 - val_acc: 0.6584\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5867 - acc: 0.6824 - val_loss: 0.6071 - val_acc: 0.6644\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5843 - acc: 0.6852 - val_loss: 0.6059 - val_acc: 0.6556\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5856 - acc: 0.6841 - val_loss: 0.5784 - val_acc: 0.6824\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5769 - acc: 0.6888 - val_loss: 0.5738 - val_acc: 0.6940\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5753 - acc: 0.6922 - val_loss: 0.5831 - val_acc: 0.6856\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5714 - acc: 0.6936 - val_loss: 0.5680 - val_acc: 0.6984\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5645 - acc: 0.7038 - val_loss: 0.5657 - val_acc: 0.7052\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5659 - acc: 0.6996 - val_loss: 0.5673 - val_acc: 0.7176\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5585 - acc: 0.7040 - val_loss: 0.5614 - val_acc: 0.7156\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5634 - acc: 0.6999 - val_loss: 0.5554 - val_acc: 0.7140\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_64 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_142 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_143 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_144 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_145 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 11,942\n",
      "Trainable params: 11,322\n",
      "Non-trainable params: 620\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 229us/sample - loss: 0.7716 - acc: 0.5545 - val_loss: 0.7453 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6962 - acc: 0.5806 - val_loss: 0.7315 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6811 - acc: 0.5874 - val_loss: 0.7358 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6644 - acc: 0.6054 - val_loss: 0.7787 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6488 - acc: 0.6149 - val_loss: 0.9101 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6376 - acc: 0.6259 - val_loss: 1.0810 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6267 - acc: 0.6382 - val_loss: 1.1509 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6161 - acc: 0.6506 - val_loss: 1.0751 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6014 - acc: 0.6650 - val_loss: 1.0304 - val_acc: 0.4912\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6007 - acc: 0.6666 - val_loss: 0.9119 - val_acc: 0.4916\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5889 - acc: 0.6728 - val_loss: 0.8336 - val_acc: 0.5024\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5798 - acc: 0.6815 - val_loss: 0.7431 - val_acc: 0.5268\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5716 - acc: 0.6896 - val_loss: 0.7757 - val_acc: 0.5288\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5649 - acc: 0.6908 - val_loss: 0.7219 - val_acc: 0.5448\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5579 - acc: 0.7005 - val_loss: 0.7242 - val_acc: 0.5464\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5541 - acc: 0.7030 - val_loss: 0.6286 - val_acc: 0.6076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5464 - acc: 0.7124 - val_loss: 0.6562 - val_acc: 0.5880\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5430 - acc: 0.7121 - val_loss: 0.5782 - val_acc: 0.6864\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5365 - acc: 0.7212 - val_loss: 0.5486 - val_acc: 0.7168\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5296 - acc: 0.7202 - val_loss: 0.5615 - val_acc: 0.7016\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5249 - acc: 0.7244 - val_loss: 0.5471 - val_acc: 0.7228\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5259 - acc: 0.7274 - val_loss: 0.5453 - val_acc: 0.7244\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5163 - acc: 0.7324 - val_loss: 0.5336 - val_acc: 0.7276\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5120 - acc: 0.7362 - val_loss: 0.5690 - val_acc: 0.6788\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5096 - acc: 0.7388 - val_loss: 0.5485 - val_acc: 0.7156\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5078 - acc: 0.7402 - val_loss: 0.5410 - val_acc: 0.7252\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5013 - acc: 0.7455 - val_loss: 0.5278 - val_acc: 0.7352\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4981 - acc: 0.7493 - val_loss: 0.5461 - val_acc: 0.7044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4894 - acc: 0.7531 - val_loss: 0.5792 - val_acc: 0.6708\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4928 - acc: 0.7545 - val_loss: 0.5187 - val_acc: 0.7480\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_65 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_146 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 28,242\n",
      "Trainable params: 27,322\n",
      "Non-trainable params: 920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 233us/sample - loss: 0.7770 - acc: 0.5584 - val_loss: 0.7119 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6993 - acc: 0.5839 - val_loss: 0.7075 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.6771 - acc: 0.5997 - val_loss: 0.7348 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6594 - acc: 0.6059 - val_loss: 0.8154 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6417 - acc: 0.6275 - val_loss: 1.0565 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6304 - acc: 0.6355 - val_loss: 1.2674 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6153 - acc: 0.6506 - val_loss: 1.3447 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6018 - acc: 0.6642 - val_loss: 1.2615 - val_acc: 0.4880\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5962 - acc: 0.6704 - val_loss: 1.1096 - val_acc: 0.4908\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5900 - acc: 0.6761 - val_loss: 0.9309 - val_acc: 0.5012\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5814 - acc: 0.6776 - val_loss: 0.9583 - val_acc: 0.5012\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5698 - acc: 0.6916 - val_loss: 0.8755 - val_acc: 0.5088\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5618 - acc: 0.6945 - val_loss: 0.7980 - val_acc: 0.5344\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5550 - acc: 0.7015 - val_loss: 0.7496 - val_acc: 0.5504\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5496 - acc: 0.7061 - val_loss: 0.6405 - val_acc: 0.6136\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5425 - acc: 0.7159 - val_loss: 0.7665 - val_acc: 0.5544\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5377 - acc: 0.7169 - val_loss: 0.7689 - val_acc: 0.5596\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5340 - acc: 0.7201 - val_loss: 0.6146 - val_acc: 0.6392\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5257 - acc: 0.7268 - val_loss: 0.5776 - val_acc: 0.6740\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5214 - acc: 0.7298 - val_loss: 0.5621 - val_acc: 0.6932\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5122 - acc: 0.7365 - val_loss: 0.5487 - val_acc: 0.7176\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5067 - acc: 0.7394 - val_loss: 0.6105 - val_acc: 0.6460\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5067 - acc: 0.7426 - val_loss: 0.5967 - val_acc: 0.6504\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5007 - acc: 0.7455 - val_loss: 0.5688 - val_acc: 0.6832\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4990 - acc: 0.7452 - val_loss: 0.5149 - val_acc: 0.7532\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4952 - acc: 0.7495 - val_loss: 0.5292 - val_acc: 0.7352\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4843 - acc: 0.7514 - val_loss: 0.5733 - val_acc: 0.6844\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4839 - acc: 0.7559 - val_loss: 0.5257 - val_acc: 0.7428\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4823 - acc: 0.7572 - val_loss: 0.5112 - val_acc: 0.7588\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4782 - acc: 0.7609 - val_loss: 0.5092 - val_acc: 0.7540\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_66 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_151 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_153 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 54,402\n",
      "Trainable params: 53,122\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 238us/sample - loss: 0.7887 - acc: 0.5591 - val_loss: 0.7123 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.7046 - acc: 0.5845 - val_loss: 0.6898 - val_acc: 0.5040\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6723 - acc: 0.6030 - val_loss: 0.7372 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6514 - acc: 0.6202 - val_loss: 0.9428 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6339 - acc: 0.6344 - val_loss: 1.3551 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6227 - acc: 0.6456 - val_loss: 1.5357 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6075 - acc: 0.6581 - val_loss: 1.5712 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5954 - acc: 0.6735 - val_loss: 1.4604 - val_acc: 0.4880\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5846 - acc: 0.6804 - val_loss: 1.3483 - val_acc: 0.4884\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5765 - acc: 0.6869 - val_loss: 1.1355 - val_acc: 0.4924\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5681 - acc: 0.6918 - val_loss: 1.0066 - val_acc: 0.4996\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5594 - acc: 0.6981 - val_loss: 0.8246 - val_acc: 0.5236\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5519 - acc: 0.7045 - val_loss: 0.9610 - val_acc: 0.5164\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5482 - acc: 0.7108 - val_loss: 0.7484 - val_acc: 0.5544\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5402 - acc: 0.7142 - val_loss: 0.6589 - val_acc: 0.6044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5292 - acc: 0.7249 - val_loss: 0.7646 - val_acc: 0.5700\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5282 - acc: 0.7262 - val_loss: 0.7006 - val_acc: 0.5852\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5192 - acc: 0.7301 - val_loss: 0.6697 - val_acc: 0.6040\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5139 - acc: 0.7322 - val_loss: 0.6833 - val_acc: 0.6028\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5085 - acc: 0.7396 - val_loss: 0.6763 - val_acc: 0.6092\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5058 - acc: 0.7403 - val_loss: 0.5601 - val_acc: 0.7064\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4972 - acc: 0.7451 - val_loss: 0.6189 - val_acc: 0.6416\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4925 - acc: 0.7514 - val_loss: 0.6843 - val_acc: 0.6044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4882 - acc: 0.7562 - val_loss: 0.5478 - val_acc: 0.7208\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4792 - acc: 0.7586 - val_loss: 0.5522 - val_acc: 0.7152\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4764 - acc: 0.7612 - val_loss: 0.5797 - val_acc: 0.6800\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4657 - acc: 0.7699 - val_loss: 0.6203 - val_acc: 0.6488\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4675 - acc: 0.7674 - val_loss: 0.5271 - val_acc: 0.7292\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4593 - acc: 0.7742 - val_loss: 0.4961 - val_acc: 0.7708\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4577 - acc: 0.7729 - val_loss: 0.5010 - val_acc: 0.7656\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_67 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_154 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_155 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_156 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_157 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 466,242\n",
      "Trainable params: 462,322\n",
      "Non-trainable params: 3,920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 243us/sample - loss: 0.8821 - acc: 0.5611 - val_loss: 0.7290 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.7006 - acc: 0.6044 - val_loss: 0.6899 - val_acc: 0.5412\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6546 - acc: 0.6227 - val_loss: 0.7970 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.6312 - acc: 0.6459 - val_loss: 1.2642 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6164 - acc: 0.6534 - val_loss: 2.1253 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.6005 - acc: 0.6669 - val_loss: 2.7866 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5821 - acc: 0.6841 - val_loss: 2.9603 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5715 - acc: 0.6942 - val_loss: 2.6924 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5574 - acc: 0.7038 - val_loss: 2.5248 - val_acc: 0.4888\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5495 - acc: 0.7138 - val_loss: 2.2444 - val_acc: 0.4896\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5410 - acc: 0.7144 - val_loss: 2.1199 - val_acc: 0.4960\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5224 - acc: 0.7305 - val_loss: 2.0498 - val_acc: 0.4944\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5142 - acc: 0.7369 - val_loss: 1.9780 - val_acc: 0.5012\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5062 - acc: 0.7383 - val_loss: 1.7047 - val_acc: 0.5080\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4949 - acc: 0.7475 - val_loss: 1.2188 - val_acc: 0.5212\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4830 - acc: 0.7591 - val_loss: 1.5755 - val_acc: 0.5096\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4753 - acc: 0.7589 - val_loss: 1.2292 - val_acc: 0.5220\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4696 - acc: 0.7672 - val_loss: 0.9617 - val_acc: 0.5468\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4576 - acc: 0.7741 - val_loss: 1.2182 - val_acc: 0.5304\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4504 - acc: 0.7783 - val_loss: 0.6596 - val_acc: 0.6512\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4429 - acc: 0.7861 - val_loss: 0.9731 - val_acc: 0.5696\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4269 - acc: 0.7931 - val_loss: 1.2060 - val_acc: 0.5500\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4253 - acc: 0.7968 - val_loss: 0.7215 - val_acc: 0.6332\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4136 - acc: 0.8017 - val_loss: 0.5374 - val_acc: 0.7332\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4091 - acc: 0.8052 - val_loss: 0.7191 - val_acc: 0.6376\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4027 - acc: 0.8089 - val_loss: 0.5640 - val_acc: 0.7268\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3841 - acc: 0.8205 - val_loss: 0.5215 - val_acc: 0.7432\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3818 - acc: 0.8186 - val_loss: 0.5635 - val_acc: 0.7268\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3801 - acc: 0.8219 - val_loss: 0.5524 - val_acc: 0.7472\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3771 - acc: 0.8228 - val_loss: 0.6706 - val_acc: 0.6748\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_68 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_158 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_159 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_160 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_161 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_162 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_163 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,692\n",
      "Trainable params: 2,272\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 581us/sample - loss: 0.7806 - acc: 0.5476 - val_loss: 0.6658 - val_acc: 0.5956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6927 - acc: 0.5738 - val_loss: 0.6572 - val_acc: 0.6132\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6673 - acc: 0.5981 - val_loss: 0.6585 - val_acc: 0.5984\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6540 - acc: 0.6170 - val_loss: 0.6540 - val_acc: 0.6020\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6408 - acc: 0.6305 - val_loss: 0.6291 - val_acc: 0.6436\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6317 - acc: 0.6415 - val_loss: 0.6241 - val_acc: 0.6484\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.6201 - acc: 0.6515 - val_loss: 0.6263 - val_acc: 0.6420\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.6071 - acc: 0.6669 - val_loss: 0.5991 - val_acc: 0.6856\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6043 - acc: 0.6721 - val_loss: 0.6127 - val_acc: 0.6472\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.5911 - acc: 0.6802 - val_loss: 0.6041 - val_acc: 0.6596\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5860 - acc: 0.6842 - val_loss: 0.5789 - val_acc: 0.7076\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5771 - acc: 0.6925 - val_loss: 0.6054 - val_acc: 0.6304\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.5739 - acc: 0.6966 - val_loss: 0.5952 - val_acc: 0.6488\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5675 - acc: 0.7019 - val_loss: 0.5733 - val_acc: 0.7008\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5654 - acc: 0.7056 - val_loss: 0.5983 - val_acc: 0.6516\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5622 - acc: 0.7038 - val_loss: 0.6183 - val_acc: 0.6184\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5568 - acc: 0.7126 - val_loss: 0.5944 - val_acc: 0.6432\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5566 - acc: 0.7126 - val_loss: 0.6214 - val_acc: 0.6108\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5553 - acc: 0.7102 - val_loss: 0.6056 - val_acc: 0.6308\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5544 - acc: 0.7169 - val_loss: 0.6565 - val_acc: 0.5840\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5430 - acc: 0.7173 - val_loss: 0.6136 - val_acc: 0.6156\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 6s 359us/sample - loss: 0.5494 - acc: 0.7181 - val_loss: 0.6480 - val_acc: 0.5836\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 6s 359us/sample - loss: 0.5468 - acc: 0.7173 - val_loss: 0.6335 - val_acc: 0.6004\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.5414 - acc: 0.7245 - val_loss: 0.6581 - val_acc: 0.5784\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5414 - acc: 0.7221 - val_loss: 0.6041 - val_acc: 0.6284\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5430 - acc: 0.7219 - val_loss: 0.6370 - val_acc: 0.6024\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5393 - acc: 0.7288 - val_loss: 0.5689 - val_acc: 0.6724\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 6s 362us/sample - loss: 0.5420 - acc: 0.7247 - val_loss: 0.6021 - val_acc: 0.6260\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.5338 - acc: 0.7306 - val_loss: 0.6278 - val_acc: 0.6116\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5422 - acc: 0.7262 - val_loss: 0.5804 - val_acc: 0.6600\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_69 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_164 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_165 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_166 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_167 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_168 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_169 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 14,892\n",
      "Trainable params: 14,072\n",
      "Non-trainable params: 820\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 582us/sample - loss: 0.7448 - acc: 0.5578 - val_loss: 0.6556 - val_acc: 0.6076\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.6705 - acc: 0.5930 - val_loss: 0.6712 - val_acc: 0.5604\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6380 - acc: 0.6291 - val_loss: 0.7299 - val_acc: 0.5260\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6045 - acc: 0.6647 - val_loss: 0.5943 - val_acc: 0.6800\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5885 - acc: 0.6766 - val_loss: 0.6442 - val_acc: 0.6080\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5717 - acc: 0.6925 - val_loss: 0.6160 - val_acc: 0.6436\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.5557 - acc: 0.7017 - val_loss: 0.5907 - val_acc: 0.6600\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5508 - acc: 0.7078 - val_loss: 0.6041 - val_acc: 0.6500\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.5349 - acc: 0.7187 - val_loss: 0.5774 - val_acc: 0.6848\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5383 - acc: 0.7179 - val_loss: 0.6136 - val_acc: 0.6296\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.5283 - acc: 0.7269 - val_loss: 0.5941 - val_acc: 0.6600\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.5209 - acc: 0.7343 - val_loss: 0.7012 - val_acc: 0.6016\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.5132 - acc: 0.7354 - val_loss: 0.5918 - val_acc: 0.6644\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5183 - acc: 0.7303 - val_loss: 0.6107 - val_acc: 0.6320\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5087 - acc: 0.7438 - val_loss: 0.6061 - val_acc: 0.6564\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5049 - acc: 0.7422 - val_loss: 0.6685 - val_acc: 0.6076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.5009 - acc: 0.7476 - val_loss: 0.6254 - val_acc: 0.6264\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.5020 - acc: 0.7476 - val_loss: 0.6568 - val_acc: 0.6212\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.4937 - acc: 0.7526 - val_loss: 0.6669 - val_acc: 0.6112\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.4933 - acc: 0.7514 - val_loss: 0.6028 - val_acc: 0.6500\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.4887 - acc: 0.7559 - val_loss: 0.6475 - val_acc: 0.6244\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.4900 - acc: 0.7551 - val_loss: 0.7113 - val_acc: 0.5924\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.4829 - acc: 0.7618 - val_loss: 0.6626 - val_acc: 0.6144\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.4849 - acc: 0.7601 - val_loss: 0.6832 - val_acc: 0.6108\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.4839 - acc: 0.7580 - val_loss: 0.7045 - val_acc: 0.5988\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.4754 - acc: 0.7628 - val_loss: 0.6902 - val_acc: 0.6052\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.4780 - acc: 0.7638 - val_loss: 0.6802 - val_acc: 0.6092\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.4713 - acc: 0.7698 - val_loss: 0.6520 - val_acc: 0.6236\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.4670 - acc: 0.7718 - val_loss: 0.6863 - val_acc: 0.6184\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4698 - acc: 0.7692 - val_loss: 0.6373 - val_acc: 0.6408\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_70 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_170 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_171 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_172 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_173 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_174 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_175 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 39,142\n",
      "Trainable params: 37,822\n",
      "Non-trainable params: 1,320\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 584us/sample - loss: 0.7598 - acc: 0.5621 - val_loss: 0.6600 - val_acc: 0.6036\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 6s 358us/sample - loss: 0.6599 - acc: 0.6121 - val_loss: 0.8719 - val_acc: 0.4960\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.6205 - acc: 0.6501 - val_loss: 0.7073 - val_acc: 0.5440\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5951 - acc: 0.6758 - val_loss: 0.6823 - val_acc: 0.5788\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.5731 - acc: 0.6913 - val_loss: 0.6275 - val_acc: 0.6304\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.5661 - acc: 0.6969 - val_loss: 0.6578 - val_acc: 0.6008\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.5491 - acc: 0.7083 - val_loss: 0.6473 - val_acc: 0.6284\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.5409 - acc: 0.7175 - val_loss: 0.6232 - val_acc: 0.6308\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.5278 - acc: 0.7255 - val_loss: 0.6269 - val_acc: 0.6248\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.5238 - acc: 0.7292 - val_loss: 0.6730 - val_acc: 0.6100\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.5175 - acc: 0.7333 - val_loss: 0.6625 - val_acc: 0.6232\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.5111 - acc: 0.7385 - val_loss: 0.6548 - val_acc: 0.6240\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.5125 - acc: 0.7382 - val_loss: 0.7006 - val_acc: 0.6152\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.4972 - acc: 0.7501 - val_loss: 0.7462 - val_acc: 0.6032\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 6s 357us/sample - loss: 0.4921 - acc: 0.7557 - val_loss: 0.7776 - val_acc: 0.5788\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4902 - acc: 0.7521 - val_loss: 0.6875 - val_acc: 0.6176\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4828 - acc: 0.7614 - val_loss: 0.6117 - val_acc: 0.6584\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4786 - acc: 0.7646 - val_loss: 0.8059 - val_acc: 0.5944\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.4759 - acc: 0.7631 - val_loss: 0.7563 - val_acc: 0.5968\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4748 - acc: 0.7678 - val_loss: 0.6379 - val_acc: 0.6536\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4720 - acc: 0.7676 - val_loss: 0.7291 - val_acc: 0.6240\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 6s 353us/sample - loss: 0.4687 - acc: 0.7715 - val_loss: 0.7296 - val_acc: 0.6244\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4647 - acc: 0.7711 - val_loss: 0.7635 - val_acc: 0.5992\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 6s 353us/sample - loss: 0.4595 - acc: 0.7748 - val_loss: 0.7511 - val_acc: 0.5988\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4528 - acc: 0.7812 - val_loss: 0.6325 - val_acc: 0.6620\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 6s 356us/sample - loss: 0.4510 - acc: 0.7833 - val_loss: 0.6886 - val_acc: 0.6388\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.4468 - acc: 0.7841 - val_loss: 0.8109 - val_acc: 0.5860\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.4542 - acc: 0.7810 - val_loss: 0.8220 - val_acc: 0.5820\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 6s 354us/sample - loss: 0.4483 - acc: 0.7869 - val_loss: 0.6163 - val_acc: 0.6864\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 6s 355us/sample - loss: 0.4456 - acc: 0.7855 - val_loss: 0.7962 - val_acc: 0.5968\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_71 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_176 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_177 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_178 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_179 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_180 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_181 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 81,442\n",
      "Trainable params: 79,522\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 604us/sample - loss: 0.7563 - acc: 0.5702 - val_loss: 0.6721 - val_acc: 0.5568\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 6s 369us/sample - loss: 0.6486 - acc: 0.6215 - val_loss: 0.9011 - val_acc: 0.5032\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.6097 - acc: 0.6584 - val_loss: 0.7309 - val_acc: 0.5500\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.5830 - acc: 0.6827 - val_loss: 0.6106 - val_acc: 0.6668\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.5650 - acc: 0.6955 - val_loss: 0.6534 - val_acc: 0.6176\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.5512 - acc: 0.7072 - val_loss: 0.6807 - val_acc: 0.6200\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.5430 - acc: 0.7161 - val_loss: 0.6456 - val_acc: 0.6216\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 6s 369us/sample - loss: 0.5365 - acc: 0.7192 - val_loss: 0.6132 - val_acc: 0.6556\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.5192 - acc: 0.7321 - val_loss: 0.5748 - val_acc: 0.6956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 6s 369us/sample - loss: 0.5168 - acc: 0.7358 - val_loss: 0.7289 - val_acc: 0.6016\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.5088 - acc: 0.7382 - val_loss: 0.6100 - val_acc: 0.6620\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.5008 - acc: 0.7442 - val_loss: 0.6578 - val_acc: 0.6420\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4964 - acc: 0.7516 - val_loss: 0.8173 - val_acc: 0.5572\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4921 - acc: 0.7521 - val_loss: 0.6272 - val_acc: 0.6552\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 6s 366us/sample - loss: 0.4879 - acc: 0.7534 - val_loss: 0.7072 - val_acc: 0.6028\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4806 - acc: 0.7606 - val_loss: 0.6798 - val_acc: 0.6396\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 6s 366us/sample - loss: 0.4766 - acc: 0.7641 - val_loss: 0.6582 - val_acc: 0.6376\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4722 - acc: 0.7659 - val_loss: 0.7608 - val_acc: 0.6052\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.4661 - acc: 0.7694 - val_loss: 0.9908 - val_acc: 0.5696\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4570 - acc: 0.7750 - val_loss: 0.7936 - val_acc: 0.6008\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.4562 - acc: 0.7743 - val_loss: 0.8727 - val_acc: 0.5880\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4522 - acc: 0.7784 - val_loss: 0.7284 - val_acc: 0.6220\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4426 - acc: 0.7848 - val_loss: 0.8350 - val_acc: 0.5980\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.4434 - acc: 0.7873 - val_loss: 0.7930 - val_acc: 0.5956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 6s 366us/sample - loss: 0.4402 - acc: 0.7855 - val_loss: 0.7350 - val_acc: 0.6104\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 6s 368us/sample - loss: 0.4334 - acc: 0.7919 - val_loss: 0.8472 - val_acc: 0.6100\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4332 - acc: 0.7922 - val_loss: 0.8182 - val_acc: 0.6164\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4246 - acc: 0.7964 - val_loss: 0.7861 - val_acc: 0.6372\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4188 - acc: 0.8012 - val_loss: 0.7211 - val_acc: 0.6408\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 6s 367us/sample - loss: 0.4167 - acc: 0.7975 - val_loss: 0.7243 - val_acc: 0.6488\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_72 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_182 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_183 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_184 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_185 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_186 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_187 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 831,642\n",
      "Trainable params: 825,322\n",
      "Non-trainable params: 6,320\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 621us/sample - loss: 0.7769 - acc: 0.5868 - val_loss: 1.0371 - val_acc: 0.4972\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.6416 - acc: 0.6398 - val_loss: 1.8569 - val_acc: 0.4912\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.6007 - acc: 0.6665 - val_loss: 1.2613 - val_acc: 0.5160\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.5848 - acc: 0.6870 - val_loss: 0.6386 - val_acc: 0.6240\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.5750 - acc: 0.6904 - val_loss: 0.5606 - val_acc: 0.7132\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 6s 369us/sample - loss: 0.5530 - acc: 0.7081 - val_loss: 0.6908 - val_acc: 0.6384\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 6s 372us/sample - loss: 0.5454 - acc: 0.7119 - val_loss: 0.6510 - val_acc: 0.6268\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.5288 - acc: 0.7298 - val_loss: 0.6727 - val_acc: 0.6200\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.5217 - acc: 0.7318 - val_loss: 0.6157 - val_acc: 0.6788\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.5122 - acc: 0.7363 - val_loss: 0.6565 - val_acc: 0.6300\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 6s 369us/sample - loss: 0.5037 - acc: 0.7461 - val_loss: 0.6917 - val_acc: 0.6288\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.4922 - acc: 0.7505 - val_loss: 0.5942 - val_acc: 0.6988\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.4901 - acc: 0.7577 - val_loss: 0.6271 - val_acc: 0.6664\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 6s 369us/sample - loss: 0.4812 - acc: 0.7541 - val_loss: 0.7275 - val_acc: 0.6224\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.4715 - acc: 0.7674 - val_loss: 0.7788 - val_acc: 0.6256\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.4652 - acc: 0.7731 - val_loss: 0.7172 - val_acc: 0.6452\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.4506 - acc: 0.7815 - val_loss: 0.8696 - val_acc: 0.5872\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.4471 - acc: 0.7844 - val_loss: 0.7755 - val_acc: 0.6244\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.4377 - acc: 0.7865 - val_loss: 0.6120 - val_acc: 0.7028\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.4362 - acc: 0.7869 - val_loss: 0.8128 - val_acc: 0.6140\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 6s 369us/sample - loss: 0.4211 - acc: 0.7977 - val_loss: 0.7981 - val_acc: 0.6372\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 6s 373us/sample - loss: 0.4121 - acc: 0.8022 - val_loss: 0.9647 - val_acc: 0.6168\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.4096 - acc: 0.8021 - val_loss: 0.8650 - val_acc: 0.6168\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.4108 - acc: 0.8018 - val_loss: 0.8528 - val_acc: 0.6180\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.4083 - acc: 0.8068 - val_loss: 0.7620 - val_acc: 0.6380\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 6s 372us/sample - loss: 0.3941 - acc: 0.8142 - val_loss: 1.0888 - val_acc: 0.5836\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.3904 - acc: 0.8168 - val_loss: 0.9589 - val_acc: 0.6012\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.3868 - acc: 0.8165 - val_loss: 0.8939 - val_acc: 0.6192\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 6s 370us/sample - loss: 0.3841 - acc: 0.8191 - val_loss: 1.0005 - val_acc: 0.5992\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.3772 - acc: 0.8243 - val_loss: 0.8694 - val_acc: 0.6364\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_73 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_188 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_189 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_190 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_191 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_192 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_193 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,692\n",
      "Trainable params: 2,272\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 442us/sample - loss: 0.8013 - acc: 0.5461 - val_loss: 0.6852 - val_acc: 0.5592\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.7067 - acc: 0.5641 - val_loss: 0.6622 - val_acc: 0.6072\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 188us/sample - loss: 0.6785 - acc: 0.5845 - val_loss: 0.6544 - val_acc: 0.6104\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6659 - acc: 0.5956 - val_loss: 0.6513 - val_acc: 0.6068\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6553 - acc: 0.6092 - val_loss: 0.6472 - val_acc: 0.6172\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6424 - acc: 0.6247 - val_loss: 0.6366 - val_acc: 0.6376\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6282 - acc: 0.6405 - val_loss: 0.6240 - val_acc: 0.6576\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6220 - acc: 0.6475 - val_loss: 0.6144 - val_acc: 0.6552\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6156 - acc: 0.6554 - val_loss: 0.6131 - val_acc: 0.6684\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6143 - acc: 0.6571 - val_loss: 0.6144 - val_acc: 0.6688\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6041 - acc: 0.6645 - val_loss: 0.6014 - val_acc: 0.6680\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.5944 - acc: 0.6799 - val_loss: 0.6124 - val_acc: 0.6500\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.5936 - acc: 0.6770 - val_loss: 0.6105 - val_acc: 0.6448\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5866 - acc: 0.6834 - val_loss: 0.6013 - val_acc: 0.6600\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5856 - acc: 0.6813 - val_loss: 0.5726 - val_acc: 0.6996\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.5758 - acc: 0.6906 - val_loss: 0.5706 - val_acc: 0.7136\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.5755 - acc: 0.6918 - val_loss: 0.5799 - val_acc: 0.6888\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.5675 - acc: 0.6997 - val_loss: 0.5758 - val_acc: 0.6932\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.5665 - acc: 0.7012 - val_loss: 0.5791 - val_acc: 0.6912\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5623 - acc: 0.7031 - val_loss: 0.5870 - val_acc: 0.6752\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5598 - acc: 0.7099 - val_loss: 0.5691 - val_acc: 0.7012\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5565 - acc: 0.7085 - val_loss: 0.5680 - val_acc: 0.7016\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5543 - acc: 0.7104 - val_loss: 0.5980 - val_acc: 0.6344\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5558 - acc: 0.7110 - val_loss: 0.5961 - val_acc: 0.6424\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5513 - acc: 0.7116 - val_loss: 0.5622 - val_acc: 0.7132\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.5448 - acc: 0.7188 - val_loss: 0.5846 - val_acc: 0.6676\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.5478 - acc: 0.7165 - val_loss: 0.5603 - val_acc: 0.7032\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.5440 - acc: 0.7202 - val_loss: 0.5660 - val_acc: 0.6900\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.5470 - acc: 0.7151 - val_loss: 0.6112 - val_acc: 0.6184\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.5410 - acc: 0.7164 - val_loss: 0.5501 - val_acc: 0.7292\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_74 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_194 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_195 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_196 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_197 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_198 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_199 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 14,892\n",
      "Trainable params: 14,072\n",
      "Non-trainable params: 820\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 456us/sample - loss: 0.7567 - acc: 0.5556 - val_loss: 0.6817 - val_acc: 0.5960\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6888 - acc: 0.5812 - val_loss: 0.6641 - val_acc: 0.5956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6673 - acc: 0.5924 - val_loss: 0.6864 - val_acc: 0.5248\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6424 - acc: 0.6246 - val_loss: 0.7954 - val_acc: 0.4976\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6206 - acc: 0.6438 - val_loss: 0.7812 - val_acc: 0.5228\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6011 - acc: 0.6662 - val_loss: 0.6502 - val_acc: 0.5856\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.5866 - acc: 0.6774 - val_loss: 0.5928 - val_acc: 0.6800\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.5709 - acc: 0.6928 - val_loss: 0.6363 - val_acc: 0.6044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.5655 - acc: 0.6951 - val_loss: 0.5806 - val_acc: 0.6928\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.5550 - acc: 0.7034 - val_loss: 0.5997 - val_acc: 0.6476\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.5465 - acc: 0.7115 - val_loss: 0.5925 - val_acc: 0.6628\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.5408 - acc: 0.7141 - val_loss: 0.5681 - val_acc: 0.6904\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.5347 - acc: 0.7244 - val_loss: 0.5541 - val_acc: 0.7232\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.5226 - acc: 0.7298 - val_loss: 0.5891 - val_acc: 0.6648\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.5181 - acc: 0.7349 - val_loss: 0.6205 - val_acc: 0.6412\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.5112 - acc: 0.7388 - val_loss: 0.5576 - val_acc: 0.7016\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.5073 - acc: 0.7406 - val_loss: 0.5722 - val_acc: 0.6852\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.5025 - acc: 0.7448 - val_loss: 0.6668 - val_acc: 0.6060\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.5013 - acc: 0.7470 - val_loss: 0.5276 - val_acc: 0.7488\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.4987 - acc: 0.7481 - val_loss: 0.5744 - val_acc: 0.6872\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.4926 - acc: 0.7538 - val_loss: 0.5317 - val_acc: 0.7332\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.4890 - acc: 0.7515 - val_loss: 0.5454 - val_acc: 0.7096\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.4875 - acc: 0.7541 - val_loss: 0.5196 - val_acc: 0.7424\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.4800 - acc: 0.7596 - val_loss: 0.5523 - val_acc: 0.7052\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.4789 - acc: 0.7622 - val_loss: 0.6245 - val_acc: 0.6500\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.4757 - acc: 0.7642 - val_loss: 0.6342 - val_acc: 0.6424\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.4734 - acc: 0.7648 - val_loss: 0.5647 - val_acc: 0.7008\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.4695 - acc: 0.7697 - val_loss: 0.6169 - val_acc: 0.6396\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.4623 - acc: 0.7726 - val_loss: 0.5592 - val_acc: 0.7092\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.4575 - acc: 0.7767 - val_loss: 0.5412 - val_acc: 0.7192\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_75 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_200 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_201 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_202 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_203 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_204 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_205 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 39,142\n",
      "Trainable params: 37,822\n",
      "Non-trainable params: 1,320\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 468us/sample - loss: 0.7552 - acc: 0.5581 - val_loss: 0.7118 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6740 - acc: 0.5950 - val_loss: 0.8303 - val_acc: 0.4900\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6418 - acc: 0.6282 - val_loss: 0.9154 - val_acc: 0.4960\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6149 - acc: 0.6520 - val_loss: 0.7789 - val_acc: 0.5228\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.5898 - acc: 0.6763 - val_loss: 0.7288 - val_acc: 0.5564\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.5740 - acc: 0.6899 - val_loss: 0.6923 - val_acc: 0.5804\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 200us/sample - loss: 0.5577 - acc: 0.6972 - val_loss: 0.6274 - val_acc: 0.6264\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.5478 - acc: 0.7134 - val_loss: 0.6567 - val_acc: 0.6048\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.5351 - acc: 0.7222 - val_loss: 0.5944 - val_acc: 0.6728\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.5251 - acc: 0.7284 - val_loss: 0.6358 - val_acc: 0.6452\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.5207 - acc: 0.7294 - val_loss: 0.5960 - val_acc: 0.6680\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.5153 - acc: 0.7336 - val_loss: 0.5401 - val_acc: 0.7268\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.5094 - acc: 0.7416 - val_loss: 0.6364 - val_acc: 0.6560\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.5102 - acc: 0.7388 - val_loss: 0.6302 - val_acc: 0.6484\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4945 - acc: 0.7551 - val_loss: 0.5654 - val_acc: 0.6948\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.4975 - acc: 0.7512 - val_loss: 0.5388 - val_acc: 0.7240\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.4843 - acc: 0.7572 - val_loss: 0.5251 - val_acc: 0.7420\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4888 - acc: 0.7553 - val_loss: 0.6467 - val_acc: 0.6420\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.4805 - acc: 0.7613 - val_loss: 0.6326 - val_acc: 0.6492\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4783 - acc: 0.7588 - val_loss: 0.5767 - val_acc: 0.6800\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.4731 - acc: 0.7648 - val_loss: 0.5525 - val_acc: 0.7148\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.4679 - acc: 0.7691 - val_loss: 0.5364 - val_acc: 0.7272\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4601 - acc: 0.7747 - val_loss: 0.6314 - val_acc: 0.6580\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4549 - acc: 0.7792 - val_loss: 0.5786 - val_acc: 0.6940\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.4547 - acc: 0.7755 - val_loss: 0.5709 - val_acc: 0.6996\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4497 - acc: 0.7783 - val_loss: 0.6354 - val_acc: 0.6568\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4553 - acc: 0.7788 - val_loss: 0.5109 - val_acc: 0.7508\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4407 - acc: 0.7870 - val_loss: 0.5311 - val_acc: 0.7284\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.4383 - acc: 0.7888 - val_loss: 0.6055 - val_acc: 0.6864\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.4369 - acc: 0.7881 - val_loss: 0.5907 - val_acc: 0.6892\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_76 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_206 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_207 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_208 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_209 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_210 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_211 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 81,442\n",
      "Trainable params: 79,522\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 477us/sample - loss: 0.7594 - acc: 0.5685 - val_loss: 0.6927 - val_acc: 0.4908\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.6740 - acc: 0.6011 - val_loss: 0.8597 - val_acc: 0.4920\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.6352 - acc: 0.6309 - val_loss: 1.1070 - val_acc: 0.4892\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.6023 - acc: 0.6624 - val_loss: 1.0228 - val_acc: 0.5152\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.5797 - acc: 0.6885 - val_loss: 0.8107 - val_acc: 0.5404\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.5678 - acc: 0.6948 - val_loss: 0.6615 - val_acc: 0.6140\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.5517 - acc: 0.7068 - val_loss: 0.6724 - val_acc: 0.6016\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.5375 - acc: 0.7176 - val_loss: 0.6094 - val_acc: 0.6632\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.5293 - acc: 0.7256 - val_loss: 0.5403 - val_acc: 0.7308\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.5177 - acc: 0.7372 - val_loss: 0.7163 - val_acc: 0.5948\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.5129 - acc: 0.7389 - val_loss: 0.5980 - val_acc: 0.6736\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.5048 - acc: 0.7424 - val_loss: 0.5934 - val_acc: 0.6728\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.4984 - acc: 0.7488 - val_loss: 0.7187 - val_acc: 0.5976\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4934 - acc: 0.7500 - val_loss: 0.5756 - val_acc: 0.6912\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4841 - acc: 0.7565 - val_loss: 0.5414 - val_acc: 0.7368\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.4793 - acc: 0.7612 - val_loss: 0.6067 - val_acc: 0.6620\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4697 - acc: 0.7664 - val_loss: 0.5700 - val_acc: 0.7024\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4657 - acc: 0.7682 - val_loss: 0.5242 - val_acc: 0.7424\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4638 - acc: 0.7699 - val_loss: 0.5770 - val_acc: 0.6956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.4570 - acc: 0.7732 - val_loss: 0.5620 - val_acc: 0.7092\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4549 - acc: 0.7778 - val_loss: 0.6502 - val_acc: 0.6484\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.4446 - acc: 0.7841 - val_loss: 0.6075 - val_acc: 0.6776\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4387 - acc: 0.7870 - val_loss: 0.6707 - val_acc: 0.6508\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.4340 - acc: 0.7925 - val_loss: 0.4978 - val_acc: 0.7556\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.4304 - acc: 0.7899 - val_loss: 0.5908 - val_acc: 0.6848\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 4s 214us/sample - loss: 0.4246 - acc: 0.7949 - val_loss: 0.5060 - val_acc: 0.7444\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4218 - acc: 0.7971 - val_loss: 0.5847 - val_acc: 0.6952\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.4127 - acc: 0.8031 - val_loss: 0.6286 - val_acc: 0.6772\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.4069 - acc: 0.8064 - val_loss: 0.6548 - val_acc: 0.6724\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 200us/sample - loss: 0.4058 - acc: 0.8096 - val_loss: 0.6399 - val_acc: 0.6804\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_77 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_212 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_213 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_214 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_215 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_216 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_217 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 831,642\n",
      "Trainable params: 825,322\n",
      "Non-trainable params: 6,320\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 486us/sample - loss: 0.8110 - acc: 0.5793 - val_loss: 0.8472 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.6618 - acc: 0.6205 - val_loss: 1.8592 - val_acc: 0.4892\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 204us/sample - loss: 0.6133 - acc: 0.6628 - val_loss: 2.2601 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.5839 - acc: 0.6813 - val_loss: 1.5094 - val_acc: 0.5116\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.5674 - acc: 0.6995 - val_loss: 1.1617 - val_acc: 0.5304\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.5493 - acc: 0.7119 - val_loss: 0.9286 - val_acc: 0.5464\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.5327 - acc: 0.7182 - val_loss: 0.7228 - val_acc: 0.5916\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.5180 - acc: 0.7362 - val_loss: 0.7818 - val_acc: 0.6000\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.5089 - acc: 0.7404 - val_loss: 0.5445 - val_acc: 0.7296\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 204us/sample - loss: 0.4980 - acc: 0.7485 - val_loss: 0.5595 - val_acc: 0.7200\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.4841 - acc: 0.7569 - val_loss: 0.6586 - val_acc: 0.6512\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 204us/sample - loss: 0.4704 - acc: 0.7652 - val_loss: 0.6514 - val_acc: 0.6632\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.4705 - acc: 0.7687 - val_loss: 0.6093 - val_acc: 0.6832\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.4546 - acc: 0.7781 - val_loss: 0.6522 - val_acc: 0.6644\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.4477 - acc: 0.7800 - val_loss: 0.5325 - val_acc: 0.7412\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.4404 - acc: 0.7839 - val_loss: 0.7449 - val_acc: 0.6328\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.4328 - acc: 0.7895 - val_loss: 0.5996 - val_acc: 0.6932\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.4277 - acc: 0.7952 - val_loss: 0.6296 - val_acc: 0.6744\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.4185 - acc: 0.7969 - val_loss: 0.5725 - val_acc: 0.7212\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.4116 - acc: 0.8009 - val_loss: 0.5137 - val_acc: 0.7580\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.3985 - acc: 0.8116 - val_loss: 0.6758 - val_acc: 0.6772\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.3938 - acc: 0.8131 - val_loss: 0.5723 - val_acc: 0.7248\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 204us/sample - loss: 0.3895 - acc: 0.8154 - val_loss: 0.6770 - val_acc: 0.6800\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.3816 - acc: 0.8233 - val_loss: 0.6382 - val_acc: 0.7080\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.3793 - acc: 0.8256 - val_loss: 0.5938 - val_acc: 0.7100\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 201us/sample - loss: 0.3717 - acc: 0.8290 - val_loss: 0.6928 - val_acc: 0.6808\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.3630 - acc: 0.8314 - val_loss: 0.6133 - val_acc: 0.7192\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.3561 - acc: 0.8314 - val_loss: 0.6373 - val_acc: 0.6972\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 204us/sample - loss: 0.3552 - acc: 0.8338 - val_loss: 0.6194 - val_acc: 0.7184\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.3411 - acc: 0.8409 - val_loss: 0.6510 - val_acc: 0.7104\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_78 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_218 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_219 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_220 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_221 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_222 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_223 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,692\n",
      "Trainable params: 2,272\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 401us/sample - loss: 0.7128 - acc: 0.5565 - val_loss: 0.7053 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6771 - acc: 0.5738 - val_loss: 0.6822 - val_acc: 0.5564\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6714 - acc: 0.5884 - val_loss: 0.6680 - val_acc: 0.6120\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6647 - acc: 0.5964 - val_loss: 0.6640 - val_acc: 0.5988\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6609 - acc: 0.6062 - val_loss: 0.6628 - val_acc: 0.5964\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6545 - acc: 0.6136 - val_loss: 0.6754 - val_acc: 0.5464\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6451 - acc: 0.6244 - val_loss: 0.6612 - val_acc: 0.5764\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6370 - acc: 0.6314 - val_loss: 0.6589 - val_acc: 0.5700\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6290 - acc: 0.6435 - val_loss: 0.6411 - val_acc: 0.6224\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6254 - acc: 0.6473 - val_loss: 0.6377 - val_acc: 0.6144\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6177 - acc: 0.6519 - val_loss: 0.6153 - val_acc: 0.6620\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6121 - acc: 0.6609 - val_loss: 0.6084 - val_acc: 0.6684\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6065 - acc: 0.6649 - val_loss: 0.6018 - val_acc: 0.6700\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5987 - acc: 0.6736 - val_loss: 0.6108 - val_acc: 0.6624\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5975 - acc: 0.6745 - val_loss: 0.5912 - val_acc: 0.6864\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5926 - acc: 0.6806 - val_loss: 0.5895 - val_acc: 0.6944\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5881 - acc: 0.6844 - val_loss: 0.5837 - val_acc: 0.7016\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5833 - acc: 0.6835 - val_loss: 0.5819 - val_acc: 0.6988\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5819 - acc: 0.6894 - val_loss: 0.5737 - val_acc: 0.7100\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5796 - acc: 0.6888 - val_loss: 0.5836 - val_acc: 0.6944\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5723 - acc: 0.7000 - val_loss: 0.5736 - val_acc: 0.6880\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5703 - acc: 0.6977 - val_loss: 0.5838 - val_acc: 0.6800\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5740 - acc: 0.6973 - val_loss: 0.5832 - val_acc: 0.6860\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5703 - acc: 0.6979 - val_loss: 0.5642 - val_acc: 0.7112\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5649 - acc: 0.7015 - val_loss: 0.5709 - val_acc: 0.7064\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5606 - acc: 0.7037 - val_loss: 0.5842 - val_acc: 0.6780\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5636 - acc: 0.7045 - val_loss: 0.5540 - val_acc: 0.7120\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5590 - acc: 0.7078 - val_loss: 0.5583 - val_acc: 0.7148\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5577 - acc: 0.7042 - val_loss: 0.5572 - val_acc: 0.7288\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5539 - acc: 0.7080 - val_loss: 0.5505 - val_acc: 0.7312\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_79 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_224 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_225 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_226 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_227 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_228 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_229 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 14,892\n",
      "Trainable params: 14,072\n",
      "Non-trainable params: 820\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 415us/sample - loss: 0.7734 - acc: 0.5478 - val_loss: 0.7027 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.7040 - acc: 0.5729 - val_loss: 0.6896 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6812 - acc: 0.5818 - val_loss: 0.6872 - val_acc: 0.4988\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6587 - acc: 0.6072 - val_loss: 0.7568 - val_acc: 0.4928\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6410 - acc: 0.6252 - val_loss: 0.8844 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6221 - acc: 0.6441 - val_loss: 0.9254 - val_acc: 0.4912\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6051 - acc: 0.6572 - val_loss: 0.9288 - val_acc: 0.4964\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5911 - acc: 0.6718 - val_loss: 0.6973 - val_acc: 0.5568\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5786 - acc: 0.6858 - val_loss: 0.6465 - val_acc: 0.5860\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5654 - acc: 0.6894 - val_loss: 0.6330 - val_acc: 0.6044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5573 - acc: 0.7015 - val_loss: 0.6356 - val_acc: 0.6032\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5500 - acc: 0.7092 - val_loss: 0.6022 - val_acc: 0.6388\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5406 - acc: 0.7115 - val_loss: 0.6289 - val_acc: 0.6116\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5361 - acc: 0.7165 - val_loss: 0.6233 - val_acc: 0.6124\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5300 - acc: 0.7230 - val_loss: 0.6148 - val_acc: 0.6360\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5284 - acc: 0.7249 - val_loss: 0.5298 - val_acc: 0.7416\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5202 - acc: 0.7352 - val_loss: 0.5726 - val_acc: 0.6820\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5162 - acc: 0.7331 - val_loss: 0.5211 - val_acc: 0.7488\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5081 - acc: 0.7421 - val_loss: 0.5839 - val_acc: 0.6656\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5027 - acc: 0.7430 - val_loss: 0.5339 - val_acc: 0.7268\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5035 - acc: 0.7427 - val_loss: 0.6448 - val_acc: 0.6200\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4969 - acc: 0.7484 - val_loss: 0.5393 - val_acc: 0.7288\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4958 - acc: 0.7525 - val_loss: 0.5169 - val_acc: 0.7556\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4883 - acc: 0.7554 - val_loss: 0.5403 - val_acc: 0.7284\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4875 - acc: 0.7567 - val_loss: 0.5274 - val_acc: 0.7296\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4816 - acc: 0.7596 - val_loss: 0.5915 - val_acc: 0.6688\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4799 - acc: 0.7582 - val_loss: 0.5237 - val_acc: 0.7388\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4790 - acc: 0.7622 - val_loss: 0.5379 - val_acc: 0.7192\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4780 - acc: 0.7602 - val_loss: 0.5222 - val_acc: 0.7380\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4735 - acc: 0.7665 - val_loss: 0.5126 - val_acc: 0.7488\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_80 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_230 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_231 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_232 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_233 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_234 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_235 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 39,142\n",
      "Trainable params: 37,822\n",
      "Non-trainable params: 1,320\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 424us/sample - loss: 0.7869 - acc: 0.5559 - val_loss: 0.6918 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.7097 - acc: 0.5782 - val_loss: 0.7003 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6720 - acc: 0.5981 - val_loss: 0.7817 - val_acc: 0.4876\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6503 - acc: 0.6188 - val_loss: 1.1407 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6279 - acc: 0.6373 - val_loss: 1.1413 - val_acc: 0.4880\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6119 - acc: 0.6573 - val_loss: 1.0976 - val_acc: 0.4972\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5923 - acc: 0.6729 - val_loss: 0.9538 - val_acc: 0.5040\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5803 - acc: 0.6835 - val_loss: 0.6840 - val_acc: 0.5680\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5681 - acc: 0.6908 - val_loss: 0.6565 - val_acc: 0.5912\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5575 - acc: 0.7027 - val_loss: 0.7687 - val_acc: 0.5524\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5513 - acc: 0.7051 - val_loss: 0.6912 - val_acc: 0.5772\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5391 - acc: 0.7149 - val_loss: 0.6111 - val_acc: 0.6472\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5325 - acc: 0.7219 - val_loss: 0.5457 - val_acc: 0.7280\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5244 - acc: 0.7320 - val_loss: 0.5834 - val_acc: 0.6752\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5167 - acc: 0.7353 - val_loss: 0.5852 - val_acc: 0.6712\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5095 - acc: 0.7398 - val_loss: 0.6844 - val_acc: 0.6128\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5043 - acc: 0.7453 - val_loss: 0.5827 - val_acc: 0.6828\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4988 - acc: 0.7471 - val_loss: 0.6842 - val_acc: 0.6176\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4908 - acc: 0.7551 - val_loss: 0.5266 - val_acc: 0.7388\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4861 - acc: 0.7568 - val_loss: 0.6173 - val_acc: 0.6616\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4825 - acc: 0.7622 - val_loss: 0.5139 - val_acc: 0.7432\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4777 - acc: 0.7608 - val_loss: 0.6053 - val_acc: 0.6640\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4734 - acc: 0.7653 - val_loss: 0.5699 - val_acc: 0.7040\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4718 - acc: 0.7649 - val_loss: 0.5605 - val_acc: 0.7104\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4651 - acc: 0.7703 - val_loss: 0.5360 - val_acc: 0.7224\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4637 - acc: 0.7716 - val_loss: 0.6177 - val_acc: 0.6644\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4518 - acc: 0.7803 - val_loss: 0.5723 - val_acc: 0.6940\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4539 - acc: 0.7776 - val_loss: 0.4984 - val_acc: 0.7604\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4457 - acc: 0.7839 - val_loss: 0.5103 - val_acc: 0.7484\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4460 - acc: 0.7839 - val_loss: 0.7701 - val_acc: 0.6072\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_81 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_236 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_237 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_238 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_206 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_239 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_240 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_241 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 81,442\n",
      "Trainable params: 79,522\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 434us/sample - loss: 0.7715 - acc: 0.5571 - val_loss: 0.6908 - val_acc: 0.5120\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6978 - acc: 0.5816 - val_loss: 0.7181 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6570 - acc: 0.6119 - val_loss: 0.9156 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6366 - acc: 0.6326 - val_loss: 1.4444 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6206 - acc: 0.6445 - val_loss: 1.3405 - val_acc: 0.4888\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6019 - acc: 0.6626 - val_loss: 1.0656 - val_acc: 0.4904\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5840 - acc: 0.6812 - val_loss: 1.0337 - val_acc: 0.5032\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5680 - acc: 0.6929 - val_loss: 0.8861 - val_acc: 0.5248\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5566 - acc: 0.7005 - val_loss: 0.7502 - val_acc: 0.5576\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5448 - acc: 0.7114 - val_loss: 0.7595 - val_acc: 0.5656\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5347 - acc: 0.7205 - val_loss: 0.6445 - val_acc: 0.6208\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5247 - acc: 0.7276 - val_loss: 0.6458 - val_acc: 0.6180\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.5118 - acc: 0.7380 - val_loss: 0.6120 - val_acc: 0.6568\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.5040 - acc: 0.7417 - val_loss: 0.5194 - val_acc: 0.7452\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4938 - acc: 0.7493 - val_loss: 0.7714 - val_acc: 0.5840\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4850 - acc: 0.7574 - val_loss: 0.5340 - val_acc: 0.7276\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4825 - acc: 0.7560 - val_loss: 0.5294 - val_acc: 0.7368\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4714 - acc: 0.7640 - val_loss: 0.5702 - val_acc: 0.7056\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.4653 - acc: 0.7709 - val_loss: 0.5006 - val_acc: 0.7628\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4636 - acc: 0.7725 - val_loss: 0.6979 - val_acc: 0.6228\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4495 - acc: 0.7822 - val_loss: 0.5049 - val_acc: 0.7580\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4528 - acc: 0.7805 - val_loss: 0.5244 - val_acc: 0.7476\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4439 - acc: 0.7821 - val_loss: 0.4980 - val_acc: 0.7500\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4376 - acc: 0.7871 - val_loss: 0.5228 - val_acc: 0.7524\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4361 - acc: 0.7870 - val_loss: 0.6577 - val_acc: 0.6604\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4351 - acc: 0.7892 - val_loss: 0.6558 - val_acc: 0.6620\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4285 - acc: 0.7966 - val_loss: 0.5145 - val_acc: 0.7456\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4237 - acc: 0.7983 - val_loss: 0.7187 - val_acc: 0.6312\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4184 - acc: 0.7989 - val_loss: 0.4779 - val_acc: 0.7852\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4102 - acc: 0.8042 - val_loss: 0.4751 - val_acc: 0.7860\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_82 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_242 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_243 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_244 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_245 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_246 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_247 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 831,642\n",
      "Trainable params: 825,322\n",
      "Non-trainable params: 6,320\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 445us/sample - loss: 0.8524 - acc: 0.5692 - val_loss: 0.7069 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6826 - acc: 0.6123 - val_loss: 1.0461 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6304 - acc: 0.6503 - val_loss: 2.4443 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6016 - acc: 0.6716 - val_loss: 2.9474 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.5787 - acc: 0.6874 - val_loss: 2.9173 - val_acc: 0.4880\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.5581 - acc: 0.7068 - val_loss: 2.7119 - val_acc: 0.4912\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.5447 - acc: 0.7176 - val_loss: 2.1996 - val_acc: 0.4944\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.5262 - acc: 0.7266 - val_loss: 1.4552 - val_acc: 0.5124\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.5109 - acc: 0.7381 - val_loss: 1.6709 - val_acc: 0.5168\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.5028 - acc: 0.7411 - val_loss: 1.0802 - val_acc: 0.5568\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.4855 - acc: 0.7579 - val_loss: 0.6580 - val_acc: 0.6360\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.4758 - acc: 0.7629 - val_loss: 0.6703 - val_acc: 0.6432\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.4677 - acc: 0.7679 - val_loss: 0.7593 - val_acc: 0.6168\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.4620 - acc: 0.7715 - val_loss: 0.5782 - val_acc: 0.7092\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.4517 - acc: 0.7779 - val_loss: 0.6743 - val_acc: 0.6548\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.4490 - acc: 0.7809 - val_loss: 0.7297 - val_acc: 0.6432\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.4273 - acc: 0.7921 - val_loss: 0.5105 - val_acc: 0.7564\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.4197 - acc: 0.7991 - val_loss: 0.4724 - val_acc: 0.7760\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.4098 - acc: 0.8042 - val_loss: 0.5299 - val_acc: 0.7312\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.4127 - acc: 0.8032 - val_loss: 0.4879 - val_acc: 0.7636\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.4015 - acc: 0.8081 - val_loss: 0.5166 - val_acc: 0.7536\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3939 - acc: 0.8136 - val_loss: 0.4829 - val_acc: 0.7744\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3868 - acc: 0.8159 - val_loss: 0.5886 - val_acc: 0.7176\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3782 - acc: 0.8256 - val_loss: 0.4882 - val_acc: 0.7696\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3755 - acc: 0.8247 - val_loss: 0.5229 - val_acc: 0.7532\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3691 - acc: 0.8271 - val_loss: 0.6645 - val_acc: 0.6936\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3624 - acc: 0.8320 - val_loss: 0.5543 - val_acc: 0.7380\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3546 - acc: 0.8361 - val_loss: 0.4887 - val_acc: 0.7692\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3497 - acc: 0.8376 - val_loss: 0.4801 - val_acc: 0.7856\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.3423 - acc: 0.8444 - val_loss: 0.6592 - val_acc: 0.7112\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_83 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_248 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 10)                1610      \n",
      "_________________________________________________________________\n",
      "batch_normalization_249 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_250 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_251 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_252 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_253 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,692\n",
      "Trainable params: 2,272\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 400us/sample - loss: 0.8273 - acc: 0.5186 - val_loss: 0.6927 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.7541 - acc: 0.5376 - val_loss: 0.6916 - val_acc: 0.5236\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.7317 - acc: 0.5485 - val_loss: 0.6887 - val_acc: 0.5864\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.7110 - acc: 0.5601 - val_loss: 0.6833 - val_acc: 0.5972\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.7009 - acc: 0.5664 - val_loss: 0.6759 - val_acc: 0.5968\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6893 - acc: 0.5761 - val_loss: 0.6695 - val_acc: 0.5932\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6828 - acc: 0.5792 - val_loss: 0.6651 - val_acc: 0.5976\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6764 - acc: 0.5842 - val_loss: 0.6632 - val_acc: 0.6028\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6723 - acc: 0.5916 - val_loss: 0.6612 - val_acc: 0.6040\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6674 - acc: 0.6015 - val_loss: 0.6571 - val_acc: 0.6060\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6639 - acc: 0.6023 - val_loss: 0.6558 - val_acc: 0.6016\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6581 - acc: 0.6134 - val_loss: 0.6585 - val_acc: 0.5984\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6548 - acc: 0.6188 - val_loss: 0.6620 - val_acc: 0.5836\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6512 - acc: 0.6177 - val_loss: 0.6501 - val_acc: 0.6172\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6474 - acc: 0.6166 - val_loss: 0.6491 - val_acc: 0.6120\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6406 - acc: 0.6309 - val_loss: 0.6685 - val_acc: 0.5684\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6374 - acc: 0.6333 - val_loss: 0.6615 - val_acc: 0.5948\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6297 - acc: 0.6446 - val_loss: 0.6802 - val_acc: 0.5512\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6218 - acc: 0.6546 - val_loss: 0.6873 - val_acc: 0.5352\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6210 - acc: 0.6526 - val_loss: 0.6734 - val_acc: 0.5516\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6143 - acc: 0.6549 - val_loss: 0.6281 - val_acc: 0.6540\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6092 - acc: 0.6622 - val_loss: 0.6338 - val_acc: 0.6308\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6058 - acc: 0.6664 - val_loss: 0.6196 - val_acc: 0.6508\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.5975 - acc: 0.6714 - val_loss: 0.5967 - val_acc: 0.6856\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.5936 - acc: 0.6786 - val_loss: 0.5915 - val_acc: 0.6832\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.5957 - acc: 0.6789 - val_loss: 0.5866 - val_acc: 0.6816\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.5858 - acc: 0.6844 - val_loss: 0.5852 - val_acc: 0.6964\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5892 - acc: 0.6806 - val_loss: 0.5834 - val_acc: 0.6960\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5811 - acc: 0.6907 - val_loss: 0.5866 - val_acc: 0.6864\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5789 - acc: 0.6905 - val_loss: 0.5747 - val_acc: 0.7020\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_84 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_254 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_255 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_256 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_257 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_258 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_259 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 14,892\n",
      "Trainable params: 14,072\n",
      "Non-trainable params: 820\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 411us/sample - loss: 0.7892 - acc: 0.5502 - val_loss: 0.7131 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.7044 - acc: 0.5702 - val_loss: 0.6941 - val_acc: 0.5116\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6861 - acc: 0.5826 - val_loss: 0.6852 - val_acc: 0.6024\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6783 - acc: 0.5882 - val_loss: 0.6867 - val_acc: 0.4896\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6612 - acc: 0.6012 - val_loss: 0.7281 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6514 - acc: 0.6120 - val_loss: 0.8349 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6391 - acc: 0.6260 - val_loss: 0.9692 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6259 - acc: 0.6415 - val_loss: 1.1501 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6127 - acc: 0.6548 - val_loss: 1.1078 - val_acc: 0.4908\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6054 - acc: 0.6643 - val_loss: 0.9882 - val_acc: 0.4932\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5954 - acc: 0.6712 - val_loss: 0.8856 - val_acc: 0.5024\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5848 - acc: 0.6759 - val_loss: 0.8065 - val_acc: 0.5188\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5781 - acc: 0.6805 - val_loss: 0.6489 - val_acc: 0.6068\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5708 - acc: 0.6888 - val_loss: 0.7187 - val_acc: 0.5580\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5643 - acc: 0.6970 - val_loss: 0.6668 - val_acc: 0.5880\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5568 - acc: 0.7026 - val_loss: 0.6903 - val_acc: 0.5768\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5531 - acc: 0.7035 - val_loss: 0.6580 - val_acc: 0.5916\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5480 - acc: 0.7093 - val_loss: 0.6013 - val_acc: 0.6368\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5373 - acc: 0.7214 - val_loss: 0.5918 - val_acc: 0.6592\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5302 - acc: 0.7241 - val_loss: 0.5892 - val_acc: 0.6624\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5311 - acc: 0.7230 - val_loss: 0.5713 - val_acc: 0.6932\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5230 - acc: 0.7288 - val_loss: 0.5405 - val_acc: 0.7328\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5219 - acc: 0.7305 - val_loss: 0.6060 - val_acc: 0.6424\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5123 - acc: 0.7342 - val_loss: 0.5515 - val_acc: 0.7168\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5095 - acc: 0.7407 - val_loss: 0.5273 - val_acc: 0.7460\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5018 - acc: 0.7442 - val_loss: 0.5692 - val_acc: 0.6812\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5009 - acc: 0.7435 - val_loss: 0.5232 - val_acc: 0.7420\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4970 - acc: 0.7525 - val_loss: 0.5183 - val_acc: 0.7512\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4958 - acc: 0.7504 - val_loss: 0.5452 - val_acc: 0.7228\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4871 - acc: 0.7571 - val_loss: 0.5002 - val_acc: 0.7524\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_85 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_260 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_261 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_262 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_222 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_263 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_264 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_223 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_265 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 39,142\n",
      "Trainable params: 37,822\n",
      "Non-trainable params: 1,320\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 427us/sample - loss: 0.7994 - acc: 0.5505 - val_loss: 0.6981 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.7176 - acc: 0.5765 - val_loss: 0.6980 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6886 - acc: 0.5926 - val_loss: 0.7092 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6717 - acc: 0.5971 - val_loss: 0.7392 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6527 - acc: 0.6139 - val_loss: 0.9087 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6413 - acc: 0.6258 - val_loss: 1.1750 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6249 - acc: 0.6426 - val_loss: 1.4525 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6120 - acc: 0.6552 - val_loss: 1.5191 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5970 - acc: 0.6676 - val_loss: 1.4331 - val_acc: 0.4892\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5852 - acc: 0.6771 - val_loss: 1.3064 - val_acc: 0.4900\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5765 - acc: 0.6829 - val_loss: 1.1221 - val_acc: 0.4996\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5661 - acc: 0.6962 - val_loss: 1.0438 - val_acc: 0.5060\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5579 - acc: 0.7009 - val_loss: 1.1196 - val_acc: 0.5108\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5503 - acc: 0.7057 - val_loss: 0.9124 - val_acc: 0.5332\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5415 - acc: 0.7133 - val_loss: 0.8565 - val_acc: 0.5424\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5305 - acc: 0.7239 - val_loss: 0.7398 - val_acc: 0.5796\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5272 - acc: 0.7245 - val_loss: 0.6516 - val_acc: 0.6144\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5196 - acc: 0.7310 - val_loss: 0.6083 - val_acc: 0.6468\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5147 - acc: 0.7345 - val_loss: 0.6828 - val_acc: 0.5980\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5061 - acc: 0.7355 - val_loss: 0.8388 - val_acc: 0.5748\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5059 - acc: 0.7414 - val_loss: 0.6696 - val_acc: 0.6048\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4961 - acc: 0.7461 - val_loss: 0.5182 - val_acc: 0.7628\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4934 - acc: 0.7497 - val_loss: 0.5101 - val_acc: 0.7628\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4910 - acc: 0.7498 - val_loss: 0.5989 - val_acc: 0.6612\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4852 - acc: 0.7574 - val_loss: 0.6143 - val_acc: 0.6556\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4792 - acc: 0.7606 - val_loss: 0.6132 - val_acc: 0.6508\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4742 - acc: 0.7613 - val_loss: 0.5976 - val_acc: 0.6708\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4693 - acc: 0.7649 - val_loss: 0.5192 - val_acc: 0.7496\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4666 - acc: 0.7662 - val_loss: 0.5134 - val_acc: 0.7596\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4600 - acc: 0.7728 - val_loss: 0.5100 - val_acc: 0.7496\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_86 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_266 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_267 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_268 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_269 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_270 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_271 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 81,442\n",
      "Trainable params: 79,522\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 438us/sample - loss: 0.7922 - acc: 0.5511 - val_loss: 0.7596 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.7094 - acc: 0.5776 - val_loss: 0.7458 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6802 - acc: 0.5975 - val_loss: 0.7740 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6593 - acc: 0.6104 - val_loss: 0.8895 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6410 - acc: 0.6256 - val_loss: 1.1996 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6240 - acc: 0.6431 - val_loss: 1.5737 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.6095 - acc: 0.6575 - val_loss: 1.7950 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5958 - acc: 0.6714 - val_loss: 1.7233 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5835 - acc: 0.6767 - val_loss: 1.5664 - val_acc: 0.4888\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5705 - acc: 0.6922 - val_loss: 1.4795 - val_acc: 0.4908\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5577 - acc: 0.7030 - val_loss: 1.1939 - val_acc: 0.5016\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5467 - acc: 0.7116 - val_loss: 1.0141 - val_acc: 0.5212\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5428 - acc: 0.7123 - val_loss: 0.9330 - val_acc: 0.5348\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5324 - acc: 0.7224 - val_loss: 0.9662 - val_acc: 0.5448\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5278 - acc: 0.7289 - val_loss: 0.7635 - val_acc: 0.5784\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5165 - acc: 0.7342 - val_loss: 0.6575 - val_acc: 0.6256\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5111 - acc: 0.7380 - val_loss: 0.7858 - val_acc: 0.5808\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5030 - acc: 0.7442 - val_loss: 0.7203 - val_acc: 0.5984\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4993 - acc: 0.7470 - val_loss: 0.6774 - val_acc: 0.6296\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4938 - acc: 0.7511 - val_loss: 0.5315 - val_acc: 0.7372\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4839 - acc: 0.7571 - val_loss: 0.5545 - val_acc: 0.7108\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4768 - acc: 0.7608 - val_loss: 0.5578 - val_acc: 0.7076\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4742 - acc: 0.7658 - val_loss: 0.5021 - val_acc: 0.7616\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4666 - acc: 0.7710 - val_loss: 0.5282 - val_acc: 0.7356\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4662 - acc: 0.7729 - val_loss: 0.6931 - val_acc: 0.6304\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4551 - acc: 0.7750 - val_loss: 0.5204 - val_acc: 0.7472\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4485 - acc: 0.7817 - val_loss: 0.5372 - val_acc: 0.7340\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4483 - acc: 0.7822 - val_loss: 0.5121 - val_acc: 0.7464\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4388 - acc: 0.7889 - val_loss: 0.4784 - val_acc: 0.7788\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4379 - acc: 0.7871 - val_loss: 0.5535 - val_acc: 0.7144\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_87 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_272 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 600)               96600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_273 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_274 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_230 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_275 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_276 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_231 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_277 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_232 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 831,642\n",
      "Trainable params: 825,322\n",
      "Non-trainable params: 6,320\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 447us/sample - loss: 0.8825 - acc: 0.5664 - val_loss: 0.7123 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.7025 - acc: 0.5989 - val_loss: 0.7411 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6594 - acc: 0.6268 - val_loss: 0.8900 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6296 - acc: 0.6458 - val_loss: 1.6193 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6087 - acc: 0.6641 - val_loss: 2.8409 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5823 - acc: 0.6862 - val_loss: 3.5429 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5713 - acc: 0.6934 - val_loss: 3.7198 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5488 - acc: 0.7114 - val_loss: 3.5519 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5410 - acc: 0.7229 - val_loss: 3.1375 - val_acc: 0.4880\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5207 - acc: 0.7329 - val_loss: 3.1046 - val_acc: 0.4896\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5118 - acc: 0.7416 - val_loss: 2.6644 - val_acc: 0.4940\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5001 - acc: 0.7482 - val_loss: 2.7079 - val_acc: 0.4964\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4889 - acc: 0.7539 - val_loss: 2.5922 - val_acc: 0.4940\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4815 - acc: 0.7584 - val_loss: 2.3071 - val_acc: 0.4976\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4675 - acc: 0.7700 - val_loss: 2.0295 - val_acc: 0.5080\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4558 - acc: 0.7766 - val_loss: 1.6052 - val_acc: 0.5140\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4529 - acc: 0.7788 - val_loss: 1.1172 - val_acc: 0.5572\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4382 - acc: 0.7879 - val_loss: 1.5215 - val_acc: 0.5240\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4328 - acc: 0.7916 - val_loss: 0.7328 - val_acc: 0.6364\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4268 - acc: 0.7934 - val_loss: 0.6013 - val_acc: 0.7040\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4151 - acc: 0.8004 - val_loss: 0.9207 - val_acc: 0.5984\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4119 - acc: 0.8053 - val_loss: 0.6500 - val_acc: 0.6684\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3985 - acc: 0.8091 - val_loss: 0.8192 - val_acc: 0.6284\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3979 - acc: 0.8124 - val_loss: 0.5824 - val_acc: 0.7212\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3838 - acc: 0.8165 - val_loss: 0.7802 - val_acc: 0.6504\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3850 - acc: 0.8182 - val_loss: 0.5143 - val_acc: 0.7676\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3757 - acc: 0.8226 - val_loss: 0.5966 - val_acc: 0.7172\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3662 - acc: 0.8292 - val_loss: 0.4944 - val_acc: 0.7744\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3677 - acc: 0.8305 - val_loss: 0.6013 - val_acc: 0.7160\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3632 - acc: 0.8287 - val_loss: 0.5719 - val_acc: 0.7364\n"
     ]
    }
   ],
   "source": [
    "Tiefe = [0,1,2,3]\n",
    "Batchgrose = [32,64,128,254]\n",
    "Breite = [10,50,100,160,600]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-Charge-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\ChargePerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTrainingC.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                x= layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "            history=model.fit(XTrainingC,YTraining,\n",
    "                              validation_data=(XValC,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Netz mit den besten Parametern speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_89 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_282 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_283 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_284 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_285 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 54,402\n",
      "Trainable params: 53,122\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.5662\n",
      "Epoch 00001: val_acc improved from -inf to 0.57440, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.57.model\n",
      "17000/17000 [==============================] - 31s 2ms/sample - loss: 0.7768 - acc: 0.5662 - val_loss: 0.6901 - val_acc: 0.5744\n",
      "Epoch 2/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.6887 - acc: 0.5969\n",
      "Epoch 00002: val_acc did not improve from 0.57440\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6878 - acc: 0.5979 - val_loss: 0.7522 - val_acc: 0.4884\n",
      "Epoch 3/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.6168\n",
      "Epoch 00003: val_acc did not improve from 0.57440\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6570 - acc: 0.6167 - val_loss: 1.1032 - val_acc: 0.4884\n",
      "Epoch 4/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.6308 - acc: 0.6409\n",
      "Epoch 00004: val_acc did not improve from 0.57440\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6300 - acc: 0.6418 - val_loss: 1.3255 - val_acc: 0.4884\n",
      "Epoch 5/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.6087 - acc: 0.6622\n",
      "Epoch 00005: val_acc did not improve from 0.57440\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6088 - acc: 0.6630 - val_loss: 1.1209 - val_acc: 0.4924\n",
      "Epoch 6/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.5962 - acc: 0.6723\n",
      "Epoch 00006: val_acc did not improve from 0.57440\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.5958 - acc: 0.6724 - val_loss: 1.0188 - val_acc: 0.4932\n",
      "Epoch 7/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.6809\n",
      "Epoch 00007: val_acc did not improve from 0.57440\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5842 - acc: 0.6809 - val_loss: 0.6879 - val_acc: 0.5616\n",
      "Epoch 8/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.6843\n",
      "Epoch 00008: val_acc did not improve from 0.57440\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5746 - acc: 0.6842 - val_loss: 0.8030 - val_acc: 0.5336\n",
      "Epoch 9/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.5630 - acc: 0.6937\n",
      "Epoch 00009: val_acc did not improve from 0.57440\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5631 - acc: 0.6933 - val_loss: 0.6818 - val_acc: 0.5732\n",
      "Epoch 10/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7077\n",
      "Epoch 00010: val_acc improved from 0.57440 to 0.63480, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.63.model\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.5490 - acc: 0.7075 - val_loss: 0.6242 - val_acc: 0.6348\n",
      "Epoch 11/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.7158\n",
      "Epoch 00011: val_acc did not improve from 0.63480\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.5413 - acc: 0.7157 - val_loss: 0.6830 - val_acc: 0.5860\n",
      "Epoch 12/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7206\n",
      "Epoch 00012: val_acc did not improve from 0.63480\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5319 - acc: 0.7208 - val_loss: 0.6307 - val_acc: 0.6224\n",
      "Epoch 13/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.7307\n",
      "Epoch 00013: val_acc improved from 0.63480 to 0.65720, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.66.model\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5227 - acc: 0.7309 - val_loss: 0.6026 - val_acc: 0.6572\n",
      "Epoch 14/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.5208 - acc: 0.7300\n",
      "Epoch 00014: val_acc did not improve from 0.65720\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5209 - acc: 0.7303 - val_loss: 0.6596 - val_acc: 0.6056\n",
      "Epoch 15/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.7385\n",
      "Epoch 00015: val_acc improved from 0.65720 to 0.69680, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.70.model\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5098 - acc: 0.7391 - val_loss: 0.5650 - val_acc: 0.6968\n",
      "Epoch 16/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.5066 - acc: 0.7394\n",
      "Epoch 00016: val_acc did not improve from 0.69680\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.5064 - acc: 0.7396 - val_loss: 0.6372 - val_acc: 0.6364\n",
      "Epoch 17/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.7449\n",
      "Epoch 00017: val_acc improved from 0.69680 to 0.70840, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.71.model\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4998 - acc: 0.7459 - val_loss: 0.5476 - val_acc: 0.7084\n",
      "Epoch 18/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.7505\n",
      "Epoch 00018: val_acc improved from 0.70840 to 0.71920, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.72.model\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4923 - acc: 0.7515 - val_loss: 0.5418 - val_acc: 0.7192\n",
      "Epoch 19/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.7506\n",
      "Epoch 00019: val_acc improved from 0.71920 to 0.76760, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.77.model\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4894 - acc: 0.7508 - val_loss: 0.5008 - val_acc: 0.7676\n",
      "Epoch 20/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4789 - acc: 0.7608\n",
      "Epoch 00020: val_acc did not improve from 0.76760\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.4791 - acc: 0.7608 - val_loss: 0.5876 - val_acc: 0.6744\n",
      "Epoch 21/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4765 - acc: 0.7598\n",
      "Epoch 00021: val_acc did not improve from 0.76760\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.4763 - acc: 0.7602 - val_loss: 0.5167 - val_acc: 0.7420\n",
      "Epoch 22/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.7675\n",
      "Epoch 00022: val_acc did not improve from 0.76760\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.4688 - acc: 0.7678 - val_loss: 0.5609 - val_acc: 0.7024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.4670 - acc: 0.7671\n",
      "Epoch 00023: val_acc did not improve from 0.76760\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.4670 - acc: 0.7672 - val_loss: 0.5617 - val_acc: 0.7032\n",
      "Epoch 24/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4535 - acc: 0.7767\n",
      "Epoch 00024: val_acc did not improve from 0.76760\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4536 - acc: 0.7764 - val_loss: 0.5171 - val_acc: 0.7444\n",
      "Epoch 25/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4525 - acc: 0.7800\n",
      "Epoch 00025: val_acc improved from 0.76760 to 0.77200, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.77.model\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.4527 - acc: 0.7801 - val_loss: 0.4836 - val_acc: 0.7720\n",
      "Epoch 26/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.7829- ETA: 0s - loss: 0.437\n",
      "Epoch 00026: val_acc did not improve from 0.77200\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.4452 - acc: 0.7828 - val_loss: 0.4974 - val_acc: 0.7644\n",
      "Epoch 27/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.7856\n",
      "Epoch 00027: val_acc did not improve from 0.77200\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.4413 - acc: 0.7858 - val_loss: 0.4953 - val_acc: 0.7664\n",
      "Epoch 28/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.7914\n",
      "Epoch 00028: val_acc did not improve from 0.77200\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.4338 - acc: 0.7913 - val_loss: 0.5748 - val_acc: 0.6924\n",
      "Epoch 29/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.7900\n",
      "Epoch 00029: val_acc did not improve from 0.77200\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.4296 - acc: 0.7903 - val_loss: 0.4986 - val_acc: 0.7696\n",
      "Epoch 30/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.7972\n",
      "Epoch 00030: val_acc did not improve from 0.77200\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.4254 - acc: 0.7971 - val_loss: 0.5045 - val_acc: 0.7588\n",
      "Epoch 31/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4219 - acc: 0.7943\n",
      "Epoch 00031: val_acc did not improve from 0.77200\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.4221 - acc: 0.7946 - val_loss: 0.5934 - val_acc: 0.6948\n",
      "Epoch 32/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.7998\n",
      "Epoch 00032: val_acc improved from 0.77200 to 0.77280, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.77.model\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4165 - acc: 0.7996 - val_loss: 0.4868 - val_acc: 0.7728\n",
      "Epoch 33/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4117 - acc: 0.8006\n",
      "Epoch 00033: val_acc did not improve from 0.77280\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.4114 - acc: 0.8002 - val_loss: 0.4854 - val_acc: 0.7696\n",
      "Epoch 34/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8064\n",
      "Epoch 00034: val_acc did not improve from 0.77280\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.4091 - acc: 0.8062 - val_loss: 0.5481 - val_acc: 0.7256\n",
      "Epoch 35/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.8052\n",
      "Epoch 00035: val_acc did not improve from 0.77280\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.4077 - acc: 0.8046 - val_loss: 0.4841 - val_acc: 0.7716\n",
      "Epoch 36/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8121\n",
      "Epoch 00036: val_acc improved from 0.77280 to 0.77480, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.77.model\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.3989 - acc: 0.8106 - val_loss: 0.4964 - val_acc: 0.7748\n",
      "Epoch 37/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8082\n",
      "Epoch 00037: val_acc improved from 0.77480 to 0.78000, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.78.model\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4013 - acc: 0.8083 - val_loss: 0.4764 - val_acc: 0.7800\n",
      "Epoch 38/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3964 - acc: 0.8078\n",
      "Epoch 00038: val_acc did not improve from 0.78000\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3960 - acc: 0.8079 - val_loss: 0.5451 - val_acc: 0.7284\n",
      "Epoch 39/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.3903 - acc: 0.8144\n",
      "Epoch 00039: val_acc did not improve from 0.78000\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3915 - acc: 0.8136 - val_loss: 0.5151 - val_acc: 0.7476\n",
      "Epoch 40/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8159\n",
      "Epoch 00040: val_acc did not improve from 0.78000\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3902 - acc: 0.8158 - val_loss: 0.5333 - val_acc: 0.7252\n",
      "Epoch 41/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8189\n",
      "Epoch 00041: val_acc did not improve from 0.78000\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3860 - acc: 0.8192 - val_loss: 0.5167 - val_acc: 0.7448\n",
      "Epoch 42/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8206\n",
      "Epoch 00042: val_acc did not improve from 0.78000\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.3815 - acc: 0.8201 - val_loss: 0.5094 - val_acc: 0.7528\n",
      "Epoch 43/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8242\n",
      "Epoch 00043: val_acc did not improve from 0.78000\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.3793 - acc: 0.8242 - val_loss: 0.5620 - val_acc: 0.7152\n",
      "Epoch 44/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8277\n",
      "Epoch 00044: val_acc did not improve from 0.78000\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.3724 - acc: 0.8279 - val_loss: 0.5203 - val_acc: 0.7420\n",
      "Epoch 45/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8282\n",
      "Epoch 00045: val_acc improved from 0.78000 to 0.78160, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.78.model\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.3701 - acc: 0.8277 - val_loss: 0.4726 - val_acc: 0.7816\n",
      "Epoch 46/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.3699 - acc: 0.8245\n",
      "Epoch 00046: val_acc did not improve from 0.78160\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3699 - acc: 0.8245 - val_loss: 0.5712 - val_acc: 0.7172\n",
      "Epoch 47/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.3639 - acc: 0.8325\n",
      "Epoch 00047: val_acc did not improve from 0.78160\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3643 - acc: 0.8321 - val_loss: 0.4959 - val_acc: 0.7688\n",
      "Epoch 48/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8321\n",
      "Epoch 00048: val_acc improved from 0.78160 to 0.78880, saving model to Perceptron-PMT-Charge-MuEl-val-acc_0.79.model\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3589 - acc: 0.8323 - val_loss: 0.4628 - val_acc: 0.7888\n",
      "Epoch 49/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8346\n",
      "Epoch 00049: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.3550 - acc: 0.8344 - val_loss: 0.5672 - val_acc: 0.7252\n",
      "Epoch 50/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8352\n",
      "Epoch 00050: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3527 - acc: 0.8352 - val_loss: 0.5547 - val_acc: 0.7408\n",
      "Epoch 51/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.3531 - acc: 0.8386\n",
      "Epoch 00051: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.3537 - acc: 0.8378 - val_loss: 0.5808 - val_acc: 0.7184\n",
      "Epoch 52/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.3508 - acc: 0.8380\n",
      "Epoch 00052: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3500 - acc: 0.8384 - val_loss: 0.4845 - val_acc: 0.7720\n",
      "Epoch 53/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8393\n",
      "Epoch 00053: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3495 - acc: 0.8385 - val_loss: 0.4965 - val_acc: 0.7620\n",
      "Epoch 54/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8413\n",
      "Epoch 00054: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.3502 - acc: 0.8413 - val_loss: 0.6251 - val_acc: 0.7028\n",
      "Epoch 55/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8400\n",
      "Epoch 00055: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3488 - acc: 0.8404 - val_loss: 0.5248 - val_acc: 0.7448\n",
      "Epoch 56/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8483\n",
      "Epoch 00056: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.3395 - acc: 0.8479 - val_loss: 0.4808 - val_acc: 0.7872\n",
      "Epoch 57/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.8480\n",
      "Epoch 00057: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.3356 - acc: 0.8472 - val_loss: 0.5369 - val_acc: 0.7488\n",
      "Epoch 58/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8440\n",
      "Epoch 00058: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3351 - acc: 0.8431 - val_loss: 0.5001 - val_acc: 0.7636\n",
      "Epoch 59/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8477\n",
      "Epoch 00059: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.3398 - acc: 0.8472 - val_loss: 0.5515 - val_acc: 0.7332\n",
      "Epoch 60/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8520\n",
      "Epoch 00060: val_acc did not improve from 0.78880\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.3262 - acc: 0.8522 - val_loss: 0.6012 - val_acc: 0.7172\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "Tiefe = [2]\n",
    "Batchgrose = [128]\n",
    "Breite = [160]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-Charge-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\ChargePerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTrainingC.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                x= layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "            #filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "            filepath=\"Perceptron-PMT-Charge-MuEl-val-acc_{val_acc:.2f}.model\" \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "\n",
    "            history=model.fit(XTrainingC,YTraining,\n",
    "                              validation_data=(XValC,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 60)\n",
    "\n",
    "\n",
    "            print(history.history.keys())\n",
    "            # summarize history for accuracy\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  1.7602712278422574\n",
      "Test accuracy:  0.5920533\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"Perceptron-PMT-Charge-MuEl-val-acc_0.79.model\")\n",
    "score = model.evaluate(XTestC, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 485us/sample - loss: 0.7348 - acc: 0.6366 - val_loss: 0.6626 - val_acc: 0.5520\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6140 - acc: 0.6793 - val_loss: 0.9117 - val_acc: 0.4896\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.5785 - acc: 0.7013 - val_loss: 0.9193 - val_acc: 0.4920\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.5549 - acc: 0.7169 - val_loss: 0.7828 - val_acc: 0.5236\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.5447 - acc: 0.7264 - val_loss: 0.6609 - val_acc: 0.5980\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.5300 - acc: 0.7359 - val_loss: 0.5940 - val_acc: 0.6600\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.5186 - acc: 0.7442 - val_loss: 0.5460 - val_acc: 0.7176\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.5107 - acc: 0.7486 - val_loss: 0.5270 - val_acc: 0.7340\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.5013 - acc: 0.7541 - val_loss: 0.5224 - val_acc: 0.7376\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4868 - acc: 0.7659 - val_loss: 0.5169 - val_acc: 0.7532\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4819 - acc: 0.7684 - val_loss: 0.5122 - val_acc: 0.7484\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.4710 - acc: 0.7732 - val_loss: 0.5263 - val_acc: 0.7328\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.4694 - acc: 0.7764 - val_loss: 0.5131 - val_acc: 0.7540\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.4552 - acc: 0.7844 - val_loss: 0.5183 - val_acc: 0.7468\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.4510 - acc: 0.7879 - val_loss: 0.5199 - val_acc: 0.7436\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.4458 - acc: 0.7874 - val_loss: 0.5218 - val_acc: 0.7384\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.4408 - acc: 0.7919 - val_loss: 0.5231 - val_acc: 0.7512\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.4334 - acc: 0.7945 - val_loss: 0.5249 - val_acc: 0.7468\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.4239 - acc: 0.8014 - val_loss: 0.5328 - val_acc: 0.7312\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.4213 - acc: 0.8031 - val_loss: 0.5343 - val_acc: 0.7504\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.4145 - acc: 0.8066 - val_loss: 0.5459 - val_acc: 0.7256\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.4119 - acc: 0.8074 - val_loss: 0.5436 - val_acc: 0.7264\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.4078 - acc: 0.8085 - val_loss: 0.5450 - val_acc: 0.7384\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.4010 - acc: 0.8129 - val_loss: 0.5480 - val_acc: 0.7312\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.4056 - acc: 0.8119 - val_loss: 0.5502 - val_acc: 0.7352\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.3995 - acc: 0.8109 - val_loss: 0.5433 - val_acc: 0.7360\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.3922 - acc: 0.8191 - val_loss: 0.5556 - val_acc: 0.7472\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.3894 - acc: 0.8186 - val_loss: 0.5497 - val_acc: 0.7328\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.3838 - acc: 0.8239 - val_loss: 0.5551 - val_acc: 0.7360\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.3901 - acc: 0.8171 - val_loss: 0.5589 - val_acc: 0.7380\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 489us/sample - loss: 0.6863 - acc: 0.6563 - val_loss: 0.9084 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.5885 - acc: 0.6955 - val_loss: 1.4699 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.5548 - acc: 0.7178 - val_loss: 1.4511 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.5326 - acc: 0.7335 - val_loss: 1.1499 - val_acc: 0.4908\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.5137 - acc: 0.7449 - val_loss: 0.8670 - val_acc: 0.5288\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.5013 - acc: 0.7554 - val_loss: 0.7223 - val_acc: 0.5912\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.4877 - acc: 0.7597 - val_loss: 0.5849 - val_acc: 0.6784\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.4762 - acc: 0.7708 - val_loss: 0.5838 - val_acc: 0.6776\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.4615 - acc: 0.7781 - val_loss: 0.5073 - val_acc: 0.7420\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.4504 - acc: 0.7834 - val_loss: 0.5471 - val_acc: 0.7148\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4433 - acc: 0.7898 - val_loss: 0.5556 - val_acc: 0.7124\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.4351 - acc: 0.7931 - val_loss: 0.5476 - val_acc: 0.7116\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.4236 - acc: 0.8013 - val_loss: 0.5206 - val_acc: 0.7412\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.4118 - acc: 0.8072 - val_loss: 0.5265 - val_acc: 0.7296\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.4051 - acc: 0.8090 - val_loss: 0.5198 - val_acc: 0.7500\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.3972 - acc: 0.8114 - val_loss: 0.5504 - val_acc: 0.7260\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.3898 - acc: 0.8170 - val_loss: 0.5473 - val_acc: 0.7316\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.3802 - acc: 0.8217 - val_loss: 0.5394 - val_acc: 0.7408\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.3759 - acc: 0.8279 - val_loss: 0.5491 - val_acc: 0.7404\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.3630 - acc: 0.8313 - val_loss: 0.5622 - val_acc: 0.7304\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.3585 - acc: 0.8366 - val_loss: 0.5562 - val_acc: 0.7464\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.3515 - acc: 0.8392 - val_loss: 0.5668 - val_acc: 0.7432\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.3514 - acc: 0.8381 - val_loss: 0.5798 - val_acc: 0.7352\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.3420 - acc: 0.8434 - val_loss: 0.5963 - val_acc: 0.7292\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.3388 - acc: 0.8451 - val_loss: 0.5879 - val_acc: 0.7396\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.3279 - acc: 0.8525 - val_loss: 0.5871 - val_acc: 0.7500\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.3290 - acc: 0.8490 - val_loss: 0.6032 - val_acc: 0.7404\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.3265 - acc: 0.8562 - val_loss: 0.6095 - val_acc: 0.7352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.3106 - acc: 0.8611 - val_loss: 0.6224 - val_acc: 0.7304\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.3066 - acc: 0.8596 - val_loss: 0.6347 - val_acc: 0.7396\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 484us/sample - loss: 0.6727 - acc: 0.6574 - val_loss: 1.0409 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.5921 - acc: 0.6964 - val_loss: 1.7078 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.5518 - acc: 0.7191 - val_loss: 1.7784 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.5325 - acc: 0.7310 - val_loss: 1.4331 - val_acc: 0.4892\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.5118 - acc: 0.7488 - val_loss: 1.1197 - val_acc: 0.4948\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4957 - acc: 0.7633 - val_loss: 0.8624 - val_acc: 0.5440\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4835 - acc: 0.7642 - val_loss: 0.6574 - val_acc: 0.6396\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4679 - acc: 0.7780 - val_loss: 0.5858 - val_acc: 0.6856\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4503 - acc: 0.7866 - val_loss: 0.5697 - val_acc: 0.7056\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.4398 - acc: 0.7891 - val_loss: 0.6014 - val_acc: 0.6832\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.4233 - acc: 0.8002 - val_loss: 0.5313 - val_acc: 0.7328\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.4141 - acc: 0.8088 - val_loss: 0.5797 - val_acc: 0.7044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.4047 - acc: 0.8096 - val_loss: 0.6161 - val_acc: 0.6888\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.3897 - acc: 0.8194 - val_loss: 0.5582 - val_acc: 0.7240\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.3789 - acc: 0.8244 - val_loss: 0.5409 - val_acc: 0.7384\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.3710 - acc: 0.8292 - val_loss: 0.5662 - val_acc: 0.7212\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.3654 - acc: 0.8319 - val_loss: 0.6074 - val_acc: 0.7028\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.3531 - acc: 0.8392 - val_loss: 0.5771 - val_acc: 0.7268\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.3479 - acc: 0.8443 - val_loss: 0.5835 - val_acc: 0.7160\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.3402 - acc: 0.8472 - val_loss: 0.5775 - val_acc: 0.7332\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.3309 - acc: 0.8499 - val_loss: 0.5842 - val_acc: 0.7336\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.3224 - acc: 0.8550 - val_loss: 0.6157 - val_acc: 0.7252\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.3104 - acc: 0.8586 - val_loss: 0.6081 - val_acc: 0.7400\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.3083 - acc: 0.8622 - val_loss: 0.6126 - val_acc: 0.7296\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.2931 - acc: 0.8687 - val_loss: 0.6097 - val_acc: 0.7472\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.2924 - acc: 0.8714 - val_loss: 0.6284 - val_acc: 0.7312\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.2852 - acc: 0.8755 - val_loss: 0.6347 - val_acc: 0.7440\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.2787 - acc: 0.8768 - val_loss: 0.6491 - val_acc: 0.7496\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.2689 - acc: 0.8825 - val_loss: 0.6565 - val_acc: 0.7488\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.2620 - acc: 0.8838 - val_loss: 0.6669 - val_acc: 0.7324\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 480us/sample - loss: 0.7073 - acc: 0.6506 - val_loss: 1.0966 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.5978 - acc: 0.6928 - val_loss: 2.5045 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.5543 - acc: 0.7209 - val_loss: 3.1152 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.5272 - acc: 0.7382 - val_loss: 3.0299 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.5087 - acc: 0.7524 - val_loss: 2.3381 - val_acc: 0.4888\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.4888 - acc: 0.7622 - val_loss: 1.5958 - val_acc: 0.4908\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4727 - acc: 0.7736 - val_loss: 1.1175 - val_acc: 0.5212\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.4512 - acc: 0.7853 - val_loss: 0.7784 - val_acc: 0.6168\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.4347 - acc: 0.7932 - val_loss: 0.6763 - val_acc: 0.6540\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.4140 - acc: 0.8062 - val_loss: 0.6082 - val_acc: 0.6824\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.4017 - acc: 0.8130 - val_loss: 0.6303 - val_acc: 0.6852\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.3844 - acc: 0.8216 - val_loss: 0.6102 - val_acc: 0.6980\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.3696 - acc: 0.8304 - val_loss: 0.6834 - val_acc: 0.6724\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.3566 - acc: 0.8351 - val_loss: 0.7585 - val_acc: 0.6520\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.3414 - acc: 0.8415 - val_loss: 0.6163 - val_acc: 0.7072\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.3255 - acc: 0.8518 - val_loss: 0.6649 - val_acc: 0.6892\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.3124 - acc: 0.8599 - val_loss: 0.6200 - val_acc: 0.7208\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.3000 - acc: 0.8679 - val_loss: 0.6352 - val_acc: 0.7196\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.2875 - acc: 0.8728 - val_loss: 0.6380 - val_acc: 0.7240\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.2733 - acc: 0.8815 - val_loss: 0.6664 - val_acc: 0.7144\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.2586 - acc: 0.8862 - val_loss: 0.7077 - val_acc: 0.7080\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.2422 - acc: 0.8963 - val_loss: 0.7001 - val_acc: 0.7220\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.2247 - acc: 0.9064 - val_loss: 0.7119 - val_acc: 0.7392\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.2118 - acc: 0.9126 - val_loss: 0.7306 - val_acc: 0.7288\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.2019 - acc: 0.9167 - val_loss: 0.7631 - val_acc: 0.7220\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.1862 - acc: 0.9231 - val_loss: 0.7887 - val_acc: 0.7248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.1707 - acc: 0.9294 - val_loss: 0.8356 - val_acc: 0.7384\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.1577 - acc: 0.9358 - val_loss: 0.8812 - val_acc: 0.7136\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.1457 - acc: 0.9427 - val_loss: 0.8814 - val_acc: 0.7352\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.1394 - acc: 0.9462 - val_loss: 0.8979 - val_acc: 0.7420\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 454us/sample - loss: 0.7216 - acc: 0.6304 - val_loss: 0.7798 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.6215 - acc: 0.6734 - val_loss: 0.8413 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5930 - acc: 0.6881 - val_loss: 0.9892 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5735 - acc: 0.7010 - val_loss: 1.1081 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5617 - acc: 0.7120 - val_loss: 1.1227 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5491 - acc: 0.7209 - val_loss: 1.0774 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.5354 - acc: 0.7301 - val_loss: 0.9553 - val_acc: 0.4936\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5281 - acc: 0.7366 - val_loss: 0.8203 - val_acc: 0.5200\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5234 - acc: 0.7411 - val_loss: 0.7117 - val_acc: 0.5692\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5150 - acc: 0.7472 - val_loss: 0.6866 - val_acc: 0.5984\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5037 - acc: 0.7526 - val_loss: 0.5849 - val_acc: 0.6648\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4977 - acc: 0.7556 - val_loss: 0.5538 - val_acc: 0.7048\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4901 - acc: 0.7648 - val_loss: 0.5352 - val_acc: 0.7288\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4822 - acc: 0.7692 - val_loss: 0.5241 - val_acc: 0.7380\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4753 - acc: 0.7704 - val_loss: 0.5370 - val_acc: 0.7292\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4660 - acc: 0.7783 - val_loss: 0.5393 - val_acc: 0.7264\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4579 - acc: 0.7828 - val_loss: 0.5254 - val_acc: 0.7388\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4555 - acc: 0.7838 - val_loss: 0.5603 - val_acc: 0.7060\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4457 - acc: 0.7868 - val_loss: 0.5251 - val_acc: 0.7376\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4440 - acc: 0.7887 - val_loss: 0.5304 - val_acc: 0.7376\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4338 - acc: 0.7937 - val_loss: 0.5186 - val_acc: 0.7468\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4336 - acc: 0.7944 - val_loss: 0.5229 - val_acc: 0.7436\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4265 - acc: 0.7984 - val_loss: 0.5238 - val_acc: 0.7416\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4212 - acc: 0.8024 - val_loss: 0.5261 - val_acc: 0.7420\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4190 - acc: 0.8021 - val_loss: 0.5305 - val_acc: 0.7400\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4203 - acc: 0.8012 - val_loss: 0.5545 - val_acc: 0.7224\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4078 - acc: 0.8078 - val_loss: 0.5462 - val_acc: 0.7260\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4092 - acc: 0.8108 - val_loss: 0.5555 - val_acc: 0.7276\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4016 - acc: 0.8084 - val_loss: 0.5517 - val_acc: 0.7304\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3950 - acc: 0.8170 - val_loss: 0.5417 - val_acc: 0.7516\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 454us/sample - loss: 0.7424 - acc: 0.6325 - val_loss: 0.6990 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.6187 - acc: 0.6794 - val_loss: 0.9268 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5841 - acc: 0.6956 - val_loss: 1.3288 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5613 - acc: 0.7139 - val_loss: 1.5618 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5420 - acc: 0.7292 - val_loss: 1.6785 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5296 - acc: 0.7359 - val_loss: 1.5194 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5211 - acc: 0.7414 - val_loss: 1.3904 - val_acc: 0.4888\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5082 - acc: 0.7511 - val_loss: 1.1452 - val_acc: 0.4916\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5003 - acc: 0.7569 - val_loss: 0.9977 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4874 - acc: 0.7649 - val_loss: 0.8126 - val_acc: 0.5448\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4769 - acc: 0.7728 - val_loss: 0.7394 - val_acc: 0.5844\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4708 - acc: 0.7737 - val_loss: 0.6416 - val_acc: 0.6468\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4590 - acc: 0.7823 - val_loss: 0.5771 - val_acc: 0.6932\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4526 - acc: 0.7864 - val_loss: 0.6049 - val_acc: 0.6780\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4413 - acc: 0.7922 - val_loss: 0.5499 - val_acc: 0.7132\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4324 - acc: 0.7930 - val_loss: 0.5745 - val_acc: 0.6980\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4219 - acc: 0.8032 - val_loss: 0.5311 - val_acc: 0.7328\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4165 - acc: 0.8049 - val_loss: 0.5738 - val_acc: 0.7080\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4143 - acc: 0.8051 - val_loss: 0.5517 - val_acc: 0.7244\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4047 - acc: 0.8109 - val_loss: 0.5559 - val_acc: 0.7260\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3948 - acc: 0.8129 - val_loss: 0.5509 - val_acc: 0.7292\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3904 - acc: 0.8162 - val_loss: 0.5837 - val_acc: 0.7124\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3846 - acc: 0.8227 - val_loss: 0.5677 - val_acc: 0.7224\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.3807 - acc: 0.8219 - val_loss: 0.5810 - val_acc: 0.7140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3751 - acc: 0.8235 - val_loss: 0.5502 - val_acc: 0.7404\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.3670 - acc: 0.8322 - val_loss: 0.5706 - val_acc: 0.7248\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3650 - acc: 0.8313 - val_loss: 0.5689 - val_acc: 0.7248\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3577 - acc: 0.8344 - val_loss: 0.5652 - val_acc: 0.7300\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3543 - acc: 0.8377 - val_loss: 0.5712 - val_acc: 0.7264\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3459 - acc: 0.8428 - val_loss: 0.5751 - val_acc: 0.7336\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 461us/sample - loss: 0.7191 - acc: 0.6405 - val_loss: 0.7220 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6243 - acc: 0.6727 - val_loss: 0.8029 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.5848 - acc: 0.6978 - val_loss: 1.3248 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5576 - acc: 0.7167 - val_loss: 1.6894 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.5370 - acc: 0.7306 - val_loss: 1.9365 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5251 - acc: 0.7375 - val_loss: 1.8529 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.5072 - acc: 0.7502 - val_loss: 1.6474 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4975 - acc: 0.7587 - val_loss: 1.3910 - val_acc: 0.4892\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4825 - acc: 0.7655 - val_loss: 1.1910 - val_acc: 0.4984\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4701 - acc: 0.7722 - val_loss: 0.9473 - val_acc: 0.5260\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4590 - acc: 0.7822 - val_loss: 0.7793 - val_acc: 0.5916\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4504 - acc: 0.7845 - val_loss: 0.6661 - val_acc: 0.6536\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4346 - acc: 0.7940 - val_loss: 0.6367 - val_acc: 0.6704\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4276 - acc: 0.7968 - val_loss: 0.6245 - val_acc: 0.6872\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4194 - acc: 0.8063 - val_loss: 0.6560 - val_acc: 0.6592\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4138 - acc: 0.8048 - val_loss: 0.6306 - val_acc: 0.6752\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4005 - acc: 0.8162 - val_loss: 0.5566 - val_acc: 0.7172\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3961 - acc: 0.8181 - val_loss: 0.6467 - val_acc: 0.6764\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3851 - acc: 0.8219 - val_loss: 0.5530 - val_acc: 0.7280\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3756 - acc: 0.8254 - val_loss: 0.6069 - val_acc: 0.7008\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3732 - acc: 0.8260 - val_loss: 0.5729 - val_acc: 0.7124\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3639 - acc: 0.8322 - val_loss: 0.5714 - val_acc: 0.7236\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3558 - acc: 0.8368 - val_loss: 0.5953 - val_acc: 0.7124\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3470 - acc: 0.8411 - val_loss: 0.5958 - val_acc: 0.7176\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3387 - acc: 0.8476 - val_loss: 0.5799 - val_acc: 0.7244\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3283 - acc: 0.8526 - val_loss: 0.5893 - val_acc: 0.7260\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3242 - acc: 0.8515 - val_loss: 0.5711 - val_acc: 0.7468\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3184 - acc: 0.8586 - val_loss: 0.5907 - val_acc: 0.7300\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3108 - acc: 0.8563 - val_loss: 0.6281 - val_acc: 0.7192\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3035 - acc: 0.8627 - val_loss: 0.6250 - val_acc: 0.7228\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 462us/sample - loss: 0.7203 - acc: 0.6431 - val_loss: 0.7397 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.6234 - acc: 0.6774 - val_loss: 1.4869 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.5774 - acc: 0.7018 - val_loss: 2.5334 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5522 - acc: 0.7209 - val_loss: 3.2824 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5289 - acc: 0.7375 - val_loss: 3.6423 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5153 - acc: 0.7465 - val_loss: 3.6750 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4966 - acc: 0.7605 - val_loss: 3.5779 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4842 - acc: 0.7654 - val_loss: 3.1631 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4692 - acc: 0.7786 - val_loss: 2.7908 - val_acc: 0.4884\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4533 - acc: 0.7825 - val_loss: 2.1155 - val_acc: 0.4896\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4416 - acc: 0.7900 - val_loss: 1.7470 - val_acc: 0.4936\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4316 - acc: 0.7958 - val_loss: 1.7881 - val_acc: 0.4936\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4167 - acc: 0.8039 - val_loss: 1.2757 - val_acc: 0.5224\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4103 - acc: 0.8068 - val_loss: 1.3361 - val_acc: 0.5248\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3957 - acc: 0.8181 - val_loss: 0.9874 - val_acc: 0.5848\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3836 - acc: 0.8215 - val_loss: 0.8460 - val_acc: 0.6324\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3797 - acc: 0.8228 - val_loss: 0.9921 - val_acc: 0.5960\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3611 - acc: 0.8337 - val_loss: 0.8448 - val_acc: 0.6280\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3550 - acc: 0.8391 - val_loss: 0.9292 - val_acc: 0.6052\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3383 - acc: 0.8430 - val_loss: 0.7270 - val_acc: 0.6704\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3311 - acc: 0.8501 - val_loss: 0.7255 - val_acc: 0.6872\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3141 - acc: 0.8586 - val_loss: 0.8270 - val_acc: 0.6508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3095 - acc: 0.8616 - val_loss: 0.7036 - val_acc: 0.6924\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3006 - acc: 0.8680 - val_loss: 0.7266 - val_acc: 0.6932\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2862 - acc: 0.8732 - val_loss: 0.7502 - val_acc: 0.6976\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2753 - acc: 0.8816 - val_loss: 0.6990 - val_acc: 0.7160\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2598 - acc: 0.8881 - val_loss: 0.6349 - val_acc: 0.7464\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2485 - acc: 0.8924 - val_loss: 0.7335 - val_acc: 0.7116\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2431 - acc: 0.8949 - val_loss: 0.7165 - val_acc: 0.7288\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2224 - acc: 0.9059 - val_loss: 0.7096 - val_acc: 0.7260\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 547us/sample - loss: 0.7337 - acc: 0.6205 - val_loss: 0.6857 - val_acc: 0.4912\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6325 - acc: 0.6606 - val_loss: 0.7094 - val_acc: 0.4980\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.5994 - acc: 0.6869 - val_loss: 0.7789 - val_acc: 0.5056\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.5737 - acc: 0.7045 - val_loss: 0.8214 - val_acc: 0.5180\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.5561 - acc: 0.7189 - val_loss: 0.7574 - val_acc: 0.5516\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5451 - acc: 0.7238 - val_loss: 0.6591 - val_acc: 0.6136\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.5367 - acc: 0.7341 - val_loss: 0.6287 - val_acc: 0.6380\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.5258 - acc: 0.7394 - val_loss: 0.5523 - val_acc: 0.7024\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.5187 - acc: 0.7431 - val_loss: 0.5499 - val_acc: 0.7036\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5141 - acc: 0.7477 - val_loss: 0.5247 - val_acc: 0.7276\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5036 - acc: 0.7551 - val_loss: 0.5390 - val_acc: 0.7112\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4950 - acc: 0.7615 - val_loss: 0.5132 - val_acc: 0.7456\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4858 - acc: 0.7690 - val_loss: 0.5108 - val_acc: 0.7476\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.4770 - acc: 0.7741 - val_loss: 0.5151 - val_acc: 0.7476\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.4728 - acc: 0.7724 - val_loss: 0.5167 - val_acc: 0.7372\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4673 - acc: 0.7788 - val_loss: 0.5160 - val_acc: 0.7516\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4578 - acc: 0.7862 - val_loss: 0.5200 - val_acc: 0.7368\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4569 - acc: 0.7860 - val_loss: 0.5176 - val_acc: 0.7444\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4504 - acc: 0.7947 - val_loss: 0.5215 - val_acc: 0.7548\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4428 - acc: 0.7946 - val_loss: 0.5272 - val_acc: 0.7432\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.4365 - acc: 0.7969 - val_loss: 0.5254 - val_acc: 0.7428\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4296 - acc: 0.8036 - val_loss: 0.5296 - val_acc: 0.7564\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4288 - acc: 0.8009 - val_loss: 0.5169 - val_acc: 0.7568\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4273 - acc: 0.8009 - val_loss: 0.5295 - val_acc: 0.7436\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4182 - acc: 0.8078 - val_loss: 0.5229 - val_acc: 0.7508\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.4170 - acc: 0.8065 - val_loss: 0.5312 - val_acc: 0.7444\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4163 - acc: 0.8059 - val_loss: 0.5258 - val_acc: 0.7556\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4069 - acc: 0.8145 - val_loss: 0.5436 - val_acc: 0.7324\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.4015 - acc: 0.8180 - val_loss: 0.5350 - val_acc: 0.7508\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.3995 - acc: 0.8203 - val_loss: 0.5405 - val_acc: 0.7632\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 566us/sample - loss: 0.7069 - acc: 0.6366 - val_loss: 0.7515 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6213 - acc: 0.6738 - val_loss: 0.8755 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5826 - acc: 0.6968 - val_loss: 1.0521 - val_acc: 0.4888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.5622 - acc: 0.7105 - val_loss: 0.9924 - val_acc: 0.4960\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.5383 - acc: 0.7284 - val_loss: 1.0095 - val_acc: 0.5020\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5232 - acc: 0.7406 - val_loss: 0.9197 - val_acc: 0.5276\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5091 - acc: 0.7496 - val_loss: 0.6756 - val_acc: 0.6160\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4992 - acc: 0.7542 - val_loss: 0.6546 - val_acc: 0.6416\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4867 - acc: 0.7652 - val_loss: 0.5863 - val_acc: 0.6864\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4771 - acc: 0.7710 - val_loss: 0.6004 - val_acc: 0.6672\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4638 - acc: 0.7807 - val_loss: 0.5470 - val_acc: 0.7208\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4570 - acc: 0.7854 - val_loss: 0.5299 - val_acc: 0.7348\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.4432 - acc: 0.7902 - val_loss: 0.5598 - val_acc: 0.7136\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.4365 - acc: 0.7960 - val_loss: 0.5451 - val_acc: 0.7268\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4269 - acc: 0.8024 - val_loss: 0.5438 - val_acc: 0.7320\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4198 - acc: 0.8072 - val_loss: 0.5687 - val_acc: 0.7220\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.4089 - acc: 0.8115 - val_loss: 0.5609 - val_acc: 0.7164\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4034 - acc: 0.8134 - val_loss: 0.5477 - val_acc: 0.7388\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4003 - acc: 0.8151 - val_loss: 0.5311 - val_acc: 0.7492\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.3861 - acc: 0.8234 - val_loss: 0.5911 - val_acc: 0.7120\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.3828 - acc: 0.8256 - val_loss: 0.5514 - val_acc: 0.7384\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3765 - acc: 0.8295 - val_loss: 0.5646 - val_acc: 0.7376\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3668 - acc: 0.8359 - val_loss: 0.5816 - val_acc: 0.7256\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3592 - acc: 0.8364 - val_loss: 0.5851 - val_acc: 0.7308\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3555 - acc: 0.8398 - val_loss: 0.5906 - val_acc: 0.7356\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.3549 - acc: 0.8409 - val_loss: 0.5893 - val_acc: 0.7316\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3411 - acc: 0.8478 - val_loss: 0.5853 - val_acc: 0.7444\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3359 - acc: 0.8529 - val_loss: 0.6396 - val_acc: 0.7176\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.3354 - acc: 0.8502 - val_loss: 0.6213 - val_acc: 0.7184\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.3280 - acc: 0.8541 - val_loss: 0.7110 - val_acc: 0.6704\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 560us/sample - loss: 0.7089 - acc: 0.6442 - val_loss: 0.6809 - val_acc: 0.5272\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6203 - acc: 0.6738 - val_loss: 0.8322 - val_acc: 0.4888\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5732 - acc: 0.7043 - val_loss: 1.3866 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5477 - acc: 0.7238 - val_loss: 1.5268 - val_acc: 0.4892\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.5240 - acc: 0.7399 - val_loss: 1.3017 - val_acc: 0.4908\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5088 - acc: 0.7495 - val_loss: 1.1540 - val_acc: 0.5052\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4941 - acc: 0.7616 - val_loss: 0.8100 - val_acc: 0.5712\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.4805 - acc: 0.7698 - val_loss: 0.6836 - val_acc: 0.6312\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.4666 - acc: 0.7749 - val_loss: 0.5909 - val_acc: 0.6824\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4552 - acc: 0.7852 - val_loss: 0.6753 - val_acc: 0.6524\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.4442 - acc: 0.7896 - val_loss: 0.5980 - val_acc: 0.6792\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.4332 - acc: 0.7965 - val_loss: 0.5501 - val_acc: 0.7080\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4229 - acc: 0.8005 - val_loss: 0.6081 - val_acc: 0.6800\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4131 - acc: 0.8060 - val_loss: 0.5622 - val_acc: 0.7076\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4023 - acc: 0.8145 - val_loss: 0.5351 - val_acc: 0.7416\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3941 - acc: 0.8189 - val_loss: 0.5880 - val_acc: 0.7008\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3797 - acc: 0.8278 - val_loss: 0.6239 - val_acc: 0.6896\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3711 - acc: 0.8299 - val_loss: 0.5641 - val_acc: 0.7288\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.3638 - acc: 0.8337 - val_loss: 0.5607 - val_acc: 0.7460\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.3569 - acc: 0.8401 - val_loss: 0.5985 - val_acc: 0.7108\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3489 - acc: 0.8426 - val_loss: 0.6910 - val_acc: 0.6732\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3412 - acc: 0.8490 - val_loss: 0.6283 - val_acc: 0.7120\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3362 - acc: 0.8479 - val_loss: 0.5907 - val_acc: 0.7368\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.3250 - acc: 0.8520 - val_loss: 0.6246 - val_acc: 0.7240\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.3167 - acc: 0.8593 - val_loss: 0.6124 - val_acc: 0.7296\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.3108 - acc: 0.8616 - val_loss: 0.6561 - val_acc: 0.7104\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.3031 - acc: 0.8675 - val_loss: 0.7008 - val_acc: 0.6980\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.2915 - acc: 0.8761 - val_loss: 0.6493 - val_acc: 0.7404\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.2899 - acc: 0.8725 - val_loss: 0.7371 - val_acc: 0.6912\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.2826 - acc: 0.8751 - val_loss: 0.7087 - val_acc: 0.7260\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 568us/sample - loss: 0.7561 - acc: 0.6452 - val_loss: 0.6856 - val_acc: 0.4972\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5900 - acc: 0.6938 - val_loss: 1.2818 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5506 - acc: 0.7184 - val_loss: 2.6474 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.5225 - acc: 0.7401 - val_loss: 3.4272 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4981 - acc: 0.7552 - val_loss: 2.9326 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4699 - acc: 0.7724 - val_loss: 2.8272 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4531 - acc: 0.7832 - val_loss: 2.2925 - val_acc: 0.4904\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4302 - acc: 0.7969 - val_loss: 1.4614 - val_acc: 0.5168\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.4116 - acc: 0.8059 - val_loss: 0.9887 - val_acc: 0.5892\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3857 - acc: 0.8217 - val_loss: 0.6978 - val_acc: 0.6676\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.3677 - acc: 0.8342 - val_loss: 0.7608 - val_acc: 0.6688\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.3529 - acc: 0.8405 - val_loss: 0.5650 - val_acc: 0.7524\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.3345 - acc: 0.8467 - val_loss: 0.6599 - val_acc: 0.7164\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.3101 - acc: 0.8605 - val_loss: 0.6592 - val_acc: 0.7164\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.2948 - acc: 0.8709 - val_loss: 0.6017 - val_acc: 0.7428\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.2781 - acc: 0.8788 - val_loss: 0.5953 - val_acc: 0.7456\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.2624 - acc: 0.8852 - val_loss: 0.6968 - val_acc: 0.7148\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.2448 - acc: 0.8935 - val_loss: 0.6440 - val_acc: 0.7584\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.2256 - acc: 0.9056 - val_loss: 0.6594 - val_acc: 0.7476\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.2095 - acc: 0.9108 - val_loss: 0.6837 - val_acc: 0.7492\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.2026 - acc: 0.9152 - val_loss: 0.7104 - val_acc: 0.7340\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.1861 - acc: 0.9218 - val_loss: 0.7030 - val_acc: 0.7444\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.1744 - acc: 0.9290 - val_loss: 0.8058 - val_acc: 0.7416\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.1541 - acc: 0.9361 - val_loss: 0.7518 - val_acc: 0.7488\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.1514 - acc: 0.9399 - val_loss: 0.7991 - val_acc: 0.7652\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.1349 - acc: 0.9469 - val_loss: 0.8236 - val_acc: 0.7488\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.1335 - acc: 0.9466 - val_loss: 0.8443 - val_acc: 0.7476\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.1206 - acc: 0.9528 - val_loss: 0.8569 - val_acc: 0.7476\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.1142 - acc: 0.9552 - val_loss: 0.8757 - val_acc: 0.7476\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.1104 - acc: 0.9568 - val_loss: 0.8966 - val_acc: 0.7564\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 509us/sample - loss: 0.7347 - acc: 0.6142 - val_loss: 1.0850 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.6438 - acc: 0.6575 - val_loss: 0.8785 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6136 - acc: 0.6735 - val_loss: 0.8330 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5970 - acc: 0.6895 - val_loss: 0.8577 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5773 - acc: 0.6995 - val_loss: 0.9396 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5627 - acc: 0.7128 - val_loss: 0.9705 - val_acc: 0.4900\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5556 - acc: 0.7180 - val_loss: 0.9361 - val_acc: 0.4920\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5433 - acc: 0.7222 - val_loss: 0.8933 - val_acc: 0.5064\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5388 - acc: 0.7315 - val_loss: 0.8146 - val_acc: 0.5344\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.5297 - acc: 0.7342 - val_loss: 0.7424 - val_acc: 0.5680\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5236 - acc: 0.7409 - val_loss: 0.6963 - val_acc: 0.5956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5136 - acc: 0.7469 - val_loss: 0.6099 - val_acc: 0.6540\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.5075 - acc: 0.7521 - val_loss: 0.5888 - val_acc: 0.6740\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.4993 - acc: 0.7571 - val_loss: 0.5687 - val_acc: 0.6996\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4911 - acc: 0.7636 - val_loss: 0.5828 - val_acc: 0.6820\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4810 - acc: 0.7688 - val_loss: 0.5679 - val_acc: 0.6944\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4798 - acc: 0.7756 - val_loss: 0.5512 - val_acc: 0.7064\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4734 - acc: 0.7756 - val_loss: 0.5223 - val_acc: 0.7356\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4682 - acc: 0.7799 - val_loss: 0.5360 - val_acc: 0.7276\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4614 - acc: 0.7826 - val_loss: 0.5323 - val_acc: 0.7300\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4596 - acc: 0.7831 - val_loss: 0.5535 - val_acc: 0.7124\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4481 - acc: 0.7919 - val_loss: 0.5400 - val_acc: 0.7256\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4450 - acc: 0.7901 - val_loss: 0.5684 - val_acc: 0.7092\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4385 - acc: 0.7962 - val_loss: 0.5531 - val_acc: 0.7200\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4350 - acc: 0.7950 - val_loss: 0.5463 - val_acc: 0.7196\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4280 - acc: 0.8013 - val_loss: 0.5384 - val_acc: 0.7372\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4266 - acc: 0.8046 - val_loss: 0.5295 - val_acc: 0.7416\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4224 - acc: 0.8059 - val_loss: 0.5508 - val_acc: 0.7216\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4224 - acc: 0.8039 - val_loss: 0.5364 - val_acc: 0.7368\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4172 - acc: 0.8069 - val_loss: 0.5553 - val_acc: 0.7220\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 516us/sample - loss: 0.7324 - acc: 0.6244 - val_loss: 0.7644 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.6407 - acc: 0.6652 - val_loss: 0.7443 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.6099 - acc: 0.6762 - val_loss: 0.8230 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.5785 - acc: 0.7018 - val_loss: 0.9898 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5638 - acc: 0.7088 - val_loss: 1.1903 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5441 - acc: 0.7238 - val_loss: 1.3302 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5318 - acc: 0.7328 - val_loss: 1.3359 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5180 - acc: 0.7441 - val_loss: 1.2610 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5051 - acc: 0.7525 - val_loss: 1.1133 - val_acc: 0.4952\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.5011 - acc: 0.7562 - val_loss: 0.9444 - val_acc: 0.5208\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4884 - acc: 0.7629 - val_loss: 0.8290 - val_acc: 0.5592\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4842 - acc: 0.7665 - val_loss: 0.8029 - val_acc: 0.5768\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4706 - acc: 0.7726 - val_loss: 0.7768 - val_acc: 0.5908\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4576 - acc: 0.7815 - val_loss: 0.7656 - val_acc: 0.6036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4485 - acc: 0.7852 - val_loss: 0.7120 - val_acc: 0.6256\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4414 - acc: 0.7911 - val_loss: 0.5959 - val_acc: 0.6820\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4373 - acc: 0.7924 - val_loss: 0.6223 - val_acc: 0.6692\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4302 - acc: 0.7978 - val_loss: 0.6649 - val_acc: 0.6436\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4212 - acc: 0.8028 - val_loss: 0.6322 - val_acc: 0.6672\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4148 - acc: 0.8098 - val_loss: 0.6157 - val_acc: 0.6832\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4039 - acc: 0.8154 - val_loss: 0.6354 - val_acc: 0.6712\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3971 - acc: 0.8170 - val_loss: 0.5743 - val_acc: 0.7200\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3940 - acc: 0.8184 - val_loss: 0.5886 - val_acc: 0.7136\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3852 - acc: 0.8232 - val_loss: 0.6055 - val_acc: 0.6980\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3791 - acc: 0.8265 - val_loss: 0.6372 - val_acc: 0.7012\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3717 - acc: 0.8316 - val_loss: 0.6242 - val_acc: 0.7012\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3714 - acc: 0.8294 - val_loss: 0.6144 - val_acc: 0.7136\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3608 - acc: 0.8318 - val_loss: 0.6133 - val_acc: 0.7128\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3536 - acc: 0.8385 - val_loss: 0.6420 - val_acc: 0.6868\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3536 - acc: 0.8419 - val_loss: 0.5920 - val_acc: 0.7284\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 547us/sample - loss: 0.7142 - acc: 0.6411 - val_loss: 0.6833 - val_acc: 0.5404\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.6335 - acc: 0.6659 - val_loss: 0.6865 - val_acc: 0.4888\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5972 - acc: 0.6902 - val_loss: 0.7936 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5737 - acc: 0.7045 - val_loss: 1.1024 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.5475 - acc: 0.7216 - val_loss: 1.4695 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5364 - acc: 0.7304 - val_loss: 1.7006 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5187 - acc: 0.7416 - val_loss: 1.7435 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5086 - acc: 0.7466 - val_loss: 1.6084 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4955 - acc: 0.7618 - val_loss: 1.5088 - val_acc: 0.4892\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4824 - acc: 0.7656 - val_loss: 1.2832 - val_acc: 0.4936\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4685 - acc: 0.7766 - val_loss: 1.0824 - val_acc: 0.5152\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4600 - acc: 0.7795 - val_loss: 1.0718 - val_acc: 0.5236\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4498 - acc: 0.7862 - val_loss: 0.9877 - val_acc: 0.5448\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4376 - acc: 0.7949 - val_loss: 0.9036 - val_acc: 0.5716\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4275 - acc: 0.7995 - val_loss: 0.9516 - val_acc: 0.5708\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4182 - acc: 0.8038 - val_loss: 0.7109 - val_acc: 0.6400\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4102 - acc: 0.8106 - val_loss: 0.7716 - val_acc: 0.6296\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3990 - acc: 0.8182 - val_loss: 0.8983 - val_acc: 0.5916\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3921 - acc: 0.8231 - val_loss: 0.7316 - val_acc: 0.6472\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3858 - acc: 0.8216 - val_loss: 0.7764 - val_acc: 0.6248\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3723 - acc: 0.8298 - val_loss: 0.6798 - val_acc: 0.6780\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3661 - acc: 0.8318 - val_loss: 0.7871 - val_acc: 0.6316\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3588 - acc: 0.8358 - val_loss: 0.7525 - val_acc: 0.6476\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3528 - acc: 0.8409 - val_loss: 0.9066 - val_acc: 0.6036\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3444 - acc: 0.8445 - val_loss: 0.7332 - val_acc: 0.6624\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3369 - acc: 0.8484 - val_loss: 0.7269 - val_acc: 0.6640\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3275 - acc: 0.8543 - val_loss: 0.7576 - val_acc: 0.6644\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3187 - acc: 0.8596 - val_loss: 0.8638 - val_acc: 0.6276\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3195 - acc: 0.8609 - val_loss: 0.8732 - val_acc: 0.6320\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3074 - acc: 0.8643 - val_loss: 0.7620 - val_acc: 0.6536\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 539us/sample - loss: 0.7852 - acc: 0.6360 - val_loss: 0.6932 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.6166 - acc: 0.6792 - val_loss: 0.7238 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5814 - acc: 0.6995 - val_loss: 0.9801 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5471 - acc: 0.7224 - val_loss: 1.9222 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5188 - acc: 0.7399 - val_loss: 2.9664 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4895 - acc: 0.7591 - val_loss: 3.6143 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4754 - acc: 0.7674 - val_loss: 3.8712 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4580 - acc: 0.7821 - val_loss: 3.9363 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4415 - acc: 0.7918 - val_loss: 3.9131 - val_acc: 0.4884\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4255 - acc: 0.8040 - val_loss: 3.4372 - val_acc: 0.4884\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4052 - acc: 0.8108 - val_loss: 3.5421 - val_acc: 0.4888\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3879 - acc: 0.8221 - val_loss: 3.3019 - val_acc: 0.4888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3696 - acc: 0.8319 - val_loss: 2.7849 - val_acc: 0.4908\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.3573 - acc: 0.8376 - val_loss: 2.2040 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3427 - acc: 0.8445 - val_loss: 2.2024 - val_acc: 0.5024\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3259 - acc: 0.8534 - val_loss: 1.4748 - val_acc: 0.5396\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3068 - acc: 0.8652 - val_loss: 1.1895 - val_acc: 0.5892\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2984 - acc: 0.8682 - val_loss: 0.9851 - val_acc: 0.6320\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2737 - acc: 0.8814 - val_loss: 1.2997 - val_acc: 0.5940\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2592 - acc: 0.8848 - val_loss: 1.0513 - val_acc: 0.6396\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2447 - acc: 0.8959 - val_loss: 0.7514 - val_acc: 0.6952\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2313 - acc: 0.9038 - val_loss: 0.6350 - val_acc: 0.7476\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2101 - acc: 0.9124 - val_loss: 0.6927 - val_acc: 0.7244\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2007 - acc: 0.9162 - val_loss: 0.7527 - val_acc: 0.7240\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1852 - acc: 0.9241 - val_loss: 0.7608 - val_acc: 0.7252\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1771 - acc: 0.9276 - val_loss: 0.9367 - val_acc: 0.6752\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1613 - acc: 0.9352 - val_loss: 0.7429 - val_acc: 0.7408\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1532 - acc: 0.9387 - val_loss: 0.7617 - val_acc: 0.7368\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1458 - acc: 0.9418 - val_loss: 0.7772 - val_acc: 0.7440\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1342 - acc: 0.9464 - val_loss: 0.7791 - val_acc: 0.7548\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 644us/sample - loss: 0.7538 - acc: 0.6021 - val_loss: 0.6830 - val_acc: 0.5996\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6368 - acc: 0.6591 - val_loss: 0.6592 - val_acc: 0.5572\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6039 - acc: 0.6826 - val_loss: 0.7299 - val_acc: 0.5220\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5739 - acc: 0.7000 - val_loss: 0.8409 - val_acc: 0.5240\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5562 - acc: 0.7190 - val_loss: 0.8330 - val_acc: 0.5392\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5451 - acc: 0.7253 - val_loss: 0.7227 - val_acc: 0.5952\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5343 - acc: 0.7326 - val_loss: 0.6026 - val_acc: 0.6644\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5230 - acc: 0.7435 - val_loss: 0.5813 - val_acc: 0.6836\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.5157 - acc: 0.7496 - val_loss: 0.5344 - val_acc: 0.7300\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.5071 - acc: 0.7516 - val_loss: 0.5329 - val_acc: 0.7304\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4943 - acc: 0.7625 - val_loss: 0.5163 - val_acc: 0.7472\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4904 - acc: 0.7638 - val_loss: 0.5190 - val_acc: 0.7384\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4853 - acc: 0.7712 - val_loss: 0.5303 - val_acc: 0.7332\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4759 - acc: 0.7746 - val_loss: 0.5248 - val_acc: 0.7412\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.4663 - acc: 0.7806 - val_loss: 0.5287 - val_acc: 0.7372\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4577 - acc: 0.7859 - val_loss: 0.5195 - val_acc: 0.7424\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4572 - acc: 0.7851 - val_loss: 0.5190 - val_acc: 0.7488\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4514 - acc: 0.7898 - val_loss: 0.5127 - val_acc: 0.7436\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4460 - acc: 0.7941 - val_loss: 0.5416 - val_acc: 0.7356\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4413 - acc: 0.7945 - val_loss: 0.5137 - val_acc: 0.7484\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4329 - acc: 0.8046 - val_loss: 0.5357 - val_acc: 0.7372\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4296 - acc: 0.8032 - val_loss: 0.5178 - val_acc: 0.7412\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4261 - acc: 0.8062 - val_loss: 0.5332 - val_acc: 0.7408\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4207 - acc: 0.8114 - val_loss: 0.5349 - val_acc: 0.7428\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4200 - acc: 0.8115 - val_loss: 0.5278 - val_acc: 0.7468\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4154 - acc: 0.8151 - val_loss: 0.5548 - val_acc: 0.7324\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4104 - acc: 0.8190 - val_loss: 0.5284 - val_acc: 0.7484\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.4045 - acc: 0.8211 - val_loss: 0.5525 - val_acc: 0.7380\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4079 - acc: 0.8182 - val_loss: 0.5495 - val_acc: 0.7428\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4026 - acc: 0.8217 - val_loss: 0.5177 - val_acc: 0.7476\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 655us/sample - loss: 0.7222 - acc: 0.6224 - val_loss: 0.6962 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6176 - acc: 0.6756 - val_loss: 0.7267 - val_acc: 0.4912\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5864 - acc: 0.6959 - val_loss: 1.0446 - val_acc: 0.4888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5570 - acc: 0.7181 - val_loss: 1.2238 - val_acc: 0.4904\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5396 - acc: 0.7311 - val_loss: 1.1007 - val_acc: 0.4936\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.5178 - acc: 0.7435 - val_loss: 1.0540 - val_acc: 0.5064\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.5072 - acc: 0.7506 - val_loss: 0.7388 - val_acc: 0.6120\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4937 - acc: 0.7610 - val_loss: 0.6657 - val_acc: 0.6652\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4827 - acc: 0.7675 - val_loss: 0.6693 - val_acc: 0.6676\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4703 - acc: 0.7769 - val_loss: 0.5710 - val_acc: 0.7068\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4606 - acc: 0.7822 - val_loss: 0.5806 - val_acc: 0.7068\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4523 - acc: 0.7866 - val_loss: 0.5891 - val_acc: 0.7096\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4408 - acc: 0.7936 - val_loss: 0.5569 - val_acc: 0.7248\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4310 - acc: 0.8001 - val_loss: 0.5838 - val_acc: 0.7180\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4214 - acc: 0.8066 - val_loss: 0.5218 - val_acc: 0.7360\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4149 - acc: 0.8116 - val_loss: 0.5976 - val_acc: 0.7156\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4083 - acc: 0.8117 - val_loss: 0.5693 - val_acc: 0.7324\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3977 - acc: 0.8204 - val_loss: 0.5855 - val_acc: 0.7236\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.3970 - acc: 0.8229 - val_loss: 0.5487 - val_acc: 0.7376\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.3878 - acc: 0.8255 - val_loss: 0.5734 - val_acc: 0.7272\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3870 - acc: 0.8276 - val_loss: 0.5786 - val_acc: 0.7336\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3719 - acc: 0.8365 - val_loss: 0.6217 - val_acc: 0.7068\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3674 - acc: 0.8360 - val_loss: 0.5604 - val_acc: 0.7420\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3644 - acc: 0.8386 - val_loss: 0.5665 - val_acc: 0.7512\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3620 - acc: 0.8411 - val_loss: 0.5539 - val_acc: 0.7556\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3562 - acc: 0.8434 - val_loss: 0.5903 - val_acc: 0.7364\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.3473 - acc: 0.8490 - val_loss: 0.5906 - val_acc: 0.7332\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3412 - acc: 0.8533 - val_loss: 0.6010 - val_acc: 0.7276\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3398 - acc: 0.8538 - val_loss: 0.6088 - val_acc: 0.7416\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3300 - acc: 0.8570 - val_loss: 0.6629 - val_acc: 0.7184\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 680us/sample - loss: 0.6977 - acc: 0.6352 - val_loss: 0.7017 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.6049 - acc: 0.6809 - val_loss: 0.8092 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5766 - acc: 0.6985 - val_loss: 1.1918 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5442 - acc: 0.7254 - val_loss: 1.4781 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5250 - acc: 0.7362 - val_loss: 1.5296 - val_acc: 0.4880\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5063 - acc: 0.7518 - val_loss: 1.2770 - val_acc: 0.4952\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4926 - acc: 0.7615 - val_loss: 1.0091 - val_acc: 0.5428\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4741 - acc: 0.7716 - val_loss: 0.7999 - val_acc: 0.6084\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4636 - acc: 0.7828 - val_loss: 0.6708 - val_acc: 0.6536\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.4467 - acc: 0.7918 - val_loss: 0.7121 - val_acc: 0.6584\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4366 - acc: 0.7955 - val_loss: 0.6037 - val_acc: 0.6976\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4260 - acc: 0.8039 - val_loss: 0.7005 - val_acc: 0.6756\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4153 - acc: 0.8110 - val_loss: 0.5593 - val_acc: 0.7080\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4003 - acc: 0.8160 - val_loss: 0.6804 - val_acc: 0.6828\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3950 - acc: 0.8205 - val_loss: 0.6160 - val_acc: 0.7116\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3798 - acc: 0.8295 - val_loss: 0.5621 - val_acc: 0.7304\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3681 - acc: 0.8354 - val_loss: 0.6404 - val_acc: 0.6988\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3700 - acc: 0.8339 - val_loss: 0.6621 - val_acc: 0.7000\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3525 - acc: 0.8451 - val_loss: 0.6045 - val_acc: 0.7116\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3485 - acc: 0.8444 - val_loss: 0.6557 - val_acc: 0.7080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3357 - acc: 0.8549 - val_loss: 0.6913 - val_acc: 0.6896\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3314 - acc: 0.8548 - val_loss: 0.7074 - val_acc: 0.6976\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.3237 - acc: 0.8578 - val_loss: 0.7809 - val_acc: 0.6608\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.3176 - acc: 0.8634 - val_loss: 0.6556 - val_acc: 0.7216\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3111 - acc: 0.8659 - val_loss: 0.6699 - val_acc: 0.7188\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3043 - acc: 0.8658 - val_loss: 0.6820 - val_acc: 0.7184\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.2916 - acc: 0.8746 - val_loss: 0.6503 - val_acc: 0.7164\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.2914 - acc: 0.8732 - val_loss: 0.7127 - val_acc: 0.7112\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.2822 - acc: 0.8801 - val_loss: 0.7192 - val_acc: 0.7116\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.2774 - acc: 0.8815 - val_loss: 0.8087 - val_acc: 0.7020\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 691us/sample - loss: 0.7849 - acc: 0.6383 - val_loss: 0.7030 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6058 - acc: 0.6825 - val_loss: 0.8957 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.5550 - acc: 0.7171 - val_loss: 2.0350 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.5158 - acc: 0.7426 - val_loss: 2.8157 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.4882 - acc: 0.7616 - val_loss: 2.8004 - val_acc: 0.4892\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4675 - acc: 0.7766 - val_loss: 2.7393 - val_acc: 0.4884\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4401 - acc: 0.7915 - val_loss: 1.9951 - val_acc: 0.4928\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4170 - acc: 0.8040 - val_loss: 1.5630 - val_acc: 0.5080\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3974 - acc: 0.8191 - val_loss: 1.1892 - val_acc: 0.5672\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3809 - acc: 0.8285 - val_loss: 0.9881 - val_acc: 0.6164\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3575 - acc: 0.8392 - val_loss: 0.7492 - val_acc: 0.6672\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3345 - acc: 0.8497 - val_loss: 0.7223 - val_acc: 0.6720\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.3154 - acc: 0.8585 - val_loss: 0.8618 - val_acc: 0.6624\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.2961 - acc: 0.8692 - val_loss: 0.6499 - val_acc: 0.7264\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.2754 - acc: 0.8814 - val_loss: 0.6741 - val_acc: 0.7120\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.2610 - acc: 0.8865 - val_loss: 0.7583 - val_acc: 0.7160\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.2346 - acc: 0.9022 - val_loss: 0.6923 - val_acc: 0.7412\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.2194 - acc: 0.9069 - val_loss: 0.7475 - val_acc: 0.7244\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.1987 - acc: 0.9174 - val_loss: 0.8369 - val_acc: 0.7168\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.1891 - acc: 0.9224 - val_loss: 0.8251 - val_acc: 0.7396\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.1771 - acc: 0.9274 - val_loss: 0.8710 - val_acc: 0.7184\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.1640 - acc: 0.9334 - val_loss: 0.8774 - val_acc: 0.7300\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.1468 - acc: 0.9420 - val_loss: 0.9365 - val_acc: 0.7328\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.1324 - acc: 0.9467 - val_loss: 1.0399 - val_acc: 0.7300\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.1382 - acc: 0.9455 - val_loss: 0.9004 - val_acc: 0.7396\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.1231 - acc: 0.9508 - val_loss: 0.9660 - val_acc: 0.7408\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.1183 - acc: 0.9533 - val_loss: 1.0460 - val_acc: 0.7296\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.1130 - acc: 0.9546 - val_loss: 1.0126 - val_acc: 0.7312\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.1017 - acc: 0.9604 - val_loss: 0.9969 - val_acc: 0.7372\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.1036 - acc: 0.9585 - val_loss: 1.0189 - val_acc: 0.7456\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 630us/sample - loss: 0.7424 - acc: 0.5985 - val_loss: 0.8984 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6496 - acc: 0.6494 - val_loss: 0.7977 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6190 - acc: 0.6672 - val_loss: 0.7539 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5984 - acc: 0.6835 - val_loss: 0.7553 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.5832 - acc: 0.6934 - val_loss: 0.8640 - val_acc: 0.4888\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5626 - acc: 0.7122 - val_loss: 0.9632 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5582 - acc: 0.7169 - val_loss: 1.0026 - val_acc: 0.4896\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5439 - acc: 0.7242 - val_loss: 0.9469 - val_acc: 0.4940\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5358 - acc: 0.7324 - val_loss: 0.9564 - val_acc: 0.5048\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5271 - acc: 0.7367 - val_loss: 0.8710 - val_acc: 0.5328\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5228 - acc: 0.7445 - val_loss: 0.7662 - val_acc: 0.5840\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5115 - acc: 0.7515 - val_loss: 0.7258 - val_acc: 0.6076\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5053 - acc: 0.7545 - val_loss: 0.6550 - val_acc: 0.6308\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4958 - acc: 0.7629 - val_loss: 0.6229 - val_acc: 0.6528\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4878 - acc: 0.7643 - val_loss: 0.5582 - val_acc: 0.7068\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4818 - acc: 0.7738 - val_loss: 0.5779 - val_acc: 0.6908\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4764 - acc: 0.7723 - val_loss: 0.5660 - val_acc: 0.7072\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4687 - acc: 0.7800 - val_loss: 0.5831 - val_acc: 0.6952\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4616 - acc: 0.7825 - val_loss: 0.5569 - val_acc: 0.7180\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4571 - acc: 0.7847 - val_loss: 0.5587 - val_acc: 0.7140\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4524 - acc: 0.7892 - val_loss: 0.5320 - val_acc: 0.7400\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4447 - acc: 0.7953 - val_loss: 0.5503 - val_acc: 0.7216\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4421 - acc: 0.7935 - val_loss: 0.5433 - val_acc: 0.7340\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4384 - acc: 0.7956 - val_loss: 0.5486 - val_acc: 0.7332\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4310 - acc: 0.8019 - val_loss: 0.5802 - val_acc: 0.7128\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4232 - acc: 0.8124 - val_loss: 0.5435 - val_acc: 0.7300\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4235 - acc: 0.8094 - val_loss: 0.5568 - val_acc: 0.7208\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4186 - acc: 0.8109 - val_loss: 0.5371 - val_acc: 0.7428\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4098 - acc: 0.8166 - val_loss: 0.5449 - val_acc: 0.7388\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4085 - acc: 0.8165 - val_loss: 0.5568 - val_acc: 0.7328\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 635us/sample - loss: 0.7137 - acc: 0.6152 - val_loss: 0.6907 - val_acc: 0.4900\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6349 - acc: 0.6640 - val_loss: 0.6999 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6000 - acc: 0.6889 - val_loss: 0.7218 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5810 - acc: 0.6941 - val_loss: 0.8527 - val_acc: 0.4884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5662 - acc: 0.7084 - val_loss: 1.0508 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5457 - acc: 0.7231 - val_loss: 1.3428 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5333 - acc: 0.7326 - val_loss: 1.5300 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5164 - acc: 0.7439 - val_loss: 1.5141 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5076 - acc: 0.7501 - val_loss: 1.4607 - val_acc: 0.4888\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4970 - acc: 0.7600 - val_loss: 1.2853 - val_acc: 0.4944\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4852 - acc: 0.7667 - val_loss: 1.0596 - val_acc: 0.5192\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4753 - acc: 0.7749 - val_loss: 0.8712 - val_acc: 0.5764\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4652 - acc: 0.7806 - val_loss: 0.8196 - val_acc: 0.5944\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4582 - acc: 0.7835 - val_loss: 0.8222 - val_acc: 0.5916\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4489 - acc: 0.7912 - val_loss: 0.8343 - val_acc: 0.6060\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4424 - acc: 0.7951 - val_loss: 0.7612 - val_acc: 0.6336\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4309 - acc: 0.8008 - val_loss: 0.7035 - val_acc: 0.6708\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4238 - acc: 0.8058 - val_loss: 0.7162 - val_acc: 0.6472\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.4148 - acc: 0.8127 - val_loss: 0.7883 - val_acc: 0.6352\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4076 - acc: 0.8148 - val_loss: 0.7483 - val_acc: 0.6460\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4021 - acc: 0.8219 - val_loss: 0.6733 - val_acc: 0.6848\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3928 - acc: 0.8285 - val_loss: 0.6203 - val_acc: 0.7188\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3879 - acc: 0.8271 - val_loss: 0.6240 - val_acc: 0.7172\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.3844 - acc: 0.8281 - val_loss: 0.6937 - val_acc: 0.6872\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3741 - acc: 0.8368 - val_loss: 0.7482 - val_acc: 0.6708\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3698 - acc: 0.8375 - val_loss: 0.6926 - val_acc: 0.6940\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3549 - acc: 0.8446 - val_loss: 0.6767 - val_acc: 0.6972\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.3566 - acc: 0.8444 - val_loss: 0.6378 - val_acc: 0.7236\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.3505 - acc: 0.8452 - val_loss: 0.6279 - val_acc: 0.7180\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3413 - acc: 0.8528 - val_loss: 0.7296 - val_acc: 0.7040\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 657us/sample - loss: 0.7301 - acc: 0.6225 - val_loss: 0.6945 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6329 - acc: 0.6625 - val_loss: 0.6819 - val_acc: 0.6084\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5989 - acc: 0.6842 - val_loss: 0.7180 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5722 - acc: 0.7095 - val_loss: 0.9733 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5515 - acc: 0.7178 - val_loss: 1.3229 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5316 - acc: 0.7340 - val_loss: 1.7527 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5170 - acc: 0.7458 - val_loss: 1.7417 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5030 - acc: 0.7566 - val_loss: 1.6946 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4945 - acc: 0.7619 - val_loss: 1.6332 - val_acc: 0.4896\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4810 - acc: 0.7665 - val_loss: 1.3982 - val_acc: 0.4944\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4685 - acc: 0.7778 - val_loss: 1.3445 - val_acc: 0.4996\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4577 - acc: 0.7831 - val_loss: 1.2197 - val_acc: 0.5196\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4514 - acc: 0.7891 - val_loss: 1.1169 - val_acc: 0.5292\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4355 - acc: 0.7959 - val_loss: 0.9185 - val_acc: 0.5732\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4261 - acc: 0.7998 - val_loss: 1.0563 - val_acc: 0.5508\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.4192 - acc: 0.8064 - val_loss: 0.9663 - val_acc: 0.5664\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.4079 - acc: 0.8141 - val_loss: 0.8390 - val_acc: 0.6184\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.3953 - acc: 0.8221 - val_loss: 0.8796 - val_acc: 0.6036\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3903 - acc: 0.8221 - val_loss: 0.9278 - val_acc: 0.5984\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3831 - acc: 0.8239 - val_loss: 0.9040 - val_acc: 0.6080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3744 - acc: 0.8301 - val_loss: 0.8303 - val_acc: 0.6272\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3655 - acc: 0.8355 - val_loss: 0.9642 - val_acc: 0.5972\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3575 - acc: 0.8418 - val_loss: 0.9403 - val_acc: 0.6144\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3481 - acc: 0.8456 - val_loss: 0.9795 - val_acc: 0.5984\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3397 - acc: 0.8492 - val_loss: 0.8513 - val_acc: 0.6340\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3326 - acc: 0.8540 - val_loss: 0.8193 - val_acc: 0.6488\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3288 - acc: 0.8563 - val_loss: 0.8109 - val_acc: 0.6608\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3230 - acc: 0.8591 - val_loss: 0.7110 - val_acc: 0.6996\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3109 - acc: 0.8654 - val_loss: 0.8079 - val_acc: 0.6628\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3066 - acc: 0.8697 - val_loss: 0.8739 - val_acc: 0.6668\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 680us/sample - loss: 0.7790 - acc: 0.6365 - val_loss: 0.6911 - val_acc: 0.5116\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6190 - acc: 0.6828 - val_loss: 0.6945 - val_acc: 0.4936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5837 - acc: 0.6981 - val_loss: 0.7250 - val_acc: 0.4888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.5455 - acc: 0.7201 - val_loss: 1.4696 - val_acc: 0.4884\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5157 - acc: 0.7413 - val_loss: 2.2129 - val_acc: 0.4884\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4877 - acc: 0.7634 - val_loss: 2.4840 - val_acc: 0.4884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4655 - acc: 0.7773 - val_loss: 3.0106 - val_acc: 0.4884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4518 - acc: 0.7830 - val_loss: 2.6811 - val_acc: 0.4884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.4313 - acc: 0.7945 - val_loss: 3.1345 - val_acc: 0.4884\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4093 - acc: 0.8104 - val_loss: 3.0362 - val_acc: 0.4884\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3921 - acc: 0.8189 - val_loss: 3.1822 - val_acc: 0.4888\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3777 - acc: 0.8265 - val_loss: 2.8846 - val_acc: 0.4884\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3568 - acc: 0.8372 - val_loss: 2.8946 - val_acc: 0.4908\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3379 - acc: 0.8476 - val_loss: 2.7558 - val_acc: 0.4904\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3185 - acc: 0.8595 - val_loss: 2.1178 - val_acc: 0.5088\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3069 - acc: 0.8634 - val_loss: 1.8871 - val_acc: 0.5196\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2849 - acc: 0.8751 - val_loss: 1.7302 - val_acc: 0.5452\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2641 - acc: 0.8831 - val_loss: 1.4354 - val_acc: 0.5708\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.2483 - acc: 0.8942 - val_loss: 0.9943 - val_acc: 0.6448\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.2358 - acc: 0.8992 - val_loss: 1.0953 - val_acc: 0.6392\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.2158 - acc: 0.9077 - val_loss: 1.0232 - val_acc: 0.6804\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2000 - acc: 0.9155 - val_loss: 0.8974 - val_acc: 0.7012\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1833 - acc: 0.9236 - val_loss: 0.8419 - val_acc: 0.7128\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1683 - acc: 0.9322 - val_loss: 0.8113 - val_acc: 0.7460\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1539 - acc: 0.9372 - val_loss: 0.8190 - val_acc: 0.7240\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1420 - acc: 0.9443 - val_loss: 0.9587 - val_acc: 0.7008\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1434 - acc: 0.9421 - val_loss: 0.8410 - val_acc: 0.7528\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1277 - acc: 0.9510 - val_loss: 0.8687 - val_acc: 0.7408\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1124 - acc: 0.9562 - val_loss: 0.8951 - val_acc: 0.7476\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1106 - acc: 0.9566 - val_loss: 0.9401 - val_acc: 0.7552\n"
     ]
    }
   ],
   "source": [
    "Tiefe = [1,2,3]\n",
    "Batchgrose = [128,254]\n",
    "Breite = [50,100,160,600]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-Time-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\TimePerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTrainingT.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                x= layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            #model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "            history=model.fit(XTrainingT,YTraining,\n",
    "                              validation_data=(XValT,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Netz mit den besten Parametern speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_114 (Flatten)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_382 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_383 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_197 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_384 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_385 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_198 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_313 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 11,942\n",
      "Trainable params: 11,322\n",
      "Non-trainable params: 620\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.7404 - acc: 0.6164\n",
      "Epoch 00001: val_acc improved from -inf to 0.48840, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.49.model\n",
      "17000/17000 [==============================] - 50s 3ms/sample - loss: 0.7395 - acc: 0.6168 - val_loss: 0.7048 - val_acc: 0.4884\n",
      "Epoch 2/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.6327 - acc: 0.6642\n",
      "Epoch 00002: val_acc improved from 0.48840 to 0.49280, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.49.model\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6314 - acc: 0.6649 - val_loss: 0.7284 - val_acc: 0.4928\n",
      "Epoch 3/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.6846\n",
      "Epoch 00003: val_acc improved from 0.49280 to 0.50480, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.50.model\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.5974 - acc: 0.6842 - val_loss: 0.8044 - val_acc: 0.5048\n",
      "Epoch 4/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.7043\n",
      "Epoch 00004: val_acc improved from 0.50480 to 0.52000, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.52.model\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.5698 - acc: 0.7036 - val_loss: 0.8147 - val_acc: 0.5200\n",
      "Epoch 5/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.7182\n",
      "Epoch 00005: val_acc improved from 0.52000 to 0.57480, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.57.model\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5532 - acc: 0.7179 - val_loss: 0.7108 - val_acc: 0.5748\n",
      "Epoch 6/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.7242\n",
      "Epoch 00006: val_acc improved from 0.57480 to 0.59680, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.60.model\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.5444 - acc: 0.7244 - val_loss: 0.6743 - val_acc: 0.5968\n",
      "Epoch 7/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.7302\n",
      "Epoch 00007: val_acc improved from 0.59680 to 0.69960, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.70.model\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.5363 - acc: 0.7302 - val_loss: 0.5596 - val_acc: 0.6996\n",
      "Epoch 8/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.7409\n",
      "Epoch 00008: val_acc improved from 0.69960 to 0.70400, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.70.model\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.5236 - acc: 0.7411 - val_loss: 0.5599 - val_acc: 0.7040\n",
      "Epoch 9/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.7443\n",
      "Epoch 00009: val_acc improved from 0.70400 to 0.70960, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.71.model\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.5205 - acc: 0.7438 - val_loss: 0.5510 - val_acc: 0.7096\n",
      "Epoch 10/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7534\n",
      "Epoch 00010: val_acc did not improve from 0.70960\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5089 - acc: 0.7531 - val_loss: 0.5509 - val_acc: 0.7040\n",
      "Epoch 11/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.7574\n",
      "Epoch 00011: val_acc improved from 0.70960 to 0.72640, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.73.model\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5019 - acc: 0.7564 - val_loss: 0.5380 - val_acc: 0.7264\n",
      "Epoch 12/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.7631\n",
      "Epoch 00012: val_acc did not improve from 0.72640\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4941 - acc: 0.7628 - val_loss: 0.5418 - val_acc: 0.7244\n",
      "Epoch 13/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.7673\n",
      "Epoch 00013: val_acc improved from 0.72640 to 0.74120, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.74.model\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.4855 - acc: 0.7678 - val_loss: 0.5235 - val_acc: 0.7412\n",
      "Epoch 14/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4798 - acc: 0.7733\n",
      "Epoch 00014: val_acc did not improve from 0.74120\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.4802 - acc: 0.7730 - val_loss: 0.5272 - val_acc: 0.7348\n",
      "Epoch 15/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.7764\n",
      "Epoch 00015: val_acc did not improve from 0.74120\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4714 - acc: 0.7764 - val_loss: 0.5290 - val_acc: 0.7392\n",
      "Epoch 16/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4613 - acc: 0.7807\n",
      "Epoch 00016: val_acc improved from 0.74120 to 0.74800, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.75.model\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.4616 - acc: 0.7806 - val_loss: 0.5186 - val_acc: 0.7480\n",
      "Epoch 17/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4565 - acc: 0.7843\n",
      "Epoch 00017: val_acc improved from 0.74800 to 0.76320, saving model to Perceptron-PMT-Time-MuEl-val-acc_0.76.model\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.4565 - acc: 0.7845 - val_loss: 0.5191 - val_acc: 0.7632\n",
      "Epoch 18/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.7898\n",
      "Epoch 00018: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4487 - acc: 0.7897 - val_loss: 0.5330 - val_acc: 0.7276\n",
      "Epoch 19/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.4432 - acc: 0.7929\n",
      "Epoch 00019: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4444 - acc: 0.7923 - val_loss: 0.5161 - val_acc: 0.7620\n",
      "Epoch 20/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.7954\n",
      "Epoch 00020: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4402 - acc: 0.7958 - val_loss: 0.5175 - val_acc: 0.7440\n",
      "Epoch 21/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8020\n",
      "Epoch 00021: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.4302 - acc: 0.8025 - val_loss: 0.5230 - val_acc: 0.7504\n",
      "Epoch 22/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.4289 - acc: 0.8006\n",
      "Epoch 00022: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.4292 - acc: 0.8005 - val_loss: 0.5346 - val_acc: 0.7328\n",
      "Epoch 23/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8032\n",
      "Epoch 00023: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4253 - acc: 0.8027 - val_loss: 0.5459 - val_acc: 0.7284\n",
      "Epoch 24/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.8018\n",
      "Epoch 00024: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4240 - acc: 0.8017 - val_loss: 0.5187 - val_acc: 0.7420\n",
      "Epoch 25/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8126\n",
      "Epoch 00025: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4180 - acc: 0.8126 - val_loss: 0.5237 - val_acc: 0.7488\n",
      "Epoch 26/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8114\n",
      "Epoch 00026: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.4123 - acc: 0.8118 - val_loss: 0.5308 - val_acc: 0.7468\n",
      "Epoch 27/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8152\n",
      "Epoch 00027: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.4038 - acc: 0.8150 - val_loss: 0.5343 - val_acc: 0.7520\n",
      "Epoch 28/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8167\n",
      "Epoch 00028: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4078 - acc: 0.8168 - val_loss: 0.5372 - val_acc: 0.7400\n",
      "Epoch 29/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8168\n",
      "Epoch 00029: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.4051 - acc: 0.8164 - val_loss: 0.5316 - val_acc: 0.7432\n",
      "Epoch 30/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8177\n",
      "Epoch 00030: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4026 - acc: 0.8179 - val_loss: 0.5334 - val_acc: 0.7484\n",
      "Epoch 31/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8233\n",
      "Epoch 00031: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3948 - acc: 0.8234 - val_loss: 0.5459 - val_acc: 0.7464\n",
      "Epoch 32/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8236\n",
      "Epoch 00032: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3944 - acc: 0.8238 - val_loss: 0.5331 - val_acc: 0.7492\n",
      "Epoch 33/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8245\n",
      "Epoch 00033: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3938 - acc: 0.8247 - val_loss: 0.5402 - val_acc: 0.7400\n",
      "Epoch 34/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8288\n",
      "Epoch 00034: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3863 - acc: 0.8289 - val_loss: 0.5462 - val_acc: 0.7400\n",
      "Epoch 35/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8282\n",
      "Epoch 00035: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.3823 - acc: 0.8279 - val_loss: 0.5504 - val_acc: 0.7428\n",
      "Epoch 36/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8296\n",
      "Epoch 00036: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3861 - acc: 0.8292 - val_loss: 0.5713 - val_acc: 0.7300\n",
      "Epoch 37/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3835 - acc: 0.8279\n",
      "Epoch 00037: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.3841 - acc: 0.8277 - val_loss: 0.5456 - val_acc: 0.7396\n",
      "Epoch 38/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8278\n",
      "Epoch 00038: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.3808 - acc: 0.8280 - val_loss: 0.5545 - val_acc: 0.7444\n",
      "Epoch 39/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8329\n",
      "Epoch 00039: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3761 - acc: 0.8327 - val_loss: 0.5569 - val_acc: 0.7396\n",
      "Epoch 40/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8328\n",
      "Epoch 00040: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.3719 - acc: 0.8334 - val_loss: 0.5637 - val_acc: 0.7448\n",
      "Epoch 41/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8343\n",
      "Epoch 00041: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3729 - acc: 0.8335 - val_loss: 0.5579 - val_acc: 0.7408\n",
      "Epoch 42/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3695 - acc: 0.8356\n",
      "Epoch 00042: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.3690 - acc: 0.8359 - val_loss: 0.5822 - val_acc: 0.7284\n",
      "Epoch 43/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8358\n",
      "Epoch 00043: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3701 - acc: 0.8351 - val_loss: 0.5658 - val_acc: 0.7440\n",
      "Epoch 44/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8378\n",
      "Epoch 00044: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3650 - acc: 0.8375 - val_loss: 0.5635 - val_acc: 0.7436\n",
      "Epoch 45/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8385\n",
      "Epoch 00045: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.3687 - acc: 0.8378 - val_loss: 0.5498 - val_acc: 0.7476\n",
      "Epoch 46/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8424\n",
      "Epoch 00046: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.3615 - acc: 0.8422 - val_loss: 0.5779 - val_acc: 0.7296\n",
      "Epoch 47/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8429\n",
      "Epoch 00047: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.3579 - acc: 0.8427 - val_loss: 0.5700 - val_acc: 0.7400\n",
      "Epoch 48/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8405\n",
      "Epoch 00048: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.3576 - acc: 0.8402 - val_loss: 0.5665 - val_acc: 0.7400\n",
      "Epoch 49/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8414\n",
      "Epoch 00049: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3593 - acc: 0.8422 - val_loss: 0.5758 - val_acc: 0.7404\n",
      "Epoch 50/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8447\n",
      "Epoch 00050: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3533 - acc: 0.8451 - val_loss: 0.5832 - val_acc: 0.7296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8429\n",
      "Epoch 00051: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.3545 - acc: 0.8423 - val_loss: 0.5845 - val_acc: 0.7380\n",
      "Epoch 52/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8431\n",
      "Epoch 00052: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.3528 - acc: 0.8434 - val_loss: 0.5706 - val_acc: 0.7372\n",
      "Epoch 53/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8475\n",
      "Epoch 00053: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.3487 - acc: 0.8476 - val_loss: 0.5865 - val_acc: 0.7336\n",
      "Epoch 54/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.8476\n",
      "Epoch 00054: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.3473 - acc: 0.8471 - val_loss: 0.5870 - val_acc: 0.7332\n",
      "Epoch 55/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8474\n",
      "Epoch 00055: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3497 - acc: 0.8472 - val_loss: 0.5737 - val_acc: 0.7388\n",
      "Epoch 56/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8496\n",
      "Epoch 00056: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.3448 - acc: 0.8495 - val_loss: 0.5728 - val_acc: 0.7460\n",
      "Epoch 57/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8448\n",
      "Epoch 00057: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.3480 - acc: 0.8447 - val_loss: 0.5829 - val_acc: 0.7372\n",
      "Epoch 58/60\n",
      "16384/17000 [===========================>..] - ETA: 0s - loss: 0.3375 - acc: 0.8513\n",
      "Epoch 00058: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.3362 - acc: 0.8517 - val_loss: 0.5894 - val_acc: 0.7296\n",
      "Epoch 59/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8520\n",
      "Epoch 00059: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.3394 - acc: 0.8519 - val_loss: 0.5762 - val_acc: 0.7460\n",
      "Epoch 60/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8511\n",
      "Epoch 00060: val_acc did not improve from 0.76320\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.3433 - acc: 0.8512 - val_loss: 0.5826 - val_acc: 0.7336\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "Tiefe = [2]\n",
    "Batchgrose = [128]\n",
    "Breite = [50]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-Time-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\ChargePerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTrainingT.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                x= layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "            #filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "            filepath=\"Perceptron-PMT-Time-MuEl-val-acc_{val_acc:.2f}.model\" \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "\n",
    "            history=model.fit(XTrainingT,YTraining,\n",
    "                              validation_data=(XValT,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 60)\n",
    "\n",
    "\n",
    "            print(history.history.keys())\n",
    "            # summarize history for accuracy\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.8165153689822228\n",
      "Test accuracy:  0.5234452\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"Perceptron-PMT-Time-MuEl-val-acc_0.76.model\")\n",
    "score = model.evaluate(XTestC, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMT Charge+Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 695us/sample - loss: 0.6689 - acc: 0.6681 - val_loss: 0.6782 - val_acc: 0.5124\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5669 - acc: 0.7113 - val_loss: 0.9835 - val_acc: 0.4892\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.5365 - acc: 0.7302 - val_loss: 0.8793 - val_acc: 0.5092\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.5162 - acc: 0.7425 - val_loss: 0.7344 - val_acc: 0.5776\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4953 - acc: 0.7589 - val_loss: 0.5887 - val_acc: 0.6696\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4781 - acc: 0.7687 - val_loss: 0.5837 - val_acc: 0.6784\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.4581 - acc: 0.7811 - val_loss: 0.4919 - val_acc: 0.7580\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.4480 - acc: 0.7896 - val_loss: 0.4873 - val_acc: 0.7528\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.4351 - acc: 0.7964 - val_loss: 0.4747 - val_acc: 0.7672\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4228 - acc: 0.8011 - val_loss: 0.4551 - val_acc: 0.7776\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4083 - acc: 0.8091 - val_loss: 0.4645 - val_acc: 0.7700\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3927 - acc: 0.8184 - val_loss: 0.4540 - val_acc: 0.7788\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.3866 - acc: 0.8226 - val_loss: 0.4619 - val_acc: 0.7832\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3771 - acc: 0.8275 - val_loss: 0.4549 - val_acc: 0.7836\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.3678 - acc: 0.8329 - val_loss: 0.4535 - val_acc: 0.7884\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3555 - acc: 0.8365 - val_loss: 0.4802 - val_acc: 0.7676\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3463 - acc: 0.8445 - val_loss: 0.4551 - val_acc: 0.7884\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3428 - acc: 0.8477 - val_loss: 0.4901 - val_acc: 0.7556\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.3350 - acc: 0.8471 - val_loss: 0.4604 - val_acc: 0.7900\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3224 - acc: 0.8561 - val_loss: 0.4629 - val_acc: 0.7872\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.3216 - acc: 0.8568 - val_loss: 0.4831 - val_acc: 0.7768\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3085 - acc: 0.8607 - val_loss: 0.4855 - val_acc: 0.7812\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.3081 - acc: 0.8652 - val_loss: 0.4791 - val_acc: 0.7864\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.3011 - acc: 0.8669 - val_loss: 0.4727 - val_acc: 0.7904\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.2905 - acc: 0.8721 - val_loss: 0.4927 - val_acc: 0.7804\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.2886 - acc: 0.8738 - val_loss: 0.4756 - val_acc: 0.7924\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.2861 - acc: 0.8712 - val_loss: 0.4998 - val_acc: 0.7828\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.2743 - acc: 0.8805 - val_loss: 0.4956 - val_acc: 0.7872\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.2705 - acc: 0.8821 - val_loss: 0.4987 - val_acc: 0.7832\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.2634 - acc: 0.8829 - val_loss: 0.5068 - val_acc: 0.7872\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 727us/sample - loss: 0.6520 - acc: 0.6737 - val_loss: 0.8290 - val_acc: 0.4888\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.5592 - acc: 0.7152 - val_loss: 1.4027 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.5177 - acc: 0.7392 - val_loss: 1.2043 - val_acc: 0.4900\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4879 - acc: 0.7629 - val_loss: 0.8724 - val_acc: 0.5276\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.4620 - acc: 0.7799 - val_loss: 0.6865 - val_acc: 0.6164\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.4355 - acc: 0.7940 - val_loss: 0.5104 - val_acc: 0.7324\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4100 - acc: 0.8106 - val_loss: 0.5138 - val_acc: 0.7408\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3907 - acc: 0.8210 - val_loss: 0.4904 - val_acc: 0.7652\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.3719 - acc: 0.8309 - val_loss: 0.5176 - val_acc: 0.7344\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.3472 - acc: 0.8422 - val_loss: 0.4620 - val_acc: 0.7720\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.3303 - acc: 0.8549 - val_loss: 0.4792 - val_acc: 0.7732\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.3178 - acc: 0.8588 - val_loss: 0.4579 - val_acc: 0.7884\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.2955 - acc: 0.8717 - val_loss: 0.5340 - val_acc: 0.7472\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.2855 - acc: 0.8779 - val_loss: 0.4993 - val_acc: 0.7808\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.2687 - acc: 0.8814 - val_loss: 0.4830 - val_acc: 0.7888\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.2552 - acc: 0.8901 - val_loss: 0.5036 - val_acc: 0.7916\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.2436 - acc: 0.8949 - val_loss: 0.5138 - val_acc: 0.7884\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.2327 - acc: 0.8998 - val_loss: 0.5425 - val_acc: 0.7716\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.2163 - acc: 0.9105 - val_loss: 0.5354 - val_acc: 0.7892\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.2076 - acc: 0.9104 - val_loss: 0.5445 - val_acc: 0.7876\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1981 - acc: 0.9166 - val_loss: 0.5567 - val_acc: 0.7992\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.1838 - acc: 0.9238 - val_loss: 0.5756 - val_acc: 0.7832\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1828 - acc: 0.9258 - val_loss: 0.5809 - val_acc: 0.7812\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1698 - acc: 0.9305 - val_loss: 0.5958 - val_acc: 0.7988\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.1619 - acc: 0.9354 - val_loss: 0.6155 - val_acc: 0.8016\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.1604 - acc: 0.9336 - val_loss: 0.6116 - val_acc: 0.7940\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1496 - acc: 0.9394 - val_loss: 0.6386 - val_acc: 0.8012\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.1445 - acc: 0.9407 - val_loss: 0.6339 - val_acc: 0.8012\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.1388 - acc: 0.9449 - val_loss: 0.6607 - val_acc: 0.7988\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.1335 - acc: 0.9466 - val_loss: 0.6539 - val_acc: 0.7900\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 693us/sample - loss: 0.6728 - acc: 0.6759 - val_loss: 0.8569 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.5590 - acc: 0.7175 - val_loss: 1.8512 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5129 - acc: 0.7447 - val_loss: 1.6678 - val_acc: 0.4888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4832 - acc: 0.7688 - val_loss: 1.1209 - val_acc: 0.4992\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4557 - acc: 0.7868 - val_loss: 0.7531 - val_acc: 0.5916\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.4319 - acc: 0.8006 - val_loss: 0.5765 - val_acc: 0.6944\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.4102 - acc: 0.8158 - val_loss: 0.4769 - val_acc: 0.7716\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3849 - acc: 0.8268 - val_loss: 0.4799 - val_acc: 0.7672\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.3601 - acc: 0.8362 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.3389 - acc: 0.8466 - val_loss: 0.4569 - val_acc: 0.7916\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.3135 - acc: 0.8636 - val_loss: 0.4628 - val_acc: 0.8016\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.2999 - acc: 0.8691 - val_loss: 0.4901 - val_acc: 0.7672\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.2783 - acc: 0.8796 - val_loss: 0.5158 - val_acc: 0.7624\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.2595 - acc: 0.8874 - val_loss: 0.4803 - val_acc: 0.7928\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.2403 - acc: 0.8981 - val_loss: 0.5024 - val_acc: 0.7964\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.2224 - acc: 0.9070 - val_loss: 0.5464 - val_acc: 0.7744\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.2102 - acc: 0.9101 - val_loss: 0.5124 - val_acc: 0.8100\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.1904 - acc: 0.9216 - val_loss: 0.5305 - val_acc: 0.7960\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1828 - acc: 0.9265 - val_loss: 0.5386 - val_acc: 0.8060\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.1700 - acc: 0.9298 - val_loss: 0.5597 - val_acc: 0.7972\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.1569 - acc: 0.9358 - val_loss: 0.5629 - val_acc: 0.8084\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1425 - acc: 0.9440 - val_loss: 0.5972 - val_acc: 0.7896\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.1350 - acc: 0.9450 - val_loss: 0.6160 - val_acc: 0.7996\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.1309 - acc: 0.9460 - val_loss: 0.6374 - val_acc: 0.8036\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.1221 - acc: 0.9522 - val_loss: 0.6609 - val_acc: 0.7880\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.1155 - acc: 0.9561 - val_loss: 0.6616 - val_acc: 0.8008\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.1109 - acc: 0.9573 - val_loss: 0.6796 - val_acc: 0.7956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.1008 - acc: 0.9612 - val_loss: 0.7277 - val_acc: 0.8100\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.0970 - acc: 0.9635 - val_loss: 0.6963 - val_acc: 0.8108\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.0881 - acc: 0.9673 - val_loss: 0.7217 - val_acc: 0.8104\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 707us/sample - loss: 0.6995 - acc: 0.6724 - val_loss: 0.7386 - val_acc: 0.4904\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.5671 - acc: 0.7168 - val_loss: 1.8878 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.5174 - acc: 0.7443 - val_loss: 2.0120 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.4759 - acc: 0.7659 - val_loss: 1.5204 - val_acc: 0.4900\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.4475 - acc: 0.7884 - val_loss: 1.1972 - val_acc: 0.5064\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.4231 - acc: 0.8048 - val_loss: 0.7373 - val_acc: 0.6268\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.3953 - acc: 0.8167 - val_loss: 0.5620 - val_acc: 0.7100\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.3637 - acc: 0.8344 - val_loss: 0.5047 - val_acc: 0.7508\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.3382 - acc: 0.8509 - val_loss: 0.4937 - val_acc: 0.7648\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.3256 - acc: 0.8566 - val_loss: 0.5151 - val_acc: 0.7492\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.2970 - acc: 0.8711 - val_loss: 0.4829 - val_acc: 0.7896\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.2745 - acc: 0.8794 - val_loss: 0.5889 - val_acc: 0.7420\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.2533 - acc: 0.8892 - val_loss: 0.5573 - val_acc: 0.7580\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.2329 - acc: 0.8991 - val_loss: 0.5333 - val_acc: 0.7676\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.2147 - acc: 0.9081 - val_loss: 0.5372 - val_acc: 0.7752\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.1914 - acc: 0.9207 - val_loss: 0.5105 - val_acc: 0.8152\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.1805 - acc: 0.9253 - val_loss: 0.5727 - val_acc: 0.7772\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.1652 - acc: 0.9329 - val_loss: 0.5658 - val_acc: 0.7976\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.1465 - acc: 0.9417 - val_loss: 0.5844 - val_acc: 0.8032\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1330 - acc: 0.9459 - val_loss: 0.6491 - val_acc: 0.7872\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1205 - acc: 0.9524 - val_loss: 0.6199 - val_acc: 0.8056\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.1097 - acc: 0.9576 - val_loss: 0.6616 - val_acc: 0.7960\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.1011 - acc: 0.9628 - val_loss: 0.6563 - val_acc: 0.7992\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.0927 - acc: 0.9656 - val_loss: 0.6673 - val_acc: 0.8032\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.0853 - acc: 0.9696 - val_loss: 0.6997 - val_acc: 0.8152\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.0817 - acc: 0.9685 - val_loss: 0.7067 - val_acc: 0.8048\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.0680 - acc: 0.9759 - val_loss: 0.7129 - val_acc: 0.8136\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.0702 - acc: 0.9741 - val_loss: 0.7497 - val_acc: 0.8004\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.0684 - acc: 0.9754 - val_loss: 0.7484 - val_acc: 0.8076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.0649 - acc: 0.9760 - val_loss: 0.7573 - val_acc: 0.8136\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 742us/sample - loss: 0.6679 - acc: 0.6570 - val_loss: 0.6624 - val_acc: 0.6208\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5740 - acc: 0.7001 - val_loss: 0.6507 - val_acc: 0.5656\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.5486 - acc: 0.7205 - val_loss: 0.7591 - val_acc: 0.5324\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.5243 - acc: 0.7356 - val_loss: 0.8323 - val_acc: 0.5336\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.5035 - acc: 0.7478 - val_loss: 0.6879 - val_acc: 0.6196\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.4902 - acc: 0.7618 - val_loss: 0.5422 - val_acc: 0.7136\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4759 - acc: 0.7709 - val_loss: 0.5072 - val_acc: 0.7344\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4572 - acc: 0.7831 - val_loss: 0.4687 - val_acc: 0.7700\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4402 - acc: 0.7965 - val_loss: 0.4614 - val_acc: 0.7740\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.4290 - acc: 0.8005 - val_loss: 0.4542 - val_acc: 0.7812\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.4178 - acc: 0.8059 - val_loss: 0.4459 - val_acc: 0.7928\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4056 - acc: 0.8138 - val_loss: 0.4465 - val_acc: 0.7896\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3906 - acc: 0.8217 - val_loss: 0.4397 - val_acc: 0.7996\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3771 - acc: 0.8290 - val_loss: 0.4497 - val_acc: 0.7884\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3691 - acc: 0.8352 - val_loss: 0.4422 - val_acc: 0.7912\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3604 - acc: 0.8386 - val_loss: 0.4479 - val_acc: 0.7768\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3508 - acc: 0.8454 - val_loss: 0.4528 - val_acc: 0.7900\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3440 - acc: 0.8491 - val_loss: 0.4488 - val_acc: 0.7932\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 147us/sample - loss: 0.3364 - acc: 0.8529 - val_loss: 0.4465 - val_acc: 0.8004\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3207 - acc: 0.8625 - val_loss: 0.4551 - val_acc: 0.7940\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.3212 - acc: 0.8604 - val_loss: 0.4721 - val_acc: 0.7784\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3095 - acc: 0.8673 - val_loss: 0.4595 - val_acc: 0.7884\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3055 - acc: 0.8697 - val_loss: 0.4734 - val_acc: 0.7828\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3007 - acc: 0.8699 - val_loss: 0.4635 - val_acc: 0.7932\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.2901 - acc: 0.8751 - val_loss: 0.4691 - val_acc: 0.7968\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.2886 - acc: 0.8725 - val_loss: 0.4687 - val_acc: 0.8040\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.2846 - acc: 0.8795 - val_loss: 0.4784 - val_acc: 0.8056\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.2790 - acc: 0.8806 - val_loss: 0.4789 - val_acc: 0.8036\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.2709 - acc: 0.8827 - val_loss: 0.4823 - val_acc: 0.8032\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.2711 - acc: 0.8846 - val_loss: 0.4826 - val_acc: 0.8044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 760us/sample - loss: 0.6565 - acc: 0.6688 - val_loss: 0.6616 - val_acc: 0.7040\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.5692 - acc: 0.7069 - val_loss: 0.8004 - val_acc: 0.4932\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5234 - acc: 0.7377 - val_loss: 1.3044 - val_acc: 0.4892\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.4983 - acc: 0.7550 - val_loss: 1.0325 - val_acc: 0.5036\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.4722 - acc: 0.7734 - val_loss: 0.7212 - val_acc: 0.6104\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.4464 - acc: 0.7884 - val_loss: 0.6496 - val_acc: 0.6408\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4247 - acc: 0.8012 - val_loss: 0.5435 - val_acc: 0.7204\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.4021 - acc: 0.8173 - val_loss: 0.5332 - val_acc: 0.7284\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3797 - acc: 0.8272 - val_loss: 0.4968 - val_acc: 0.7548\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.3586 - acc: 0.8398 - val_loss: 0.5812 - val_acc: 0.7084\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.3456 - acc: 0.8468 - val_loss: 0.5145 - val_acc: 0.7556\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.3298 - acc: 0.8539 - val_loss: 0.4697 - val_acc: 0.7736\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.3182 - acc: 0.8571 - val_loss: 0.4550 - val_acc: 0.7960\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.3024 - acc: 0.8663 - val_loss: 0.4745 - val_acc: 0.7928\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.2888 - acc: 0.8740 - val_loss: 0.5035 - val_acc: 0.7704\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.2728 - acc: 0.8818 - val_loss: 0.4821 - val_acc: 0.7984\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.2589 - acc: 0.8869 - val_loss: 0.5004 - val_acc: 0.7984\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.2483 - acc: 0.8944 - val_loss: 0.5493 - val_acc: 0.7748\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.2304 - acc: 0.9028 - val_loss: 0.5515 - val_acc: 0.7880\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.2191 - acc: 0.9061 - val_loss: 0.5121 - val_acc: 0.8128\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.2152 - acc: 0.9068 - val_loss: 0.5437 - val_acc: 0.7980\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.2095 - acc: 0.9126 - val_loss: 0.5602 - val_acc: 0.7956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.1984 - acc: 0.9165 - val_loss: 0.5611 - val_acc: 0.8112\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.1836 - acc: 0.9250 - val_loss: 0.6290 - val_acc: 0.7740\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.1792 - acc: 0.9244 - val_loss: 0.6197 - val_acc: 0.8004\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.1755 - acc: 0.9279 - val_loss: 0.6197 - val_acc: 0.7832\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.1678 - acc: 0.9311 - val_loss: 0.6751 - val_acc: 0.7680\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.1591 - acc: 0.9360 - val_loss: 0.6237 - val_acc: 0.8008\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.1500 - acc: 0.9397 - val_loss: 0.6543 - val_acc: 0.8048\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.1490 - acc: 0.9392 - val_loss: 0.6872 - val_acc: 0.7984\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 784us/sample - loss: 0.6971 - acc: 0.6675 - val_loss: 0.7173 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.5716 - acc: 0.7054 - val_loss: 1.1032 - val_acc: 0.4888\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.5223 - acc: 0.7414 - val_loss: 1.6400 - val_acc: 0.4888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.4840 - acc: 0.7621 - val_loss: 1.3500 - val_acc: 0.4940\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.4617 - acc: 0.7764 - val_loss: 1.0657 - val_acc: 0.5312\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.4301 - acc: 0.7992 - val_loss: 0.6465 - val_acc: 0.6520\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.4108 - acc: 0.8085 - val_loss: 0.7518 - val_acc: 0.6276\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.3864 - acc: 0.8217 - val_loss: 0.6428 - val_acc: 0.6852\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.3665 - acc: 0.8348 - val_loss: 0.5248 - val_acc: 0.7296\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.3480 - acc: 0.8452 - val_loss: 0.5307 - val_acc: 0.7424\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.3186 - acc: 0.8575 - val_loss: 0.4626 - val_acc: 0.7932\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.3056 - acc: 0.8645 - val_loss: 0.4825 - val_acc: 0.7800\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.2857 - acc: 0.8764 - val_loss: 0.4565 - val_acc: 0.8060\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.2656 - acc: 0.8876 - val_loss: 0.4657 - val_acc: 0.8036\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.2518 - acc: 0.8922 - val_loss: 0.4712 - val_acc: 0.7968\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.2404 - acc: 0.8975 - val_loss: 0.5060 - val_acc: 0.7964\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 147us/sample - loss: 0.2224 - acc: 0.9071 - val_loss: 0.5250 - val_acc: 0.7856\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.2102 - acc: 0.9125 - val_loss: 0.4917 - val_acc: 0.8188\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.1983 - acc: 0.9165 - val_loss: 0.4924 - val_acc: 0.8236\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.1859 - acc: 0.9236 - val_loss: 0.5204 - val_acc: 0.8052\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.1760 - acc: 0.9274 - val_loss: 0.5417 - val_acc: 0.8220\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.1652 - acc: 0.9321 - val_loss: 0.5506 - val_acc: 0.8148\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.1500 - acc: 0.9393 - val_loss: 0.5477 - val_acc: 0.8124\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.1427 - acc: 0.9435 - val_loss: 0.6013 - val_acc: 0.8028\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.1356 - acc: 0.9461 - val_loss: 0.6474 - val_acc: 0.8224\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.1338 - acc: 0.9463 - val_loss: 0.6003 - val_acc: 0.8216\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.1234 - acc: 0.9499 - val_loss: 0.6382 - val_acc: 0.8152\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.1202 - acc: 0.9521 - val_loss: 0.6086 - val_acc: 0.8168\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.1128 - acc: 0.9551 - val_loss: 0.6246 - val_acc: 0.8168\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.1122 - acc: 0.9557 - val_loss: 0.6183 - val_acc: 0.8188\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 14s 798us/sample - loss: 0.7303 - acc: 0.6667 - val_loss: 0.7403 - val_acc: 0.4884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.5574 - acc: 0.7188 - val_loss: 1.3994 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.5117 - acc: 0.7458 - val_loss: 2.0432 - val_acc: 0.4888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.4697 - acc: 0.7762 - val_loss: 1.7880 - val_acc: 0.4896\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.4364 - acc: 0.7941 - val_loss: 1.3348 - val_acc: 0.5060\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.4089 - acc: 0.8085 - val_loss: 0.9187 - val_acc: 0.5872\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.3775 - acc: 0.8275 - val_loss: 0.8167 - val_acc: 0.6156\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.3531 - acc: 0.8420 - val_loss: 0.8511 - val_acc: 0.6244\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.3285 - acc: 0.8549 - val_loss: 0.6606 - val_acc: 0.7076\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.3044 - acc: 0.8707 - val_loss: 0.6257 - val_acc: 0.7168\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.2769 - acc: 0.8794 - val_loss: 0.5026 - val_acc: 0.7816\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.2490 - acc: 0.8943 - val_loss: 0.4885 - val_acc: 0.7988\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.2317 - acc: 0.9011 - val_loss: 0.4905 - val_acc: 0.7968\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.2044 - acc: 0.9155 - val_loss: 0.5275 - val_acc: 0.7908\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.1892 - acc: 0.9215 - val_loss: 0.5399 - val_acc: 0.8092\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.1756 - acc: 0.9276 - val_loss: 0.5125 - val_acc: 0.8104\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.1551 - acc: 0.9383 - val_loss: 0.5590 - val_acc: 0.8056\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.1502 - acc: 0.9393 - val_loss: 0.5601 - val_acc: 0.8292\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.1321 - acc: 0.9468 - val_loss: 0.5975 - val_acc: 0.8276\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.1191 - acc: 0.9524 - val_loss: 0.6212 - val_acc: 0.8224\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.1123 - acc: 0.9547 - val_loss: 0.6196 - val_acc: 0.8316\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.1026 - acc: 0.9589 - val_loss: 0.6474 - val_acc: 0.8264\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.0924 - acc: 0.9625 - val_loss: 0.6571 - val_acc: 0.8268\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0885 - acc: 0.9662 - val_loss: 0.6976 - val_acc: 0.8064\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.0867 - acc: 0.9658 - val_loss: 0.6842 - val_acc: 0.8380\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.0814 - acc: 0.9685 - val_loss: 0.6698 - val_acc: 0.8240\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.0725 - acc: 0.9719 - val_loss: 0.7492 - val_acc: 0.8236\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0725 - acc: 0.9731 - val_loss: 0.6971 - val_acc: 0.8264\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.0731 - acc: 0.9719 - val_loss: 0.7510 - val_acc: 0.8208\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.0660 - acc: 0.9751 - val_loss: 0.7469 - val_acc: 0.8292\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 14s 847us/sample - loss: 0.6999 - acc: 0.6386 - val_loss: 0.6827 - val_acc: 0.5004\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6026 - acc: 0.6866 - val_loss: 0.6494 - val_acc: 0.5640\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.5683 - acc: 0.7060 - val_loss: 0.6880 - val_acc: 0.5744\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.5484 - acc: 0.7194 - val_loss: 0.7258 - val_acc: 0.5764\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.5281 - acc: 0.7340 - val_loss: 0.6841 - val_acc: 0.6132\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.5099 - acc: 0.7498 - val_loss: 0.5795 - val_acc: 0.6864\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.4953 - acc: 0.7628 - val_loss: 0.5793 - val_acc: 0.6932\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.4783 - acc: 0.7712 - val_loss: 0.4937 - val_acc: 0.7588\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.4661 - acc: 0.7804 - val_loss: 0.4897 - val_acc: 0.7564\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.4540 - acc: 0.7838 - val_loss: 0.4760 - val_acc: 0.7692\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.4334 - acc: 0.8024 - val_loss: 0.4668 - val_acc: 0.7760\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.4206 - acc: 0.8077 - val_loss: 0.4603 - val_acc: 0.7828\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.4094 - acc: 0.8176 - val_loss: 0.4539 - val_acc: 0.7956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.3983 - acc: 0.8208 - val_loss: 0.4574 - val_acc: 0.7896\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.3841 - acc: 0.8292 - val_loss: 0.4474 - val_acc: 0.7972\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.3700 - acc: 0.8351 - val_loss: 0.4437 - val_acc: 0.7944\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.3603 - acc: 0.8421 - val_loss: 0.4607 - val_acc: 0.7796\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.3540 - acc: 0.8462 - val_loss: 0.4641 - val_acc: 0.7872\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.3499 - acc: 0.8485 - val_loss: 0.4486 - val_acc: 0.8040\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.3372 - acc: 0.8557 - val_loss: 0.4636 - val_acc: 0.7960\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.3365 - acc: 0.8555 - val_loss: 0.4675 - val_acc: 0.7852\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.3278 - acc: 0.8594 - val_loss: 0.4524 - val_acc: 0.7924\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.3192 - acc: 0.8622 - val_loss: 0.4583 - val_acc: 0.7868\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.3158 - acc: 0.8681 - val_loss: 0.4836 - val_acc: 0.7792\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.3117 - acc: 0.8679 - val_loss: 0.4571 - val_acc: 0.8008\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.3050 - acc: 0.8670 - val_loss: 0.4776 - val_acc: 0.8004\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.2963 - acc: 0.8730 - val_loss: 0.4715 - val_acc: 0.7936\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.2933 - acc: 0.8755 - val_loss: 0.5231 - val_acc: 0.7648\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.2863 - acc: 0.8816 - val_loss: 0.5075 - val_acc: 0.7892\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.2846 - acc: 0.8796 - val_loss: 0.4761 - val_acc: 0.7884\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 14s 842us/sample - loss: 0.6741 - acc: 0.6541 - val_loss: 0.6811 - val_acc: 0.5636\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.5847 - acc: 0.7000 - val_loss: 0.7075 - val_acc: 0.5012\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.5477 - acc: 0.7194 - val_loss: 1.1864 - val_acc: 0.4900\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.5138 - acc: 0.7431 - val_loss: 1.1811 - val_acc: 0.5024\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.4807 - acc: 0.7692 - val_loss: 0.9296 - val_acc: 0.5580\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.4627 - acc: 0.7796 - val_loss: 0.7741 - val_acc: 0.6328\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.4419 - acc: 0.7944 - val_loss: 0.5701 - val_acc: 0.7076\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.4203 - acc: 0.8068 - val_loss: 0.5531 - val_acc: 0.7184\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.4000 - acc: 0.8167 - val_loss: 0.5632 - val_acc: 0.7288\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.3841 - acc: 0.8228 - val_loss: 0.4754 - val_acc: 0.7652\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.3671 - acc: 0.8368 - val_loss: 0.4711 - val_acc: 0.7740\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.3479 - acc: 0.8430 - val_loss: 0.4583 - val_acc: 0.7912\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.3347 - acc: 0.8521 - val_loss: 0.4796 - val_acc: 0.7708\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.3149 - acc: 0.8614 - val_loss: 0.4563 - val_acc: 0.7912\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.3020 - acc: 0.8689 - val_loss: 0.5156 - val_acc: 0.7672\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.3015 - acc: 0.8702 - val_loss: 0.4928 - val_acc: 0.7760\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.2871 - acc: 0.8766 - val_loss: 0.4582 - val_acc: 0.7964\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.2686 - acc: 0.8858 - val_loss: 0.4960 - val_acc: 0.7892\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.2581 - acc: 0.8901 - val_loss: 0.4960 - val_acc: 0.8036\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.2459 - acc: 0.8961 - val_loss: 0.5471 - val_acc: 0.7832\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.2369 - acc: 0.9002 - val_loss: 0.4899 - val_acc: 0.8028\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.2241 - acc: 0.9058 - val_loss: 0.5788 - val_acc: 0.7732\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.2199 - acc: 0.9089 - val_loss: 0.5724 - val_acc: 0.7964\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.2146 - acc: 0.9104 - val_loss: 0.5768 - val_acc: 0.7840\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.1997 - acc: 0.9168 - val_loss: 0.5589 - val_acc: 0.7940\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.1964 - acc: 0.9171 - val_loss: 0.6006 - val_acc: 0.7860\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.1890 - acc: 0.9226 - val_loss: 0.5411 - val_acc: 0.7916\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.1805 - acc: 0.9265 - val_loss: 0.5937 - val_acc: 0.7888\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.1765 - acc: 0.9291 - val_loss: 0.6028 - val_acc: 0.7968\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.1680 - acc: 0.9312 - val_loss: 0.6057 - val_acc: 0.8016\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 15s 876us/sample - loss: 0.6811 - acc: 0.6639 - val_loss: 0.6825 - val_acc: 0.5040\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.5693 - acc: 0.7092 - val_loss: 0.8298 - val_acc: 0.4892\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.5281 - acc: 0.7399 - val_loss: 1.4648 - val_acc: 0.4888\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.4900 - acc: 0.7631 - val_loss: 1.3597 - val_acc: 0.4964\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.4623 - acc: 0.7795 - val_loss: 1.3058 - val_acc: 0.5232\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.4387 - acc: 0.7940 - val_loss: 0.9462 - val_acc: 0.5788\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.4129 - acc: 0.8106 - val_loss: 0.7326 - val_acc: 0.6656\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.3846 - acc: 0.8253 - val_loss: 0.7469 - val_acc: 0.6552\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.3614 - acc: 0.8355 - val_loss: 0.5457 - val_acc: 0.7384\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.3319 - acc: 0.8545 - val_loss: 0.5667 - val_acc: 0.7396\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.3076 - acc: 0.8644 - val_loss: 0.5505 - val_acc: 0.7640\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.2855 - acc: 0.8758 - val_loss: 0.5629 - val_acc: 0.7628\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.2779 - acc: 0.8819 - val_loss: 0.4980 - val_acc: 0.7944\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.2589 - acc: 0.8899 - val_loss: 0.5781 - val_acc: 0.7532\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.2348 - acc: 0.9033 - val_loss: 0.6159 - val_acc: 0.7700\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.2264 - acc: 0.9063 - val_loss: 0.5210 - val_acc: 0.8096\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.2144 - acc: 0.9104 - val_loss: 0.5151 - val_acc: 0.8040\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.1912 - acc: 0.9205 - val_loss: 0.5378 - val_acc: 0.8116\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.1822 - acc: 0.9251 - val_loss: 0.5948 - val_acc: 0.8168\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.1702 - acc: 0.9313 - val_loss: 0.6134 - val_acc: 0.8024\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.1663 - acc: 0.9332 - val_loss: 0.5890 - val_acc: 0.8176\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.1533 - acc: 0.9384 - val_loss: 0.5878 - val_acc: 0.8156\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.1409 - acc: 0.9442 - val_loss: 0.6381 - val_acc: 0.8076\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.1361 - acc: 0.9475 - val_loss: 0.6170 - val_acc: 0.8144\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.1270 - acc: 0.9493 - val_loss: 0.6568 - val_acc: 0.8172\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.1235 - acc: 0.9517 - val_loss: 0.6590 - val_acc: 0.8056\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.1122 - acc: 0.9567 - val_loss: 0.6742 - val_acc: 0.8196\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.1102 - acc: 0.9575 - val_loss: 0.6979 - val_acc: 0.8024\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.1052 - acc: 0.9585 - val_loss: 0.7066 - val_acc: 0.8060\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.1027 - acc: 0.9605 - val_loss: 0.7123 - val_acc: 0.8028\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 15s 897us/sample - loss: 0.7353 - acc: 0.6626 - val_loss: 0.6776 - val_acc: 0.6184\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.5532 - acc: 0.7223 - val_loss: 1.2417 - val_acc: 0.4884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.4991 - acc: 0.7526 - val_loss: 2.5113 - val_acc: 0.4884\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.4543 - acc: 0.7839 - val_loss: 2.0129 - val_acc: 0.4916\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.4238 - acc: 0.8014 - val_loss: 1.8346 - val_acc: 0.4984\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.4001 - acc: 0.8156 - val_loss: 1.3852 - val_acc: 0.5380\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.3694 - acc: 0.8326 - val_loss: 1.1327 - val_acc: 0.5956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.3485 - acc: 0.8405 - val_loss: 0.7552 - val_acc: 0.6720\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.3103 - acc: 0.8646 - val_loss: 0.6529 - val_acc: 0.7196\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.3034 - acc: 0.8681 - val_loss: 0.5399 - val_acc: 0.7676\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.2663 - acc: 0.8852 - val_loss: 0.5634 - val_acc: 0.7624\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.2380 - acc: 0.9024 - val_loss: 0.5642 - val_acc: 0.7628\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.2210 - acc: 0.9072 - val_loss: 0.5511 - val_acc: 0.7940\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.2030 - acc: 0.9150 - val_loss: 0.5121 - val_acc: 0.8024\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.1817 - acc: 0.9252 - val_loss: 0.5185 - val_acc: 0.8148\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.1682 - acc: 0.9305 - val_loss: 0.5292 - val_acc: 0.8156\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.1530 - acc: 0.9384 - val_loss: 0.5765 - val_acc: 0.8144\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.1302 - acc: 0.9466 - val_loss: 0.6154 - val_acc: 0.8188\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.1258 - acc: 0.9526 - val_loss: 0.5901 - val_acc: 0.8260\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.1217 - acc: 0.9530 - val_loss: 0.6063 - val_acc: 0.8276\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.1057 - acc: 0.9591 - val_loss: 0.6735 - val_acc: 0.8144\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.0978 - acc: 0.9624 - val_loss: 0.6755 - val_acc: 0.8248\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.0937 - acc: 0.9642 - val_loss: 0.7066 - val_acc: 0.8076\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.0887 - acc: 0.9661 - val_loss: 0.7236 - val_acc: 0.8180\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.0849 - acc: 0.9678 - val_loss: 0.7193 - val_acc: 0.8044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.0791 - acc: 0.9706 - val_loss: 0.7231 - val_acc: 0.8248\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.0795 - acc: 0.9684 - val_loss: 0.7003 - val_acc: 0.8284\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.0675 - acc: 0.9739 - val_loss: 0.7521 - val_acc: 0.8220\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.0626 - acc: 0.9776 - val_loss: 0.7477 - val_acc: 0.8252\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.0663 - acc: 0.9751 - val_loss: 0.7168 - val_acc: 0.8276\n"
     ]
    }
   ],
   "source": [
    "Tiefe = [1,2,3]\n",
    "Batchgrose = [128]\n",
    "Breite = [50,160,300,600]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\PMTPerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTraining.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                x= layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            #model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "            history=model.fit(XTraining,YTraining,\n",
    "                              validation_data=(XVal,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Netz mit den besten Parametern speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 2)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_128 (Flatten)        (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_438 (Bat (None, 320)               1280      \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 600)               192600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_439 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_225 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_440 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_441 (Bat (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_226 (Dropout)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 562,882\n",
      "Trainable params: 558,642\n",
      "Non-trainable params: 4,240\n",
      "_________________________________________________________________\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.7477 - acc: 0.6664\n",
      "Epoch 00001: val_acc improved from -inf to 0.67680, saving model to Perceptron-PMT-MuEl-val-acc_0.68.model\n",
      "17000/17000 [==============================] - 62s 4ms/sample - loss: 0.7448 - acc: 0.6668 - val_loss: 0.6634 - val_acc: 0.6768\n",
      "Epoch 2/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.7161\n",
      "Epoch 00002: val_acc did not improve from 0.67680\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.5613 - acc: 0.7160 - val_loss: 1.2130 - val_acc: 0.4884\n",
      "Epoch 3/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.5102 - acc: 0.7499\n",
      "Epoch 00003: val_acc did not improve from 0.67680\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.5098 - acc: 0.7500 - val_loss: 2.2707 - val_acc: 0.4888\n",
      "Epoch 4/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.7740\n",
      "Epoch 00004: val_acc did not improve from 0.67680\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.4694 - acc: 0.7745 - val_loss: 2.2251 - val_acc: 0.4896\n",
      "Epoch 5/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.7879\n",
      "Epoch 00005: val_acc did not improve from 0.67680\n",
      "17000/17000 [==============================] - 2s 147us/sample - loss: 0.4447 - acc: 0.7881 - val_loss: 1.5600 - val_acc: 0.5084\n",
      "Epoch 6/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8056\n",
      "Epoch 00006: val_acc did not improve from 0.67680\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.4149 - acc: 0.8056 - val_loss: 1.3500 - val_acc: 0.5236\n",
      "Epoch 7/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8271\n",
      "Epoch 00007: val_acc did not improve from 0.67680\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.3809 - acc: 0.8271 - val_loss: 1.1214 - val_acc: 0.5600\n",
      "Epoch 8/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8402\n",
      "Epoch 00008: val_acc did not improve from 0.67680\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.3543 - acc: 0.8400 - val_loss: 0.7263 - val_acc: 0.6672\n",
      "Epoch 9/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3360 - acc: 0.8507\n",
      "Epoch 00009: val_acc improved from 0.67680 to 0.68600, saving model to Perceptron-PMT-MuEl-val-acc_0.69.model\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.3360 - acc: 0.8508 - val_loss: 0.6751 - val_acc: 0.6860\n",
      "Epoch 10/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.8680\n",
      "Epoch 00010: val_acc improved from 0.68600 to 0.75280, saving model to Perceptron-PMT-MuEl-val-acc_0.75.model\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.3019 - acc: 0.8681 - val_loss: 0.5242 - val_acc: 0.7528\n",
      "Epoch 11/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.2732 - acc: 0.8825\n",
      "Epoch 00011: val_acc did not improve from 0.75280\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.2731 - acc: 0.8826 - val_loss: 0.6232 - val_acc: 0.7372\n",
      "Epoch 12/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.8918\n",
      "Epoch 00012: val_acc improved from 0.75280 to 0.78240, saving model to Perceptron-PMT-MuEl-val-acc_0.78.model\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.2530 - acc: 0.8914 - val_loss: 0.5083 - val_acc: 0.7824\n",
      "Epoch 13/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9023\n",
      "Epoch 00013: val_acc did not improve from 0.78240\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.2330 - acc: 0.9011 - val_loss: 0.5622 - val_acc: 0.7620\n",
      "Epoch 14/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9099\n",
      "Epoch 00014: val_acc improved from 0.78240 to 0.81800, saving model to Perceptron-PMT-MuEl-val-acc_0.82.model\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.2148 - acc: 0.9098 - val_loss: 0.4786 - val_acc: 0.8180\n",
      "Epoch 15/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9228\n",
      "Epoch 00015: val_acc did not improve from 0.81800\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.1838 - acc: 0.9231 - val_loss: 0.5102 - val_acc: 0.8028\n",
      "Epoch 16/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9264\n",
      "Epoch 00016: val_acc did not improve from 0.81800\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.1769 - acc: 0.9269 - val_loss: 0.5078 - val_acc: 0.8068\n",
      "Epoch 17/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9318\n",
      "Epoch 00017: val_acc did not improve from 0.81800\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.1656 - acc: 0.9316 - val_loss: 0.5542 - val_acc: 0.8076\n",
      "Epoch 18/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9384- ETA: 1s -\n",
      "Epoch 00018: val_acc improved from 0.81800 to 0.82360, saving model to Perceptron-PMT-MuEl-val-acc_0.82.model\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.1526 - acc: 0.9385 - val_loss: 0.5338 - val_acc: 0.8236\n",
      "Epoch 19/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9471\n",
      "Epoch 00019: val_acc did not improve from 0.82360\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.1316 - acc: 0.9469 - val_loss: 0.5727 - val_acc: 0.8184\n",
      "Epoch 20/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9501\n",
      "Epoch 00020: val_acc did not improve from 0.82360\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.1243 - acc: 0.9505 - val_loss: 0.5900 - val_acc: 0.8160\n",
      "Epoch 21/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9554\n",
      "Epoch 00021: val_acc did not improve from 0.82360\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.1149 - acc: 0.9556 - val_loss: 0.7277 - val_acc: 0.7520\n",
      "Epoch 22/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9589\n",
      "Epoch 00022: val_acc improved from 0.82360 to 0.83080, saving model to Perceptron-PMT-MuEl-val-acc_0.83.model\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.1024 - acc: 0.9591 - val_loss: 0.6768 - val_acc: 0.8308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9629\n",
      "Epoch 00023: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.0963 - acc: 0.9626 - val_loss: 0.6393 - val_acc: 0.8008\n",
      "Epoch 24/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9616\n",
      "Epoch 00024: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.0980 - acc: 0.9617 - val_loss: 0.6116 - val_acc: 0.8216\n",
      "Epoch 25/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9658\n",
      "Epoch 00025: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 147us/sample - loss: 0.0873 - acc: 0.9656 - val_loss: 0.6499 - val_acc: 0.8120\n",
      "Epoch 26/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9657\n",
      "Epoch 00026: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0878 - acc: 0.9657 - val_loss: 0.6842 - val_acc: 0.8112\n",
      "Epoch 27/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9707\n",
      "Epoch 00027: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.0772 - acc: 0.9708 - val_loss: 0.6822 - val_acc: 0.8224\n",
      "Epoch 28/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9710\n",
      "Epoch 00028: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0746 - acc: 0.9711 - val_loss: 0.7078 - val_acc: 0.8212\n",
      "Epoch 29/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9763\n",
      "Epoch 00029: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.0675 - acc: 0.9761 - val_loss: 0.7416 - val_acc: 0.8188\n",
      "Epoch 30/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9731\n",
      "Epoch 00030: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0722 - acc: 0.9729 - val_loss: 0.7390 - val_acc: 0.8152\n",
      "Epoch 31/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9754\n",
      "Epoch 00031: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.0655 - acc: 0.9755 - val_loss: 0.7809 - val_acc: 0.8236\n",
      "Epoch 32/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9769\n",
      "Epoch 00032: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0591 - acc: 0.9769 - val_loss: 0.7585 - val_acc: 0.8136\n",
      "Epoch 33/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9778\n",
      "Epoch 00033: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0611 - acc: 0.9778 - val_loss: 0.8307 - val_acc: 0.8252\n",
      "Epoch 34/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9768\n",
      "Epoch 00034: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.0599 - acc: 0.9768 - val_loss: 0.7819 - val_acc: 0.8252\n",
      "Epoch 35/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9812\n",
      "Epoch 00035: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0499 - acc: 0.9812 - val_loss: 0.8300 - val_acc: 0.8212\n",
      "Epoch 36/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9790\n",
      "Epoch 00036: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.0570 - acc: 0.9789 - val_loss: 0.8743 - val_acc: 0.8160\n",
      "Epoch 37/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9794\n",
      "Epoch 00037: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.0585 - acc: 0.9792 - val_loss: 0.8421 - val_acc: 0.8224\n",
      "Epoch 38/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9821\n",
      "Epoch 00038: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.0510 - acc: 0.9821 - val_loss: 0.8352 - val_acc: 0.8204\n",
      "Epoch 39/60\n",
      "16512/17000 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9833\n",
      "Epoch 00039: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0452 - acc: 0.9832 - val_loss: 0.8388 - val_acc: 0.8124\n",
      "Epoch 40/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9840\n",
      "Epoch 00040: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.0411 - acc: 0.9840 - val_loss: 0.8871 - val_acc: 0.8188\n",
      "Epoch 41/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9829\n",
      "Epoch 00041: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0465 - acc: 0.9829 - val_loss: 0.8362 - val_acc: 0.8216\n",
      "Epoch 42/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9840\n",
      "Epoch 00042: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.0441 - acc: 0.9839 - val_loss: 0.8701 - val_acc: 0.8136\n",
      "Epoch 43/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9849\n",
      "Epoch 00043: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0421 - acc: 0.9848 - val_loss: 0.8302 - val_acc: 0.8188\n",
      "Epoch 44/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9838\n",
      "Epoch 00044: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0420 - acc: 0.9838 - val_loss: 0.8559 - val_acc: 0.8196\n",
      "Epoch 45/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9831\n",
      "Epoch 00045: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.0436 - acc: 0.9832 - val_loss: 0.9037 - val_acc: 0.8084\n",
      "Epoch 46/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9851- ETA: 0s - loss: 0.0425 - \n",
      "Epoch 00046: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0437 - acc: 0.9850 - val_loss: 0.8568 - val_acc: 0.8208\n",
      "Epoch 47/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9859\n",
      "Epoch 00047: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.0384 - acc: 0.9859 - val_loss: 0.9034 - val_acc: 0.8228\n",
      "Epoch 48/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9856\n",
      "Epoch 00048: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.0385 - acc: 0.9854 - val_loss: 0.8780 - val_acc: 0.8252\n",
      "Epoch 49/60\n",
      "16896/17000 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9861\n",
      "Epoch 00049: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.0388 - acc: 0.9861 - val_loss: 0.9187 - val_acc: 0.8120\n",
      "Epoch 50/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9853\n",
      "Epoch 00050: val_acc did not improve from 0.83080\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.0379 - acc: 0.9851 - val_loss: 0.8533 - val_acc: 0.8224\n",
      "Epoch 51/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9915\n",
      "Epoch 00051: val_acc improved from 0.83080 to 0.83360, saving model to Perceptron-PMT-MuEl-val-acc_0.83.model\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.0271 - acc: 0.9915 - val_loss: 0.9041 - val_acc: 0.8336\n",
      "Epoch 52/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9888\n",
      "Epoch 00052: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.0326 - acc: 0.9886 - val_loss: 0.9225 - val_acc: 0.8144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9865\n",
      "Epoch 00053: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0354 - acc: 0.9866 - val_loss: 0.9683 - val_acc: 0.8136\n",
      "Epoch 54/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9889\n",
      "Epoch 00054: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.0288 - acc: 0.9889 - val_loss: 0.9661 - val_acc: 0.8204\n",
      "Epoch 55/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.987 - ETA: 0s - loss: 0.0308 - acc: 0.9880\n",
      "Epoch 00055: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.0317 - acc: 0.9877 - val_loss: 0.9809 - val_acc: 0.8096\n",
      "Epoch 56/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9877\n",
      "Epoch 00056: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.0345 - acc: 0.9876 - val_loss: 0.9825 - val_acc: 0.8224\n",
      "Epoch 57/60\n",
      "16768/17000 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9878\n",
      "Epoch 00057: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0326 - acc: 0.9877 - val_loss: 0.9358 - val_acc: 0.8244\n",
      "Epoch 58/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9882\n",
      "Epoch 00058: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.0310 - acc: 0.9882 - val_loss: 0.9597 - val_acc: 0.8188\n",
      "Epoch 59/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9885\n",
      "Epoch 00059: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.0313 - acc: 0.9882 - val_loss: 0.9691 - val_acc: 0.8184\n",
      "Epoch 60/60\n",
      "16640/17000 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9900\n",
      "Epoch 00060: val_acc did not improve from 0.83360\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.0288 - acc: 0.9899 - val_loss: 1.0078 - val_acc: 0.8164\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "Tiefe = [2]\n",
    "Batchgrose = [128]\n",
    "Breite = [600]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\ChargePerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTraining.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                x= layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "            #filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "            filepath=\"Perceptron-PMT-MuEl-val-acc_{val_acc:.2f}.model\" \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "\n",
    "            history=model.fit(XTraining,YTraining,\n",
    "                              validation_data=(XVal,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 60)\n",
    "\n",
    "\n",
    "            print(history.history.keys())\n",
    "            # summarize history for accuracy\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.897076064307033\n",
      "Test accuracy:  0.8336624\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"Perceptron-PMT-MuEl-val-acc_0.83.model\")\n",
    "score = model.evaluate([XTest,XTest], YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just LAPPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "XL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(9x24)_23k_Files.pickle\",\"rb\"))\n",
    "YL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(9x24)_23k_Files.pickle\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "(17000, 9, 24, 2) (2500, 9, 24, 2) (4052, 9, 24, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(XL, YL))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:20]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:17000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[17000:19500]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[19500:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,9,24,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,9,24,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,9,24,1)\n",
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,9,24,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,9,24,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,9,24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cce74de550>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XTrainingC[90,:,:,0], cmap='binary', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_147 (Flatten)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_396 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_364 (Dense)            (None, 150)               24150     \n",
      "_________________________________________________________________\n",
      "batch_normalization_397 (Bat (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dropout_429 (Dropout)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_398 (Bat (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense_365 (Dense)            (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_399 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_430 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_366 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_400 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_431 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_367 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_401 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_432 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_368 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 49,592\n",
      "Trainable params: 48,272\n",
      "Non-trainable params: 1,320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=XTrainingC.shape[1:], name='img')\n",
    "x= layers.Flatten()(inputs)\n",
    "\n",
    "x= layers.BatchNormalization()(x)\n",
    "x = layers.Dense(150, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x = layers.Dense(100, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(50, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(50, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "#outputs = layers.Dense(1,activation='sigmoid')(x)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/60\n",
      "17000/17000 [==============================] - 18s 1ms/sample - loss: 0.7725 - acc: 0.5490 - val_loss: 0.6910 - val_acc: 0.5080\n",
      "Epoch 2/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.6965 - acc: 0.5804 - val_loss: 0.6971 - val_acc: 0.4972\n",
      "Epoch 3/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6659 - acc: 0.5988 - val_loss: 0.8294 - val_acc: 0.4916\n",
      "Epoch 4/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6433 - acc: 0.6241 - val_loss: 1.2116 - val_acc: 0.4924\n",
      "Epoch 5/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.6168 - acc: 0.6520 - val_loss: 1.0946 - val_acc: 0.4980\n",
      "Epoch 6/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.5943 - acc: 0.6675 - val_loss: 1.0440 - val_acc: 0.5052\n",
      "Epoch 7/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5803 - acc: 0.6821 - val_loss: 0.8442 - val_acc: 0.5368\n",
      "Epoch 8/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.5629 - acc: 0.6976 - val_loss: 0.7114 - val_acc: 0.5752\n",
      "Epoch 9/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5510 - acc: 0.7059 - val_loss: 0.7817 - val_acc: 0.5684\n",
      "Epoch 10/60\n",
      "17000/17000 [==============================] - 4s 217us/sample - loss: 0.5388 - acc: 0.7194 - val_loss: 0.5805 - val_acc: 0.6780\n",
      "Epoch 11/60\n",
      "17000/17000 [==============================] - 4s 214us/sample - loss: 0.5303 - acc: 0.7227 - val_loss: 0.5940 - val_acc: 0.6692\n",
      "Epoch 12/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.5170 - acc: 0.7356 - val_loss: 0.5833 - val_acc: 0.7000\n",
      "Epoch 13/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5137 - acc: 0.7383 - val_loss: 0.6219 - val_acc: 0.6516\n",
      "Epoch 14/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5070 - acc: 0.7411 - val_loss: 0.6265 - val_acc: 0.6596\n",
      "Epoch 15/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.4968 - acc: 0.7489 - val_loss: 0.6253 - val_acc: 0.6664\n",
      "Epoch 16/60\n",
      "17000/17000 [==============================] - 4s 225us/sample - loss: 0.4932 - acc: 0.7531 - val_loss: 0.5172 - val_acc: 0.7412\n",
      "Epoch 17/60\n",
      "17000/17000 [==============================] - 4s 216us/sample - loss: 0.4886 - acc: 0.7550 - val_loss: 0.5273 - val_acc: 0.7344\n",
      "Epoch 18/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4800 - acc: 0.7628 - val_loss: 0.5229 - val_acc: 0.7344\n",
      "Epoch 19/60\n",
      "17000/17000 [==============================] - 4s 224us/sample - loss: 0.4739 - acc: 0.7686 - val_loss: 0.6194 - val_acc: 0.6628\n",
      "Epoch 20/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4696 - acc: 0.7696 - val_loss: 0.5572 - val_acc: 0.6968\n",
      "Epoch 21/60\n",
      "17000/17000 [==============================] - 4s 219us/sample - loss: 0.4666 - acc: 0.7702 - val_loss: 0.5248 - val_acc: 0.7304\n",
      "Epoch 22/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4589 - acc: 0.7748 - val_loss: 0.6128 - val_acc: 0.6864\n",
      "Epoch 23/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4531 - acc: 0.7789 - val_loss: 0.5372 - val_acc: 0.7136\n",
      "Epoch 24/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4478 - acc: 0.7837 - val_loss: 0.5587 - val_acc: 0.7048\n",
      "Epoch 25/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4407 - acc: 0.7875 - val_loss: 0.5635 - val_acc: 0.7152\n",
      "Epoch 26/60\n",
      "17000/17000 [==============================] - 4s 228us/sample - loss: 0.4455 - acc: 0.7846 - val_loss: 0.5409 - val_acc: 0.7212\n",
      "Epoch 27/60\n",
      "17000/17000 [==============================] - 4s 228us/sample - loss: 0.4327 - acc: 0.7913 - val_loss: 0.5898 - val_acc: 0.6964\n",
      "Epoch 28/60\n",
      "17000/17000 [==============================] - 4s 218us/sample - loss: 0.4282 - acc: 0.7929 - val_loss: 0.4820 - val_acc: 0.7620\n",
      "Epoch 29/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4225 - acc: 0.7973 - val_loss: 0.7286 - val_acc: 0.6396\n",
      "Epoch 30/60\n",
      "17000/17000 [==============================] - 4s 219us/sample - loss: 0.4234 - acc: 0.7972 - val_loss: 0.6614 - val_acc: 0.6588\n",
      "Epoch 31/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4210 - acc: 0.7986 - val_loss: 0.5719 - val_acc: 0.7124\n",
      "Epoch 32/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4142 - acc: 0.8012 - val_loss: 0.6036 - val_acc: 0.7104\n",
      "Epoch 33/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4086 - acc: 0.8086 - val_loss: 0.5979 - val_acc: 0.7024\n",
      "Epoch 34/60\n",
      "17000/17000 [==============================] - 4s 226us/sample - loss: 0.4024 - acc: 0.8143 - val_loss: 0.5356 - val_acc: 0.7376\n",
      "Epoch 35/60\n",
      "17000/17000 [==============================] - 4s 229us/sample - loss: 0.4004 - acc: 0.8108 - val_loss: 0.5683 - val_acc: 0.7284\n",
      "Epoch 36/60\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.3974 - acc: 0.8162 - val_loss: 0.5234 - val_acc: 0.7380\n",
      "Epoch 37/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.4016 - acc: 0.8093 - val_loss: 0.5233 - val_acc: 0.7396\n",
      "Epoch 38/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3890 - acc: 0.8193 - val_loss: 0.5317 - val_acc: 0.7400\n",
      "Epoch 39/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3914 - acc: 0.8185 - val_loss: 0.6418 - val_acc: 0.7000\n",
      "Epoch 40/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3840 - acc: 0.8219 - val_loss: 0.6487 - val_acc: 0.6844\n",
      "Epoch 41/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3811 - acc: 0.8218 - val_loss: 0.4687 - val_acc: 0.7836\n",
      "Epoch 42/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.3787 - acc: 0.8275 - val_loss: 0.5854 - val_acc: 0.7232\n",
      "Epoch 43/60\n",
      "17000/17000 [==============================] - 4s 214us/sample - loss: 0.3798 - acc: 0.8241 - val_loss: 0.6663 - val_acc: 0.6800\n",
      "Epoch 44/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.3764 - acc: 0.8297 - val_loss: 0.4995 - val_acc: 0.7684\n",
      "Epoch 45/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3710 - acc: 0.8273 - val_loss: 0.5634 - val_acc: 0.7296\n",
      "Epoch 46/60\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.3666 - acc: 0.8316 - val_loss: 0.5257 - val_acc: 0.7656\n",
      "Epoch 47/60\n",
      "17000/17000 [==============================] - 3s 206us/sample - loss: 0.3671 - acc: 0.8316 - val_loss: 0.7167 - val_acc: 0.6768\n",
      "Epoch 48/60\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.3614 - acc: 0.8368 - val_loss: 0.5259 - val_acc: 0.7456\n",
      "Epoch 49/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3647 - acc: 0.8304 - val_loss: 0.5340 - val_acc: 0.7488\n",
      "Epoch 50/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3590 - acc: 0.8355 - val_loss: 0.5137 - val_acc: 0.7568\n",
      "Epoch 51/60\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.3590 - acc: 0.8387 - val_loss: 0.5292 - val_acc: 0.7492\n",
      "Epoch 52/60\n",
      "17000/17000 [==============================] - 4s 212us/sample - loss: 0.3514 - acc: 0.8398 - val_loss: 0.8775 - val_acc: 0.6280\n",
      "Epoch 53/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3514 - acc: 0.8401 - val_loss: 0.4859 - val_acc: 0.7756\n",
      "Epoch 54/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3473 - acc: 0.8418 - val_loss: 0.6815 - val_acc: 0.6984\n",
      "Epoch 55/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3466 - acc: 0.8432 - val_loss: 0.4904 - val_acc: 0.7676\n",
      "Epoch 56/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3449 - acc: 0.8418 - val_loss: 0.6592 - val_acc: 0.7016\n",
      "Epoch 57/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3412 - acc: 0.8446 - val_loss: 0.6384 - val_acc: 0.6980\n",
      "Epoch 58/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3340 - acc: 0.8489 - val_loss: 0.7159 - val_acc: 0.6776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3411 - acc: 0.8453 - val_loss: 0.6283 - val_acc: 0.7140\n",
      "Epoch 60/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.3342 - acc: 0.8488 - val_loss: 0.6639 - val_acc: 0.7080\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 60)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_layers = [0,1,2,3]\n",
    "\n",
    "\n",
    "#for dense_layer in dense_layers:\n",
    "\n",
    "\n",
    "NAME =\"LAPPD-Charge-3x3-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "tensorboard = TensorBoard(log_dir = 'logs\\LAPPDPerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "\n",
    "for l in range(3):\n",
    "    model.add(Dense(512-l*50 ,activation=\"relu\" ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "#model.add(Dense(32,activation=\"relu\"))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "              metrics=['accuracy']\n",
    "             )   \n",
    "#filepath=\"LAPPD_Charge_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "#model.summary()\n",
    "#         history=model.fit(XTrainingC,YTraining,\n",
    "#       validation_data=(XValC,Yval)\n",
    "#       ,batch_size=100,\n",
    "#         shuffle=True,\n",
    "#         class_weight='balanced',\n",
    "#         callbacks=[\n",
    "#                     #monitor,\n",
    "#                     #checkpoint,\n",
    "#                     tensorboard \n",
    "#         ],\n",
    "#       epochs= 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0514 14:02:27.200614  2760 deprecation.py:323] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/40\n",
      "17000/17000 [==============================] - 4s 258us/sample - loss: 0.8613 - acc: 0.5003 - val_loss: 0.7572 - val_acc: 0.4956\n",
      "Epoch 2/40\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.7364 - acc: 0.4989 - val_loss: 0.7997 - val_acc: 0.4956\n",
      "Epoch 3/40\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.7093 - acc: 0.5022 - val_loss: 0.7038 - val_acc: 0.4956\n",
      "Epoch 4/40\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.7009 - acc: 0.5011 - val_loss: 0.6967 - val_acc: 0.4956\n",
      "Epoch 5/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6969 - acc: 0.4987 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6950 - acc: 0.5035 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 7/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6942 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6945 - acc: 0.5024 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 9/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6940 - acc: 0.5039 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 10/40\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6940 - acc: 0.5033 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 11/40\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6939 - acc: 0.5005 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 12/40\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.4968 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.4986 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 14/40\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6940 - acc: 0.4962 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 15/40\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6942 - acc: 0.4928 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 16/40\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/40\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6943 - acc: 0.4919 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 18/40\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6934 - acc: 0.5065 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 19/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6942 - acc: 0.5002 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 20/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6940 - acc: 0.4986 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 21/40\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6940 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6940 - acc: 0.4917 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 23/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6939 - acc: 0.4969 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 24/40\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6940 - acc: 0.5019 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 25/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 26/40\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6940 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6939 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6939 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 30/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6940 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 31/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6936 - acc: 0.5023 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 32/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6940 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 33/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6938 - acc: 0.4988 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 34/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6959 - val_acc: 0.4956\n",
      "Epoch 35/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6939 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 36/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6940 - acc: 0.4931 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 37/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6937 - acc: 0.4977 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 38/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5033 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 39/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6938 - acc: 0.5004 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 40/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 40)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "\n",
    "cd \"Documents\\Python\\CNN_Masterarbeit\"\n",
    "\n",
    "tensorboard --logdir=logs/ --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/Overfitting_Studie --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/Modell_Studie --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/MuonElectron --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=BeamlikePI/logs/Time --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Test_18.05_PMT.pickle\",\"wb\")\n",
    "pickle.dump(XTest,pickle_out,protocol=4)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Test_18.05_PMT.pickle\",\"wb\")\n",
    "pickle.dump(YTest,pickle_out,protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"PMTOnly_PI_22k_RANDOM-improvement-val-acc_0.93.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined\n",
    "model = tf.keras.models.load_model(\"PMTOnly_Combined_PI_22k-80epoch-improvement-val-acc_0.92.model\")\n",
    "#Time\n",
    "#model = tf.keras.models.load_model(\"PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.81.model\")\n",
    "#Charge\n",
    "#model = tf.keras.models.load_model(\"PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.93.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120005"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTestC = X[:,:,:,0].reshape(120005,10,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120005, 10, 16, 1) (120005, 2)\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(XTestC.shape,Y.shape)\n",
    "for sample in Y[:20]:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cbf7094208>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XTest[7,:,:,0], cmap='binary', interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.35680383114329917\n",
      "Test accuracy:  0.90992105\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.23812034977390828\n",
      "Test accuracy:  0.90469563\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestC, Y, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.19774321766031458\n",
      "Test accuracy:  0.92719644\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestC, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.4832646434304745\n",
      "Test accuracy:  0.8005923\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestT, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1772  201]\n",
      " [  94 1985]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#prediction = model.predict(XTestC)\n",
    "#print(prediction.shape,YTest.shape)\n",
    "rounded_labels =np.argmax(YTest, axis=1)\n",
    "y_prob = np.array(model.predict(XTestC, batch_size=128, verbose=0))\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "cm = confusion_matrix(rounded_labels, y_classes)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    " \n",
    "    print(cm)\n",
    " \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89812468 0.10187532]\n",
      " [0.04521405 0.95478595]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "cm = cm.reshape((2,2))\n",
    " \n",
    "class_names = [\"e\", \"muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n CNN-model with 93% accuracy \\n Only charge vlaues form PMTs')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import pydot_ng as pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, 'my_first_model_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_12 (5, 5, 2, 130)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 130 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summarize filter shapes\n",
    "pyplot.subplots_adjust(wspace = 0.05 ,hspace = 0.05)\n",
    "for layer in model.layers:\n",
    "    #check for conv. layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    #get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)\n",
    "    f_min, f_max= filters.min(),filters.max()\n",
    "    filters = (filters-f_min)/(f_max-f_min)\n",
    "    # plot first few filters\n",
    "    \n",
    "\n",
    "    n_filters, ix = 130, 1\n",
    "\n",
    "\n",
    "    for i in range(n_filters):\n",
    "        #get the filter\n",
    "        f = filters[:, :, :, i]\n",
    "        # plot each channel separately\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        ax = pyplot.subplot(n_filters, 4, ix)\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:,:, 1], cmap='viridis')\n",
    "        ix += 1\n",
    "    # show the figure\n",
    "    #pyplot.savefig(\"PMT Model 85% layer0 ALL ConvFilters-Time.jpg\",format =\"jpg\", bbox_inches='tight')\n",
    "    pyplot.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6.4, 30*4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv2d_12 (?, 10, 16, 130)\n",
      "3 conv2d_13 (?, 5, 8, 130)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    print(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c9023abe10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[9,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6.4, 10*4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14_input (InputLayer) [(None, 10, 16, 2)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 10, 16, 130)       6630      \n",
      "=================================================================\n",
      "Total params: 6,630\n",
      "Trainable params: 6,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 10, 16, 130)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 52, not 53",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-f9c6a5616c25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# specify subplot and turn of axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     raise ValueError(\n\u001b[1;32m---> 59\u001b[1;33m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[0;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[0;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[1;31mValueError\u001b[0m: num must be 1 <= num <= 52, not 53"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x3456 with 52 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"PMTOnly_PI_22k_RANDOM-improvement-val-acc_0.93.model\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "\n",
    "\n",
    "\n",
    "ixs = [0]\n",
    "\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(XTest[9:10])\n",
    "print(feature_maps.shape)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "\n",
    "#for fmap in feature_maps:\n",
    "ix = 1\n",
    "a=130\n",
    "for _ in range(a):\n",
    "    for _ in range(a):\n",
    "        if ix==a+1:\n",
    "            break\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(26, 2, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='viridis')\n",
    "        ix += 1\n",
    "    # show the figure\n",
    "#pyplot.savefig(\"PMT layer0 ALL Conv ElectronEvent9.jpg\",format =\"jpg\", bbox_inches='tight')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
