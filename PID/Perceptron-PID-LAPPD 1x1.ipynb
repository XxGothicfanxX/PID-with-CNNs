{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  CSV erkennen by David Maksimovic 24.06.2019\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "#import keras\n",
    "\n",
    "#from keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,LeakyReLU, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot_ng as pydot\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "########### Normalisieren ###########\n",
    "\n",
    "#Ist schon normalisiert\n",
    "########### Normalisieren ###########\n",
    "\n",
    "#Ist schon normalisiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(1x1)_120k_Files.pickle\",\"rb\"))\n",
    "Y=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(1x1)_120k_Files.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much from one kind, how much from the other: \n",
      " [59977 60028]\n",
      "How do they look like? \n",
      " [[0 1]\n",
      " [1 0]]\n",
      "Percentage of one kind: \n",
      " 50.021249114620225\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(Y, return_counts=True, axis=0)\n",
    "print(\"How much from one kind, how much from the other: \\n\",counts)\n",
    "print(\"How do they look like? \\n\",unique)\n",
    "print(\"Percentage of one kind: \\n\", 100/(counts[0]+counts[1])*counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 23k Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "(17000, 10, 16, 2) (2500, 10, 16, 2) (4052, 10, 16, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(X, Y))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:5]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:17000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[17000:19500]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[19500:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)\n",
    "del X,Y,X1,X2,X3,Y1,Y2,Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 120k Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "(85000, 3, 8, 2) (20000, 3, 8, 2) (15005, 3, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(X, Y))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:5]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:85000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[85000:105000]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[105000:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)\n",
    "del X,Y,X1,X2,X3,Y1,Y2,Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Charge and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,9,24,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,9,24,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,9,24,1)\n",
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,9,24,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,9,24,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,9,24,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testen der besten Methode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85000, 3, 8, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTraining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0528 16:54:40.024735 24064 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                490       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 552\n",
      "Trainable params: 532\n",
      "Non-trainable params: 20\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 3s 35us/sample - loss: 0.6827 - acc: 0.6238 - val_loss: 0.6399 - val_acc: 0.6290\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6214 - acc: 0.6633 - val_loss: 0.6186 - val_acc: 0.6568\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6147 - acc: 0.6717 - val_loss: 0.6141 - val_acc: 0.6615\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 33us/sample - loss: 0.6136 - acc: 0.6695 - val_loss: 0.6124 - val_acc: 0.6657\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.6125 - acc: 0.6714 - val_loss: 0.6124 - val_acc: 0.6644\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 31us/sample - loss: 0.6126 - acc: 0.6722 - val_loss: 0.6112 - val_acc: 0.6684\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6121 - acc: 0.6739 - val_loss: 0.6120 - val_acc: 0.6654\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6114 - acc: 0.6739 - val_loss: 0.6116 - val_acc: 0.6665\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6114 - acc: 0.6723 - val_loss: 0.6113 - val_acc: 0.6680\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.6109 - acc: 0.6724 - val_loss: 0.6108 - val_acc: 0.6677\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.6112 - acc: 0.6740 - val_loss: 0.6110 - val_acc: 0.6671\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.6110 - acc: 0.6733 - val_loss: 0.6112 - val_acc: 0.6659\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.6096 - acc: 0.6739 - val_loss: 0.6098 - val_acc: 0.6701\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6099 - acc: 0.6747 - val_loss: 0.6081 - val_acc: 0.6740\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.6089 - acc: 0.6751 - val_loss: 0.6077 - val_acc: 0.6709\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.6083 - acc: 0.6759 - val_loss: 0.6061 - val_acc: 0.6718\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6083 - acc: 0.6758 - val_loss: 0.6050 - val_acc: 0.6751\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6074 - acc: 0.6775 - val_loss: 0.6052 - val_acc: 0.6733\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6068 - acc: 0.6774 - val_loss: 0.6033 - val_acc: 0.6757\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6045 - acc: 0.6806 - val_loss: 0.6011 - val_acc: 0.6770\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6049 - acc: 0.6786 - val_loss: 0.5995 - val_acc: 0.6798\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6029 - acc: 0.6822 - val_loss: 0.5969 - val_acc: 0.6823\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6019 - acc: 0.6806 - val_loss: 0.5946 - val_acc: 0.6857\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6012 - acc: 0.6826 - val_loss: 0.5936 - val_acc: 0.6849\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5994 - acc: 0.6845 - val_loss: 0.5909 - val_acc: 0.6896\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.5988 - acc: 0.6843 - val_loss: 0.5897 - val_acc: 0.6896\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.5968 - acc: 0.6863 - val_loss: 0.5874 - val_acc: 0.6921\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5960 - acc: 0.6875 - val_loss: 0.5859 - val_acc: 0.6941\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5946 - acc: 0.6889 - val_loss: 0.5835 - val_acc: 0.6978\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5930 - acc: 0.6901 - val_loss: 0.5818 - val_acc: 0.7015\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 1,322\n",
      "Trainable params: 1,274\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.6521 - acc: 0.6429 - val_loss: 0.6439 - val_acc: 0.6258\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6157 - acc: 0.6686 - val_loss: 0.6223 - val_acc: 0.6522\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.6126 - acc: 0.6717 - val_loss: 0.6149 - val_acc: 0.6608\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.6108 - acc: 0.6733 - val_loss: 0.6156 - val_acc: 0.6583\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6106 - acc: 0.6735 - val_loss: 0.6130 - val_acc: 0.6629\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.6097 - acc: 0.6736 - val_loss: 0.6111 - val_acc: 0.6664\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6096 - acc: 0.6747 - val_loss: 0.6106 - val_acc: 0.6646\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.6091 - acc: 0.6747 - val_loss: 0.6096 - val_acc: 0.6678\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 3s 33us/sample - loss: 0.6081 - acc: 0.6756 - val_loss: 0.6088 - val_acc: 0.6676\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6071 - acc: 0.6766 - val_loss: 0.6061 - val_acc: 0.6726\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6059 - acc: 0.6791 - val_loss: 0.6053 - val_acc: 0.6710\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6043 - acc: 0.6784 - val_loss: 0.6018 - val_acc: 0.6770\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6033 - acc: 0.6796 - val_loss: 0.5999 - val_acc: 0.6795\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6013 - acc: 0.6810 - val_loss: 0.5966 - val_acc: 0.6806\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5994 - acc: 0.6847 - val_loss: 0.5939 - val_acc: 0.6850\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5972 - acc: 0.6863 - val_loss: 0.5907 - val_acc: 0.6882\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5942 - acc: 0.6881 - val_loss: 0.5878 - val_acc: 0.6874\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5921 - acc: 0.6913 - val_loss: 0.5831 - val_acc: 0.6956\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5917 - acc: 0.6914 - val_loss: 0.5820 - val_acc: 0.6948\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5879 - acc: 0.6947 - val_loss: 0.5776 - val_acc: 0.6980\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5857 - acc: 0.6967 - val_loss: 0.5742 - val_acc: 0.7022\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 3s 33us/sample - loss: 0.5835 - acc: 0.6995 - val_loss: 0.5712 - val_acc: 0.7055\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5809 - acc: 0.7002 - val_loss: 0.5673 - val_acc: 0.7113\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5789 - acc: 0.7012 - val_loss: 0.5648 - val_acc: 0.7149\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5773 - acc: 0.7019 - val_loss: 0.5615 - val_acc: 0.7174\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 3s 31us/sample - loss: 0.5741 - acc: 0.7066 - val_loss: 0.5607 - val_acc: 0.7186\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5742 - acc: 0.7066 - val_loss: 0.5584 - val_acc: 0.7197\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5712 - acc: 0.7090 - val_loss: 0.5554 - val_acc: 0.7229\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5677 - acc: 0.7120 - val_loss: 0.5531 - val_acc: 0.7238\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5668 - acc: 0.7126 - val_loss: 0.5514 - val_acc: 0.7257\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2450      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,752\n",
      "Trainable params: 2,652\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 3s 33us/sample - loss: 0.6364 - acc: 0.6528 - val_loss: 0.6621 - val_acc: 0.5991\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6134 - acc: 0.6713 - val_loss: 0.6320 - val_acc: 0.6457\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6113 - acc: 0.6729 - val_loss: 0.6191 - val_acc: 0.6565\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 33us/sample - loss: 0.6102 - acc: 0.6739 - val_loss: 0.6151 - val_acc: 0.6608\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6092 - acc: 0.6737 - val_loss: 0.6163 - val_acc: 0.6604\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 31us/sample - loss: 0.6084 - acc: 0.6751 - val_loss: 0.6128 - val_acc: 0.6640\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6079 - acc: 0.6752 - val_loss: 0.6116 - val_acc: 0.6634\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6072 - acc: 0.6764 - val_loss: 0.6081 - val_acc: 0.6712\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6060 - acc: 0.6767 - val_loss: 0.6061 - val_acc: 0.6729\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6053 - acc: 0.6798 - val_loss: 0.6054 - val_acc: 0.6713\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6021 - acc: 0.6819 - val_loss: 0.6033 - val_acc: 0.6727\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5993 - acc: 0.6833 - val_loss: 0.6020 - val_acc: 0.6722\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5966 - acc: 0.6868 - val_loss: 0.5923 - val_acc: 0.6830\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5948 - acc: 0.6892 - val_loss: 0.5932 - val_acc: 0.6786\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5916 - acc: 0.6898 - val_loss: 0.5879 - val_acc: 0.6891\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5886 - acc: 0.6934 - val_loss: 0.5818 - val_acc: 0.6933\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5852 - acc: 0.6949 - val_loss: 0.5755 - val_acc: 0.7013\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 3s 31us/sample - loss: 0.5828 - acc: 0.6972 - val_loss: 0.5729 - val_acc: 0.7061\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5798 - acc: 0.7003 - val_loss: 0.5684 - val_acc: 0.7128\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5777 - acc: 0.7018 - val_loss: 0.5647 - val_acc: 0.7111\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5748 - acc: 0.7039 - val_loss: 0.5600 - val_acc: 0.7186\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5712 - acc: 0.7084 - val_loss: 0.5563 - val_acc: 0.7236\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5695 - acc: 0.7091 - val_loss: 0.5543 - val_acc: 0.7243\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5686 - acc: 0.7105 - val_loss: 0.5524 - val_acc: 0.7274\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5639 - acc: 0.7140 - val_loss: 0.5496 - val_acc: 0.7258\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5629 - acc: 0.7143 - val_loss: 0.5493 - val_acc: 0.7242\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5606 - acc: 0.7172 - val_loss: 0.5439 - val_acc: 0.7331\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5575 - acc: 0.7190 - val_loss: 0.5440 - val_acc: 0.7294\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.5567 - acc: 0.7212 - val_loss: 0.5415 - val_acc: 0.7318\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5544 - acc: 0.7218 - val_loss: 0.5420 - val_acc: 0.7326\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 160)               7840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 8,802\n",
      "Trainable params: 8,482\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 3s 31us/sample - loss: 0.6367 - acc: 0.6540 - val_loss: 0.7491 - val_acc: 0.5316\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6146 - acc: 0.6696 - val_loss: 0.6546 - val_acc: 0.6220\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.6123 - acc: 0.6711 - val_loss: 0.6277 - val_acc: 0.6457\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6115 - acc: 0.6719 - val_loss: 0.6229 - val_acc: 0.6547\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6112 - acc: 0.6727 - val_loss: 0.6178 - val_acc: 0.6608\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6105 - acc: 0.6736 - val_loss: 0.6198 - val_acc: 0.6593\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6098 - acc: 0.6728 - val_loss: 0.6164 - val_acc: 0.6601\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6094 - acc: 0.6753 - val_loss: 0.6122 - val_acc: 0.6645\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6081 - acc: 0.6755 - val_loss: 0.6133 - val_acc: 0.6630\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6073 - acc: 0.6768 - val_loss: 0.6076 - val_acc: 0.6718\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 3s 33us/sample - loss: 0.6060 - acc: 0.6782 - val_loss: 0.6060 - val_acc: 0.6727\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 31us/sample - loss: 0.6041 - acc: 0.6802 - val_loss: 0.6032 - val_acc: 0.6735\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.6017 - acc: 0.6811 - val_loss: 0.6057 - val_acc: 0.6700\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.5988 - acc: 0.6852 - val_loss: 0.5925 - val_acc: 0.6865\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5954 - acc: 0.6879 - val_loss: 0.5951 - val_acc: 0.6797\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5915 - acc: 0.6916 - val_loss: 0.5986 - val_acc: 0.6768\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5877 - acc: 0.6940 - val_loss: 0.5846 - val_acc: 0.6921\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.5835 - acc: 0.6985 - val_loss: 0.5825 - val_acc: 0.6909\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.5807 - acc: 0.7015 - val_loss: 0.5808 - val_acc: 0.6906\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5766 - acc: 0.7028 - val_loss: 0.5626 - val_acc: 0.7175\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5744 - acc: 0.7038 - val_loss: 0.5604 - val_acc: 0.7175\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5704 - acc: 0.7099 - val_loss: 0.5608 - val_acc: 0.7153\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.5669 - acc: 0.7123 - val_loss: 0.5561 - val_acc: 0.7183\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.5631 - acc: 0.7146 - val_loss: 0.5510 - val_acc: 0.7227\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.5618 - acc: 0.7153 - val_loss: 0.5497 - val_acc: 0.7242\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5600 - acc: 0.7170 - val_loss: 0.5469 - val_acc: 0.7273\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5570 - acc: 0.7186 - val_loss: 0.5459 - val_acc: 0.7286\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5532 - acc: 0.7223 - val_loss: 0.5363 - val_acc: 0.7376\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5521 - acc: 0.7233 - val_loss: 0.5458 - val_acc: 0.7232\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.5504 - acc: 0.7255 - val_loss: 0.5360 - val_acc: 0.7382\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 600)               29400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 33,002\n",
      "Trainable params: 31,802\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.6321 - acc: 0.6586 - val_loss: 0.8360 - val_acc: 0.5031\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6173 - acc: 0.6681 - val_loss: 0.6858 - val_acc: 0.6019\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6149 - acc: 0.6684 - val_loss: 0.6377 - val_acc: 0.6382\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6148 - acc: 0.6697 - val_loss: 0.6330 - val_acc: 0.6448\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 3s 32us/sample - loss: 0.6147 - acc: 0.6694 - val_loss: 0.6264 - val_acc: 0.6490\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 29us/sample - loss: 0.6135 - acc: 0.6702 - val_loss: 0.6272 - val_acc: 0.6531\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6134 - acc: 0.6713 - val_loss: 0.6246 - val_acc: 0.6569\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6131 - acc: 0.6710 - val_loss: 0.6205 - val_acc: 0.6604\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6126 - acc: 0.6721 - val_loss: 0.6195 - val_acc: 0.6604\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6122 - acc: 0.6729 - val_loss: 0.6169 - val_acc: 0.6616\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6116 - acc: 0.6723 - val_loss: 0.6140 - val_acc: 0.6621\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6121 - acc: 0.6725 - val_loss: 0.6147 - val_acc: 0.6629\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6113 - acc: 0.6727 - val_loss: 0.6144 - val_acc: 0.6659\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6114 - acc: 0.6728 - val_loss: 0.6146 - val_acc: 0.6646\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6106 - acc: 0.6739 - val_loss: 0.6157 - val_acc: 0.6624\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6094 - acc: 0.6754 - val_loss: 0.6121 - val_acc: 0.6686\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 3s 34us/sample - loss: 0.6093 - acc: 0.6740 - val_loss: 0.6112 - val_acc: 0.6686\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 3s 31us/sample - loss: 0.6085 - acc: 0.6747 - val_loss: 0.6096 - val_acc: 0.6713\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 3s 30us/sample - loss: 0.6077 - acc: 0.6767 - val_loss: 0.6073 - val_acc: 0.6714\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 2s 29us/sample - loss: 0.6054 - acc: 0.6781 - val_loss: 0.6059 - val_acc: 0.6733\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6032 - acc: 0.6806 - val_loss: 0.6037 - val_acc: 0.6752\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.6010 - acc: 0.6807 - val_loss: 0.5984 - val_acc: 0.6800\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5982 - acc: 0.6845 - val_loss: 0.5976 - val_acc: 0.6798\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5948 - acc: 0.6888 - val_loss: 0.5912 - val_acc: 0.6866\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5913 - acc: 0.6910 - val_loss: 0.5842 - val_acc: 0.6946\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 2s 28us/sample - loss: 0.5871 - acc: 0.6941 - val_loss: 0.5793 - val_acc: 0.6968\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5846 - acc: 0.6968 - val_loss: 0.5793 - val_acc: 0.6966\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5802 - acc: 0.6992 - val_loss: 0.5721 - val_acc: 0.7050\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 2s 27us/sample - loss: 0.5785 - acc: 0.7029 - val_loss: 0.5675 - val_acc: 0.7095\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 3s 31us/sample - loss: 0.5741 - acc: 0.7067 - val_loss: 0.5607 - val_acc: 0.7157\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                490       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 702\n",
      "Trainable params: 662\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 45us/sample - loss: 0.6557 - acc: 0.6361 - val_loss: 0.6192 - val_acc: 0.6549\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 3s 38us/sample - loss: 0.6211 - acc: 0.6622 - val_loss: 0.6143 - val_acc: 0.6600\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.6151 - acc: 0.6702 - val_loss: 0.6125 - val_acc: 0.6622\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.6134 - acc: 0.6706 - val_loss: 0.6099 - val_acc: 0.6689\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.6125 - acc: 0.6714 - val_loss: 0.6083 - val_acc: 0.6700\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.6108 - acc: 0.6742 - val_loss: 0.6067 - val_acc: 0.6737\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.6087 - acc: 0.6767 - val_loss: 0.6046 - val_acc: 0.6742\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.6070 - acc: 0.6770 - val_loss: 0.6010 - val_acc: 0.6796\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.6045 - acc: 0.6786 - val_loss: 0.5999 - val_acc: 0.6784\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.6042 - acc: 0.6803 - val_loss: 0.5972 - val_acc: 0.6834\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.6011 - acc: 0.6833 - val_loss: 0.5964 - val_acc: 0.6837\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.6007 - acc: 0.6832 - val_loss: 0.5935 - val_acc: 0.6876\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5993 - acc: 0.6846 - val_loss: 0.5920 - val_acc: 0.6870\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5988 - acc: 0.6853 - val_loss: 0.5908 - val_acc: 0.6900\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5971 - acc: 0.6854 - val_loss: 0.5894 - val_acc: 0.6898\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5967 - acc: 0.6872 - val_loss: 0.5883 - val_acc: 0.6912\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 4s 46us/sample - loss: 0.5963 - acc: 0.6862 - val_loss: 0.5865 - val_acc: 0.6930\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5945 - acc: 0.6881 - val_loss: 0.5852 - val_acc: 0.6941\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5949 - acc: 0.6885 - val_loss: 0.5873 - val_acc: 0.6925\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5946 - acc: 0.6888 - val_loss: 0.5867 - val_acc: 0.6924\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5930 - acc: 0.6892 - val_loss: 0.5834 - val_acc: 0.6959\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5931 - acc: 0.6882 - val_loss: 0.5830 - val_acc: 0.6977\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5933 - acc: 0.6901 - val_loss: 0.5820 - val_acc: 0.6985\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5912 - acc: 0.6922 - val_loss: 0.5820 - val_acc: 0.6966\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.5903 - acc: 0.6926 - val_loss: 0.5793 - val_acc: 0.6998\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5911 - acc: 0.6915 - val_loss: 0.5791 - val_acc: 0.7021\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5906 - acc: 0.6913 - val_loss: 0.5799 - val_acc: 0.6990\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5893 - acc: 0.6934 - val_loss: 0.5772 - val_acc: 0.7033\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5905 - acc: 0.6915 - val_loss: 0.5783 - val_acc: 0.7000\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5888 - acc: 0.6950 - val_loss: 0.5762 - val_acc: 0.7024\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 2,018\n",
      "Trainable params: 1,922\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 45us/sample - loss: 0.6607 - acc: 0.6354 - val_loss: 0.6291 - val_acc: 0.6407\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.6157 - acc: 0.6674 - val_loss: 0.6150 - val_acc: 0.6590\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.6104 - acc: 0.6728 - val_loss: 0.6093 - val_acc: 0.6633\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.6041 - acc: 0.6796 - val_loss: 0.6074 - val_acc: 0.6637\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5990 - acc: 0.6852 - val_loss: 0.5978 - val_acc: 0.6722\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5925 - acc: 0.6912 - val_loss: 0.5874 - val_acc: 0.6891\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5879 - acc: 0.6956 - val_loss: 0.5842 - val_acc: 0.6877\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5844 - acc: 0.6974 - val_loss: 0.5752 - val_acc: 0.6995\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5805 - acc: 0.7014 - val_loss: 0.5734 - val_acc: 0.7005\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5782 - acc: 0.7029 - val_loss: 0.5693 - val_acc: 0.7040\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5758 - acc: 0.7051 - val_loss: 0.5674 - val_acc: 0.7060\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5730 - acc: 0.7065 - val_loss: 0.5656 - val_acc: 0.7071\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5711 - acc: 0.7099 - val_loss: 0.5624 - val_acc: 0.7105\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5708 - acc: 0.7092 - val_loss: 0.5625 - val_acc: 0.7104\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5687 - acc: 0.7111 - val_loss: 0.5568 - val_acc: 0.7183\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5675 - acc: 0.7130 - val_loss: 0.5603 - val_acc: 0.7110\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5671 - acc: 0.7108 - val_loss: 0.5562 - val_acc: 0.7150\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5668 - acc: 0.7138 - val_loss: 0.5540 - val_acc: 0.7173\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5632 - acc: 0.7146 - val_loss: 0.5538 - val_acc: 0.7189\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5645 - acc: 0.7149 - val_loss: 0.5574 - val_acc: 0.7135\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5622 - acc: 0.7156 - val_loss: 0.5521 - val_acc: 0.7194\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5611 - acc: 0.7181 - val_loss: 0.5502 - val_acc: 0.7214\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5608 - acc: 0.7170 - val_loss: 0.5521 - val_acc: 0.7183\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5607 - acc: 0.7169 - val_loss: 0.5517 - val_acc: 0.7193\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5604 - acc: 0.7172 - val_loss: 0.5500 - val_acc: 0.7206\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5576 - acc: 0.7198 - val_loss: 0.5494 - val_acc: 0.7209\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5566 - acc: 0.7208 - val_loss: 0.5464 - val_acc: 0.7244\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5562 - acc: 0.7204 - val_loss: 0.5472 - val_acc: 0.7242\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5558 - acc: 0.7198 - val_loss: 0.5441 - val_acc: 0.7276\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5559 - acc: 0.7196 - val_loss: 0.5421 - val_acc: 0.7294\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 50)                2450      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 5,502\n",
      "Trainable params: 5,302\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 48us/sample - loss: 0.6548 - acc: 0.6438 - val_loss: 0.6511 - val_acc: 0.6159\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.6105 - acc: 0.6735 - val_loss: 0.6251 - val_acc: 0.6456\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.6014 - acc: 0.6808 - val_loss: 0.6169 - val_acc: 0.6488\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5866 - acc: 0.6953 - val_loss: 0.5984 - val_acc: 0.6687\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5766 - acc: 0.7046 - val_loss: 0.5752 - val_acc: 0.6924\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 38us/sample - loss: 0.5665 - acc: 0.7111 - val_loss: 0.5641 - val_acc: 0.7042\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5629 - acc: 0.7158 - val_loss: 0.5553 - val_acc: 0.7124\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5577 - acc: 0.7194 - val_loss: 0.5456 - val_acc: 0.7202\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 45us/sample - loss: 0.5562 - acc: 0.7212 - val_loss: 0.5425 - val_acc: 0.7261\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5506 - acc: 0.7238 - val_loss: 0.5452 - val_acc: 0.7182\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5499 - acc: 0.7245 - val_loss: 0.5425 - val_acc: 0.7224\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5481 - acc: 0.7275 - val_loss: 0.5435 - val_acc: 0.7188\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5454 - acc: 0.7287 - val_loss: 0.5490 - val_acc: 0.7130\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5448 - acc: 0.7287 - val_loss: 0.5337 - val_acc: 0.7287\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5441 - acc: 0.7299 - val_loss: 0.5376 - val_acc: 0.7249\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5423 - acc: 0.7304 - val_loss: 0.5403 - val_acc: 0.7186\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5425 - acc: 0.7307 - val_loss: 0.5310 - val_acc: 0.7309\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5405 - acc: 0.7320 - val_loss: 0.5311 - val_acc: 0.7280\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5389 - acc: 0.7319 - val_loss: 0.5346 - val_acc: 0.7265\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5375 - acc: 0.7326 - val_loss: 0.5314 - val_acc: 0.7301\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5364 - acc: 0.7339 - val_loss: 0.5376 - val_acc: 0.7232\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5362 - acc: 0.7362 - val_loss: 0.5281 - val_acc: 0.7333\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 3s 38us/sample - loss: 0.5361 - acc: 0.7355 - val_loss: 0.5264 - val_acc: 0.7344\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5351 - acc: 0.7356 - val_loss: 0.5268 - val_acc: 0.7336\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5343 - acc: 0.7355 - val_loss: 0.5232 - val_acc: 0.7383\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5323 - acc: 0.7380 - val_loss: 0.5251 - val_acc: 0.7355\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5326 - acc: 0.7374 - val_loss: 0.5223 - val_acc: 0.7351\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5336 - acc: 0.7358 - val_loss: 0.5282 - val_acc: 0.7317\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5302 - acc: 0.7383 - val_loss: 0.5195 - val_acc: 0.7401\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 3s 39us/sample - loss: 0.5303 - acc: 0.7384 - val_loss: 0.5202 - val_acc: 0.7385\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 160)               7840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 35,202\n",
      "Trainable params: 34,562\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 47us/sample - loss: 0.6492 - acc: 0.6507 - val_loss: 0.6714 - val_acc: 0.5890\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.6001 - acc: 0.6828 - val_loss: 0.7004 - val_acc: 0.5998\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5760 - acc: 0.7041 - val_loss: 0.6447 - val_acc: 0.6266\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5554 - acc: 0.7203 - val_loss: 0.5819 - val_acc: 0.6790\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5436 - acc: 0.7287 - val_loss: 0.5541 - val_acc: 0.7073\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5361 - acc: 0.7354 - val_loss: 0.5221 - val_acc: 0.7449\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5315 - acc: 0.7376 - val_loss: 0.5251 - val_acc: 0.7362\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5272 - acc: 0.7390 - val_loss: 0.5151 - val_acc: 0.7492\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5233 - acc: 0.7431 - val_loss: 0.5217 - val_acc: 0.7336\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5215 - acc: 0.7432 - val_loss: 0.5179 - val_acc: 0.7372\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5196 - acc: 0.7450 - val_loss: 0.5134 - val_acc: 0.7418\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5168 - acc: 0.7448 - val_loss: 0.5066 - val_acc: 0.7492\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5140 - acc: 0.7474 - val_loss: 0.5073 - val_acc: 0.7485\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5119 - acc: 0.7480 - val_loss: 0.5037 - val_acc: 0.7536\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5111 - acc: 0.7493 - val_loss: 0.5038 - val_acc: 0.7510\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5095 - acc: 0.7511 - val_loss: 0.5126 - val_acc: 0.7395\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5081 - acc: 0.7515 - val_loss: 0.4997 - val_acc: 0.7577\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5077 - acc: 0.7534 - val_loss: 0.5005 - val_acc: 0.7519\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5058 - acc: 0.7536 - val_loss: 0.5015 - val_acc: 0.7527\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5050 - acc: 0.7529 - val_loss: 0.4997 - val_acc: 0.7521\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5040 - acc: 0.7546 - val_loss: 0.4943 - val_acc: 0.7606\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5024 - acc: 0.7567 - val_loss: 0.4937 - val_acc: 0.7614\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 4s 46us/sample - loss: 0.5031 - acc: 0.7545 - val_loss: 0.4980 - val_acc: 0.7535\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.5006 - acc: 0.7565 - val_loss: 0.4969 - val_acc: 0.7534\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5003 - acc: 0.7560 - val_loss: 0.4981 - val_acc: 0.7537\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.5009 - acc: 0.7552 - val_loss: 0.4991 - val_acc: 0.7524\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4978 - acc: 0.7594 - val_loss: 0.4945 - val_acc: 0.7569\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4980 - acc: 0.7580 - val_loss: 0.4895 - val_acc: 0.7624\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.4968 - acc: 0.7581 - val_loss: 0.4905 - val_acc: 0.7620\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 4s 41us/sample - loss: 0.4947 - acc: 0.7598 - val_loss: 0.4901 - val_acc: 0.7580\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 600)               29400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 396,002\n",
      "Trainable params: 393,602\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.6665 - acc: 0.6484 - val_loss: 0.7579 - val_acc: 0.5202\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.5902 - acc: 0.6909 - val_loss: 0.8010 - val_acc: 0.5216\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5557 - acc: 0.7187 - val_loss: 0.6189 - val_acc: 0.6375\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5360 - acc: 0.7331 - val_loss: 0.5550 - val_acc: 0.7095\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5255 - acc: 0.7397 - val_loss: 0.5218 - val_acc: 0.7387\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5165 - acc: 0.7451 - val_loss: 0.5086 - val_acc: 0.7495\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5083 - acc: 0.7498 - val_loss: 0.5112 - val_acc: 0.7465\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.5048 - acc: 0.7531 - val_loss: 0.5001 - val_acc: 0.7559\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.4988 - acc: 0.7556 - val_loss: 0.4964 - val_acc: 0.7537\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4962 - acc: 0.7581 - val_loss: 0.4965 - val_acc: 0.7592\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 3s 41us/sample - loss: 0.4914 - acc: 0.7616 - val_loss: 0.4972 - val_acc: 0.7544\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 3s 40us/sample - loss: 0.4883 - acc: 0.7633 - val_loss: 0.4873 - val_acc: 0.7630\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.4830 - acc: 0.7656 - val_loss: 0.4952 - val_acc: 0.7586\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.4825 - acc: 0.7676 - val_loss: 0.4885 - val_acc: 0.7589\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.4784 - acc: 0.7695 - val_loss: 0.4906 - val_acc: 0.7611\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.4752 - acc: 0.7694 - val_loss: 0.4849 - val_acc: 0.7628\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 4s 44us/sample - loss: 0.4716 - acc: 0.7717 - val_loss: 0.4857 - val_acc: 0.7652\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 46us/sample - loss: 0.4698 - acc: 0.7746 - val_loss: 0.4936 - val_acc: 0.7581\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 45us/sample - loss: 0.4672 - acc: 0.7763 - val_loss: 0.4823 - val_acc: 0.7620\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.4643 - acc: 0.7771 - val_loss: 0.4879 - val_acc: 0.7619\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.4648 - acc: 0.7768 - val_loss: 0.4829 - val_acc: 0.7662\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4620 - acc: 0.7788 - val_loss: 0.4828 - val_acc: 0.7663\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.4582 - acc: 0.7807 - val_loss: 0.4843 - val_acc: 0.7678\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4594 - acc: 0.7803 - val_loss: 0.4877 - val_acc: 0.7624\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4573 - acc: 0.7818 - val_loss: 0.4831 - val_acc: 0.7682\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 4s 46us/sample - loss: 0.4554 - acc: 0.7824 - val_loss: 0.4845 - val_acc: 0.7663\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 4s 43us/sample - loss: 0.4532 - acc: 0.7841 - val_loss: 0.4899 - val_acc: 0.7584\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4542 - acc: 0.7834 - val_loss: 0.4819 - val_acc: 0.7666\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4515 - acc: 0.7846 - val_loss: 0.4849 - val_acc: 0.7638\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 4s 42us/sample - loss: 0.4486 - acc: 0.7881 - val_loss: 0.4837 - val_acc: 0.7668\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                490       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 852\n",
      "Trainable params: 792\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.6953 - acc: 0.6102 - val_loss: 0.6234 - val_acc: 0.6542\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.6269 - acc: 0.6599 - val_loss: 0.6162 - val_acc: 0.6610\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.6188 - acc: 0.6683 - val_loss: 0.6146 - val_acc: 0.6634\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.6159 - acc: 0.6718 - val_loss: 0.6114 - val_acc: 0.6677\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.6144 - acc: 0.6723 - val_loss: 0.6086 - val_acc: 0.6734\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.6126 - acc: 0.6754 - val_loss: 0.6087 - val_acc: 0.6701\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.6109 - acc: 0.6769 - val_loss: 0.6052 - val_acc: 0.6759\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.6091 - acc: 0.6774 - val_loss: 0.6035 - val_acc: 0.6785\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.6050 - acc: 0.6808 - val_loss: 0.6001 - val_acc: 0.6801\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.6030 - acc: 0.6851 - val_loss: 0.5966 - val_acc: 0.6844\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.6023 - acc: 0.6853 - val_loss: 0.5948 - val_acc: 0.6875\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.6007 - acc: 0.6878 - val_loss: 0.5933 - val_acc: 0.6889\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5996 - acc: 0.6888 - val_loss: 0.5927 - val_acc: 0.6902\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5976 - acc: 0.6911 - val_loss: 0.5889 - val_acc: 0.6948\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5957 - acc: 0.6910 - val_loss: 0.5877 - val_acc: 0.6967\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5948 - acc: 0.6925 - val_loss: 0.5870 - val_acc: 0.6955\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5935 - acc: 0.6933 - val_loss: 0.5834 - val_acc: 0.7001\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 4s 52us/sample - loss: 0.5917 - acc: 0.6947 - val_loss: 0.5820 - val_acc: 0.7014\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.5911 - acc: 0.6958 - val_loss: 0.5810 - val_acc: 0.7013\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.5898 - acc: 0.6975 - val_loss: 0.5793 - val_acc: 0.7043\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 4s 51us/sample - loss: 0.5886 - acc: 0.6981 - val_loss: 0.5761 - val_acc: 0.7061\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5871 - acc: 0.6989 - val_loss: 0.5746 - val_acc: 0.7071\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.5873 - acc: 0.6989 - val_loss: 0.5747 - val_acc: 0.7070\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5861 - acc: 0.6984 - val_loss: 0.5705 - val_acc: 0.7127\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5849 - acc: 0.7002 - val_loss: 0.5706 - val_acc: 0.7095\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5843 - acc: 0.7001 - val_loss: 0.5686 - val_acc: 0.7111\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5823 - acc: 0.7038 - val_loss: 0.5686 - val_acc: 0.7131\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5822 - acc: 0.7023 - val_loss: 0.5679 - val_acc: 0.7107\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.5813 - acc: 0.7035 - val_loss: 0.5660 - val_acc: 0.7129\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5816 - acc: 0.7034 - val_loss: 0.5643 - val_acc: 0.7147\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 2,714\n",
      "Trainable params: 2,570\n",
      "Non-trainable params: 144\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.6816 - acc: 0.6258 - val_loss: 0.6247 - val_acc: 0.6480\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.6195 - acc: 0.6663 - val_loss: 0.6169 - val_acc: 0.6565\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.6120 - acc: 0.6732 - val_loss: 0.6140 - val_acc: 0.6585\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.6080 - acc: 0.6779 - val_loss: 0.6110 - val_acc: 0.6618\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.6037 - acc: 0.6798 - val_loss: 0.5999 - val_acc: 0.6737\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5978 - acc: 0.6858 - val_loss: 0.5943 - val_acc: 0.6809\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5911 - acc: 0.6934 - val_loss: 0.5832 - val_acc: 0.6934\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5856 - acc: 0.6986 - val_loss: 0.5770 - val_acc: 0.6980\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5794 - acc: 0.7034 - val_loss: 0.5752 - val_acc: 0.6977\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5767 - acc: 0.7050 - val_loss: 0.5710 - val_acc: 0.7050\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.5735 - acc: 0.7083 - val_loss: 0.5612 - val_acc: 0.7139\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5707 - acc: 0.7106 - val_loss: 0.5565 - val_acc: 0.7184\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5700 - acc: 0.7105 - val_loss: 0.5579 - val_acc: 0.7145\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5679 - acc: 0.7110 - val_loss: 0.5566 - val_acc: 0.7165\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5641 - acc: 0.7167 - val_loss: 0.5510 - val_acc: 0.7218\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5641 - acc: 0.7157 - val_loss: 0.5477 - val_acc: 0.7257\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5616 - acc: 0.7176 - val_loss: 0.5509 - val_acc: 0.7221\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.5615 - acc: 0.7169 - val_loss: 0.5485 - val_acc: 0.7265\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5603 - acc: 0.7185 - val_loss: 0.5430 - val_acc: 0.7291\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5578 - acc: 0.7203 - val_loss: 0.5476 - val_acc: 0.7232\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5585 - acc: 0.7181 - val_loss: 0.5465 - val_acc: 0.7259\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5568 - acc: 0.7215 - val_loss: 0.5423 - val_acc: 0.7291\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5560 - acc: 0.7199 - val_loss: 0.5487 - val_acc: 0.7235\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.5527 - acc: 0.7242 - val_loss: 0.5383 - val_acc: 0.7338\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.5541 - acc: 0.7212 - val_loss: 0.5402 - val_acc: 0.7306\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5528 - acc: 0.7234 - val_loss: 0.5384 - val_acc: 0.7311\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5517 - acc: 0.7240 - val_loss: 0.5335 - val_acc: 0.7369\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5500 - acc: 0.7262 - val_loss: 0.5399 - val_acc: 0.7308\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.5506 - acc: 0.7239 - val_loss: 0.5357 - val_acc: 0.7333\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5503 - acc: 0.7259 - val_loss: 0.5299 - val_acc: 0.7382\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 50)                2450      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 8,252\n",
      "Trainable params: 7,952\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.6444 - acc: 0.6459 - val_loss: 0.6336 - val_acc: 0.6374\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.6114 - acc: 0.6723 - val_loss: 0.6255 - val_acc: 0.6514\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.6023 - acc: 0.6798 - val_loss: 0.6299 - val_acc: 0.6385\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5903 - acc: 0.6912 - val_loss: 0.6126 - val_acc: 0.6646\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5778 - acc: 0.7022 - val_loss: 0.5959 - val_acc: 0.6786\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 59us/sample - loss: 0.5693 - acc: 0.7093 - val_loss: 0.5696 - val_acc: 0.7033\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5635 - acc: 0.7142 - val_loss: 0.5600 - val_acc: 0.7105\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5575 - acc: 0.7190 - val_loss: 0.5591 - val_acc: 0.7104\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5533 - acc: 0.7229 - val_loss: 0.5449 - val_acc: 0.7239\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5511 - acc: 0.7240 - val_loss: 0.5374 - val_acc: 0.7331\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5476 - acc: 0.7267 - val_loss: 0.5451 - val_acc: 0.7229\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5459 - acc: 0.7274 - val_loss: 0.5363 - val_acc: 0.7283\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.5431 - acc: 0.7292 - val_loss: 0.5403 - val_acc: 0.7277\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5407 - acc: 0.7294 - val_loss: 0.5459 - val_acc: 0.7238\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5392 - acc: 0.7306 - val_loss: 0.5348 - val_acc: 0.7328\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5367 - acc: 0.7326 - val_loss: 0.5277 - val_acc: 0.7369\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5354 - acc: 0.7332 - val_loss: 0.5279 - val_acc: 0.7402\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5348 - acc: 0.7345 - val_loss: 0.5261 - val_acc: 0.7383\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5335 - acc: 0.7343 - val_loss: 0.5261 - val_acc: 0.7395\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5317 - acc: 0.7363 - val_loss: 0.5198 - val_acc: 0.7420\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5306 - acc: 0.7381 - val_loss: 0.5214 - val_acc: 0.7420\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5300 - acc: 0.7366 - val_loss: 0.5261 - val_acc: 0.7379\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5294 - acc: 0.7381 - val_loss: 0.5152 - val_acc: 0.7462\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5270 - acc: 0.7394 - val_loss: 0.5184 - val_acc: 0.7426\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5272 - acc: 0.7404 - val_loss: 0.5198 - val_acc: 0.7430\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.5241 - acc: 0.7413 - val_loss: 0.5227 - val_acc: 0.7429\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5251 - acc: 0.7409 - val_loss: 0.5152 - val_acc: 0.7466\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5231 - acc: 0.7427 - val_loss: 0.5186 - val_acc: 0.7426\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5229 - acc: 0.7417 - val_loss: 0.5143 - val_acc: 0.7456\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5227 - acc: 0.7415 - val_loss: 0.5176 - val_acc: 0.7459\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 160)               7840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 61,602\n",
      "Trainable params: 60,642\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.6593 - acc: 0.6478 - val_loss: 0.6545 - val_acc: 0.6142\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.6007 - acc: 0.6809 - val_loss: 0.6578 - val_acc: 0.6192\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5807 - acc: 0.6963 - val_loss: 0.6524 - val_acc: 0.6370\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5623 - acc: 0.7127 - val_loss: 0.5852 - val_acc: 0.6798\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.5485 - acc: 0.7234 - val_loss: 0.5592 - val_acc: 0.7031\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5398 - acc: 0.7302 - val_loss: 0.5454 - val_acc: 0.7167\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5329 - acc: 0.7363 - val_loss: 0.5353 - val_acc: 0.7255\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5275 - acc: 0.7381 - val_loss: 0.5288 - val_acc: 0.7312\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5230 - acc: 0.7425 - val_loss: 0.5197 - val_acc: 0.7397\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5198 - acc: 0.7440 - val_loss: 0.5211 - val_acc: 0.7372\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5160 - acc: 0.7472 - val_loss: 0.5077 - val_acc: 0.7484\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5121 - acc: 0.7495 - val_loss: 0.5198 - val_acc: 0.7430\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5110 - acc: 0.7496 - val_loss: 0.5109 - val_acc: 0.7445\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.5085 - acc: 0.7517 - val_loss: 0.5085 - val_acc: 0.7458\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.5062 - acc: 0.7532 - val_loss: 0.5008 - val_acc: 0.7534\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5047 - acc: 0.7533 - val_loss: 0.5135 - val_acc: 0.7454\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5011 - acc: 0.7543 - val_loss: 0.5094 - val_acc: 0.7477\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.5026 - acc: 0.7538 - val_loss: 0.4980 - val_acc: 0.7580\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.5005 - acc: 0.7543 - val_loss: 0.4981 - val_acc: 0.7584\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4980 - acc: 0.7566 - val_loss: 0.4992 - val_acc: 0.7552\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4967 - acc: 0.7574 - val_loss: 0.5023 - val_acc: 0.7518\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.4951 - acc: 0.7588 - val_loss: 0.4988 - val_acc: 0.7542\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4928 - acc: 0.7597 - val_loss: 0.4949 - val_acc: 0.7598\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4926 - acc: 0.7600 - val_loss: 0.4957 - val_acc: 0.7577\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4913 - acc: 0.7611 - val_loss: 0.4987 - val_acc: 0.7555\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4915 - acc: 0.7606 - val_loss: 0.4893 - val_acc: 0.7632\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4880 - acc: 0.7625 - val_loss: 0.4857 - val_acc: 0.7640\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.4872 - acc: 0.7644 - val_loss: 0.4904 - val_acc: 0.7639\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4867 - acc: 0.7634 - val_loss: 0.4876 - val_acc: 0.7620\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 4s 53us/sample - loss: 0.4844 - acc: 0.7650 - val_loss: 0.4874 - val_acc: 0.7639\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 600)               29400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 600)               360600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 759,002\n",
      "Trainable params: 755,402\n",
      "Non-trainable params: 3,600\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.6557 - acc: 0.6564 - val_loss: 0.6444 - val_acc: 0.6086\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5777 - acc: 0.6991 - val_loss: 0.6334 - val_acc: 0.6639\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5570 - acc: 0.7157 - val_loss: 0.6016 - val_acc: 0.6572\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.5408 - acc: 0.7276 - val_loss: 0.5897 - val_acc: 0.6786\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5275 - acc: 0.7377 - val_loss: 0.5463 - val_acc: 0.7222\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5151 - acc: 0.7453 - val_loss: 0.5260 - val_acc: 0.7333\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.5084 - acc: 0.7492 - val_loss: 0.5031 - val_acc: 0.7505\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.5013 - acc: 0.7543 - val_loss: 0.5049 - val_acc: 0.7473\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4943 - acc: 0.7594 - val_loss: 0.4980 - val_acc: 0.7580\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 57us/sample - loss: 0.4929 - acc: 0.7602 - val_loss: 0.4995 - val_acc: 0.7531\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.4872 - acc: 0.7631 - val_loss: 0.4940 - val_acc: 0.7585\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4817 - acc: 0.7665 - val_loss: 0.4935 - val_acc: 0.7604\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4767 - acc: 0.7686 - val_loss: 0.4949 - val_acc: 0.7576\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4745 - acc: 0.7714 - val_loss: 0.4946 - val_acc: 0.7596\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.4695 - acc: 0.7745 - val_loss: 0.4911 - val_acc: 0.7570\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4670 - acc: 0.7750 - val_loss: 0.4948 - val_acc: 0.7569\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.4619 - acc: 0.7784 - val_loss: 0.4898 - val_acc: 0.7627\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 55us/sample - loss: 0.4581 - acc: 0.7800 - val_loss: 0.4926 - val_acc: 0.7598\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4558 - acc: 0.7830 - val_loss: 0.4962 - val_acc: 0.7591\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4523 - acc: 0.7848 - val_loss: 0.4998 - val_acc: 0.7555\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4485 - acc: 0.7858 - val_loss: 0.4955 - val_acc: 0.7606\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4457 - acc: 0.7878 - val_loss: 0.4983 - val_acc: 0.7630\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.4427 - acc: 0.7896 - val_loss: 0.4984 - val_acc: 0.7552\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 56us/sample - loss: 0.4400 - acc: 0.7908 - val_loss: 0.4955 - val_acc: 0.7604\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4330 - acc: 0.7961 - val_loss: 0.5091 - val_acc: 0.7577\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4325 - acc: 0.7955 - val_loss: 0.4934 - val_acc: 0.7585\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4273 - acc: 0.7984 - val_loss: 0.4990 - val_acc: 0.7605\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 54us/sample - loss: 0.4262 - acc: 0.8014 - val_loss: 0.4992 - val_acc: 0.7596\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 53us/sample - loss: 0.4230 - acc: 0.8008 - val_loss: 0.4985 - val_acc: 0.7629\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 58us/sample - loss: 0.4208 - acc: 0.8027 - val_loss: 0.4991 - val_acc: 0.7621\n"
     ]
    }
   ],
   "source": [
    "Tiefe = [1,2,3]\n",
    "Batchgrose = [128]\n",
    "Breite = [10,24,50,160,600]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-Charge-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\LAPPD1x1Perceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTraining.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                \n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "            history=model.fit(XTraining,YTraining,\n",
    "                              validation_data=(XVal,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 3, 8, 2)]         0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1600)              78400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 1600)              6400      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1600)              2561600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 1600)              6400      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1600)              2561600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 1600)              6400      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 2)                 3202      \n",
      "=================================================================\n",
      "Total params: 5,224,002\n",
      "Trainable params: 5,214,402\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/20\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.6953 - acc: 0.6545 - val_loss: 0.6988 - val_acc: 0.6378\n",
      "Epoch 2/20\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.5888 - acc: 0.6935 - val_loss: 0.6108 - val_acc: 0.6611\n",
      "Epoch 3/20\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.5663 - acc: 0.7090 - val_loss: 0.7201 - val_acc: 0.5792\n",
      "Epoch 4/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5497 - acc: 0.7212 - val_loss: 0.6158 - val_acc: 0.6554\n",
      "Epoch 5/20\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.5315 - acc: 0.7362 - val_loss: 0.5708 - val_acc: 0.7069\n",
      "Epoch 6/20\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.5245 - acc: 0.7405 - val_loss: 0.5649 - val_acc: 0.7157\n",
      "Epoch 7/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5154 - acc: 0.7461 - val_loss: 0.5386 - val_acc: 0.7216\n",
      "Epoch 8/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5062 - acc: 0.7520 - val_loss: 0.5325 - val_acc: 0.7188\n",
      "Epoch 9/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.5038 - acc: 0.7529 - val_loss: 0.5226 - val_acc: 0.7379\n",
      "Epoch 10/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4942 - acc: 0.7594 - val_loss: 0.5142 - val_acc: 0.7445\n",
      "Epoch 11/20\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4916 - acc: 0.7623 - val_loss: 0.5098 - val_acc: 0.7485\n",
      "Epoch 12/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4844 - acc: 0.7658 - val_loss: 0.5061 - val_acc: 0.7500\n",
      "Epoch 13/20\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4791 - acc: 0.7693 - val_loss: 0.5091 - val_acc: 0.7516\n",
      "Epoch 14/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4737 - acc: 0.7716 - val_loss: 0.5035 - val_acc: 0.7552\n",
      "Epoch 15/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4717 - acc: 0.7723 - val_loss: 0.5212 - val_acc: 0.7450\n",
      "Epoch 16/20\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4662 - acc: 0.7746 - val_loss: 0.5110 - val_acc: 0.7546\n",
      "Epoch 17/20\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4599 - acc: 0.7792 - val_loss: 0.5006 - val_acc: 0.7538\n",
      "Epoch 18/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4528 - acc: 0.7839 - val_loss: 0.5250 - val_acc: 0.7455\n",
      "Epoch 19/20\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4479 - acc: 0.7865 - val_loss: 0.5137 - val_acc: 0.7510\n",
      "Epoch 20/20\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4422 - acc: 0.7908 - val_loss: 0.5037 - val_acc: 0.7539\n"
     ]
    }
   ],
   "source": [
    "Tiefe = [3]\n",
    "Batchgrose = [128]\n",
    "Breite = [600]\n",
    "    \n",
    "for deep in Tiefe:\n",
    "    for batch in Batchgrose:\n",
    "        for breit in Breite:\n",
    "\n",
    "            \n",
    "            \n",
    "            NAME =\"Perceptron-PMT-Charge-MuEl-{}-deep-{}-nodes-{}-batchsize\".format(deep, breit, batch) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\LAPPD1x1Perceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            inputs = tf.keras.Input(shape=XTraining.shape[1:], name='img')\n",
    "            x= layers.Flatten()(inputs)\n",
    "            for d in range(deep):\n",
    "                \n",
    "                x = layers.Dense(breit, activation='sigmoid')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "                \n",
    "            outputs = layers.Dense(2, activation='softmax')(x)\n",
    "            model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "            filepath=\"Perceptron_LAPPD(1x1)_PID_120k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "                \n",
    "            model.summary()\n",
    "\n",
    "            model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "            history=model.fit(XTraining,YTraining,\n",
    "                              validation_data=(XVal,Yval)\n",
    "                              ,batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85000, 10, 16, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTraining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0528 17:57:27.722968 24064 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0528 17:57:27.722968 24064 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0528 17:57:27.722968 24064 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"Perceptron_LAPPD(1x1)_PID_120k-improvement-val-acc_0.77.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.47987645894279085\n",
      "Test accuracy:  0.76754415\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    " \n",
    "    print(cm)\n",
    " \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5264 2223]\n",
      " [1265 6253]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rounded_labels =np.argmax(YTest, axis=1)\n",
    "y_prob = np.array(model.predict(XTest, batch_size=128, verbose=0))\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "cm = confusion_matrix(rounded_labels, y_classes)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70308535 0.29691465]\n",
      " [0.16826284 0.83173716]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "cm = cm.reshape((2,2))\n",
    " \n",
    "class_names = [r\"$e^{-}$\", \"muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n Perceptron-model with 77% accuracy \\n Pure LAPPD (1x1 res)')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuctinal API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout,LeakyReLU, Activation,ZeroPadding2D, Flatten, Conv2D, MaxPooling2D, BatchNormalization,Concatenate, Reshape, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot_ng as pydot\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cbook as cbook\n",
    "from matplotlib.colors import DivergingNorm\n",
    "from matplotlib import ticker, cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23552, 10, 16, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "XCharge= X[:,:,:,0].reshape(23552,10,16,1)\n",
    "XTime=X[:,:,:,1].reshape(23552,10,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Test\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputC (InputLayer)             [(None, 10, 16, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "InputT (InputLayer)             [(None, 10, 16, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 10, 16, 130)  3380        inputC[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 10, 16, 150)  3900        InputT[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling2D) (None, 5, 8, 130)    0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling2D) (None, 5, 8, 150)    0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 5, 8, 130)    520         max_pooling2d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 5, 8, 150)    600         max_pooling2d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 5, 8, 130)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 5, 8, 150)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 5, 8, 230)    269330      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 5, 8, 300)    405300      dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling2D) (None, 2, 4, 230)    0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling2D) (None, 2, 4, 300)    0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 2, 4, 230)    920         max_pooling2d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 2, 4, 300)    1200        max_pooling2d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 2, 4, 230)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 2, 4, 300)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 2, 4, 500)    1035500     dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 2, 4, 500)    1350500     dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling2D) (None, 1, 2, 500)    0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling2D) (None, 1, 2, 500)    0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 1, 2, 500)    2000        max_pooling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 1, 2, 500)    2000        max_pooling2d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1, 2, 1000)   0           batch_normalization_51[0][0]     \n",
      "                                                                 batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 2000)         0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 658)          1316658     flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 658)          2632        dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 200)          131800      batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 200)          800         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 2)            402         batch_normalization_56[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 4,527,442\n",
      "Trainable params: 4,522,106\n",
      "Non-trainable params: 5,336\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Program\n",
    "\n",
    "#Ist schon normalisiert\n",
    "\n",
    "#NAME =\"BigData_Beamlike-32k-toy_res\"\n",
    "#tensorboard = TensorBoard(log_dir = 'logs\\{}'.format(NAME))\n",
    "inputC = tf.keras.Input(shape=(10,16,1), name='inputC')\n",
    "\n",
    "xC =layers.Conv2D(130, kernel_size=(5,5), strides=(1, 1),activation ='relu',padding='same')(inputC)\n",
    "xC =layers.MaxPooling2D(2)(xC)\n",
    "xC =layers.BatchNormalization()(xC)\n",
    "xC =layers.Dropout(0.2)(xC)\n",
    "\n",
    "\n",
    "xC =layers.Conv2D(230, kernel_size=(3,3), strides=(1, 1),activation ='relu',padding='same')(xC)\n",
    "xC =layers.MaxPooling2D(2)(xC)\n",
    "xC =layers.BatchNormalization()(xC)\n",
    "xC =layers.Dropout(0.2)(xC)\n",
    "\n",
    "xC =layers.Conv2D(500, kernel_size=(3,3), strides=(1, 1),activation ='relu',padding='same')(xC)\n",
    "xC =layers.MaxPooling2D(2)(xC)\n",
    "outputC =layers.BatchNormalization()(xC)\n",
    "#outputC =layers.Dropout(0.2)(xC)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Time\n",
    "\n",
    "\n",
    "\n",
    "inputT = tf.keras.Input(shape=(10,16,1), name='InputT')\n",
    "\n",
    "x= layers.Conv2D(150, kernel_size=(5,5), strides=(1, 1),activation ='relu',padding='same')(inputT)\n",
    "x= layers.MaxPooling2D(2)(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x= layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "x= layers.Conv2D(300, kernel_size=(3,3), strides=(1, 1),activation ='relu',padding='same')(x)\n",
    "x= layers.MaxPooling2D(2)(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x= layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "x= layers.Conv2D(500, kernel_size=(3,3), strides=(1, 1),activation ='relu',padding='same')(x)\n",
    "x= layers.MaxPooling2D(2)(x)\n",
    "outputT= layers.BatchNormalization()(x)\n",
    "#outputT= layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Zusammenfhrung\n",
    "\n",
    "\n",
    "\n",
    "summary     = layers.concatenate([outputC,outputT])\n",
    "\n",
    "######\n",
    "\n",
    "inter= layers.Flatten()(summary)\n",
    "\n",
    "inter= layers.Dense(658, activation='relu')(inter)\n",
    "inter= layers.BatchNormalization()(inter)\n",
    "inter= layers.Dense(200, activation='relu')(inter)\n",
    "inter= layers.BatchNormalization()(inter)\n",
    "outputs = layers.Dense(2, activation='softmax')(inter)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputC, inputT], outputs=[outputs], name='Test')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#NVIDIA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKAAAAejCAYAAAB4JViPAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdf2wb52H/8c/Vcdau6CRkgeS4nfNtkCYI0E5NijlKf6yIayCI1mPaLUosqWr6h1RQQ5ulsDCkLokgkOtmAIUUaYAIlP6YIFAk7D/WkdiMAbEAG10kZ2gnre0fNgo3VNfMIlCAXIECa5Y83z+0O/OnRFI83pF8vwDC1vF4z3NHUvfRc889j2WMMQIAAAAAAAA88j6/KwAAAAAAAIDuRgMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8dZvfFegVN2/e1Le+9S29++67flcFANBBJicnZdu239UAAieTyWhlZcXvagAAOsihQ4f08ssv68iRI35XpSfRA6pN1tbWlEql/K4GoKtXr+rq1at+V6MjXLhwQdvb235XAz3swoULnDuAGlKplC5cuOB3NQDyQp22t7f5zsJ3qVRKa2trflejZ9EDqs3Onz/vdxXQ4yYmJiRJiUTC55oEn2VZevbZZzU+Pu53VdCjnO8rgOrGx8c5n8F35IX6rK6uamJigr+H4CvLsvyuQk+jBxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFICmRaNRRaNRv6sRGJZllTyqyeVymp+fb3PN4KX5+XkVCoWqz9XzmQAAQCJXVUO26k1kq+5FAxSAjlUoFAJ54jHGyBhTsTyXy+mFF17Qgw8+6J40awXN8pNrEPfTUSgUtLGxocXFRYVCoZrrZTIZhUIhhUIhZTIZz8tzbG1tues2cxz3K+/kyZOanJxULpereK7WZwEAgKAJaq6SyFa1kK3QaW7zuwIAOtfc3Jyv5V+5csXX8htRKBQ0NTWlM2fOaHh4WPl8XhcvXtTY2JikymNpjFEul9Pg4KB2dnY0MDDgR7XrEovFJElnz56tuU4qldLq6qpWVlYkSc8//7xu3ryp6elpT8pzzM/P6/Lly5qentarr76qdDrd8vKGhoZ05swZTU1NaWVlRX19fQ2XAQAAuaoxZCuyFTqPZWg+bIvV1VVNTEzQWgvfTUxMSJISiYTPNTmYQqGgyclJZTIZz75XlmUpkUhofHy87vUlVa3P/Py88vl8RRhyXpNMJnXq1Kmq2+yU3xu19n97e1t333231tfXNTw8LGn3ytknP/lJbW5uamhoqKXlOWZmZnTnnXdqdna2JcGlnvLuvfdenT59uuHX1tIt31fAC3w/EBSN5oUgakeuaubvIbIV2arV2aobvq+djFvwADQll8splUq53WbLf85kMrIsS6FQSNvb2+46TldhSVpcXJRlWZqZmdH169fdbVfrGl2+LBaLuV2Ni5cHcfyEXC6n2dlZPfroo1Wfj8ViGhsbUyqVqmt7hUJBqVTK3e/FxcWSLsr1vBfF687Pz7vPr62tNbmXtb3xxhuSpKNHj7rL7rrrLknSm2++2fLyJLmfgbm5ubZdNRsdHdXs7GzV7uIAAOyFXNUYshXZCh3KoC0SiYThcCMIxsfHzfj4+IG3Y9u2keR+rot/Xl9fN8YYk81mjSQTDoeNMcZ9vnidfD5vwuGwkWSuXbtmjDFmZ2enZNvF2ypeVv6zMcZEIhETiUQOvH/O9hOJREPrV/uep9NpI8lks9mqrzFmt96SzObmZtXni9m2beLxuDFm91jZtm1s2zb5fN59fr/3ovi1yWTSGGPMpUuXqtahXrX233l/q61v23ZTZe1V3ubmppFk0um0icfjbjmXLl1quqy9ynM4xzidTjf82lpa9X0FuhHfDwRFo3mhml7IVc38PUS2Ilu1Olu14vuK5tEi0iY0QCEoWhnY6wku9azjnNRisdiBt9VKrWqAcgJQrdcYsxsYnXDjBMbi5x1OkNnZ2XGXra+vG0lu2KlVl/JlyWSy6jrNBs1a+9/o8oOWF4vFSsJecRh3QmMry3Pk8/mKz3G9r62FP7CB2vh+ICha9Qdtt+eqVjZAka3IVjRAdSZuwQPgO+c+9dnZWZ9r4o16BnTs6+vT0tKSJO3Z1fjChQuSVDJw5gMPPCBpd2yFRjjrl3fDr6e+QeZ8jpzPVV9fn8LhsCRpeXnZs3Kd7ujd+jkGAHSGbs9VEtmq3chWaBUaoAAgIAYGBrS5ualMJqOpqSkVCoWKdRYWFiqWOSfnRqffddY3/zedbfGjlWzbrvmcE1685gSmascPAAB0J7KVd8hWaAYNUAACo10nzCAbGhpSOp1WJpNxp6gt5gSOalfxmj1+xQOVeqFanZ0BOx966KGWl+cch2ohc6/ABgBANyFX7SJbHRzZCq1CAxQA3zkn6ZGREZ9r4g0n7FQ7aVdj27aSyWTV7trOlLE3btxwlznbHR0dbahe8XhckrSysuJuw5m5pZUee+wxSaV1fvvtt0ueayXnOLz11lvuMmf/2jHlbiQS8bwMAABq6fZcJZGtyFboVDRAAWhK+dS0xT87J6TiUFB+VcmZFrdQKGhlZUW2bZdcQXGutDghamNjw31uZmZGUunVH+fEHsTpgu+77z5JlSHJOSbVrridOnWq6sn28ccfl23bOnfunPu6ixcvKhwO68SJExXb2+u9eOKJJyTtjkvQ398vy7I0ODjohgxnCuGtra1997F4++X7eezYMcXjcS0vL6tQKKhQKGh5eVnxeFzHjh1z12tVeSdOnFAkElE0GnX39fz587JtW6dOnWp5eQ7nyuPx48f33R4AAMXIVY0hW5Gt0JlogALQlMHBwZL/F//c399f8m/5+tLu4I6hUEj9/f06duyYVlZWSp7/9re/Ldu2df/99yuTyWh4eNi9evXiiy9Kkubm5iRJP/jBDzQ5OdnaHWyhhx9+WNKtK1OS3EAi7R4bZ5DKYnNzcxXdmp0BNW3bLnndSy+95K5T73sxMDCgbDbrhrFwOKxsNusGl3w+r3A4vG/wtCyrZPtO4Co2PT2tkZER9ff3a3JyUqOjo5qeni5Zp5XlOceu+BiVf8ZaWZ506/113m8AAOpFrmoM2Ypshc5kmVaPiIaqVldXNTEx0fIB6IBGTUxMSJISiYQv5Tsnl074LliWpUQiUXfX4r32zbmSePr06YbqUCgU3IEw/RIKhZROpylvH9FoVP39/VXf42Y/935/X4Eg4/uBoGg0L7S6bKkzclUzfw+RrXq7PC+ylZ/fV9ADCgDaYmpqSpcvXy7p8l4PvwPSxsaGzpw5Q3n72Nra0tbWlqamplpQKwAAsB+yVXeXR7bqTjRAAWib8vENeonTvfvcuXN13RcfBGtra7rjjjs0PDxMeXu4fv26FhYWtLS05HuoBQD0jl7OVRLZqpvLI1t1r9v8rgBqc+6dde7HBjpd+fgGndBdvBm1ugQPDAxoZWVFS0tLGhoa8qNqDXEG3qS8vWUyGb344osaGBioeK7aeAYAALRCr+QqiWzVa+WRrboXPaBQU6FQ2PMLvrGxoWg0KsuyZFmWotGotra2lMvlGvrF4Ly+/OGH8n0OUt26gTGm5NFt6tm/vr6+hscqQLCdPn26akCSuv8zD3SSIJ3T680bB6lzJ+4vGtML5xiyVW8iW3UvGqACbG5uztfeT1euXKn5XDQa1fLysiYnJ90v/ze/+U1tb29XzMqxH2OM8vm8+3M+n/ftl0n5PhtjtLOz4/7sZ90AAEDzgpw3JCmZTFb9o6p4WTKZrLuMIO8v+QoAehO34KGqQqGgxcXFqs85PZ3KZzYYGBiQbdtaX1/XI4880lB5xff2+nWfb619Lm595x5kAAA6V5DzxqlTp/Z97eOPP95QWUHeX/IVAPQeekAFVC6XUyqVUigUqvpzJpORZVkKhULa3t5218lkMu46i4uLsixLMzMzun79urvtal2dy5fFYjFlMpmS56Td2+7Onj2758wG5YPORaNRdzyrgxwDv/a5EU7IKr4tMZfLaX5+vqQ8Z9pYSSXPFe+XszwUCmltba1ifwuFgmZmZpo6tgAAYFcQ8kY2m62rrn19fe665CvyFQB0HIO2SCQSppHDbdu2keS+pvjn9fV1Y4wx2WzWSDLhcNgYY9zni9fJ5/MmHA4bSebatWvGGGN2dnZKtl28reJl5T8bY0wkEjGSzM7OTt37EolETCQS2Xe98vKCss97LS/nlLuzs1NR1/X19ZKfi9m27R7TnZ0dY9u2SSaTxhhjLl26ZCSZzc3NimOyublZdXt7GR8fN+Pj4w29pldJMolEwu9qoIfxfQVqa/b7EeS8sVc9y5GvgpOvyAv1afTvIcALfF/9xW+ANmnmF249J+961tnc3DSSTCwWO/C26g0Kzai3vHbv817Ly0UikZLAUv66WCxmJJlsNltSVycMGWNMMpmsWk8nZDrbzOfz+9anGv6grR8nKPiN7ytQW6saoOpd1o68sV+dmhHk/e2WfEVeqA8NUAgCvq/+4jdAm/jZANXKbXVKA1S967U6IDmy2awbhopf5wS3eDzuLovFYiWBqfgqXPmjmbqUGx8fr7l9Hjx4BO9BAxRQXRAaoOpdr95t7VenZgR5fxvdx6DmK7/PEzx48GjsQQOUfxiEHA0Jh8NaWFhQoVBgwMgaFhcXlclkFIvFNDs7W/Lc0NCQwuGwvv71r+upp56SJP3iF7/QsWPH3HWccRKMh7PBfPazn9Wzzz7r2fa7xVNPPaVnn31Wn/3sZ/2uCnrUK6+84ncVACAQgp6vyAv7+9GPfqRXXnlF58+f97sq6GHO7wj4gwaoHhIOhw+8jZGRES0sLOitt97S0NBQC2rlrVbscz1mZmb02muvKZVK6etf/7qy2WxJ6Cmv08LCgi5evKgPfvCDeuaZZ6qud/36dd13332e1PfYsWMaHR31ZNvd5uGHH+ZYwTc//OEP/a4CgDq0K28EBfmqEnlhf++8844kcZyAHsYseD3Ama1kZGTkwNuybVu2bWthYaHmOtvb2yWzkPihlfu8n42NDX3+85+XJI2NjUlSzXAk3bpKNzY2psXFxYpZA+PxuCRpZWVFhUJB0q1ZWwAAQHC0M28EAfkKAHAQNEAFVC6XK/l/8c/OSdP5t3x9SUqlUu46KysrbsORw7ly5QSJjY0N97mZmRlJctcvPzkvLS3p17/+dcVUvNJu49M3vvENTU5OusvqmSa4eF+KQ0EQ9rm8nGIbGxt65JFH9MADD5S8fnt7u+TYlG/DuSpXXD/HE088IUk6e/as+vv7ZVmWBgcHNTo6umddAADA3oKcN4qV58BqyFfkKwDoOH4PQtUrGh2EXPsMnFZtneJlxVPKxuPxihk9stms+3w6nTbGGHdqWme6WmdAx0gk4i5z5PN5k06n3WlxJRnbtk08Hi8Z8NGY/acJ3m9f/dzneuvmlFX+emfWlvJj4pTtTGNcLpvNmkgkYiSVvL64TNu2ax7TvTCrVv3EIIXwGd9XoLZGvx9Bzhv11LMc+So4+Yq8UB9mwUMQ8H31l2WMhyMdw7W6uqqJiQlPB5aWJMuyJHk7gHXQdOI+FwoFPf/883rttdfaXvbExIQkKZFItL3sTmNZlhKJhMbHx/2uCnoU31egtnZ/PzoxbxxEJ+6vX/mKvFCfdv09BOyF76u/uAUP8MH58+cZgBEAAKCFyFcAEGw0QHWResYL6DadtM/RaFSWZcmyLG1vb+vEiRN+Vwkt5ry/zqMaBjztPvPz8yXjpxSr5zMBIPg6KW+0QiftL/mqu5GtehPZqnvRANVFBgcHq/6/m3XSPjszt8Tjcc3NzflcG/8UCgVPTxZeb78expiq3ctzuZxeeOEFPfjgg+5Js9YAsuUnV7/3aS+FQkEbGxtaXFxUKBSquV4mk1EoFFIoFFImk/G8PMfW1pa7bjPHcb/yTp48qcnJyap/pNX6LADoLJ2UN1qhk/a31/NVL+QqiWxVC9kKneY2vyuA1unFL2In7fP09LSmp6f9robvrly50tHbb1ahUNDU1JTOnDmj4eFh5fN5Xbx40Z1aujw0G2OUy+U0ODionZ0dDQwM+FHtusRiMUm7MwvVkkqltLq6qpWVFUnS888/r5s3bzb1nainPMf8/LwuX76s6elpvfrqq0qn0y0vb2hoSGfOnNHU1JRWVlbU19fXcBkAgq2T8kYrdNL+9nq+6tVcJZGtyFboSO0c8byXMesDgsLPWbXy+bw7U04nbF8NzpKhGjMVGWNMLBarOluR85pkMllzm52i1v5ns1kjyayvr7vLnBmNNjc3W16eIxwOm0gkUjFjk5flxWKxpl5bC7PgAbXx/UBQNJoXWqXTclUzfw+RrchWrc5Wfn1fsYtb8ADUpVAoKJVKuV2WFxcXS7rFVuvOXL4sFou53YOd5blczu0+LEmLi4uyLEszMzO6fv36gbcv7Y4PUas7djvkcjnNzs7q0Ucfrfp8LBbT2NiYUqlUXdvb773I5XJKpVLuMc1kMrIsS6FQSNvb2xV1m5+fd59fW1trci9re+ONNyRJR48edZfdddddkqQ333yz5eVJct/vubm5tl01Gx0d1ezsbODHSwEA+I9cdTBkK7IVOhMNUADqMjk5qd/+9rcyxmhnZ0eZTEZTU1PuAIE7OzsVr8lmsyU/F3eFNv93//bg4KB73/rGxoamp6eVz+clSffff78blprdfhBcvXpVknTvvfdWff706dOKRCIaGxvT1tbWvtvb772YmprS2NiYe0xt21Y2m1Umk9H3vvc9dzu5XE5TU1P68Ic/LGOMnnvuOX3hC1+oqw6NuHz5sqRb43RIcru9H2S8glq2trZ09uxZjYyMuMHbqwBYzHl/nfcbAIBayFUHQ7YiW6FD+dHtqhdxCx6CoplbFi5dumQkmZ2dHXfZ+vp6RfdmVekKW76snnWMudWNuLjbbbPbb5ZadAteJBKpWSdneXE392vXrlU872jle5FMJquuU607ez1q7X+jyw9aXiwWK+mCns/nTTgcruiq3qryHPl8vuIzW+9ra+EWI6A2vh8IikbzQq/mqlbegke2Ils1u6+Nfl/RWvSAArCvCxcuSFLJYI0PPPCAJGl1ddWTMoeGhiRJs7Oznmy/neoZ0LGvr09LS0uStGdX41a+F8765V3u66lvkDmfGecz1NfXp3A4LElaXl72rFynO3o3fGYBAN4hVx0c2aq9yFZoFRqgAOxrYWGhYplzQvCim2+vGhgY0ObmZkW372KtfC+c9c3/dasvfrSSbds1n3PCi9ecwFTt+AEA0E7kqvYhW3mHbIVm0AAFYF/OSa7alSOvT3LtOokGxdDQkNLptDKZjDtFbTEv3oviQUm9UK3OzoCdDz30UMvLc45DtZC5V2ADAKAdyFXtRbY6OLIVWoUGKAD7Gh8flyTduHHDXeacgEZHRz0p0zlxj4yMeLL9dnLCTrWTdjW2bSuZTFbtrt3K9yIej0uSVlZW3G04M7e00mOPPSaptM5vv/12yXOt5ByHt956y13m7J9z/LwUiUQ8LwMA0LnIVQdHtiJboTPRAAVgX48//rhs29a5c+fcKy0XL15UOBzWiRMn3PWcqyNOyNnY2HCfm5mZkVR6xab8ZOxMlVsoFLSysiLbtkuuqjS7fb+nC77vvvskVYYk51hWu+J26tSpqifbet6L4u05ZRaX7Tz/xBNPSNodl6C/v1+WZWlwcNANGc4UwvXM3FK8/fL9PHbsmOLxuJaXl1UoFFQoFLS8vKx4PF4ye0uryjtx4oQikYii0ai7r+fPn5dt2zp16lTLy3M4Vx6PHz++7/YAAL2LXHVwZCuyFTqUX6Of9xpmwUNQNDtr0M7OjonH4+6ME8lk0uTz+ZJ1stmsO9tIOp02xhhj27ZJJpPuzCLOLCyRSMRd5mxzc3PTfX08Hm/Z9iORSFOzj6hFs+Dt7OxUzBLirFv8qMa27arb2+u9qLbdWmVls1l3JplwOGyy2az7XCQSMeFwuGodqu33fvuTTqeNJGPbtrl06VLF860ur/gYVfs8tbo8Z8ac4ll0yrfRKGb5Amrj+4GgaDQvGNObuaqVs+CRrXaRrZgFr9NYxrR4RDRUtbq6qomJiZYPQAc0amJiQpKUSCR8rsktzgwhQft+WJalRCJRd9fivfbDuWp4+vTphupQKBTcgTD9EgqFlE6nKW8f0WhU/f39Vd/jZj/jQfy+AkHB9wNB0Whe8FpQc1Uzfw+RrXq7PC+yVdC+r72GW/AAoA2mpqZ0+fLlku7t9fA7IG1sbOjMmTOUt4+trS1tbW1pamqqBbUCAAD7IVt1d3lkq+5EAxQAXxXfU1/tfv1u0dfXp6WlJZ07d66u++KDYG1tTXfccYeGh4cpbw/Xr1/XwsKClpaWfA+1AIDe1iu5SiJbdXN5ZKvudZvfFQDQ2wYHB0v+H7Tu4s2o1SV4YGBAKysrWlpa0tDQkB9Va0jxQKiUV1smk9GLL76ogYGBiueczwIAAO3QjblKIlv1Wnlkq+5FAxQAX3VLMJLq25e+vr6GxypAsO31fnbT5xsAEHzddt4hW/UmslX34hY8AAAAAAAAeIoGKAAAAAAAAHiKBigAAAAAAAB4igYoAAAAAAAAeIpByNvswoULflcBPW57e1sSn8V6Xb16VYcPH/a7GuhRFy5c0OjoqN/VAALrwoUL+tKXvuR3NQDyQh2uXr0qiQwK9DLLMIx8W7z55pt6+OGH/a4GAKDDfOc739HZs2f9rgYQOJFIRN/97nf9rgYAoMNcvXpVx48f97saPYkGKAC+syxLiURC4+PjflcFAACgo62urmpiYoLp6gEEDmNAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAU7f5XQEAvWVzc1P/8i//UrE8k8noV7/6lfvzvffeq7/6q79qZ9UAAAA6zvnz5/XLX/7S/Xlzc1OS9Hd/93cl6/3FX/yFPv7xj7e1bgBQzDLGGL8rAaB3/M3f/I1eeeUV/cEf/EHNdf7nf/5HksSvJwAAgL1ZliVJ+2arv/3bv61olAKAduIWPABt9Zd/+ZeSdoNQrcftt9+ub3zjGz7XFAAAIPi+8Y1v6Pbbb98zW0nSyMiIzzUF0OvoAQWgrd577z19+MMf1s2bN/dc70c/+pE+85nPtKlWAAAAnelf//Vf9dnPfnbPdY4cOaJf//rXet/76H8AwD/8BgLQVu973/s0MTGh22+/veY6R48e1ac//ek21goAAKAzffrTn9bRo0drPn/77bdrYmKCxicAvuO3EIC2Gxsb0+9///uqzx0+fFhf/epX3fEMAAAAUJtlWfrqV7+qw4cPV33+97//vcbGxtpcKwCoxC14AHxxzz33lMzYUuw//uM/9IlPfKLNNQIAAOhMP/3pT/Wnf/qnVZ/76Ec/qhs3brS5RgBQiR5QAHzxta99reqVuo997GM0PgEAADTgE5/4hD72sY9VLD98+LC+9rWvtb9CAFAFDVAAfDE2NqZ33nmnZNnhw4f1zDPP+FQjAACAzvXMM89UXNx75513uP0OQGBwCx4A3wwNDemnP/2pnF9DlmXpF7/4he655x6fawYAANBZbty4oXvvvbckV33iE5/Q1taWzzUDgF30gALgm2eeeUaHDh2StBuSHnroIRqfAAAAmnDPPffooYcecidyOXToED3LAQQKDVAAfHPq1Cm9++67knZD0uTkpM81AgAA6FyTk5Puxb13331Xp06d8rlGAHALDVAAfHP06FF97nOfkyS99957evrpp32uEQAAQOd6+umn9d5770mSPve5z+no0aM+1wgAbqEBCoCvJiYmJEmf+tSndOTIEZ9rAwAA0LmOHDmiT33qU5JuZSwACAoGIW+xN998Uw8//LDf1QAAdLnvfOc7Onv2rN/VADwXiUT03e9+1+9qAAC63NWrV3X8+HG/q9HVbvO7At3mF7/4hSTp/PnzPtcEqO2pp57Ss88+q89+9rN+V0WSVCgU9Ed/9EfuoJlB8aMf/UivvPIK32cEzsTEhH75y1/6XQ2gLX75y1/q8OHDSiQSflcFqCpoecEYo//+7/9WX1+f31Wp8Morr0iSnn32WZ9rApR66qmn9Itf/IIGKI/RAOWR0dFRv6sA7Onhhx/mc7qPd955RxLfZwTPD3/4Q7+rALTV6Ogov4sRWOSF+jnnL44V0JsYAwoAAAAAAACeogEKAAAAAAAAnqIBCgAAAAAAAJ6iAQoAAAAAAACeogEKAAAAAAAAnqIBCkDTotGootGo39XoaLlcTvPz835XAy00Pz+vQqHgdzUAAB2IbHUw5KruQ67qLjRAAehYhUJBlmX5XY2m5XI5vfDCC3rwwQdlWZYsy6oZOp3nix9BVSgUtLGxocXFRYVCoZrrZTIZhUIhhUIhZTIZz8tzbG1tues2cxz3K+/kyZOanJxULpdreNsAAPipk7MVuYpcheC7ze8KAOhcc3NzvpZ/5coVX8s/iEKhoKmpKZ05c0bDw8PK5/O6ePGixsbGJFUeW2OMcrmcBgcHtbOzo4GBAT+qXZdYLCZJOnv2bM11UqmUVldXtbKyIkl6/vnndfPmTU1PT3tSnmN+fl6XL1/W9PS0Xn31VaXT6ZaXNzQ0pDNnzmhqakorKyvq6+truAwAQG8iWzWHXEWuQmewjDHG7zM3bn0AACAASURBVEp0k9XVVU1MTIjDiiCzLEuJRELj4+N+V6VphUJBk5OTymQynn3fvPw+z8/PK5/PVwQi58pRMpnUqVOnKl5nWVbH/H5x9qW8vtvb27r77ru1vr6u4eFhSbtXzz75yU9qc3NTQ0NDLS3PMTMzozvvvFOzs7MtCS/1lHfvvffq9OnTBy6r3MTEhCQpkUi0fNtA0PB5R9B1S/5vR7by6vtMriJXtaL8Tv/7qBNwCx6ApuRyOaVSKberbPnPmUxGlmUpFAppe3vbXcfpHixJi4uLsixLMzMzun79urvtat2hy5fFYjG3e3Hx8k4YOyGXy2l2dlaPPvpo1edjsZjGxsaUSqXq2l6hUFAqlXKPw+LiYkk35Xrem+J15+fn3efX1taa3Mva3njjDUnS0aNH3WV33XWXJOnNN99seXmS3M/E3Nxc266cjY6OanZ2li7jAIC6kK2aQ64iV6GDGLRUIpEwHFYEnSSTSCQOtA3bto0k9/Ne/PP6+roxxphsNmskmXA47JZbvk4+nzfhcNhIMteuXTPGGLOzs1Oy7eJtFS8r/9kYYyKRiIlEIgfaN4dX3+d0Om0kmWw2W/GcU14kEjGSzObmZtXni9m2beLxuDFm99jZtm1s2zb5fN59fr/3pvi1yWTSGGPMpUuXqtahXtXeH2OM+35XW9+27abK2qu8zc1NI8mk02kTj8fdci5dutR0WXuV53COcTqdPlA51YyPj5vx8fGWbxcIIj7vCLpW5YVeyFZefJ/JVeSqVmjF30fYHy0lLUYDFDpBq37B1hNa6lnHOZHFYrEDb6uVvPo+OyGoGmd5Pp93A44THoufdzhhZmdnx122vr5uJLmBx3ndfsczmUxWXafZ0Fnr/Wl0+UHLi8ViJYGvOJg7wbGV5Tny+XzF57pV+IMcvYTPO4KulXmh27OVF99nchW5qhVogGoPbsED4Dvn3vTZ2Vmfa9Ie9Qzq2NfXp6WlJUnas7vxhQsXJKlk8MwHHnhA0u6YFI1w1i/vkl9PfYPM+Vw5n7O+vj6Fw2FJ0vLysmflOl3Se+VzDQAIjl7KVuSq9iJX4SBogAKAgBoYGNDm5qYymYympqZUKBQq1llYWKhY5pygG52C11nf7PaOLXm0km3bNZ9zAozXnNBU7fgBAIDuQ67yDrkK9aIBCkBgtOsk2UmGhoaUTqeVyWTcaWqLOaGj2pW8Zo9n8aClXqhWZ2fQzoceeqjl5TnHoVrQ3Cu0AQDQ6chWpchVB0euwkHQAAXAd86JeWRkxOeatIcTeKqduKuxbVvJZLJql21nqtgbN264y5ztjo6ONlSveDwuSVpZWXG34cze0kqPPfaYpNI6v/322yXPtZJzHN566y13mbN/7ZhqNxKJeF4GAADFeilbkavIVegcNEABaEr5dLTFPzsnoeIgUH4lyZkKt1AoaGVlRbZtl1w1ca6uOAFqY2PDfW5mZkZS6RUf52Qe9KmCJem+++6TVBmUnGNU7arbqVOnqp5wH3/8cdm2rXPnzrmvu3jxosLhsE6cOFGxvb3emyeeeELS7tgE/f39sixLg4ODbtBwphHe2tradx+Lt1++n8eOHVM8Htfy8rIKhYIKhYKWl5cVj8d17Ngxd71WlXfixAlFIhFFo1F3X8+fPy/btnXq1KmWl+dwrj4eP3583+0BAEC2ag65ilyFzkEDFICmDA4Olvy/+Of+/v6Sf8vXl3YHdAyFQurv79exY8e0srJS8vy3v/1t2bat+++/X5lMRsPDw+4VqxdffFGSNDc3J0n6wQ9+oMnJydbuoIcefvhhSbeuTklyQ4m0e6ycgSqLzc3NVXRtdgbVtG275HUvvfSSu069783AwICy2awbyMLhsLLZrBte8vm8wuHwviHUsqyS7Tuhq9j09LRGRkbU39+vyclJjY6Oanp6umSdVpbnHLviY1T+mWtledKt99d5vwEA2AvZqjnkKnIVOodlWj0KWo9bXV3VxMREyweXA1rJsiwlEom2dJOtVrakjviOePl9dq4qnj59uqHXFQoFdzBMv4RCIaXTacrbRzQaVX9/f8PvcT0mJiYkSYlEouXbBoKGzzuCzu/830nZyqvvM7mq+8vzMldJ/v591EvoAQUAPpiamtLly5dLur/Xw++QtLGxoTNnzlDePra2trS1taWpqakW1AoAAOyFXNXd5ZGrugcNUGipXC6nVCqlUCjkd1UQQOVjG/Qyp4v3uXPn6ro3PgjW1tZ0xx13aHh4mPL2cP36dS0sLGhpacn3YAug85GtsBey1S5yVfeWR67qLjRAoart7W3NzMzIsizNzMxobW2trte98MILGhsbUyaTOVD5W1tbWlxcVCgUqnoPcL02NjYUjUZlWZYsy1I0GtXW1pZyudyBtntQ+x1fp77VHvPz88pkMnXP9BEk5WMb9LqBgQGtrKzo9ddf97sqdTlx4oQ70Cfl1ZbJZPTiiy9qYGCgBbUC0C38yFa5XE6Li4tuhnAGqT4IslWwkK1uIVd1Z3nkqu5CAxQqFAoFbW1t6bXXXlM+n9fnP/95feELX6gr+Lz22msHLn9+fl7RaFRHjhzRq6++2vT97NFoVMvLy5qcnJQxRsYYffOb39T29ravJ+h6jq8xRjs7O+7P+Xze3YeTJ09qcXFRk5OTHXely9kH54HdK3Ze3csOf5w+fZqQBKCEH9mqUCi4t6s4uWJ1dfVAs5mRrYKHbFWKXNV9yFXdhQYoVLhy5Yo7I0RfX587nWY7un7PzMwon8+7U8cWTx3aCOdq3GuvvVbS8j4wMCDbtrW+vt6qKjes3uNb/Iu2uLvp0NCQlpaWJO3e796JV+sAAOglfmSrixcvKpPJ6KmnnpK0myvm5uZ09uzZuntfFSNbAQAOigaogCgUCkqlUm5X4MXFxbrWKb/vu3iMgEwmI8uyFAqFtL29rY2NjYoux475+Xl32dDQUNU6hsPhPesUCoV0/fr1po+Bc0Vubm6u5v290Wh03yt3GxsbOnv27J4D3lW7FzmIx7eWgYEBPffcc8pkMrpy5UrdrwMAoFf0erZaXV2VVNrQ8v/+3/+TJF24cMFdRrbaRbYCAO/RABUQk5OT+vnPf+52n/3JT35SEQYmJyf129/+1u1CnMlkSq7STE1NuWMEbGxsyLZtZbNZZTIZfe9739Pw8LAuXbokSYpEIiXddE+fPq1IJKLNzc2KXkfO9kdGRqrW+/Lly8rn80qn0/rJT37S1P5vbW3p7NmzGhkZcccqCIVCTV2h+6d/+idJ0j333LPneuXdlIN4fPfyqU99SpL0z//8zw29DgCAXtDr2ara7X1OY9TCwkJD2yJbAQBawqClEomEafSwJpNJI8ns7Oy4y9bX141t2+7Ply5dqrqOJJNMJt1lkirKL18WiUSMJJPP591l+XzeRCKRqvW7dOmSsW27ZH1jjEmn00aSuXbtWsl2qtVhP7FYzEgym5ub7nbC4bCRZNbX1xvaVjPlB/H41rMvzeyr87pEItHw63pNM99noB3Gx8fN+Pi439UA2qKZzzvZyrg5qnhbtfZnP2SrvZEX6sf5C0HF30ftcVurGrLQPKeLdPF96cPDw0qn0+7PTlfp4nUeeOAB9/XOve71ePLJJ3X27FldvHjRfd2Pf/xjPfnkk1XX//73v68zZ85U3BbnXB0qHgeg2akxZ2dnJcntQt3X16dwOKyFhQUtLy97Pl1oEI+v165evarDhw+3tcxOc/XqVUmltyoAQbC9vd30GHlALyBbSc8884wWFhb08ssv66WXXlJfX587PX0sFmtqm40I4vH1Gnlhf9vb25I4VkDP8rsFrNs0cwVEdVxpqbVO+fJq61VbZtt2yVXAWleQksmkicfjB6pTPVq5LeeKX7WrXgctv53Hd696GXPrimitbe/F2S4PHjw698EVZPSKZnpMON+TZtYpX15tvWrLgpatjLnVE0iSicfjbq8kp8d5vchWe3PyPw8ePDr7QQ8o7zEGVAA4s3Y4V6X2Wqfa1LCNDLDoGB8fd++3397e1vHjxyvW2dra0s9//nNNT083vP1GOftQbdYRZ9/r5dzv/9Zbb9X9mk48vj/+8Y8lSY8++mhTr08kEhVT9/IofSQSCUnyvR48eJQ/xsfHm/reA72CbLXrxIkTSqfTMsZoenpa//7v/65IJFJz0O5ayFb18fvc0AmP8fFxjY+P+14PHjzKH2gPGqACwDlBLywsuA0w29vbmpmZcddx/ti4ceOGu8xZd3R0tOEyT5w4IUlaXl7WG2+8oT//8z8veT6Xy+n111/X3Nycu2xra6ukTvF43F1+UM4+FAcbZ/8a/UPLtm3Ztr3nAJvb29uan593fw7i8d1LLpfT97//fdm27ZYFAAB2ka0qpVIpXb582R32oBFkKwBASxi0VDO34O3s7Ljdo51HOByuGIDS6XrsDOaYTCZNOBwu2Y7zeqeLdPHAlcWDQBpza0DHWCy2b32cRzqddtfLZrNGkrFt22SzWWPMrQEnnX1oRCQSKdm/eDxe0tXaWaeebtHOPpQfR6fexeUYE8zjW7zt4i7vm5ubFXVtlOhiWhcGFUVQMYgrekkzn3ey1a193NzcNOFwuKJOxXUmWx0sW5EX6sf5C0HF30ftwW/KFmv2BLSzs+OeVCORSMWJ3VknHo+7J85kMlly8iw/4dZa5tjc3DRS5ewozn3+1R7VAoezfjgcdgNAMpls6gRevH/xeLxirIF6Q5IxuyEjnU6X7I9t2yYej7uhrliQjm+t553Q1ejMgOX4BVsfAiWCigCPXtLs573Xs1VxntprzCey1cGzFXmhfpy/EFT8fdQeljHc8NhKq6urmpiY4D5SBJplWUokEowjsw++zwiqiYkJSXLHKQO6GZ93BB15oX58nxFU/H3UHowBBQAAAAAAAE/RAAUAAAAAAABP0QAFT1mWVdcD6Ea5XK5kRiDAMT8/784GBQCNIFuhl5GtUAvZqjPQAAVPmd2B7vd9oHcUCgVPg7HX269XLpfTCy+8oAcffND9YyAajVZdt5P+cCgUCtrY2NDi4qJCoVDN9TKZjEKhkEKhkDKZjOflOba2ttx1mzmO9ZSXy+UUjUbd9yqVSlVdb69jcPLkSU1OTiqXyzVcRwC9jWyFcmSrSmSrg5fnaEe2csopfr9mZmYq1iFbdb7b/K4AgN5y5cqVjt5+PQqFgqampnTmzBkNDw8rn8/r4sWLGhsbkyTNzc2VrG+MUS6X0+DgoHZ2djQwMOBHtesSi8UkSWfPnq25TiqV0urqqlZWViRJzz//vG7evKnp6WlPynPMz8/r8uXLmp6e1quvvqp0Ot3y8nK5nG7cuKG5uTnNzc0plUppbGxMv/71r3X69Gl3vf2OwdDQkM6cOaOpqSmtrKyor6+v4boCACCRrSSyVavLc7QjWznefPPNkp9HRkZKfiZbdYk2zbbXM5iGFZ1APk0zms/njW3bnn1HWr39Zr/PsVis6pTWKpqGuppO+t2hKtNjG7M7fbikkumsnWmz95oGvNnyHOFw2EQikZLpvQ+iVnnVpukuX7eRYxAOh00sFmu4fkxjjV7C5x1B52f+77Rs1ez3mWzVvdnKkU6naz7Xjmzl199HvYZb8ADUpVAoKJVKud1iFxcXS7q4VuviXL4sFou53WWd5blczu1OK0mLi4tut9vr168fePuSFI1Ga3bRbrVcLqfZ2Vk9+uijVZ+PxWIaGxuredtWuf2Oey6XUyqVco9fJpORZVkKhULa3t6uqNv8/Lz7/NraWpN7Wdsbb7whSTp69Ki77K677pJUeWWrVZz3dm5uzvOrXcPDwyU/O2MNRCIRd1kjx2B0dFSzs7N0FweAHkS2qg/ZqruzlSRtb28rFAopGo1qY2Oj4nmyVfegAQpAXSYnJ/Xb3/5Wxhjt7Owok8loamrK/QN8Z2en4jXZbLbk5+Lu0eb/xqgYHBx07+Pe2NjQ9PS08vm8JOn+++93g1Kz22+3q1evSpLuvffeqs+fPn1akUhEY2Nj2tra2nd7+x33qakpjY2NucfPtm1ls1llMhl973vfc7eTy+U0NTWlD3/4wzLG6LnnntMXvvCFuurQiMuXL0uSjh075i5zur0fZLyCWra2tnT27FmNjIy4AdurAFhue3vb7VY+OTnpLm/kGDifE+dzAwDoHWSr+pCtuj9bOcfs7NmzeuSRRxQKhUoakMhWXcSPblfdjFvw0AnUYBfTS5cuGUlmZ2fHXba+vl7R5VlVutaWL6tnHWNudast7kLb7Pab1cz3ORKJ1HyNs7y4O/u1a9cqnne08rgnk8mq61Trzl6PWse50eUHLS8Wi5V0v87n8yYcDld0025VeQ6nK7jz2O9zWmt5Pp+veH09uCUJvYTPO4KumbzQq9mqme8z2ao3slU+nzebm5vu+x2Px/d9bSuzVaN/H6E59IACsK8LFy5IUskAjg888IAkaXV11ZMyh4aGJEmzs7OebN8r9Qzo2NfXp6WlJUnas4twK4+7s3551/p66htkzufD+bz09fUpHA5LkpaXlz0r99ixYzLGaHNzU5FIRLOzs1pcXGx4O0639k77nAMADoZsVT+yVXv5la36+vo0NDSkubk5xePxpnt3ka2CjQYoAPtaWFioWOb8cvei628vGBgY0ObmZkW372KtPO7O+sbjqbpt2675nBNevOYEpmrHz4uynNvvvv71r0sKxjEAAAQb2ar1yFbeaWe2kqSnnnqq5P0IwjFAa9AABWBfzi/9aleTvP6l380nlaGhIaXTaWUyGXcsoWJeHPfiwUe9UK3OzoCdDz30UMvLc45DtZC5V1hppfvuu69que06BgCAzkO28gbZ6uCCkK2Ke10Vl0u26nw0QAHY1/j4uCTpxo0b7jLnpDQ6OupJmc7JfGRkxJPte8UJO9VO2tXYtq1kMlm1u3Yrj3s8HpckraysuNtwZm5ppccee0xSaZ3ffvvtkudayTkOb731lrvM2T/n+HnNKS+ZTEpq7hgUz6IHAOh+ZKv6ka16M1sVvx9kq+5BAxSAfT3++OOybVvnzp1zrzxcvHhR4XBYJ06ccNdzrlQ4Aad4GtWZmRlJpVcwyk/QzvS5hUJBKysrsm275EpLs9tv51TBTm+Y8pDkHLdqV9xOnTpV9SRZz3Ev3p5TZnHZzvNPPPGEpN1xCfr7+2VZlgYHB92TuzOFcD0ztxRvv3w/jx07png8ruXlZRUKBRUKBS0vLysej5fMXNKq8k6cOKFIJKJoNOru6/nz52Xbtk6dOtXy8kKhkObn592rboVCQbFYTJFIxC2v3mMg3bp6d/z48X3rBQDoHmSr+pGtujtbpVKpkhn2tre3deXKlZLvAdmqi/g0+HnXYhY8dAI1McvDzs6Oicfj7mwTyWTS5PP5knWy2aw7A0k6nTbGGGPbtkkmk+5sI84MLJFIxF3mbHNzc9N9fTweb9n2I5FIUzOSNPN93tnZqZglxNm/4kc1tm1X3d5ex73admuVlc1m3ZlFwuGwyWaz7nORSMSEw+GqdShWbV+q7U86nTaSjG3b5tKlSxXPt7q84mNU7bPTqvKc/XIesVis5oww+x0DY27NvFM8G089mBUMvYTPO4Ku2fzfi9mqme8z2WpXL2SrSCTizr5XjZfZqpm/j9A4y5gWj5LW41ZXVzUxMdHyweeAVrIsS4lEom3daPfjzBoStO9Ns99n5+rg6dOnG3pdoVBwB8L0SygUUjqdprw2iUaj6u/vb/izMjExIUlKJBJeVAsIFD7vCLog5v+gZqtmv89kK8qrV7PZKmh/H3UrbsEDgBabmprS5cuXS7qx18PvgLSxsaEzZ85QXptsbW1pa2tLU1NTflcFAIBAI1tRXj3IVsFHAxQAXxXfZ1/tHv5O1NfXp6WlJZ07d66u++KDYG1tTXfccYeGh4cprw2uX7+uhYUFLS0t+R6OAQDdhWwVDN2edchWaMZtflcAQG8bHBws+X/Quoo3a2BgQCsrK1paWtLQ0JDf1dlX8UCPlOe9TCajF198UQMDA35XBQDQZchWwdDtWYdshWbQAAXAV90Siqrp6+tr+P5z9AY+FwAAr5Ct0Iv4XHQGbsEDAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACApxiE3CNPPfWU31UA9vTKK6/ohz/8od/VCLTt7W1JfJ8RPBcuXND4+Ljf1QDaZnV1Ve+8847f1QCqIi/U7+rVq5I4VkCvskw3T5Pgg5s3b+pb3/qW3n33Xb+rAnSM119/XR//+Md15MgRv6sCdIzJyUnZtu13NQDPZTIZrays+F0NoGPcvHlTP/vZz3Ty5Em/qwJ0jEOHDunll1/m7xGP0QAFwHeWZSmRSNCjAwAA4IBWV1c1MTEh/swDEDSMAQUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE/RAAUAAAAAAABP0QAFAAAAAAAAT9EABQAAAAAAAE9ZxhjjdyUA9I6lpSX99V//te6//3532a9+9Sv98R//sf7wD/9QkvRf//Vf+sxnPqN//Md/9KuaAAAAHeHkyZPa3NzUXXfdJUn63e9+p9/85jf6kz/5E3eda9eu6e///u81Pj7uVzUBQLf5XQEAvWVnZ0fvvPOOfvazn5UsLxQKJT9nMpl2VgsAAKAjra2tyRij3/zmNyXLy7PVW2+91cZaAUAlbsED0FZjY2OyLGvPdW677Ta99NJLbaoRAABA53rppZd022179yuwLEunTp1qU40AoDpuwQPQdn/2Z3+mH//4x6r168eyLP3yl7/U3Xff3eaaAQAAdJZsNquPfvSje+aqT33qU/q3f/u3NtcMAErRAwpA233lK1/RoUOHqj73vve9T8ePH6fxCQAAoA533323jh8/rve9r/qfdocOHdJXvvKVNtcKACrRAAWg7Z5++mm99957VZ+zLEvPPPNMm2sEAADQuZ555pmaQxy89957evrpp9tcIwCoRAMUgLY7cuSIPv/5z9fsBTU6OtrmGgEAAHSuWtnp0KFD+vznP68jR460uUYAUIkGKAC++OpXv1oxVsGhQ4f06KOP6s477/SpVgAAAJ3nzjvv1KOPPlpxcc8Yo69+9as+1QoAStEABcAXX/7ylwlJAAAALVLr4t6Xv/xln2oEAKVogALgi76+Pj3++OMl0wYfPnxYX/rSl3ysFQAAQGf60pe+pMOHD7s/33bbbXr88cfV19fnY60A4BYaoAD4ZnJyUu+++66k3ZD0xS9+UR/60Id8rhUAAEDn+dCHPqQvfvGL7sW9d999V5OTkz7XCgBuoQEKgG+++MUv6gMf+ICk3ZA0MTHhc40AAAA618TEhHtx7wMf+IC++MUv+lwjALiFBigAvnn/+9+vJ598UpL0wQ9+UCMjIz7XCAAAoHONjIzogx/8oCTpySef1Pvf/36fawQAt9y2/yreWF9f13/+53/6VTyAgPjIRz4iSbr77ruVTqd9rg0Avx06dEihUKhkfDjs73//93+VTqfdng8Aetfdd9+tn//85/rIRz6iCxcu+F0dAD77yEc+okceecTvakiSLFM+VUK7CrYsP4oFAAAB9w//8A9MSNCgH/7wh8x0BQAAqvKp2aeCr5cXE4mExsfH/awCgC6yurqqiYmJwPyCDTJnvK1EIuFzTYBSlmXpd7/7nd/V6DjOMeP3H4BWIi/Uz7Is/r5F4Dh/HwUFY0ABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABAAAAAADAUzRAAQAAAAAAwFM0QAEAAAAAAMBTNEABQBXRaFTRaNTvagRWLpfT/Py839VAAM3Pz6tQKPhdDQBAwJCtaiNXoZZuy1U0QAFAABUKBVmW5Xc1qsrlcnrhhRf04IMPyrIsWZZVM1A6zxc/gqpQKGhjY0OLi4sKhUI118tkMgqFQgqFQspkMp6X59ja2nLXbeY41lNeLpdTNBp136tUKlV1vb2OwcmTJzU5OalcLtdwHQEA8EpQsxW5qntzlVNO8fs1MzNTsU4v5arb/K4AAATR3Nycr+VfuXLF1/JrKRQKmpqa0pkzZzQ8PKx8Pq+LFy9qbGxMUuVxM8Yol8tpcHBQOzs7GhgY8KPadYnFYpKks2fP1lwnlUppdXVVKysrkqTnn39eN2/e1PT0tCflOebn53X58mVNT0/r1VdfVTqdbnl5uVxON27c0NzcnObm5pRKpTQ2NqZf//rXOn36tLvefsdgaGhIZ86c0dTUlFZWVtTX19dwXQEA3YdsVYlc1b25yvHmm2+W/DwyMlLyc8/lKuMTSSaRSPhVPIAulEgkjI+/1lomn88b27Y93Zfx8XEzPj7e8OtisZiJRCIVyyUZSSaZTFZ9XSe9L86+lMtms0aSWV9fd5dtbm4aSWZzc7Pl5TnC4bCJRCImn883XUY95RXvV611GzkG4XDYxGKxpupHPmhct/z+AxAszeaFoGlHtmrm/EWu6t5c5Uin0zWfa0euClo+4BY8ACiTy+WUSqXc7rTlP2cyGVmWpVAopO3tbXcdp/usJC0uLrrdbK9fv+5uu1qX6fJlsVjM7X5bvNzvsRNyuZxmZ2f16KOPVn0+FotpbGys5m1bnl/IewAAIABJREFU5QqFglKplLuPi4uLJd2L6znuxevOz8+7z6+trTW5l7W98cYbkqSjR4+6y+666y5JlVe3WsV5v+fm5jy/4jU8PFzyszPeQCQScZc1cgxGR0c1OzvbNV3GAQDNI1tVIld1d66SpO3tbYVCIUWjUW1sbFQ835O5yq+WL3GFE0CLtaqF37lC5myr+GfnCoVzxSIcDhtjbl39KF4nn8+bcDhsJJlr164ZY4zZ2dmp2aukeFn5z8YYE4lEql4la0YzVzTT6bSRZLLZbMVzTl0jkUjVqzbV3hfbtk08HjfG7B4X27aNbdvuFal6jnvxa52rhJcuXTrQ1bNqx94Y476X1da3bbupsvYqz7kClk6nTTwed8u5dOlS02XtVV6xbDbrvpfOZ9eYxo6B817tdeWvVv3IB40L2hVOAN2hVT2geiFbNXr+Ild1f65y3mPnYdu22dnZcZ9vR64KWj6gAQpA12jlL9h6Qks96zgnu+Ius81uq5WaCZROCKrGWV7cxb244aL8dU6YKT4Jr6+vV3Q3r+dYJZPJqus0GyhrHftGlx+0vFgsVhL4ikN3tdvlDlqeozi01/PZrbU8n89XvL7e+pEPGhe0gAmgO7TyFrxuz1aNnr/IVb2Rq/L5vNnc3HTfb6eRcK/XtjJXBS0fcAseAHhoaGhIkjQ7O+tzTQ6unkEd+/r6tLS0JEl7dhO+cOGCJJUMnvnAAw9IklZXVxuql7N+eXf7euobZM5nxvkM9fX1KRwOS5KWl5c9K/fYsWMyxmhzc1ORSESzs7NaXFxseDtO1/Zu+OwDAIKjW7IVuaq9/MpVfX19Ghoa0tzcnOLxeNMz/XVLrqIBCgDQUgMDA9rc3FQmk9HU1JQ7llCxhYWFimXOibXRE7Ozvtnt1VvyaCXbtms+5wQYrzmhqdrx86KsyclJSdLXv/51ScE4BgAA9BJylXfamask6amnnip5P4JwDNqNBigAaINuPYnUMjQ0pHQ6rUwm405TW8w54Va7ktfssSoekNQL1ersDNr50EMPtbw85zhUC5p7BZZWuu+++6qW265jAABALb2UrchVBxeEXFXc66q43F7KVTRAAYCHnJP3yMiIzzU5OCfwVDtxV2PbtpLJZNUu2+Pj45KkGzduuMuc7Y6OjjZUr3g8LklaWVlxt+HM3tJKjz32mKTSOr/99tslz7WScxzeeustd5mzf87x85pTXjKZlNTcMSieRQ8AgIPqlmxFrurNXFX8fvRirqIBCgDKlE9ZW/yzc6IqDgvlV5uc6XILhYJWVlZk23bJlRXnyocToIqnZZ2ZmZFUekXEOeH7OVWwdKs3THlQcva/2lW3U6dOVT1RPv7447JtW+fOnXNfd/HiRYXDYZ04caJie3sd9yeeeELS7tgE/f39sixLg4OD7gnemUZ4a2tr330s3n75fh47dkzxeFzLy8sqFAoqFApaXl5WPB7XsWPH3PVaVd6JEycUiUQUjUbdfT1//rxs29apU6daXl4oFNL8/Lx75a1QKCgWiykSibjl1XsMpFtX8I4fP75vvQAA3Y1sVYlc1d25KpVKaW1tzf15e3tbV65ccd8PqUdzlU+DnzPLDYCWa9UsDyqaAazao9o6xcs2NzfdGUvi8bg7/a0jm826zztTqTrT3TqzlzgzvEQiEXdZK6cKbmZWG2ea4+KZQmodn3LVptPd2dlxp8HV/83SUnys6j3uxuweU2d2kXA4XDKlcSQSMeFweN8pffd6v4s5U+rWmrq31eUVH6Nqn6dWlVc+VXAsFqs5K8x+x8CYW7PvFM/IUw/yQXOCNssNgO7QqlnweiFbNXr+Ilft6oVcFYlE3Nn3qvEyVwUtH1jGtHg0sTpZlqVEItG27m4Aut/q6qomJiZaPkhivZxZQvwqvxETExOSpEQi0dDrnCuGp0+fbuh1hULBHQzTL6FQSOl0mvLaJBqNqr+/v+HPCvmgOX7//gPQnZrNC63SSdmqmfMXuYry6tVsrgpaPuAWPABA3aampnT58uWSru318DskbWxs6MyZM5TXJltbW9ra2tLU1JTfVQEAILDIVZRXj27KVTRAdbhcLqdUKqVQKOQu83ucmHLV6oj26YTPSDcoH9ugW/X19WlpaUnnzp2r6974IFhbW9Mdd9yh4eFhymuD69eva2FhQUtLS74HZKAZnXDeJFv5qxM+I92gF7IVuYry9tNtuYoGqA73wgsvaGxsTJlMxvOytre3NTMzI8uyNDMzUzKo2l4OWsetrS1ZluU+nIEE61X8Wsuy9rzCsLGxUbF+K5Rv03mEQiEtLi56elIN0mek1nGwLEvz8/PKZDJ1zwQSNIODg1X/340GBga0srKi119/3e+q1OXEiRPuQJ+U571MJqMXX3xRAwMDflcFaEqQzpu1HLSOmUxGoVDIzSLOAM/1IlsF5zNCtup85KreLm8/XZer/Bp8Sgwy2jLaY4C6Vsnn8+6Afvl83iSTyZJB/vZzkDoWDxTXSJnFstms+/pwOFxzvXA47K7X6ABv+3EGGiw+DsUD/F27dq2l5RUL0mek+DgUD/rnDC5p23bTxz5og+wFWasGFQVajXzQHH7/tVaQzpu1NFvHWCzmDupszK2BmWOxWEPbIVsF5zPiZbYiL9SP8xeCKGj5gB5QqMuVK1fcqUv7+vrcqSrb0fX7yJEjMsa4j+IpV+vlTGMZi8W0sLDgTmNZbHt7W/fee6/7c6tbmatt79ixY/rmN78pSXr55ZdbWl671fsZKT4Oxd1Ih4aGtLS0JGn3fvhOvVoHAEA9/MpWs7OzknbPu8X/Xr58uaHtkK28R7YC0G06ogGq/D7rTCbjdkN1TnapVKpimbQ7Q8Di4qLbFTUajbpdcqt1B262i3Aul3O7M0tyy5yZmdH169cr1i8UCm6dLcuq2VW43vVqHataxy4UClUEhbW1Nbc79vz8fEk5tRp9wuHwnnUOhUJV979e29vbCoVCikajNbt3N3LP/cmTJyVJb7zxRsVzb7zxhvt8OS8/R05oWFhYqCizWz8jtQwMDOi5555TJpPRlStX6n4dAKAxZKvOOm+2MlvFYjFJcnOVU+f/z979x7Zx5nce/3ATJ3cINhS2gOTYB29vsY0R9PaUH4tE2dtFLt6ggb0Ybq6tvKK06vYP2aCBvTSFhcNWpRAEcr17AIUr0j9qUPrHECQS9h9XiMAZB1RGbaSRHLQH8vrHwcbBC+qabUi0V073j2uTyz73h+4ZkxQpDSmSMyTfL4CwORw+8+UzQ85XzzzzPEtLS9465Fb9d4w0Q24FIFSC6nqlFrooOo7jdSu13YW3t7e9Lr/b29vGmEddgau7Adtuv6VSqeHr9vYu2y21VCoZx3G87bTyeezDxlOpVLzt13cBdhzHpNPpmm06jlPTbdbveqrqAlxdV/XPD6qnzc3NmnVsF1816V5cqVSadhN3HMckEgkvxuqyWmXjso9GXYiTyaRJJpOHlmW3b/dJPVsfjWLt1HHUqGxbl/Xd1wf5GDnoeGhWH36ErYtpmNGlHmHVSn6AR1r9/SO36q/zZidzK2OMd4va9va2yWQy5FYDcIx0K7ciX/CP8xfCKGx/H/VFA5Rdv77i/CxLJpM1P7aHnQBTqVTb90g3KrvRffVbW1s1J1RjHiV9mUym5fXqt3vY81bWaTYewNbWVsMTtj2RVieF9qTX7oFfqVRMPp/3kiWbELTKbt/Wqz3ZG7O3n7a2trz16mPt1HFUn+xXKpWaJNAa5GOkWVmtvN5M2H5gw4yEEmFFAt+edn7/yK2arxem82Y3citjHu2jZDLZ8FztB7lVOI6RZmW18noz5Av+cf5CGIXt76OIMcYoAJFIROvr65qenva9viRVh+t3mbTXvfjmzZvefe/Vr5fLZY2NjclxHKVSqbZHvW+27frlly5d0rVr12rWc11XIyMjchxHm5ubLa1XX/5hz/3G1OzzSHv3ni8sLOybnrJROYeV1YqVlRXlcjnvs7ciEonU1EkikdAf//EfS9rram67nh8U61GPo0ZdxpPJpH7zN3/TG4NBGuxj5LD3+Xm9mY2NDc3MzGhycrKl9w2je/fuSZJeeeWVgCMBat28ebOl/AB77O9fK7+b5FbN1wvTebMbudXy8rJOnjyps2fPKpVKqVAoaG1treUpvsmtwnGMHPY+P683MzMzow8++IB8wYebN2/qlVde8cZHA8Jgd3dX9+7dO/Lf4Z3SF2NAHdXKyop++MMfNr2PenR0VJlMRrlcTv/7f//vrsdTfz+69GjAwOrpXP2u1wn2XnI7DW+hUJD0aJyAatlsVo7jNDz5NYq5k86fP9+Rz57JZLwBM8vlsn71V3/10Pd08jgy5tGg6ktLSzUJkjTYx8hh7ACZyWSy3VABAF1GbnW4sOZW2WxW8/PzOnv2rKLRqGZnZ5XL5XTjxo0jlUtu1TpyKwBD52gdqNqnFrsoqs1u4vZe6mKx2PQ9tluvnZa2k93E7fLqLsb2vvD67bS7Xv12D3vebNnm5qZXB47j1HQztuytcM0cVAedOtzauX/dxmDZ+/AzmYzJZDLe8WHXq4+1U8eR33oY5GOkWdmW7fpuu+23ImxdTMOMLvUIq1bzA+zp5S145Fb9nVvVv+8ot/ORW4XjGGlWtnWU3Ip8wT/OXwijsP19NPANUH5OBvY+60ql4g3y2I5GZd+/f99ItYMF2hNu9X3p9uRffWLwu14nToCbm5uH3v9vk4Bq+Xy+4YCRfgaIbEelUmnr5GljqGbHB6j/TO0cV8b4O4781sMgHyPNtmffbwf6bEfYfmDDjIQSYUUC355eNkCRW/V3bmUbWOrLaufcS24VjmOk2fbs+4+SW5Ev+Mf5C2EUtr+P+qIBqlQqeT+q9ke6eln17Bj1y+xJtlgsegmLfd0OUlj9w29PMH5m/mj0maRHAxTa8ut/8O1JtHpGt0wms+9E4me9+s980HP7OauvdNly7fP6RyKR8MqpntGj+lGdANqrX47jeFe07FUXW55fmUym5kRfLBYbzvrhZ6YWWw/VV7PsIKbVCV2jY8iYzhxHjeq9mUE+RqrLrq6zfD6/77O0Kmw/sGFGQomwIoFvT6u/f+RW/XPe7GRuVf1eW6d2gO3qnIvcqr+OkW7mVuQL/nH+QhiF7e+jvmiAqv/BbWWZPREmk0lTKpW8GTfsybx63WZltPKZ7EnXnizS6XTDKxulUsm7omWTgHbWa3biavY4qJ6aneASiYQ3C0mjR/00yMVi0VvfnkBtl+JWTn521he7/5pN33xYktSsHowxDbtSd+M4OqjsZgbxGDlou6lUquZqYzvC9gMbZiSUCCsS+Pa0+vtHbtUf502rU7mVtbW1VVNefe9ycqv+OUa6nVuRL/jH+QthFLa/j/pmFrx+0KmZ3oLw4MED/bN/9s/2zdrw4MEDnT59ui8/EzqrH46RdmaBGlYzMzOSpPX19YAjAWoNYn7QC4P6+0duhUHWD8cI+YJ/nL8QRmHLD4ZiFjwcLJvN6tlnn204ZejY2JgymUwAUSFMOEYAAPCP8yYOwzECYBjRANUh5XK54f/7wcbGhlZWVrS7u1uz/MGDB7px44ampqYCigxhwTGCVpXLZS0vLwcdBjpoeXnZm8ob6AVyKwwyjhG0itxq8AxjbkUD1CEikYivx9jYmPee6v/3g7W1NX3xi1/Uj3/8Y+/zLC4u6q//+q914cKFjm/Pb50iPHp9jPQr13W7eux2u/xOKZfLevfdd/XCCy/UHC+N9Nt3v1Ao1MR66dKltsvK5XKKxWKKRCKKxWLKZrNtleO6rnZ2drSysqJYLNZwnXK5rMXFRS/uZtuyMcViMeVyuZrX3njjDc3OzvZdQwDCh9yK3ArkVn6RW+0ht/Knl7mV5C92cqs6AY09xSBtADou6EH27MD5/VB+twYVtbMH2UFPK5WKN6V1swFtG82kFFbVg9JKajgzpx+pVMobWNmYR4Px1k+17YcdLNjGVK9UKtUMQmv3R/22MpmMcRzHVCoVU6lUTCKRMOl0umad7e1tb51uIT9oT9C/fwAGU9CDkPdTbtWt8xe5lT+9zK38xh6G3Cps+QE9oACgA1zX1crKSt+W3ymrq6saHx/XxMSEJCkajXq3EVy5cqXhlajR0dGaf8Ps+PHjMnszyMoYI8dx2ipnfn5ekjQ+Pl7z7507d1oua2lpSUtLS01ff/jwobc/JHn7w8YgSbu7u4rH41pYWFA0GlU0GlUikdDFixdVKBS89SYmJnTy5Emtrq62HCcAAK0gt9pDbuVPL3Mr66DYya0aowEKwNBzXVfZbNbrPruyslLTFbZRF+b6ZalUyutWa5eXy2Wv260krayseN1zHzx4cOTyJWlxcbFpF+xeK5fLmp+f1+uvv97w9VQqpXg87rs79GH7pVwuK5vNevWby+W8Ltf1Y2rYcRPs67dv32758+3u7ioWi2lxcVE7Ozstv79aKpWSJK8cG6+fZKdV1Y1PkryxBpLJpLfsww8/lCSdOHHCW/bMM89Ikj766KOa909OTmp+fn64uosDAFpCbtUZ5Fb+9TK3suUfFDu5VRNBdb0SXewBdFi7XUwdx/G6w5ZKJeM4Tk1XWNuNubrsYrG4b1mz55Jquk0nEgkjydy/f/9I5RvzqItwq7rRpd52Yy8Wi/tes3Hb7sy2e3T969UO2y+O4+yrX1tviUTCK8e+N5PJGGOM2draahiD389nH47jHKlru62L7e1tk8lkjtxNvtHxUa9YLHrbtcefMcY7JhuV6TjOvjJ0hC7yhyE/aE/YutgDGAzt5gvDmFt14/xFbtWaXuZWh8UeltwqbPkBDVAABkY7P7D2hFl9wtje3jaSvJOqMY1PQH6SmEbLGt2T3m757epGA5Q96Tdil9txDOobQOrf18n9YsdJqF+nneSyUqmYfD7vfdb6+/hbZZOTZDJ55Hv/Dzs+qhNvP8dfs+WVSqXtMRX8ID9oT9gSTACDoZ18YVhzq26cv8itWtfL3Oqg2MOSW4UtP+AWPABD7ebNm5Jq75F/7rnnJO1NkdwN9p706jF4BsGVK1cOXScajXr3uB/U1biT+8WuX9/13k+89aLRqMbHx7W0tKR0Or1vNpNWLC8v67XXXlOlUpEkzc7OdnUq3lOnTskYo3w+r2Qyqfn5+bbGvohGo5IG7/gFAHQGuVXnkFu1pte5VadiH6rcKqiWL3GFE0CHtdPCL59XJxqt1846nS6/Xd3oAXVQfPXL7ZVK2+27X+qtWqO4/bJXDu2Vufv37x/5ql8rn9Nuz65vr5w2KrO6y30722oV+UF7wnaFE8BgaCdfGNbcqhvnL3Ir/4LOrepjD0tuFbb8gB5QAIaana2i0dWiRCLR1W13u/wwGx8f1+bmpnK5nDdoZLVu7JfqwUk7wc5m0o54PO6VIUljY2OSpIsXL3YmuEM8++yzNc8b1bcdvPPFF1/sSUwAgMFAbhUMcqtgc6v62MmtGqMBCsBQm56elrQ3Vb1lu+pOTk52ZZv2ZH3u3LmulB8Um+z47ersOI4ymUzD7tqd3C/pdFqStLa25pVhZ245Ctd12z5G6qcYtslSu1MPt8rWQyaTkSS9+eabkmrr+2c/+1nNa/WqZ9EDAMAit+occiv/wpBbVcdObtUYDVAAhtrZs2flOI6uXr3qXaG4deuWEomEzpw5461nr2jYBKd6utVLly5Jqr3SUX8CttPjuq6rtbU1OY5Tc0Jst/wwTRVse9XUJ0m2XhtdcZuammp4svWzX6rLs9us3rZ9/bvf/a6kvXEJRkZGFIlENDY25iUJdgrhQqHQ9LNls9ma6YV3d3d19+7dmmPEb1mS9M4773jlSo/2t13eSllS7eeur/9YLKbl5WXvqpvrukqlUkomk5qampK0Nz5UOp3W9evX5bquXNfV9evXlU6nderUqZrybDkvv/zyoXEBAIYPuVXnkFuFM7fyEzu5VRNB3fsnxngA0GHt3uNcKpVMOp327r3OZDL7Zs0oFovevdx2ilQ7/aydTcTee59MJr1ltsx8Pu+9P51Od6z8dqcK7sYYUHbKYzttrzG10yXrgHvb66ejteUdtF8aldtsW8Vi0ZuhJJFI1ExnnEwmTSKRaBiDVT3VbjKZbDrNsJ+yrK2tLW+mlkQiYba2ttoqq1EdV3/2+mmCU6lUzT5q9Dkdx9kXj2VnzDnq1MYHfR7yg9aFbYwHAIOh3XxhGHOrbpy/yK3Cn1sdFHv1ukHlVmHLDyLGGNNSi1WHRCIRra+ve10BAeCoNjY2NDMzo4B+1hqys4KEKSZJmpmZkSStr693tFx79fDy5cstvc91Xa+rdFBisZg2NzcHuqxOWFxc1MjISMv72C/yg/aE8fcPQP/rVr5wFGHNrbp1/iK3CndZndDN3Cps+QG34AEAOmZubk537typ6ebuR9AJ0s7OjhYWFga6rE4oFAoqFAqam5sLOhQAAIYCuVV4y+qEYcutaIACgC6pvo++0T36gygajWp1dVVXr171dX99GNy+fVtf+tKXNDExMbBldcKDBw907do1ra6uBp7UAgCGE7kVuVVYyuqEYcytHg86AAAYVHb6V/v/sHR97bbR0VGtra1pdXVV4+PjQYdzqPrBLgexrE7I5XJ67733NDo6GnQoAIAhRW5FbhWWsjphGHMrGqAAoEuGJSlqJBqNdm2MIASD/QkACBq5FefiQTKM+5Nb8AAAAAAAANBVNEABAAAAAACgq2iAAgAAAAAAQFfRAAUAAAAAAICuogEKAAAAAAAAXRUxAU0lEIlEgtgsAAAIuf/8n/+z3nrrraDD6Ct/8id/on/37/5d0GEAAIAQCssMkoE1QG1vb+uv//qvg9g0gJA5f/683n77bX3zm98MOhQAAXvssccUi8X0+OOPBx1KX/m///f/anNzU59//nnQoQAI2AcffKD3339fN27cCDoUACHwL/7Fv9Crr74adBiSAmyAAgArEolofX1d09PTQYcCAADQ1zY2NjQzMxOaHg8AYDEGFAAAAAAAALqKBigAAAAAAAB0FQ1QAAAAAAAA6CoaoAAAAAAAANBVNEABAAAAAACgq2iAAgAAAAAAQFfRAAUAAAAAAICuogEKAAAAAAAAXUUDFAAAAAAAALqKBigAAAAAAAB0FQ1QAAAAAAAA6CoaoAAAAAAAANBVNEABAAAAAACgq2iAAgAAAAAAQFfRAAUAAAAAAICuogEKAAAAAAAAXUUDFAAAAAAAALqKBigAAAAAAAB0FQ1QAAAAAAAA6CoaoAAAAAAAANBVNEABAAAAAACgq2iAAgAAAAAAQFfRAAUAAAAAAICuogEKAAAAAAAAXUUDFAAAAAAAALqKBigAAAAAAAB0FQ1QAAAAAAAA6CoaoAAAAAAAANBVNEABAAAAAACgq2iAAgAAAAAAQFfRAAUAAAAAAICuogEKAAAAAAAAXfV40AEAGC7/5//8H/3N3/zNvuXlclkPHz70nkejUf3SL/1SL0MDAADoO3/3d38n13W95+VyWZJq8ipJeuaZZ/TP//k/72lsAFAtYowxQQcBYHj87u/+rv7wD//Q17r8PAEAABwsEon4Wi+ZTGppaanL0QBAc9yCB6CnXnzxxUPXiUQi+sY3vtGDaAAAAPrbN77xDV+NUM8++2wPogGA5miAAtBTb731lp588slD1/v3//7f9yAaAACA/uYnZ3ryySf11ltv9SAaAGiOBigAPfXFL35RjuPo8cebD0H35JNPynGcHkYFAADQnxzHOfDi3uOPPy7HcfTFL36xh1EBwH40QAHouenpaX3++ecNXzt27JjeeustPfXUUz2OCgAAoP889dRTeuutt3Ts2LGGr3/++eeanp7ucVQAsB8NUAB67ty5c00bmD777DN9//vf73FEAAAA/ev73/++Pvvss4avPfXUUzp37lyPIwKA/WiAAtBzTz75pM6fP9/wSt3TTz+tX/u1XwsgKgAAgP70a7/2a3r66af3LT927JjOnz/va/xNAOg2GqAABGJmZmbflbpjx47pe9/7XtMu5AAAANivWQ712WefaWZmJqCoAKBWxBhjgg4CwPD5/PPPNTY2pr/7u7+rWf5nf/Zneu211wKKCgAAoD/duXNH//bf/tuaZb/0S7+kUqmkxx57LJigAKAKPaAABOKxxx7T97//fT3xxBPesuPHj+tb3/pWgFEBAAD0p29961s6fvy49/yJJ57Q97//fRqfAIQGDVAAAjM9Pa1PP/1U0l6SND09rS98gZ8lAACAVn3hC1/Q9PS0d3Hv008/ZfY7AKHCLXgAAvXlL39Zu7u7kqS/+Iu/0EsvvRRwRAAAAP3pL//yL/X1r39dknTq1CkVi8WAIwKAR+hqACBQs7OzkqRf/uVfpvEJAADgCF566SX98i//sqRHORYAhMXj9Qs++eQT/e7v/q4+//zzIOIBMGT+4R/+QZL0j//4jzp//nzA0QAYFrOzs3IcpyfbyuVyWltb68m2AMDe4PIXf/EX5FYAeuKxxx7Tf/pP/6lmHLpG9vWAun37trLZbNcCA4BqTz/9tL7+9a/rlVdeCTqUnrp3757u3bsXdBh94ebNm95tmkAn3Lx5s6e5Tjab1c2bN3u2PQDDbXx8XF//+tf19NNPBx1KT5Ev+LO7u8s5CR2XzWZ1+/btQ9fb1wPKunHjRkcDAgA8MjMzI0laX18POJLwi0QievvttxlIFR1jv3+9ND09zfcdALqIfMGfjY0NzczM8Pc+OioSifhajzGgAAAAAAAA0FU0QAEAAAAAAKCraIACAAAAAABAV9EABQAAAAAAgK6iAQoAAAAAAABdRQMUAPS5xcVFLS4uBh1GKJXLZS0vLwcdBjpoeXlZrusGHQYAYECRVx2M3Grw9DK3ogEKAHAkruv6nnq1l8rlst5991298MILikQiikQiTRNK+3r1I8wKhUJNrJcuXWq7rFwup1gspkgkolirfCdQAAAgAElEQVQspmw221Y5rutqZ2dHKysrisViDdcpl8taXFz04m62LRtTLBZTLperee2NN97Q7OysyuVyW3ECABBmYc2rJHIrv3qZW0n+Yg9LbvV417cAAOiqpaWlQLd/9+7dQLffiOu6mpub08LCgiYmJlSpVHTr1i3F43FJ++vMGKNyuayxsTGVSiWNjo4GEbZvH330Uc3zc+fOtVXO8vKy5ufnlc/ntbm5qUKhoOeff14ff/yxLl++3FJZqVRKknTlypWGr5fLZT18+FBLS0taWlpSNptVPB7ft61sNquNjQ2tra1Jkn70ox/pk08+0YULFyRJ4+PjWlhY0NzcnNbW1hSNRtv56AAANERe1Ri5lT+9zK2sw2IPU25FDygAQNtc19XKykrQYeyzurqq8fFxTUxMSJKi0aimpqYk7Z3EG12JsolR2BMkSTp+/LiMMd7DcZy2ypmfn5e0l3hU/3vnzp2Wy7INS808fPjQ2x+SvP1hY5Ck3d1dxeNxLSwsKBqNKhqNKpFI6OLFiyoUCt56ExMTOnnypFZXV1uOEwCAsAprXiWRW/nVy9zKOij2sOVWNEABQB8rl8vKZrNet9z657lczuv+u7u7661ju+FK0srKitdd98GDB17ZjbpM1y9LpVJeN97q5UGOn1AulzU/P6/XX3+94eupVErxeNx3d2jXdZXNZr3Pt7KyUtNF2U+dV6+7vLzsvX779u2WP9/u7q5isZgWFxe1s7PT8vur2Strthwbbzeu/lY3PknyxhpIJpPesg8//FCSdOLECW/ZM888I2n/1b3JyUnNz89zKx4AoGPIqxojt/Kvl7mVLf+g2EOXW5k66+vrpsFiAEAHTU9Pm+np6SOX4ziOkeT9blc/397eNsYYUywWjSSTSCSMMcZ7vXqdSqViEomEkWTu379vjDGmVCrVlF1dVvWy+ufGGJNMJk0ymTzy57Plr6+v+15/c3PTSDLFYrFhWTY+SSafzzd8vZrjOCadThtj9urEcRzjOI6pVCre64fVefV7M5mMMcaYra2thjH4/Xz24TiOKZVKLZVRzdbF9va2yWQyRyrLmMbHQ71iseht1x5vxhjvGGxUpuM4+8qQZDY3N1uOsVPfv7BuDwCGUav5QiPDkFe18/c+uVVreplbHRZ7r3Irv98/GqAAIACd/IPUT+LiZ518Pm8kmVQqdeSyOqnVhNKe9JuVZcxeYmiTm+oGkPr32USm+kS+vb1tJHnJjn3fYfWUyWQartNOQlmpVEw+n/c+q03i2mWTk2Qy6SV/7TrseKhOtv0cb82WVyqVfe/3iwYoABg8nWiAsuUMcl7Vzt/75Fat62VudVDsvcqt/H7/uAUPACDp0T3q1WPy9KPDBmqU9sYtsPe4H9TV+ObNm5Jqxy547rnnJEkbGxstxWXXr+9u7yfeetFoVOPj41paWlI6nd43m0krlpeX9dprr6lSqUiSZmdnuzoV76lTp2SMUT6fVzKZ1Pz8fFvjXdgBMvv9eAUADKZByaskcqtW9Tq36lTsvcitaIACAAyl0dFR5fN55XI5zc3NNUwMrl27tm+ZPTm3enK365uqQSLt4yjOnz/fdqKRzWY1Pz+vs2fPKhqNanZ2VrlcTjdu3DhSTH6Mj49rdnZWknTx4kVJOnDAz0Qi0fWYAABA+8itgs2tpP2xhy23ogEKAFBjmP7QHx8f1+bmpnK5nDdoZDV70m50Fa/deqoekLQT7Gwm7bBTJ9vEb2xsTNKjBqFue/bZZ2ueN6pvO3jniy++2JOYAADopGHKqyRyq6Bzq/rYw5Zb0QAFAJD06OR97ty5gCM5Gpvs+O3q7DiOMplMw+7a09PTkqSHDx96y2y5k5OTLcWVTqclSWtra14ZduaWo3Bdt+VYrPqrYjZZanfq4VbZeshkMpKkN998U1Jtff/sZz+rea1e9Sx6AACExaDkVRK5VSvCkFtVxx623IoGKADoY/VT1lY/tyfi6mSh/mqTnS7XdV2tra3JcZyaE6S9gmKTqOrpXS9duiSp9sqKPeEHOV2w7VVTnyTZz97oitvU1FTDk+3Zs2flOI6uXr3qve/WrVtKJBI6c+bMvvIOqvPvfve7kvbGJRgZGVEkEtHY2JiXJNgphAuFQtPPls1ma6YX3t3d1d27d71YLD9lSdI777zjlSs92r92eStlSbWfu77+Y7GYlpeXvaturusqlUopmUxqampK0t74UOl0WtevX5frunJdV9evX1c6ndapU6dqyrPlvPzyy4fGBQCAH+RVjZFbhTO38hN72HIrGqAAoI/Zbr32/9XPR0ZGav6tX1/aG/QxFotpZGREp06d0traWs3rv/d7vyfHcXT69GnlcjlNTEx4V7Xee+89SdLS0pIk6Y/+6I+8MX2C9Morr0h6dHVHkpeQSHt1YAeprLa0tNTwqtXq6qocx6l5309+8hNvHb91Pjo6qmKx6CVjiURCxWLRO/lXKhUlEokDE8ynnnpK3/72txWJRLS4uKi///u/b3hFzU9ZknTmzBltbW3pzp07ikQiun79ura2tmoSF79lRSKRms9tE0HrwoULmp+f15e//GVFIhGtrq7qO9/5jnf8VK937tw5jYyMaHZ2VpOTk7pw4cK+7dn9a/c3AABHRV7VGLlVOHMrv7GHKbeKmLoRujY2NjQzM3PkgbsAAM3NzMxIktbX1wPZvj159cNvfSQS0fr6utdl2w97xfDy5cstbct1Xa+rdFBisZg2NzcHuqxOWFxc1MjISMv7WOr99y/o7zsADIN28oVOblvqj7yq3b/3ya3CXVYnHCW38vv9owcUAGDgzM3N6c6dOzVd2/0IOkHa2dnRwsLCQJfVCYVCQYVCQXNzc0GHAgDAUCC3Cm9ZndCr3IoGKAAYMvXjGwwi27376tWrvu6vD4Pbt2/rS1/6kiYmJga2rE548OCBrl27ptXV1cCTWgAAhiGvksitwlpWJ/QytzpyA1S5XFY2m1UsFutEPH23/bBoVA+9GKwu6AHxBs2wHc8ct8GoH99gUI2OjmptbU1/+qd/GnQovpw5c8Yb5HNQy+qEXC6n9957T6Ojo0GH0jVBnwuC3n5YcI4aDMN2PHPc9t6w5FUSuVUYy+qEXuZWR26AevfddxWPx5XL5Y4cjOu6DQcv69X2+1kv6qGd/dMrhUJBkUjEe9hZJPyqfm/9Y3l5WSsrKy3H1I3j+fbt215czU7yjT5DWA37cRsUY0zNY5BFo9G27mNHeF2+fHmgG58kcquwGPZzVC6XUywWUyQSUSwW82Z08ovcKhjDftwGYZjyKoncahD1NLcyddbX102DxQeS1PJ7Gtnc3GyrnE5tv991ux7a3T+9kE6nvc8vyWxubrZcRqlUaliHW1tbRpLJZDItldet47lSqZhMJmMkmWQy2XAd+1lKpVLL2++1YT1up6enzfT0dNBh9AVJZn19PegwMEB6/f1rZ3vkVuEwrOeoVCplJJl8Pm+MMSafzxtJJpVKtVQOuVUwhvW4JV/wp52/94HD+P3+hWYMKNd127oSgt4I+/45fvx4zZWHRtNPHqZZq6+dMnNjY8N3Wd2sr2g0qqmpKUnSlStXGl6RtJ9l0HsJHCbsxy0AdBO/geEW5v0zPz8vSRofH6/5986dOy2VQ241eMJ83AIIv442QJXLZS0vL3u3QO3u7ta8bn+wqru52oHaUqmU11W0vmur67rKZrPe8oN+9HK5nLf9VgeBq79n2pYVi8Uafpb6mOoHoLNdl13X1aVLl7zP22gb1fVly62vw4Pq77DPIjXvCm3XaXX/NLun3k/d+K1nP3Z3dxWLxbS4uNh0VoZO3Jde35U5DMdzKpVSPB733S2e4zY8xy0A+EFuxTnKb9108hyVSqUkycurbBlLS0veOuRWzbfNceu/bsitgCFT3yXqKLfgbW9vG2P2uqg6jrOvm2oikfCWFYtFI8kkEol95dRzHKemO2wikah5Xr/9+/fv7yvbDxtzdVmN4rTrptPpms/rOI6pVCoNy8rn8yaRSNQst92at7e3vW0ctN1W6q96O9WvV+8P2322WCy2XH6zbbRTNwfVsx/2c9iH4zj7ukcnk8mmXaqrNTsG1aCbeNDHsy07mUzWHE/1r9dvm+M2HMctt+D5J7rUo8P66RY8civOUe3UzUH17JfNL7a3t00mkyG3MuRWzfZDWI5b8gV/uAUP3eD3+9e1MaDsD7v90TFm78f8oB+vRuXY+7GrfyS3t7eN4zgHvq/ZCaqdz1K/zN63Xh9T/YnUvs/+yLYab/2yVuvvoDqw+2dra6vt8hsta7VuDqsDvyqVisnn817CUH3ctcLGUP9IJpP79mPQx7N9XqlUvBP4/fv3971ucdw2jzGI45YGKP9IKNFp/dQAVY3cinOU1YvcyphHDRGN8iC/yK04bq1uH7fkC/7QAIVu8Pv96+og5M2WF4tFb3DDw37M7I9/q9vvZpJkT8bVKpWKkXToya6VeI9af83eb69ANBtIspX908m6OWqSZKXT6ZpttaJRDKVSySSTyYY9q4wJ7niufm4HxqyOsX59jtvmMQZx3E5PT3vv5cGDR+8f/dgAddBycit/8R61/pq9f9DOUcbsDUSeyWRMpVLx8qB2GqEaxUBu1XpcHLeHC/q8woPHsD/8NEBF/v+X1bOxsaGZmRnVLT6QvQe7/j2Nlq+srCiXyymVSun06dM1rzdav1nZh23Hz/vaLcvv522lXvwua6X+mm1/cXFRhUJBm5ub+z7/UffPUeqm3X1Wz3VdjYyMtFVOsxjK5bLGxsaUTCZrxj8I8niORCI1zwuFgp5//nk5jqO1tbV9dcBxG67jdmZmRru7u3r77bdbet8wOn/+vN5++21985vfDDoUDIj3339fp06d0vr6ek+2NzMzI0ktbY/c6mjrtbKMc9R+2WxW8XhclUpF0WhUDx480OnTp5VOp3XhwoWWyiK34rhttW6O8ltDvnC4Dz74QO+//75u3LgRdCgYIOfPn9f6+rqmp6cPXrG+RarTPaCqu2/aLrL2/uL69zUqx17VqL8H+7DtN4upnc9Sv6zRGAx2PT/3qfuNt35Zq/XXqMx0Ol1TRrV29k8n66bdfdZIu+MdHBTDUfeHMZ09nhvFae/ht7ciNto2x204jltuwfNPoks9Oqtfb8Gzy8mtOEfZ9bp1jqp/n+250qn93+y1oI9ncqv2ym+0LKjjlnzhcNyCh27w+/3r6Cx41QqFgiTptdde85bF43FJ0qlTp3yX4ziOJOnatWtyXVfS3kwcly5d6lSoLbOteg8fPvSW2dgmJye7tt126q/azs6OLl68qK2trYZlHLV8Kbi6qea6bse3ZWfiSCQS3rIwHs+O4yiTyejKlSv7XuO4bS4Mxy0AHIbcqvM4RzVmjxErGo02XH4U5Fbt47gF0LfqW6TaaRG1Ldx28Lpm9xPb9YrFojfgnapaxqtbyu17q2d9sY9EIuENCGjvz64up/oqTaP7ypupLsve496oLDswYfU94ZlMpqY1v7qsw7bR6DM0WnZQ/dWvX//czihRv0/seu3sn2Z130rdHFTPfmQymZpBE4vFotnc3Ny3np+ZWhrFZcze4Iz2qlf1QJRBHs92vWZ11egqHcdteI5bY+gB1QpxRRMd1g89oMitOEcFeY6yA0jbwaLt4NH1A1WTW3Hchum4JV/whx5Q6Aa/37+ONEAZs3eisj9GiUSi5gRl5fN5I+11YS2VSt4MC7Z7Z/3rll3XvlY/G0X1o9kyP1opq1QqeV1X7Qm6+sRa/Z5GA+0dto1Gyw6qv/r16x/1J+ZWy2/0eifq5qj7zHaLtnE16359WJJ0WN2l0+l9XZSDOp6b7b96jQZi57gNx3FrDA1QrZBIKNFZ/dAAZQy5Feeo4M5Rxuwdf3Yg6UbHH7nVo21z3IbjuJXIF/ygAQrd4Pf715FByAGEj+u6Xpd5hE87gxIPq0gk4m9QQ8CnXn//+L4Dg4HcKtzIF/zh7310g9/vX9fGgAIQLBIkAACAziG3AoCjoQEKADCwyuWylpeXgw4DHbS8vOwNZAsAAHqL3Grw9DK3GooGqEgk4uuB8GCfAd3lum5Xv0PdLt+Pcrmsd999Vy+88IL3m7G4uNhw3X77fSkUCjWxHmW2p1wup1gspkgkolgspmw221Y5rutqZ2dHKysrisViDdcpl8taXFz04m62LRtTLBZTLperee2NN97Q7OysyuVyW3GiMzhP9x/2GdA9w5BXSeRWfvUyt5L8xR6W3Orxrm8hBLi/tf+wz4Duunv3bl+XfxjXdTU3N6eFhQVNTEyoUqno1q1b3tTSS0tLNesbY1QulzU2NqZSqaTR0dEgwvbto48+qnl+7ty5tspZXl7W/Py88vm8Njc3VSgU9Pzzz+vjjz/W5cuXWyorlUpJUsOpyqW9pPXhw4daWlrS0tKSstms4vH4vm1ls1ltbGxobW1NkvSjH/1In3zyiS5cuCBJGh8f18LCgubm5rS2tsYtMQHhPN1/2GdA9wx6XiWRW/nVy9zKOiz2MOVWQ9EDCgDwiOu6WllZ6dvy/VhdXdX4+LgmJiYk7Y3bMTU1JWnvJN7oSpRNjMKeIEnS8ePHZfZmspUxRo7jtFXO/Py8pL3Eo/rfO3futFyWbVhq5uHDh97+kOTtDxuDJO3u7ioej2thYUHRaFTRaFSJREIXL15UoVDw1puYmNDJkye1urracpwAAHTSMORVErmVX73MrayDYg9bbkUDFAD0Edd1lc1mvS62KysrNd1lG3Vzrl+WSqW8rrd2eblc9rrmStLKyorXhffBgwdHLl+SFhcXm3bT7qRyuaz5+Xm9/vrrDV9PpVKKx+O+u0MfVuflclnZbNaru1wu53W53t3d3Rfb8vKy9/rt27db/ny7u7uKxWJaXFzUzs5Oy++vZq+s2XJsvH6SnVZVNz5J8sYaSCaT3rIPP/xQknTixAlv2TPPPCNp/9W9yclJzc/PcyseAKBt5FX+kFv518vcypZ/UOxhy61ogAKAPjI7O6uf//znMsaoVCopl8tpbm7O+2O+VCrte0+xWKx5Xn0CtFdKxsbGvHvCd3Z2dOHCBVUqFUnS6dOnvWSp3fJ76d69e5Kkr371qw1fv3z5spLJpOLxeM2Vn2YOq/O5uTnF43Gv7hzHUbFYVC6X049//GOvnHK5rLm5OZ08eVLGGL3zzjv69re/7SuGanb9K1eu6NVXX1UsFms7UbB18eqrr2pnZ0cffvihSqWSd7WuW3Z3d70EbXZ21lturw6eOnXKW2avmtaPV2D3r93fAAC0irzKH3Ir/3qdWx0We+hyK1NnfX3dNFgMAOig6elpMz093dJ7tra2jCRTKpW8Zdvb20aSyWQy3jJJ+37H65f5WccYY/L5vJFkUqnUkctvlySzvr7ue/1kMtl023Z5pVIxjuMYSeb+/fv7Xrc6WeeZTKbhOslk0vdnsyqVisnn895nTafTLZdRLZFIeLFUKpUjlXXYvi8Wi946fo6tZssrlcq+9/vVzvfvKHq9PQAYRq3mC8OaV7Xz9z65Vet6mVsdFHuvciu/3z96QAFAn7h586ak2vvon3vuOUnSxsZGV7Zpr9ZUj9MTdocN1CjtjVtg73E/qKtxJ+vcrl/ftd5PvPWi0ajGx8e1tLSkdDq97wpWK5aXl/Xaa695V2ZnZ2e7OhXvqVOnZIxRPp9XMpnU/Px8W2Nb2AEy++nYBACEB3mVf+RWrel1btWp2HuRW9EABQB94tq1a/uW2RPFUU6Sw2p0dFT5fH5ft+9qnaxzu76pGiTSPo7i/Pnzbe//bDar+fl5nT17VtFoVLOzs8rlcrpx48aRYvJjfHzcu/3u4sWLknTggJ+JRKLrMQEAhgd5VeeRWwWbW0n7Yw9bbkUDFAD0CXsCaXRFqdsnkEH94398fFybm5vK5XLemETVulHn1YOPdoKdzaQddupkm/iNjY1JetQg1G3PPvtszfNG9W0H73zxxRd7EhMAYDiQV3UHuVWwuVV97GHLrWiAAoA+MT09LWlvOnvLXlmanJzsyjbtCf3cuXNdKb8bbLLjt6uz4zjKZDINu2t3ss7T6bQkaW1tzSvDztxyFK7rtr3/66+K2WSp3amHW2XrIZPJSJLefPNNSbX1/bOf/azmtXrVs+gBAOAXeZV/5Fb+hSG3qo49bLkVDVAA0CfOnj0rx3F09epV7yrGrVu3lEgkdObMGW89e9XDJjnVU7JeunRJUu3VkPqTtJ1C13Vdra2tyXGcmpNmu+X3arpg26umPkmyddboitvU1FTDk62fOq8uz26zetv29e9+97uS9sYlGBkZUSQS0djYmJck2CmED5q5JZvN1kwvvLu7q7t379bsf79lSdI777zjlSs92pd2eStlSbWfu77+Y7GYlpeXvaturusqlUopmUxqampK0t74UOl0WtevX5frunJdV9evX1c6na6ZvcV+dkl6+eWXD40LAIB65FX+kVuFM7fyE3vocqv6UcmZBQ8Auq/dWbFKpZJJp9PezBWZTGbfzBrFYtGbhWRzc9MYY4zjOCaTyXgzjthZWJLJpLfMlpnP5733p9PpjpWfTCbbmpVELc5qUyqVjCSzvb1dU0b9oxHHcRqWd1CdNyq32baKxaI3Q0kikTDFYtF7LZlMmkQi0TAGa3Nz0yszmUyafD7fcD0/ZVlbW1veTC2JRMJsbW21VVajOq7+7NWx6//PsFK9jxp9Tsdx9sVj2RlzqmfR8YtZ8ABg8LSaLxgznHlVO3/vk1uFP7c6KPbqdbuVW/n9/kX+/8qejY0NzczMHHngLgBAczMzM5Kk9fX1gCN5xM4cErbf/0gkovX1da/Lth/26uDly5db2pbrul5X6aDEYjFtbm4OdFmdsLi4qJGRkZb3sdT7718Yv+8AMGjayRe6Kax5Vbt/75NbhbusTjhKbuX3+8cteACAgTM3N6c7d+7UdGP3I+gEaWdnRwsLCwNdVicUCgUVCgXNzc0FHQoAAEOB3Cq8ZXVCr3IrGqAAADX32je6j7/fRKNRra6u6urVq77urw+D27dv60tf+pImJiYGtqxOePDgga5du6bV1dXAk1oAABoZtLxKIrcKa1md0Mvc6vGulg4A6At2ilj7/7B1F2/H6Oio1tbWtLq6qvHx8aDDOVT9YJeDWFYn5HI5vffeexodHQ06FAAAGhrEvEoitwpjWZ3Qy9yKBigAwMAkRvWi0Whb97EjvNifAICwG9S8SiK3GkS93J/cggcAAAAAAICuogEKAAAAAAAAXUUDFAAAAAAAALqKBigAAAAAAAB0VdNByG/evNnLOABgqOzu7krit9ave/fu6dixY0GHgQFx8+ZNTU5O9nybb731Vk+3CQDDhnzhcPfu3ZNEDopgREzdEP0fffSRXnnllaDiAQAA6Lrf//3f15UrV3qyrWQyqT/4gz/oybYAAACCcO/ePb388ssHrrOvAQoAei0SiWh9fV3T09NBhwIAANDXNjY2NDMzI/7MAxA2jAEFAAAAAACArqIBCgAAAAAAAF1FAxQAAAAAAAC6igYoAAAAAAAAdBUNUAAAAAAAAOgqGqAAAAAAAADQVTRAAQAAAAAAoKtogAIAAAAAAEBX0QAFAAAAAACArqIBCgAAAAAAAF1FAxQAAAAAAAC6igYoAAAAAAAAdBUNUAAAAAAAAOgqGqAAAAAAAADQVTRAAQAAAAAAoKtogAIAAAAAAEBX0QAFAAAAAACArqIBCgAAAAAAAF1FAxQAAAAAAAC6igYoAAAAAAAAdBUNUAAAAAAAAOgqGqAAAAAAAADQVTRAAQAAAAAAoKtogAIAAAAAAEBX0QAFAAAAAACArqIBCgAAAAAAAF1FAxQAAAAAAAC6igYoAAAAAAAAdBUNUAAAAAAAAOgqGqAAAAAAAADQVTRAAQAAAAAAoKtogAIAAAAAAEBX0QAFAAAAAACArno86AAADJd8Pq//+l//677luVxO/+t//S/v+Ve/+lX9xm/8Ri9DAwAA6Ds3btzQT3/6U+95Pp+XJP3H//gfa9b7zne+o3/1r/5VT2MDgGoRY4wJOggAw+N3fud39P777+vJJ59sus4//dM/SZL4eQIAADhYJBKRpENzq//wH/7DvkYpAOglbsED0FO//uu/LmkvEWr2eOKJJ/TDH/4w4EgBAADC74c//KGeeOKJA3MrSTp37lzAkQIYdvSAAtBTv/jFL3Ty5El98sknB673wQcf6N/8m3/To6gAAAD605//+Z/rm9/85oHrHD9+XB9//LG+8AX6HwAIDr9AAHrqC1/4gmZmZvTEE080XefEiRP6xje+0cOoAAAA+tM3vvENnThxounrTzzxhGZmZmh8AhA4foUA9Fw8Htenn37a8LVjx47pt37rt7zxDAAAANBcJBLRb/3Wb+nYsWMNX//0008Vj8d7HBUA7McteAAC8ZWvfKVmxpZq//2//3d97Wtf63FEAAAA/emv/uqv9K//9b9u+Nq//Jf/Ug8fPuxxRACwHz2gAATit3/7txteqfuVX/kVGp8AAABa8LWvfU2/8iu/sm/5sWPH9Nu//du9DwgAGqABCkAg4vG4Pvvss5plx44d0w9+8IOAIgIAAOhfP/jBD/Zd3Pvss8+4/Q5AaHALHoDAjI+P66/+6q9kf4YikYj+5//8n/rKV74ScGQAAAD95eHDh/rqV79ak1d97WtfU6FQCDgyANhDDygAgfnBD36gxx57TNJekvTiiy/S+AQAANCGr3zlK3rxxRe9iVwee+wxepYDCBUaoAAEZmpqSp9//rmkvSRpdnY24IgAAAD61+zsrHdx7/PPP9fU1FTAEQHAIzRAAQjMiRMn9K1vfUuS9Itf/ELf+973Ao4IAACgf33ve9/TL37xC0nSt771LZ04cSLgiADgERqgAARqZmZGkvTSSy/p+PHjAUcDAADQv44fP66XXnpJ0qMcCwDCgk+Q6qUAACAASURBVEHIeyiZTOoP/uAPgg4DANAnnnjiCf3TP/1T0GEAofXkk0/q008/DToMAECf+P3f/31duXIl6DCG1uNBBzBMfvrTn+rYsWNaX18POhTAlw8++EDvv/++bty40dXtuK6rp59+2hs0sx+9//77kqS333474EgwKDY2NvQnf/InQYcBhNqnn36qt956S9PT00GHAvjSi3zBGKN/+Id/UDQa7do2euH8+fN6++239c1vfjPoUDAgZmZm9NOf/jToMIYaDVA9Njk5qcnJyaDDAHz57LPPJIlj1gfbUEBdoVM+++wzGqAAH8it0E/IF1rzyiuvUFfoGPKq4DEGFAAAAAAAALqKBigAAAAAAAB0FQ1QAAAAAAAA6CoaoAAAAAAAANBVNEABAAAAAACgq2iAAtATi4uLWlxcDDqM0CqXy1peXg46DHTQ8vKyXNcNOgwAwIAit2qOvGrwkFcNBhqgAAwF13UViUSCDqOhcrmsd999Vy+88IIikYgikUjThNK+Xv0Is0KhUBPrpUuX2i4rl8spFospEokoFospm822VY7rutrZ2dHKyopisVjDdcrlshYXF724m23LxhSLxZTL5Wpee+ONNzQ7O6tyudxWnAAAhFlYcyvyKn96mVdJ/mInrxp8jwcdAIDhsLS0FOj27969G+j2m3FdV3Nzc1pYWNDExIQqlYpu3bqleDwuaX+9GWNULpc1NjamUqmk0dHRIML27aOPPqp5fu7cubbKWV5e1vz8vPL5vDY3N1UoFPT888/r448/1uXLl1sqK5VKSZKuXLnS8PVyuayHDx9qaWlJS0tLymazisfj+7aVzWa1sbGhtbU1SdKPfvQjffLJJ7pw4YIkaXx8XAsLC5qbm9Pa2pqi0Wg7Hx0AgIbIrfYjr/Knl3mVdVjs5FXDgR5QAAae67paWVkJOoyGVldXNT4+romJCUlSNBrV1NSUpL0TeaOrUTY5CnuSJEnHjx+XMcZ7OI7TVjnz8/OS9pKP6n/v3LnTclm2YamZhw8fevtDkrc/bAyStLu7q3g8roWFBUWjUUWjUSUSCV28eFGFQsFbb2JiQidPntTq6mrLcQIAEFZhza3Iq/zpZV5lHRQ7edXwoAEKQNeVy2Vls1mvW27981wu53X/3d3d9dax3XAlaWVlxeuu++DBA6/sRl2m65elUimvG2/18qDHTiiXy5qfn9frr7/e8PVUKqV4PO67S7Truspms95nXFlZqemm7Kfeq9ddXl72Xr99+3bLn293d1exWEyLi4va2dlp+f3V7NU1W46NtxtXf6sbnyR54w0kk0lv2YcffihJOnHihLfsmWeekbT/Ct/k5KTm5+fpMg4A6Bhyq/3Iq/zrZV5lyz8odvKqIWLQM9PT02Z6ejroMADf1tfXTSd+JhzHMZK8sqqfb29vG2OMKRaLRpJJJBLGGOO9Xr1OpVIxiUTCSDL37983xhhTKpVqyq4uq3pZ/XNjjEkmkyaZTB758xnT3vd7c3PTSDLFYnHfazbWZDJpJJl8Pt/w9WqO45h0Om2M2asXx3GM4zimUql4rx9W79XvzWQyxhhjtra2Gsbg9/PZh+M4plQqtVRGNVsX29vbJpPJHKksYxofE/WKxaK3XXvMGWO847BRmY7j7CtDktnc3Gwpvk59/4BBJsmsr68HHQbgW6f+HhiG3KrV7zd5VWt6mVcdFnsv8ipj+Hs8DMhse4gDHv2mk38A+0la/KyTz+eNJJNKpY5cVie18/22J/5G7PJKpeIlONUNIPXvs8lM9cl8e3vbSPISHvu+w+oqk8k0XKedhLJSqZh8Pu99VpvItcsmKMlk0ksA23XYMVGdbPs55potr1Qq+97vBw1QwOFogEK/6eTfA4OeW7X6/Saval0v86qDYu9FXmUMf4+HAbfgAegr9h716vF4+tVhgzVKe2MX2PvcD+pufPPmTUm14xc899xzkqSNjY2W4rLr13e39xNvvWg0qvHxcS0tLSmdTu+b0aQVy8vLeu2111SpVCRJs7OzXZ2O99SpUzLGKJ/PK5lMan5+vq3xLuwgmYNwzAIABs+g5FbkVa3pdV7VqdjJq/obDVAAEHKjo6PK5/PK5XKam5trmBxcu3Zt3zJ7gm71BG/XN1UDRdrHUZw/f77tZCObzWp+fl5nz55VNBrV7Oyscrmcbty4caSY/BgfH9fs7Kwk6eLFi5J04KCfiUSi6zEBAID2kFcFm1dJ+2MnrxoeNEAB6EvDdjIaHx/X5uamcrmcN3BkNXvibnQlr926qh6QtBPsjCbtsNMn2+RvbGxM0qMGoW579tlna543qm87gOeLL77Yk5gAAOikYcqtyKuCzavqYyevGh40QAHoK/bkfe7cuYAjOTqb8Pjt7uw4jjKZTMMu29PT05Kkhw8festsuZOTky3FlU6nJUlra2teGXb2lqNwXbflWKz6K2M2YWp3+uFW2XrIZDKSpDfffFNSbX3/7Gc/q3mtXvUsegAAhMWg5FbkVf6FIa+qjp28anjQAAWg6+qnrK1+bk/E1clC/dUmO12u67paW1uT4zg1J0h7BcUmUNXTu166dElS7ZUVe8IPcqpg6VGvmvpEyX7+RlfdpqamGp5wz549K8dxdPXqVe99t27dUiKR0JkzZ/aVd1C9f/e735W0NzbByMiIIpGIxsbGvETBTiNcKBSafrZsNlszxfDu7q7u3r3rxWL5KUuS3nnnHa9c6dE+tstbKUuq/dz19R+LxbS8vOxdeXNdV6lUSslkUlNTU5L2xodKp9O6fv26XNeV67q6fv260um0Tp06VVOeLefll18+NC4AAPwgt9qPvCqceZWf2MmrhgcNUAC6znbrtf+vfj4yMlLzb/360t6gj7FYTCMjIzp16pTW1tZqXv+93/s9OY6j06dPK5fLaWJiwruq9d5770mSlpaWJEl/9Ed/5I3nE7RXXnlF0qMrPJK8pETaqwc7UGW1paWlhleuVldX5ThOzft+8pOfeOv4rffR0VEVi0UvIUskEioWi14CUKlUlEgkDkwwn3rqKX37299WJBLR4uKi/v7v/77hVTU/ZUnSmTNntLW1pTt37igSiej69eva2tqqSV78lhWJRGo+t00GrQsXLmh+fl5f/vKXFYlEtLq6qu985zveMVS93rlz5zQyMqLZ2VlNTk7qwoUL+7Zn96/d3wAAHBW51X7kVeHMq/zGTl41HCLmqKOfwbeZmRlJ0vr6esCRAP5sbGxoZmbmyIMktsuevPrhZ6rd77e9Ynj58uWW3ue6rtddOiixWEybm5sDXVYnLC4uamRkpOV9HPT3D+gHkUhE6+vr3u0yQNgF/fdAP+VW7Xy/yavCXVYntJtXScF//0APKAAI1NzcnO7cuVPTtd2PoJOknZ0dLSwsDHRZnVAoFFQoFDQ3Nxd0KAAADDzyqvCW1QnkVf2PBqg+VC6Xlc1mFYvFgg4F6Jr6sQ0Gle3iffXqVV/32IfB7du39aUvfUkTExMDW1YnPHjwQNeuXdPq6mrgiS2A5sirMCyGIbcirwpnWZ1AXjUYaIDqQ++++67i8bhyuVzQoRyJ67oN78P2q1AoKBKJeA87IKJf1e+tfywvLyuXy/meRSPsjlrXQagf22CQjY6Oam1tTX/6p38adCi+nDlzxhvoc1DL6oRcLqf33ntPo6OjQYcC4ADkVXtyuZxisZgikYhisZg3OLFf5FXhNyy5FXlV+MrqBPKqwUADVB/64z/+46BD6Ii7d+8e6f0fffRRzfNWp441xqhUKnnPK5WKjDEyxuiNN97QysqKZmdnB+IK0VHrOgh2X9jHoItGo23dy47wunz5MkkS0AfIq/bGzYnFYlpaWpIxRktLS4rH4y1NE09eFX7DlFuRVw0e8qrBQAMUAuG6rlZWVo5UxvHjx2tOoo1mUzhM9Y9YdVfO8fFxra6uStq7l7yfr9h1oq4BAEB4HfVcPz8/L2kv/6n+986dOy2VQ14FADgIDVB9wHVdZbNZr0v0gwcPal4vl8tet2nXdXXp0qWa6TKr3x+JRLSysrLvHnD7fklaWVnxbmmr35af8qq7XDdblkqlvK7u9ev6sbu7q1gspsXFxaaDDC4uLh46behBRkdH9c477yiXy3lXuoaxrgEAGCTkVfulUilJ8nKq3d1dSXvT01vkVeRVAHBUNED1gdnZWd25c0eVSkWbm5v6b//tv9W8Pjc3p1gsplwup//xP/6HEomE/vZv/7bm/T//+c+9rtG5XK7m6tPY2Jj3/p2dHV24cEGVSkWSdPr06X0n8MPKq+5+bRWLxZrn1QlNO92A7aCCV65c0auvvqpYLNaVLt0vvfSSJOm//Jf/Imk46xoAgEFCXrXf5cuXlUwm9eqrr2pnZ0cffvihSqWS1xOqU8irAGDIGfTM9PS0mZ6ebuk9m5ubRpK5f/++t6xSqRhJpnr32eeVSqXm/VtbW0aSKZVK3rLt7W0jyWQymX3vr5bP540kk0qlOlJes5jbValUTD6fN8lk0kgy6XS6rXIOi2OY63p9ff1I+2iYtPP9Bg7C9w84nCSzvr7ue33yqoMlEgkjySSTyX2f3S/yqoORL/jX6vcbOAzfv+A93nqTFXrJXiGqnoHgoGkn61+7efOmpNp78p977jlJ0sbGhqamppqWZa96zc/Pe4P4HaW8TotGoxofH9f4+LhOnTqlXC6nCxcu9HT71Qa5rm0saM7erkBdoVPu3bsXdAjAwCGvam55eVmvvfaafvKTnyiVSml2dlZra2s9m+58mOp6d3eXfMGne/fu6dixY0GHgQGxu7urU6dOBR3GUIsYQ7/RXpmZmZEkra+v+36Pva+8fjfVL/e73lHff5T1/JbVDtd1NTIy0lZZB8Vhy00mk16X62Gq642NDe+4BRAMTtNAc5FIROvr65qenva9vkReVS+bzSoej6tSqSgajerBgwc6ffq00ul0yxf3yKsONjMzo42NjZbfB6AzpqenW/p7HJ3FGFADzs4M12h8pEQi4auM6vU6UV43RKPRrmz/L//yLyVJr7/++qHrDnJdm7ppe3nsf0xPT2t6ejrwOHgMzoPkCAifQT3Xx+NxSY96IY2NjUmSLl682NHtkFftIV/w95D2LtwHHQePwXn4vViB7qEBKuTS6bSkR4Nut8p+yR4+fOgtswMtTk5OHvheO3DjuXPnOlJeN7mu2/Htl8tl/eEf/qEcx9GZM2cOXX9Y6hoAgH5FXtWYbZyxbENU/fKjIK8CANAAFXJvvvmmpL2pb+0YM7dv3/Zev3Tp0oGzv509e1aO4+jq1aveerdu3VIikWh48s9ms5L2Tshra2tyHKcm+fBbnr2SZBMAO62vjVmqvRK1vLzsqz5sjNV1sLu7q7t37+77PH6mC7aJR/3/C4WC5ubmJEmrq6ve8mGrawAABgl5VWPvvPNOTby2fLtcIq+SyKsA4MgMeqbdUfeLxaI3K0kikTClUsk4jmMymYwplUreTBySjOM4+95fKpVMOp321slkMvtmGrGv5fN54ziON6tcoxlQ/JRXLBa9cjY3N40xpiZmYx7NUJJMJmtmJDmMncHGvjefzzdcL5lMmmQy2bSc6nqrf6RSKbO9vX3ge4ahrpmFyz9m1UCn8f0DDqc2Zskir2psa2urpl62trZqXiev6kxdky/41873GzgI37/gMQh5D7UzCHmvdHJAcBysn+raDkLeD7EGLczfb/Qnvn/A4VodhLxX+ulc3+/6ra7JF/wL6/cb/YvvX/C4BQ8AAAAAAABdRQMUau6/P+hefBwddQ0AwGDjXN871DUA9BcaoOBNtVv//16LRCK+Hv0sLHWN8GEg08GzvLxcMwgvgOEQlnM9eRWGGXnV4CGvGgw0QEHGmJpHWOJo9uhng/RZesF13a4mx90u369yuax3331XL7zwgvcHQbOZhvrtj4dCoVATq51BqB25XE6xWEyRSESxWMyb8ahVrutqZ2dHKysrisViDdcpl8taXFz04m62LRtTLBZTLperee2NN97Q7OwsV+WBIROWcz15FRoZhtyKvMqfXuZVkr/YyasG3+NBBwAAzdy9e7evy/fDdV3Nzc1pYWFBExMTqlQqunXrluLxuCRpaWmpZn1jjMrlssbGxlQqlTQ6OhpE2L599NFHNc/PnTvXVjnLy8uan59XPp/X5uamCoWCnn/+eX388ce6fPlyS2WlUilJ0pUrVxq+Xi6X9fDhQy0tLWlpaUnZbFbxeHzftrLZrDY2NrS2tiZJ+tGPfqRPPvlEFy5ckCSNj49rYWFBc3NzWltbUzQabeejAwDQMYOeW5FX+dPLvMo6LHbyquFADygAoeS6rlZWVvq2fL9WV1c1Pj6uiYkJSVI0GtXU1JSkvRN5o6tRNjkKe5IkScePH6+5Ou04TlvlzM/PS9pLPqr/vXPnTstl2YalZh4+fOjtD0ne/rAxSNLu7q7i8bgWFhYUjUYVjUaVSCR08eJFFQoFb72JiQmdPHlSq6urLccJAEAnDUNuRV7lTy/zKuug2MmrhgcNUAA6znVdZbNZr4vtyspKTXfZRt2c65elUimv661dXi6Xva65krSysuJ14X3w4MGRy5ekxcXFpt20O61cLmt+fl6vv/56w9dTqZTi8bjvLtGH1Xu5XFY2m/XqL5fLed2ud3d398W2vLzsvX779u2WP9/u7q5isZgWFxe1s7PT8vur2atrthwbr5+Ep1XVjU+SvPEGksmkt+zDDz+UJJ04ccJb9swzz0jaf4VvcnJS8/PzdBkHALSN3Opw5FX+9TKvsuUfFDt51fCgAQpAx83OzurnP/+5jDEqlUrK5XKam5vz/pAvlUr73lMsFmueV58A7ZWSsbEx757wnZ0dXbhwQZVKRZJ0+vRpL1Fqt/xeu3fvniTpq1/9asPXL1++rGQyqXg8XnP1p5nD6n1ubk7xeNyrP8dxVCwWlcvl9OMf/9grp1wua25uTidPnpQxRu+8846+/e1v+4qhml3/ypUrevXVVxWLxdpOFmxdvPrqq9rZ2dGHH36oUqnkXbHrlt3dXS9Jm/1/7N1/bBvnnefxD2tn226QUkgXkpO0Tm+vSBDgAKVpz1H2ly+OsYHdDpMFateSol1gIRvUAgmysP7o6igEgbxu/5CwQXaBGJSAPUOQSNgL7C6J1v9EurOxsOTctiCBFgcbB1/JbrMhgaLkFejdbpB97g/dM+aPoTSkSA5JvV+AYHM4fOY7D2c0Xz3zzPNMTbnL7R3Co0ePusvsndPaMQvs92u/bwAAmkVutTfyKv+6nVftFTt51QFi0DUTExNmYmIi6DAA39bW1kyzvyY2NjaMJFMoFNxlW1tbRpJJJBLuMkl1Zdcu87OOMcZkMhkjySwuLu67/Fa1cn7HYrGG27fLS6WScRzHSDJ3796te99qZ70nEgnPdWKxWFP7Z+PPZDLuvsbj8abLqBSNRt1YSqXSvsra6/vP5XLuOn6Or0bLS6VS3ef9aOX8Aw4aSWZtbS3oMADfWskXDmpu1ez5TV7VvG7mVbvF3o28yhj+Hu8F9IAC0FbXr1+XVP0c/TPPPCNJWl9f78g27d2ayjF6+sFegzVKO2MX2Ofcd+tu3M56t+vXdq33E2+tcDis0dFRLSwsKB6P193FasbS0pKOHz/u3pmdmprq6HS8R48elTFGmUxGsVhMs7OzLY1tYQfJ7LfjEwDQG8it/CGvak6386p2xU5e1d9ogALQVleuXKlbZi8U+7lIHmTDw8PKZDJ1Xb8rtbPe7fqmzdN1nz17tuVjIJlManZ2VqdOnVI4HNbU1JTS6bSuXbu2r5j8GB0ddR+/u3DhgiTtOuhnNBrteEwAgIOD3Kq9yKuCzauk+tjJqw4OGqAAtJW9gHjdUer0BWSQL1Cjo6NKpVJKp9PumESVOlHvlYOPtoOd0aQVdvpkm/yNjIxIetAg1GlPPfVU1Wuv+rYDeD733HNdiQkAcDCQW7UfeVWweVVt7ORVBwcNUADaamJiQtLOVPaWvbN05syZjmzTXtBPnz7dkfI7xSY8frs7O46jRCLh2WW7nfUej8clSaurq24ZdvaW/SiXyy0fA7V3xmzC1Or0w82y9ZBIJCRJL7/8sqTq+v7www+r3qtVOYseAAB+kVv5Q17lXy/kVZWxk1cdHDRAAWirU6dOyXEcXb582b2LcePGDUWjUZ04ccJdz971sAlO5ZSsMzMzkqrvhtRepO0UuuVyWaurq3Icp+qi2Wr53ZoqWHrQq6Y2UbL15nXX7dy5c54XXD/1Xlme3Wbltu37r7zyiqSdsQmGhoYUCoU0MjLiJgp2GuHdZm9JJpNVUwzn83ndunWr6hjwW5Ykvfnmm2650oPv0y5vpiyper9r6z8SiWhpacm981Yul7W4uKhYLKZz585J2hkfKh6P6+rVqyqXyyqXy7p69ari8XjVDC523yXp2LFje8YFAEAtcit/yKt6M6/yEzt51QES0ODnBxKj7qPftDoLV6FQMPF43J25IpFI1M2skcvl3FlIUqmUMcYYx3FMIpFwZxyxM7DEYjF3mS0zk8m4n4/H420rPxaLtTQrSSvnd6FQMJLM1taWu8zuX+WPF8dxPMvbrd69ym20rVwu585SEo1GTS6Xc9+LxWImGo16xmClUim3zFgsZjKZjOd6fsqyNjY23NlaotGo2djYaKksrzqu3PfK2PX/Z1mp/I689tNxnLp4LDtrTuVMOn4wCx6wNzELHvpMq38PHMTcqtnzm7yq9/Oq3WKvXLcTeZUx/D3eC0LG7HP0M/g2OTkpSVpbWws4EsCf9fV1TU5O7nuQxHayM4f0UkxS6+e3vTt48eLFpj5XLpfd7tJBiUQiSqVSA11WO8zPz2toaKjp77gXzz+g14RCIa2trbmPywC9rhf/HujV3KqV85u8qrfLaodW8yqpN8+/g4ZH8AAgQNPT07p582ZVN3Y/gk6Stre3NTc3N9BltUM2m1U2m9X09HTQoQAAMPDIq3q3rHYgr+p/NEAB6BuVz9p7Pcffj8LhsFZWVnT58mVfz9j3gs3NTT366KMaGxsb2LLa4d69e7py5YpWVlYCT2wBAPAyaLkVeVVvltUO5FWD4XDQAQCAX3aKWPv/Xusq3qrh4WGtrq5qZWVFo6OjQYezp9oBLwexrHZIp9N6++23NTw8HHQoAAB4GsTciryq98pqB/KqwUADFIC+MQhJUSPhcLilZ9nRu/g+AQC9blBzK/KqwcP3ORh4BA8AAAAAAAAdRQMUAAAAAAAAOooGKAAAAAAAAHQUDVAAAAAAAADoKAYh77L19XV9/PHHQYcB+JLP5yVJZ8+eDTiS3nfnzh1J1BXa5/r160GHAPSFyclJ/d3f/V3QYQC+kC8059133+X8Rttcv35dExMTQYdxoIXMoE590IPS6bRWV1eDDgPoOe+//77+w3/4Dzpy5EjQoQA95ctf/rIuX74cdBhAz5qbm9P//J//M+gwgJ7y0Ucf6Uc/+pFOnjwZdChAz5mampLjOEGHcWDRAAUgcKFQSGtra9yRAAAA2Kf19XVNTk6KP/MA9BrGgAIAAAAAAEBH0QAFAAAAAACAjqIBCgAAAAAAAB1FAxQAAAAAAAA6igYoAAAAAAAAdBQNUAAAAAAAAOgoGqAAAAAAAADQUTRAAQAAAAAAoKNogAIAAAAAAEBH0QAFAAAAAACAjqIBCgAAAAAAAB1FAxQAAAAAAAA6igYoAAAAAAAAdBQNUAAAAAAAAOgoGqAAAAAAAADQUTRAAQAAAAAAoKNogAIAAAAAAEBH0QAFAAAAAACAjqIBCgAAAAAAAB1FAxQAAAAAAAA6igYoAAAAAAAAdBQNUAAAAAAAAOgoGqAAAAAAAADQUTRAAQAAAAAAoKNogAIAAAAAAEBH0QAFAAAAAACAjqIBCgAAAAAAAB1FAxQAAAAAAAA6igYoAAAAAAAAdBQNUAAAAAAAAOgoGqAAAAAAAADQUTRAAQAAAAAAoKNogAIAAAAAAEBHhYwxJuggABwcKysr+pM/+RM9/fTT7rKf/vSn+vznP69f//VflyT98z//s377t39bf//3fx9UmAAAAH3h5MmTymQyeuyxxyRJv/rVr/Tzn/9cX/ziF9117t69q//yX/6LJiYmggoTAHQ46AAAHCyFQkEff/yxfvSjH1UtL5fLVa/T6XQ3wwIAAOhLm5ubMsbo5z//edXy2tzqJz/5SRejAoB6PIIHoKvGx8cVCoV2Xefw4cP67ne/26WIAAAA+td3v/tdHT68e7+CUCikc+fOdSkiAPDGI3gAuu4//sf/qB/84Adq9OsnFArpf/2v/6Unn3yyy5EBAAD0l1wup3/37/7drnnVV7/6Vf33//7fuxwZAFSjBxSArnvttdd06NAhz/c+9alP6dixYzQ+AQAA+PDkk0/q2LFj+tSnvP+0O3TokF577bUuRwUA9WiAAtB13/rWt/Rv//Zvnu+FQiH90R/9UZcjAgAA6F9/9Ed/1HCIg3/7t3/Tt771rS5HBAD1aIAC0HVHjhzR8ePHG/aCOnPmTJcjAgAA6F+NcqdDhw7p+PHjOnLkSJcjAoB6NEABCMQf/uEf1o1VcOjQIb344ov6jd/4jYCiAgAA6D+/8Ru/oRdffLHu5p4xRn/4h38YUFQAUI0GKACB+IM/+AOSJAAAgDZpdHPvD/7gDwKKCACq0QAFIBDhcFinTp2qmjb4oYce0quvvhpgVAAAAP3p1Vdf1UMPPeS+Pnz4sE6dOqVwOBxgVADwAA1QAAIzNTWlTz75RNJOkvSNb3xDjzzySMBRAQAA9J9HHnlE3/jGN9ybe5988ommpqYCjgoAHqABCkBgvvGNb+izn/2spJ0kaXJyMuCIAAAA+tfk5KR7c++zn/2svvGNbwQcEQA8QAMUgMB85jOf0Te/+U1J0sMPP6zTp08HHBEAAED/On36tB5++GFJ0je/+U195jOfCTgiAHjgJvNvVAAAIABJREFU8N6roN1++tOfant7O+gwgJ7whS98QZL05JNPKpVKBRwN0Bu+8IUv6IUXXgg6DKAnkUcBu3vyySf14x//WF/4whd0/fr1oMMBetKhQ4cUiUSqxqNF54VM7VQJ6Lg//uM/1l//9V8HHQYAoIdxeQa8kUcBANrhb//2b5kAqcto7gvAv/zLv2hiYkJra2tBhwJ4Wl9f1+TkJH8A+2DHreJ8RrvY8w+AN/IoDIpQKKS1tTVNTEwEHUpPIy9FJ4RCIf3qV78KOowDhzGgAAAAAAAA0FE0QAEAAAAAAKCjaIACAAAAAABAR9EABQAAAAAAgI6iAQoAAAAAAAAdRQMUAAAAAAAAOooGKAAdNT8/r/n5+aDD6FnFYlFLS0tBh4E2WlpaUrlcDjoMAMABQa61O3KtwUOu1b9ogAIw0MrlskKhUNBheCoWi3rrrbf0la98RaFQSKFQqGECad+v/Oll2Wy2KtaZmZmWy0qn04pEIgqFQopEIkomky2VUy6Xtb29reXlZUUiEc91isWi5ufn3bgbbcvGFIlElE6nq947efKkpqamVCwWW4oTAIB+Qq4VjH7NtSR/sZNrDabDQQcAYLAtLCwEuv1bt24Fuv1GyuWypqenNTc3p7GxMZVKJd24cUPj4+OS6uvNGKNisaiRkREVCgUNDw8HEbZvH3zwQdXr06dPt1TO0tKSZmdnlclklEqllM1m9eyzz+pnP/uZLl682FRZi4uLkqRLly55vl8sFnX//n0tLCxoYWFByWRS4+PjddtKJpNaX1/X6uqqJOnb3/62PvroI50/f16SNDo6qrm5OU1PT2t1dVXhcLiVXQcAwBdyLW/kWv50M9ey9oqdXGtw0QMKwMAql8taXl4OOgxPKysrGh0d1djYmCQpHA7r3LlzknYu2l53nmwi1OsJkSQdOXJExhj3x3GclsqZnZ2VtJNoVP578+bNpsuyDUuN3L9/3/0+JLnfh41BkvL5vMbHxzU3N6dwOKxwOKxoNKoLFy4om826642NjemJJ57QyspK03ECANAvyLWC04+5lrVb7ORag40GKAAdUywWlUwm3S64ta/T6bTb1Tefz7vr2C63krS8vOx2zb13755btlf36Npli4uLbpfdyuVBj5VQLBY1OzurF1980fP9xcVFjY+P++7+XC6XlUwm3X1cXl6u6pLsp94r111aWnLf39zcbHr/8vm8IpGI5ufntb293fTnK9k7abYcG28n7vZWNj5JcscWiMVi7rLbt29Lkh5//HF32WOPPSap/m7emTNnNDs7S/dwAEDHkGt5I9fyr5u5li1/t9jJtQacQddNTEyYiYmJoMMAGlpbWzPt+PXgOI6R5JZV+Xpra8sYY0wulzOSTDQaNcYY9/3KdUqlkolGo0aSuXv3rjHGmEKhUFV2ZVmVy2pfG2NMLBYzsVhs3/tnTGvncyqVMpJMLpere8/GGovFjCSTyWQ836/kOI6Jx+PGmJ16cRzHOI5jSqWS+/5e9V752UQiYYwxZmNjwzMGv/tnfxzHMYVCoakyKtm62NraMolEYl9lGeN9TNTK5XLudu0xZ4xxj0OvMh3HqStDkkmlUk3F167zDxhU5FEYFJLM2travso4CLlWK9dFcq3mdDPX2iv2buRatrz9nn9oHhluAEic0Ova+QewnyTFzzqZTMZIMouLi/suq51aOZ/tRd6LXV4qldxkprIBpPZzNnGpvHBvbW0ZSW5yYz+3V10lEgnPdVpJIEulkslkMu6+2qStVTYZicVibrLXqr2Oicrk2s8x12h5qVSq+7wfNEABuyOPwqBo1x/Ag55rtXJdJNdqXjdzrd1i70auZcujAar7eAQPQF+wz6NXjsfTr/YamFHaGafAPtO+W9fi69evS6oeq+CZZ56RJK2vrzcVl12/tnu9n3hrhcNhjY6OamFhQfF4vG72kmYsLS3p+PHjKpVKkqSpqamOTr179OhRGWOUyWQUi8U0Ozvb0vgWdkDMQThmAQCDj1yLXKtbuVa7YifX6j80QAFAjxoeHlYmk1E6ndb09LRnInDlypW6ZfZi3OzF3K5vKgaFtD/7cfbs2ZYTi2QyqdnZWZ06dUrhcFhTU1NKp9O6du3avmLyY3R0VFNTU5KkCxcuSNKuA3xGo9GOxwQAANqHXCvYXEuqj51ca7DRAAWgrxy0C8/o6KhSqZTS6bQ7SGQle5H2umvXal1VDkDaDnb2klbYqZJtojcyMiLpQYNQpz311FNVr73q2w7W+dxzz3UlJgAAOolcqxq5VmfVxk6uNdhogALQF+yF+vTp0wFHsn82ufHbtdlxHCUSCc/u2RMTE5Kk+/fvu8tsuWfOnGkqrng8LklaXV11y7AztexHuVxuOhar9i6YTY5anWq4WbYeEomEJOnll1+WVF3fH374YdV7tSpn0QMAoFeRa5FrScHkWpWxk2sNNhqgAHRM7fS0la/tRbcyMai9s2Snxi2Xy1pdXZXjOFUXQ3u3xCZMlVO5zszMSKq+i2Iv7kFPDWx71dQmRXb/ve6wnTt3zvPieurUKTmOo8uXL7ufu3HjhqLRqE6cOFFX3m71/sorr0jaGYdgaGhIoVBIIyMjblJgpwzOZrMN9y2ZTFZNJ5zP53Xr1i03FstPWZL05ptvuuVKD75ju7yZsqTq/a6t/0gkoqWlJfcuW7lc1uLiomKxmM6dOydpZ3yoeDyuq1evqlwuq1wu6+rVq4rH4zp69GhVebacY8eO7RkXAACtINfyRq7Vm7mWn9jJtQYbDVAAOsZ24bX/r3w9NDRU9W/t+tLOAI+RSERDQ0M6evSoVldXq97/sz/7MzmOo6efflrpdFpjY2PuHay3335bkrSwsCBJ+su//Et3PJ+gPf/885Ie3M2R5CYg0k492EEpKy0sLHjepVpZWZHjOFWf++53v+uu47feh4eHlcvl3OQrGo0ql8u5F/tSqaRoNLprQvnwww/rpZdeUigU0vz8vH7xi1943kHzU5YknThxQhsbG7p586ZCoZCuXr2qjY2NqkTFb1mhUKhqv23iZ50/f16zs7N68sknFQqFtLKyoq9//evuMVS53unTpzU0NKSpqSmdOXNG58+fr9ue/X7t9w0AQLuRa3kj1+rNXMtv7ORagytk9jviGZo2OTkpSVpbWws4EsDb+vq6Jicn9z0gYqvshaoffj21ej7bO4QXL15s6nPlctntGh2USCSiVCo10GW1w/z8vIaGhpr+joM+/4BeRx6FQREKhbS2tuY+4tXtbUv9kWu1el0k1+rtstqh1VxLCvb8O8joAQUAAZientbNmzerurL7EXRCtL29rbm5uYEuqx2y2ayy2aymp6eDDgUAgAOJXKt3y2oHcq3+RAMUuq5YLCqZTCoSiQQdCnpQ7VgGg8p25758+bKv5+l7webmph599FGNjY0NbFntcO/ePV25ckUrKyuBJ7EABg95FPaLXKt39Wp+RK6FdqEBCi3L5/OamZlRKBTSzMxM1YByu3nrrbc0Pj6udDrd1PbK5bJCoZDnjx00rxXb29uan593y5qfn1c2m1WxWPR8Nrxb9qrfRnURCoW0tLSkdDrte+aPXlI7lsEgGx4e1urqqt5///2gQ/HlxIkT7qCeg1pWO6TTab399tsaHh4OOhQAPazbeZS0k0ttb29reXl51wasdDqtSCSiSCTS0nZqkWv1FnKt3tWr+RG5FtqFBii0pFwuK5vN6r333lOpVNLx48f10ksv+UpS3nvvvZa2+T/+x/9o+F7trA9+zc/P6+rVq5qampIxRsYYvf7668rn84FekP3UrzFGhULBfV0qldx9OHnypJaXlzU1NdV3d7bsPtifQRcOh1t6bh296+LFiyREAHYVRB4l7UxN/73vfU8XLlxouK1kMqnl5WWtrq5qdXVV3//+97W8vNzyNsm1eg+5FvoduVb/ogEKLbl165Y7Y0E4HHanKO9kd/Cf/OQnyuVyVRfMQqGgWCzW0i8ge/ftvffeq2rRHx4eluM42traamf4TfFbv5X7Xdn9dHR0VCsrK5J2nn/vx7tzAAAMqiDyKGlnhq/amT0r5fN5jY+Pa25uTuFwWOFwWNFoVBcuXGjpESZyLQBAJRqg+ki5XFYymXS7/nrdjfJap/Y578pxA9LptEKhkCKRiPL5vLa3t+u6GFtLS0vustHRUc8Yo9HorjFFIhHdu3evpf0/ceKEO0Wptbm5qW9+85tVy+bn5/ecInR7e1uXLl3adSA9r2ece7F+GxkeHtabb76pdDqtW7du+f4cAACD6KDnUX7cvn1bkvT444+7yx577DFJ0gcffOAuI9faQa4FAM2hAaqPTE1N6cc//rHb++eHP/xh3cV/ampKv/zlL93eQel0uuquzPT0tDtuwPb2thzHUS6XUzqd1ne+8x2NjY1pY2NDkhSLxaq65V68eFGxWEyZTKauIciWf/r0ac+4b968qVKppFQqpR/+8Ict7b9XL6ebN282TCB2873vfU+S9Ju/+Zu7rlfbLbkX63c3X/3qVyVJ3//+95v6HAAAg+ag51F+3Lx5U5Kq4rP5V7NjQZFrAQDqGHTdxMSEmZiYaOoziUTCSDKFQsFdtrW1ZRzHcV9vbGx4riPJJBIJd5kkU/vV1y6LxWJGkimVSu6yUqlkYrGYZ3wbGxvGcZyq9Y0xJpVKGUnm7t27VeV4xdCsTCZTtV/NaGX7vVi/fvallX1dW1vb9/dzULRyPgO74fwDdkcetf88qtHnm13eyjZ204vfgZ99afX7kGTW1taa/txBw3URncD5F4zDbWrHQoetr69Lqu4FNDY2plQq5b6+fv163TrPPPOM+3n7bLsf3/zmN3Xp0iXduHHD/dwPfvCDusfdrHfeeccdL6CSvRtU+dx/u6bK/Ju/+Ru9/vrrbSnLj16s3047e/ZsV7fXj+7cuSOJukL75PP5oEMABg55VH/oxe+g095991393d/9XVe32W/sdZFcC+h/PILXJ/x0e75y5UrdMnsRbbbb9OjoqBzHcRM2Sfqv//W/ej7ulkwm5TiO53P8XjG1gx0LoNXZD+zz/c0MGNmL9bsXu3+xWKzpzwIAMCjIo/yxg3J7aWZspMr1ybUAABY9oPqE4zhKp9PKZrMNxzyy6xSLxbqGmWaTBkmamJjQ+Pi4tre39fjjj+vYsWN162SzWf34xz/edUaVTvAafLwZp0+f1pUrV/STn/zE9xhS/Vi/P/jBDyRJL774Ykufv3btWkufO0gmJyclSWtrawFHgkGxvr7uHlcA2oM8yh+vOrC9T5577rmmyiLX8ueNN97QxMRES589KOx1kbwU7VQ5QQG6hx5QfcLekbpy5Yp7pyWfz2tmZsZdx1687t+/7y6z6545c6bpbZ44cUKSdPXqVd2+fVu/93u/V/V+sVjU+++/X3XBzmazVTHF43F3eTu1Ovi45TiOHMfZ9c5iPp/X0tKS+7oX63c3xWJR77zzjhzHcbcFAMBBRB7lz8svvyypug4+/PDDqvf8ItcCANQJehCqg6iVwTMLhYJxHMcd5FCSiUajdYNSOo5jHMdxB29MJBImGo1WlWM/bwdZrBzMsnLQR2MeDOC4uLi4Zzz2J5VKuevlcjkjyTiOY3K5nDHmwQCTdh+atdfg47FYrOEAk177UFuPNu7KejSmN+u3suzKQTMzmUxdrM1gsEf/GIQc7cb5B+yOPGp/eVSj3MGKx+MmGo2aUqlkSqWSiUajJh6P1+0Xudb+ci1jGATZL66L6ATOv2BwJgeg1T9YC4WCexGNxWJ1F3K7Tjwedy+UiUSi6mJZe4FttMzKZDJ1s68YY0w0GvW8YHutm8vl3PWj0ah7wU8kEi1dsGOx2K6f85sUGbOTVKRSqar9cRzHxONxN9Gr1Ev12+h9m2RtbW35qgMvXOj9owEK7cb5B+yOPKr1PKrRNmvZ2fccxzEbGxt175Nr7T/XsuXzB/DeuC6iEzj/ghEyxhihqxgzBr3OPmvPr4e9cT6j3Tj/gN3xexeDIhQKaW1tjTGg9sB1EZ3A+RcMxoACAAAAAABAR9EABQABKhaLVQOwov8tLS01Ne04AADoHHKtwUOu1b9ogELgQqGQrx8cHOVyuaPfeafL96tYLOqtt97SV77yFfc4n5+f91y3386JbDZbFavfGYW8pNNpRSIRhUIhRSIRJZPJlsopl8va3t7W8vKyIpGI5zrFYlHz8/Nu3I22ZWOKRCJKp9NV7508eVJTU1MqFostxQkAzSCPQivIter123nTr7mW5C92cq3BdDjoAACe50atW7du9XX5fpTLZU1PT2tubk5jY2MqlUq6ceOGxsfHJalqSmhp5zwpFosaGRlRoVDQ8PBwEGH79sEHH1S9Pn36dEvlLC0taXZ2VplMRqlUStlsVs8++6x+9rOf6eLFi02Vtbi4KEm6dOmS5/vFYlH379/XwsKCFhYWlEwmNT4+XretZDKp9fV1ra6uSpK+/e1v66OPPtL58+clSaOjo5qbm9P09LRWV1cVDodb2XUA8IU8Cq0g1yLXsrqZa1l7xU6uNbjoAQWgp5TLZS0vL/dt+X6trKxodHRUY2NjkqRwOKxz585J2rloe915solQrydEknTkyBGZnZlWZYyR4zgtlTM7OytpJ9Go/PfmzZtNl2Ublhq5f/+++31Icr8PG4Mk5fN5jY+Pa25uTuFwWOFwWNFoVBcuXFA2m3XXGxsb0xNPPKGVlZWm4wQAoJPItci1KnUz17J2i51ca7DRAAWgbcrlspLJpNuddnl5uaprrFeX5tpli4uLbjdbu7xYLLrdcCVpeXnZ7a577969fZcvSfPz8w27ZLdbsVjU7OysXnzxRc/3FxcXNT4+7rv78171XiwWlUwm3fpLp9NuF+t8Pl8X29LSkvv+5uZm0/uXz+cViUQ0Pz+v7e3tpj9fyd5Js+XYeP0kN82qbHyS5I4tEIvF3GW3b9+WJD3++OPusscee0xS/d28M2fOaHZ2lu7hAIC2Idfyh1zLv27mWrb83WIn1xpsNEABaJupqSn98pe/lDFGhUJB6XRa09PT7h/yhUKh7jO5XK7qdeXFzt4VGRkZcZ//3t7e1vnz51UqlSRJTz/9tJsYtVp+t925c0eS9OUvf9nz/YsXLyoWi2l8fLzqTk8je9X79PS0xsfH3fpzHEe5XE7pdFrf+c533HKKxaKmp6f1xBNPyBijN998Uy+99JKvGCrZ9S9duqQXXnhBkUik5cTA1sULL7yg7e1t3b59W4VCwb071yn5fN5NyKamptzl9m7g0aNH3WX2Lmnt+AT2+7XfNwAA+0Wu5Q+5ln/dzrX2ip1ca8AZdN3ExISZmJgIOgygobW1NdPsr4eNjQ0jyRQKBXfZ1taWkWQSiYS7TFJd2bXL/KxjjDGZTMZIMouLi/suv1WtnM+xWKzh9u3yUqlkHMcxkszdu3fr3rfaWe+JRMJznVgs1tT+2fgzmYy7r/F4vOkyKkWjUTeWUqm0r7L2+v5zuZy7jp/jq9HyUqlU93k/Wjn/gIOEPAqDQpJZW1vzvf5BzbVauS6SazWvm7nWbrF3I9ey5TVz/qE96AEFoC2uX78uqfqZ+WeeeUaStL6+3pFt2jszlWP09IO9BmaUdsYpsM+079a1uJ31btev7UrvJ95a4XBYo6OjWlhYUDwer7tj1YylpSUdP37cvRM7NTXV0al3jx49KmOMMpmMYrGYZmdnWxrLwg6I2W/HJwCgN5Fr+Ueu1Zxu51rtip1cq//QAAWgLa5cuVK3zF4U9nNBPMiGh4eVyWTqunlXame92/VNxaCQ9mc/zp492/IxkEwmNTs7q1OnTikcDmtqakrpdFrXrl3bV0x+jI6Ouo/fXbhwQZJ2HeAzGo12PCYAwMFFrtV+5FrB5lpSfezkWoONBigAbWEvFl53jzp9sRjki9Ho6KhSqZTS6bQ7JlGlTtR75WCj7WBnL2mFnSrZJnojIyOSHjQIddpTTz1V9dqrvu1gnc8991xXYgIAHEzkWp1BrhVsrlUbO7nWYKMBCkBbTExMSNqZyt6yd5HOnDnTkW3ai/fp06c7Un6n2OTGb9dmx3GUSCQ8u2e3s97j8bgkaXV11S3DztSyH+VyueVjoPYumE2OWp1quFm2HhKJhCTp5ZdfllRd3x9++GHVe7UqZ9EDAKBV5Fr+kWv51wu5VmXs5FqDjQYoAG1x6tQpOY6jy5cvu3csbty4oWg0qhMnTrjr2TscNqGpnH51ZmZGUvWdj9oLsp0ut1wua3V1VY7jVF0gWy2/m1MD2141tUmRrTevO2znzp3zvLj6qffK8uw2K7dt33/llVck7YxDMDQ0pFAopJGRETcpsFMG7zZTSzKZrJpOOJ/P69atW1XHgN+yJOnNN990y5UefJ92eTNlSdX7XVv/kUhES0tL7l22crmsxcVFxWIxnTt3TtLO+FDxeFxXr15VuVxWuVzW1atXFY/Hq2ZrsfsuSceOHdszLgAA9kKu5R+5Vm/mWn5iJ9cacAENfn6gMXsLel2rs3AVCgUTj8fdWSoSiUTdLBq5XM6dcSSVShljjHEcxyQSCXd2ETvjSiwWc5fZMjOZjPv5eDzetvJjsVhLM5C0cj4XCgUjyWxtbbnL7P5V/nhxHMezvN3q3avcRtvK5XLujCTRaNTkcjn3vVgsZqLRqGcMViqVcsuMxWImk8l4ruenLGtjY8OdmSUajZqNjY2WyvKq48p9r4xd/39GlcrvyGs/Hcepi8eyM+RUzprjB7PgAbsjj8KgUAuzcB3EXKuV6yK5Vu/nWrvFXrluJ3ItGyuz4HVfyJh9jniGpk1OTkqS1tbWAo4E8La+vq7Jycl9D4jYTnaWkF6KSWr9fLZ3Ay9evNjU58rlsts1OiiRSESpVGqgy2qH+fl5DQ0NNf0d9+L5B/QS8igMilAopLW1NfcRr6D1aq7V6nWRXKu3y2qHVnMtqffOv4OCR/AAIADT09O6efNmVbd1P4JOiLa3tzU3NzfQZbVDNptVNpvV9PR00KEAAHAgkWv1blntQK7Vn2iAAtDzKp+r93pmvx+Fw2GtrKzo8uXLvp6n7wWbm5t69NFHNTY2NrBltcO9e/d05coVraysBJ7EAgDgB7lWb+jV/IhcC+1yOOgAAGAvdjpY+/9e6xrequHhYa2urmplZUWjo6NBh7On2sEtB7Gsdkin03r77bc1PDwcdCgAAPhCrtUbejU/ItdCu9AABaDnDUoS5CUcDrf03Dp6F98nAKDfkGuhn/B99i8ewQMAAAAAAEBH0QAFAAAAAACAjqIBCgAAAAAAAB1FAxQAAAAAAAA6igYoAAAAAAAAdFTIDPKUBz3qj//4j/XXf/3XQYcBAOhhXJ4Bb+RRAIB2+Nu//Vu9+uqrQYdxoNAAFYCf/vSn2t7eDjoMoGecPXtWb7zxhn7nd34n6FCAnvCFL3xBL7zwQtBhAD2JPArY3T/8wz/o3Xff1bVr14IOBehZhw4dUiQS0eHDh4MO5UChtgPwxS9+UV/84heDDgPoKc8//7zOnDkTdBgAgB5HHgXs7uOPP5Yk8ioAPYcxoAAAAAAAANBRNEABAAAAAACgo2iAAgAAAAAAQEfRAAUAAAAAAICOogEKAAAAAAAAHUUDFAAAAAAAADqKBigAAAAAAAB0FA1QAAAAAAAA6CgaoAAAAAAAANBRNEABAAAAAACgo2iAAgAAAAAAQEfRAAUAAAAAAICOogEKAAAAAAAAHUUDFAAAAAAAADqKBigAAAAAAAB0FA1QAAAAAAAA6CgaoAAAAAAAANBRNEABAAAAAACgo2iAAgAAAAAAQEfRAAUAAAAAAICOogEKAAAAAAAAHUUDFAAAAAAAADqKBigAAAAAAAB0FA1QAAAAAAAA6CgaoAAAAAAAANBRNEABAAAAAACgo2iAAgAAAAAAQEfRAAUAAAAAAICOogEKAAAAAAAAHUUDFAAAAAAAADqKBigAAAAAAAB0FA1QAAAAAAAA6KjDQQcA4GD5P//n/+if//mf65YXi0Xdv3/ffR0Oh/X5z3++m6EBAAD0nZ///Ocql8vu62KxKElVeZUkPfbYY/rsZz/b1dgAoFLIGGOCDgLAwfGnf/qneuedd3yty68nAACA3YVCIV/rxWIxLSwsdDgaAGiMR/AAdNVzzz235zqhUEi/9Vu/1YVoAAAA+ttv/dZv+WqEeuqpp7oQDQA0RgMUgK569dVX9elPf3rP9V5//fUuRAMAANDf/ORMn/70p/Xqq692IRoAaIwGKABd9cgjj8hxHB0+3HgIuk9/+tNyHKeLUQEAAPQnx3F2vbl3+PBhOY6jRx55pItRAUA9GqAAdN3ExIQ++eQTz/ceeughvfrqq3r44Ye7HBUAAED/efjhh/Xqq6/qoYce8nz/k08+0cTERJejAoB6NEAB6LrTp083bGD6+OOP9dprr3U5IgAAgP712muv6eOPP/Z87+GHH9bp06e7HBEA1KMBCkDXffrTn9bZs2c979R97nOf0+///u8HEBUAAEB/+v3f/3197nOfq1v+0EMP6ezZs77G3wSATqMBCkAgJicn6+7UPfTQQ/rWt77VsAs5AAAA6jXKoT7++GNNTk4GFBUAVAsZY0zQQQA4eD755BONjIzo5z//edXy//bf/puOHz8eUFQAAAD96ebNm/pP/+k/VS37/Oc/r0KhoEOHDgUTFABUoAcUgEAcOnRIr732mn7t137NXXbkyBH97u/+boBRAQAA9Kff/d3f1ZEjR9zXv/Zrv6bXXnuNxicAPYMGKACBmZiY0L/+679K2kmSJiYm9KlP8WsJAACgWZ/61Kc0MTHh3tz713/9V2a/A9BTeAQPQKCefPJJ5fN5SdI//uM/6qtf/WrAEQEAAPSnH/zgB/ra174mSTp69KhyuVzAEQHAA3Q1ABCoqakpSdKXvvQlGp8AAAD24atf/aq+9KUvSXqQYwHvTJg9AAAgAElEQVRArzgcdACNfPTRR/rTP/1TffLJJ0GHAqCD/vf//t+SpP/7f/+vzp49G3A0ADptampKjuMEHcaBlE6ntbq6GnQYADrMPuDyj//4j+RWwIA7dOiQ/uIv/qJq/Lde1rM9oDY3N5VMJoMOA0CHfe5zn9PXvvY1Pf/8813Z3p07d3Tnzp2ubKvfXb9+3X08EmiH69evc20PUDKZ1PXr14MOA0CHjY6O6mtf+5o+97nPdWV75Av+5PN5fgej7ZLJpDY3N4MOw7ee7QFlXbt2LegQAAyQyclJSdLa2lrAkfS+UCikN954gwFM0Tb2/ENwJiYm+P0HoK3IF/xZX1/X5OQkf9+irUKhUNAhNKVne0ABAAAAAABgMNAABQAAAAAAgI6iAQoAAAAAAAAdRQMUAAAAAAAAOooGKAAAAAAAAHQUDVAA0KL5+XnNz88HHUZPKhaLWlpaCjoMtNHS0pLK5XLQYQAABhR51e7IrQbPQcytaIACgD5VLpd7curVYrGot956S1/5ylcUCoUUCoUaJpT2/cqfXpbNZqtinZmZabmsdDqtSCSiUCikSCSiZDLZUjnlclnb29taXl5WJBLxXKdYLGp+ft6Nu9G2bEyRSETpdLrqvZMnT2pqakrFYrGlOAEA6GW9mldJ5FZ+dTO3kvzFTm5V7XDQAQBAv1pYWAh0+7du3Qp0+17K5bKmp6c1NzensbExlUol3bhxQ+Pj45Lq68wYo2KxqJGRERUKBQ0PDwcRtm8ffPBB1evTp0+3VM7S0pJmZ2eVyWSUSqWUzWb17LPP6mc/+5kuXrzYVFmLi4uSpEuXLnm+XywWdf/+fS0sLGhhYUHJZFLj4+N120omk1pfX9fq6qok6dvf/rY++ugjnT9/XpI0Ojqqubk5TU9Pa3V1VeFwuJVdBwDAE3mVN3Irf7qZW1l7xU5uVY8eUADQh8rlspaXl4MOo87KyopGR0c1NjYmSQqHwzp37pyknYu4150omxj1eoIkSUeOHJExxv1xHKelcmZnZyXtJB6V/968ebPpsmzDUiP37993vw9J7vdhY5CkfD6v8fFxzc3NKRwOKxwOKxqN6sKFC8pms+56Y2NjeuKJJ7SystJ0nAAA9Kpezaskciu/uplbWbvFTm7ljQYoAGhBsVhUMpl0u+XWvk6n027333w+765ju+FK0vLysttd9969e27ZXl2ma5ctLi663Xgrlwc5fkKxWNTs7KxefPFFz/cXFxc1Pj7uuzt0uVxWMpl09295ebmqi7KfOq9cd2lpyX1/c3Oz6f3L5/OKRCKan5/X9vZ205+vZO+s2XJsvJ24+1vZ+CTJHWsgFou5y27fvi1Jevzxx91ljz32mKT6u3tnzpzR7OzsgeouDgDoLPIqb+RW/nUzt7Ll7xY7uVUDpketra2ZHg4PQJ+amJgwExMT+y7HcRwjyf09Vfl6a2vLGGNMLpczkkw0GjXGGPf9ynVKpZKJRqNGkrl7964xxphCoVBVdmVZlctqXxtjTCwWM7FYbN/7Z8tfW1vzvX4qlTKSTC6X8yzLxifJZDIZz/crOY5j4vG4MWanThzHMY7jmFKp5L6/V51XfjaRSBhjjNnY2PCMwe/+2R/HcUyhUGiqjEq2Lra2tkwikdhXWcZ4Hw+1crmcu117vBlj3GPQq0zHcerKkGRSqVTTMbbr/ENrqH8AndBsvuDlIORVrfx9S27VnG7mVnvF3q3cqh3nXzf1bAsPDVAAOqGdf4D5SVz8rJPJZIwks7i4uO+y2qnZC5q96Dcqy5idxNAmN5UNILWfs4lM5YV8a2vLSHKTHfu5veopkUh4rtNKQlkqlUwmk3H31SZxrbLJSSwWc5O/Vu11PFQm236Ot0bLS6VS3ef9ogEkWNQ/gE5o1x/Ag55XtfL3LblV87qZW+0We7dyq35rgOIRPAAImH1GvXJMnn6010CN0s64BfYZ9926Gl+/fl1S9dgFzzzzjCRpfX29qbjs+rXd7f3EWyscDmt0dFQLCwuKx+N1s5k0Y2lpScePH1epVJIkTU1NdXQq3qNHj8oYo0wmo1gsptnZ2ZbGu7ADZPb78QoAGEyDkldJ5FbN6nZu1a7YD1JuRQMUAKCrhoeHlclklE6nNT097ZkYXLlypW6ZvTg3e3G365uKQSLtz36cPXu25UQjmUxqdnZWp06dUjgc1tTUlNLptK5du7avmPwYHR3V1NSUJOnChQuStOuAn9FotOMxAQCA1pFbBZtbSfWxk1t5owEKAHrEQboYjY6OKpVKKZ1Ou4NGVrIXba+7eK3WU+WApO1gZzNphZ062SZ+IyMjkh40CHXaU089VfXaq77t4J3PPfdcV2ICAKCdDlJeJZFbBZ1b1cZObuWNBigACJi9eJ8+fTrgSPbHJjt+uzo7jqNEIuHZXXtiYkKSdP/+fXeZLffMmTNNxRWPxyVJq6urbhl25pb9KJfLTcdi1d4Vs8lSq1MPN8vWQyKRkCS9/PLLkqrr+8MPP6x6r1blLHoAAPSKQcmrJHKrZvRCblUZO7mVNxqgAKAFtVPWVr62F+LKZKH2bpOdLrdcLmt1dVWO41RdIO0dFJtEVU7vOjMzI6n6zoq94Ac5XbDtVVObJNl997rjdu7cOc+L7alTp+Q4ji5fvux+7saNG4pGozpx4kRdebvV+SuvvCJpZ1yCoaEhhUIhjYyMuEmCnUI4m8023LdkMlk1vXA+n9etW7fcWCw/ZUnSm2++6ZYrPfh+7fJmypKq97u2/iORiJaWlty7buVyWYuLi4rFYjp37pyknfGh4vG4rl69qnK5rHK5rKtXryoej+vo0aNV5dlyjh07tmdcAAD4QV7ljdyqN3MrP7GTW3mjAQoAWmC79dr/V74eGhqq+rd2fWln0MdIJKKhoSEdPXpUq6urVe//2Z/9mRzH0dNPP610Oq2xsTH3rtbbb78tSVpYWJAk/eVf/qU7pk+Qnn/+eUkP7u5IchMSaacO7CCVlRYWFjzvWq2srMhxnKrPffe733XX8Vvnw8PDyuVybjIWjUaVy+Xci3+pVFI0Gt01wXz44Yf10ksvKRQKaX5+Xr/4xS8876j5KUuSTpw4oY2NDd28eVOhUEhXr17VxsZGVeLit6xQKFS13zYRtM6fP6/Z2Vk9+eSTCoVCWllZ0de//nX3+Klc7/Tp0xoaGtLU1JTOnDmj8+fP123Pfr/2+wYAYL/Iq7yRW/VmbuU3dnKreiGz35HCOmR9fV2Tk5P7HsgMACpNTk5KktbW1gLZvr149cPvtlAopLW1NbfLth/2juHFixeb2la5XHa7SgclEokolUoNdFntMD8/r6Ghoaa/Yyn48++go/4BdEIr+UI7ty31R17V6t+35Fa9XVY77Ce3CvL8awU9oAAAbTM9Pa2bN29WdW33I+gEaXt7W3NzcwNdVjtks1lls1lNT08HHQoAAAcCuVXvltUOBy23ogEKALqkdnyDQWS7d1++fNnX8/W9YHNzU48++qjGxsYGtqx2uHfvnq5cuaKVlZXAk1oAAA5CXiWRW/VqWe1wEHMrGqAOgGKxqGQyqUgk4i4LekC9Wl4xonv64RgZBLXjGwyq4eFhra6u6v333w86FF9OnDjhDvI5qGW1Qzqd1ttvv63h4eGgQwEC1w/XTXKrYPXDMdLvDkpeJZFb9WJZ7XAQcysaoA6At956S+Pj40qn0x3fVj6f18zMjEKhkGZmZqpmB9hNu2LMZrNaXl5WJBLxHJCvkVAoVPWzWxfX7e3tuvXbobZM+xOJRLS8vNzROzu9dIw0qodQKKSlpSWl02nfU9H2GmNM1c8gC4fDLT3Hjt518eLFA5UgAbvppetmI/uJsVgsanl52b3+2lmlmkFu1TvHyKDmVgcpr5LIrQbRQcytaIA6AN577726ZQsLC3UzIO1XuVxWNpvVe++9p1KppOPHj+ull17ydeH1irFZS0tLmp+f15EjR/RXf/VXTV2IjDHK5XLu66tXrzZct/K9QqHQtgueMUaFQqHqtTFGf/VXf6V8Pq+RkRF36th266VjpLYeSqWSWxcnT57U8vKypqamBrqrNQCgt/XSdbOZGP1u045FYq/J6+vrTffcIbfqnWOE3ApAr6ABCm1z69Ytd/rJcDisc+fOSVJXun7PzMyoVCppdXVVjuO4U4A2w35mcXFRV65cUT6fr1snn8/ry1/+svu63S3WXuUdPXpUr7/+uiTpL/7iL9q6vW7ze4xU1kPl89Cjo6NaWVmRtDMgYz/erQMAwK8gcqsbN24onU7r7NmzknauyQsLC7p06ZLv3lcWuVXnkVsB6CcD0wBV+5x1Op12u6Hai10ymaxbJu3cOajsZjw/P+/eAfDqDtxqF+Fisah0Ou3GaLc5MzPjefelXC67MYdCoYZdhf2u16iuGtVdJBKpSxQ2Nzfdx9uWlpaqtmMvfrWi0eiuMUcikX3dfbJ35BYWFhoO3tbMM/cnT56UJN2+fbvuvdu3b7vv1+rkcWSThitXrtRtc1CPkUaGh4f15ptvKp1O69atW74/BwBoDrlVf10325Vbra+vS6pupPjSl74kSbp+/bq7jNyq/46RRsitAHSN6VFra2ummfAcxzGSjCSTyWSMMcZsbW0ZSSYajZqtrS1jjDG5XM5dZkWjUSPJFAoFz/fj8bj7vjHGFAoF4ziOux2/bHyS3HhKpZK7/bt379btUzwer9qm4zimVCo1vZ7dbm1d1b7erZ5SqVTVOolEomqfapVKJSPJpFKpuvccxzHRaNSNsbKsZmQyGXcb9ntyHMdsbGxUrReLxUwsFtuzPLt9+53UsvXhFWu7jiOvsm1dVpZnzGAfI7sdD43qw4+JiQkzMTHR9OcOIklmbW0t6DAwQDj/gtVs/ZNb9dd1s125VaPP1C4nt+q/Y6RTuRX5gj/N/n0L+NFv51/PngGtnKBev1T9LIvFYlW/bPe6AC4uLroXumZ5lW0bURYXF91lGxsbVRdUYx4kfYlEoun1are71+tm1qmMu9LGxobnBdteSCuTQnvRa/Y7X1xcrEqMK5NOe6Fuht2+rdfKMjKZjNuw5RVru46j2mS/VCqZWCxWF88gHyONymrm/Ub4A9i/frugofdx/gWrlfont2q8Xi9dN9uZWzVqvGv1uktu1RvHSKOymnl/t8+RL+yNBih0Qr+dfz17BnSzAcrK5XJug0bt+4VCwUg7vWtqL8j7jdFrudddIptIOI7T9HrtuAB6bWu3C5XjOJ6NQI3ugLVy0dst6Wz1Dk7l/yvLqLzLt1us+z2O7Ocqf2KxWN3dvEE+Rvb6nJ/3G5mYmPCsY3744ac7PzRABaebDVAWuVX/5VaVvdxsI4ZXg55ftXVCbjV4uVXQ1xV++DnoP/3UABUypk3TTLTZ+vq6Jicn1Ux49lnvys/4Xba8vKx0Oq3FxUU9/fTTde9LO+McjI+Pa2trS2NjY03uUeNtey3v9Hp+yqldls1m9eyzzyqRSOjcuXPu68XFxbopQZPJpH75y1/q/PnzLdeBH+0sy37OfsZ+37lcTp/5zGe0ubnpDuzYqPx2HEd+Yx/kY2SveiiXyxoaGlIsFmt6NpnJyUnl83m98cYbTX3uIDp79qzeeOMN/c7v/E7QoWBAvPvuuzp69KjW1taCDuVAmpyclKSm6p/cyv96g5JbSTtjDr3zzjtKp9OKx+P69//+3+ull15SJpPR6OhoU2WRW/XGMbJXPewntwqFQuQLPvzDP/yD3n33XV27di3oUDBAzp49q7W1NU1MTAQdij9taMTqiG72gLLPUudyuYafsd167d2XdnYTt8sr7wjZ58Jrt9PqerXb3et1o2WpVMqtA8dxqroZW5lMZtcxAXarg2a/c3tXqLabsY2vWZXbt8/hJxIJk0gk3OOjUaztOo781sMgHyONyrZs1/fasb784BEg/9Rnd1TQ+zj/gtXNHlDkVv2bW3lZXFz0Nd6TF3Kr3jhGGpVt7Se3Il/wh0fw0An9dv717BnQzQYoPxcD2+W4VCq5gzy2wqvsu3fvGql6sEB7wa3sQmu79lZeGPyu144LYCqV8nyevJJNAiplMhnPASP9DBC5F3uxrCzL7r/XxXkvtdu34wPU7lMrx5Ux/o4jv/UwyMdIo+3Zz9uBPlvBH8D+9dsFDb2P8y9Y3WyAIrfq39yqViKRaDimkB/kVr1xjDTanv38fnIr8gV/aIBCJ/Tb+dezZ0CzJ6h9/lt60Bumclnl7Bi1y+xdjFwu5yYs9n07SGHlL357gWnlTpAt2zaO2PJrf+Hbi6jjOG6ciUSi7kLiZ73afd7ttd3PyoErbbn2de1PNBp1y6mc0aPypzIBtHe/HMdx72jZhiRbXjNs/dk44/F4XX36manF1kPl3Sw75kFlQud1DBnTnuPIq94bGeRjpLLsyjrLZDJ1+9Is/gD2r98uaOh9nH/Barb+ya3657rZ7tyqVCq5DRiNxn0it+qvY6STuRX5gj80QKET+u3869kzoNkTtPYXbjPL7IUwFouZQqHgzrhhL+aV6zYqo9k47S97SSYej3ve2SgUCu4dLZtYtbJeowtXo5/d6qnRBS4ajbqPw3n91A4Kmcvl3PXtBdR2KW7l4le5/171uVeS1KgejDGeXak7cRztVnYjg3iM7LbdxcXFlmY3rMQfwP712wUNvY/zL1jN1j+5VX9cN6125VaV+VRtj6pK5Fb9c4x0OrciX/CHBih0Qr+dfwM1CHk/aHUwyF5w7949feYzn9HRo0frlj/99NN9uU9or344RloZhPegCoVC/TWoIXoe51+wBrX+ya0wyPrhGCFf8GdQ/75FsPrt/PtU0AGgPySTST311FN1Fz9JGhkZUSKRCCAq9BKOEQAA/OO6ib1wjAAYNDRAdVGxWPT8fz9YX1/X8vKy8vl81fJ79+7p2rVr7hS6OLg4RlCpWCxqaWkp6DDQRktLSyqXy0GHAVQht8Ig4xhBJXKrwXMQcysaoNogFAr5+hkZGXE/U/n/frC6uqpHHnlE3/nOd9z9mZ+f1z/90z/p/Pnzbd+e3zpF7+j2MdKvyuVyR4/dTpfvR7FY1FtvvaWvfOUrVceCl346r/P5vGZmZhQKhTQzM6PNzc22lb28vNzyvpfLZW1vb2t5eVmRSMRznWKxqPn5ebeOk8mk53rpdFqRSESRSETpdLrqvZMnT2pqaqrv/shHfyK3IrcCuZUfByGvksitWtHp3EqSstlsVT3PzMzUrUNuVSOowaf2wiBtADoh6EGQU6lUR3+3tbN8tTCooZ0ZyA5oWiqV3OmqGw1W6zVLUq8plUrubEKV+1Q5w1Cr7CC7rX5vdiDgRmUUCoWqAWZt7LUzW1VOtV4qlUw0GjXxeLxqna2trX1Nxx70+XfQUf8AOqGVfKFd+imvavXvW3Kr5nU6t7IqJyLwir0buVWQ518r6AEFAF1SLpe1vLzct+X7sbKyotHRUY2NjUmSwuGw+4jApUuXPHveDA8PV/3bi27duiXHcSRV79Nud8X8KJfL+pu/+Zt9lbGwsKCFhYWG79+/f9/9PiS5sc/OzrrL8vm8xsfHNTc3p3A4rHA4rGg0qgsXLiibzbrrjY2N6YknntDKysq+YgYAYL8OQl4lkVs1qxu5lXXkyBEZY9wfuz8SuVUjNEABgA/lclnJZNLtYru8vFzVXdarm3PtssXFRbfrrV1eLBbdrrnSg+7CMzMzunfv3r7Ll6T5+fmG3bTbqVgsanZ2Vi+++KLn+4uLixofH2/4+Fetveq8WCwqmUy6dZdOpxUKhRSJROrGy7DjJtj3m+3iXZlQVIpGo02VU2tlZUWvv/76vsrYS2XjkyR3rIFYLOYuu337tiTp8ccfd5c99thjkqQPPvig6vNnzpzR7OzsweouDgBoK/Iqf8itmteN3EraaWCKRCKan5/X9vZ23fvkVt5ogAIAH6ampvTLX/5SxhgVCgWl02lNT0+7f8wXCoW6z+RyuarXlXdS7J2SkZER95nw7e1tnT9/XqVSSZL09NNPu8lSq+V30507dyRJX/7ylz3fv3jxomKxmMbHx6vu/DSyV51PT09rfHzcrTvHcZTL5ZROp/Wd73zHLadYLGp6elpPPPGEjDF688039dJLL/mKoREbw+nTp1suY3NzU7/927/d1buT+Xxei4uLknbq17p586YkVc20ZOOqHa/Afr/2+wYAoFnkVf6QWzWnm7mV3ddLly7phRdeUCQSqWpAIrdqoOsP/fnEGFAAOqGVMVA2NjbqnqPf2toykkwikXCXyeM58dplftYx5sGz65Xj9LRafqvU5DPl9ln5RmUZ82AcA0nm7t27de9b7axzO6ZA7TqNxk3wY2NjY1/P6xcKhaoxANrxve1VRi6XqxqnYK9jq9HyUqnkOYaUH4xBFCzqH0AnNJsvHNS8qpW/b8mt/AsityqVSiaTybjfk5/ttzu3avb8Cxo9oABgD9evX5dU/Rz9M888I2lniuROGB0dlVQ9Tk+vu3Tp0p7rhMNh9xn33boat7PO7fq1Xev9xNvIO++84z7T34q///u/7/oMRkePHpUxRplMRrFYTLOzsy2NbWH3uZ+OTQBA7yCv8o/cyr8gcqtwOKzR0VEtLCwoHo/X9Wxqphyp/47PVtAABQB7uHLlSt0ye6Fo9UJzkA0PDyuTydR1+67Uzjq365uKQSLtTyuSyaQcx6kbW6mZeF5++eWWPtsOo6Oj7uN3Fy5ckNR4HAZp/2MxAABQibyq/citgs2tJOns2bNV9Uhu5Y0GKADYg72AeN1R6vQFZFAvUKOjo0qlUkqn0+6YRJU6UeeVg4+2KpvN6sc//vG+7rBFIhE9+eSTDQdA7Yannnqq6rVXfdvBRp977rmuxAQAOBjIqzqD3CrY3MrOcmeRW3mjAQoA9jAxMSFpZzp7y95ZOnPmTEe2aS/o+xmIsdtssuN1182L4zhKJBKe3bXbWefxeFyStLq66pZhZ25pRrFY1Pvvv181KGk2m9XMzExT5ex2t7DVO4fNsvWQSCQkyb1rWFnfH374YdV7tSpn0QMAwC/yKv/Irfzpldyqsh7JrbzRAAUAezh16pQcx9Hly5fduxg3btxQNBrViRMn3PXsXQ+b5FROyWovpJV3Q2ov0nYK3XK5rNXVVTmOU9V9t9XyuzVdsO1VU5sk2TrzuuN27tw5z4utnzqvLM9us3Lb9v1XXnlF0s64BENDQwqFQhoZGXGTBDuF8G4zt9jZXmZnZ6vurj377LNVyayfsvxqpqzK/a6t/0gkoqWlJfeuW7lc1uLiomKxmM6dOydpZ3yoeDyuq1evqlwuq1wu6+rVq4rH41Wzt0gP7t4dO3ZsX/sHADiYyKv8I7fqzdwqmUxqc3PTfZ3P53Xr1q2q45fcqoEuDnjeFGbBA9AJrc4CZWfW0P+fuSKRSNTN0pHL5dxZSFKplDHGGMdxTCKRcGccsbOwxGIxd5ktM5PJuJ+Px+NtKz8Wi7U0K4manFWjUCgYSWZra6uqjNofL47jeJa3W517ldtoW7lczp2hJBqNmlwu574Xi8VMNBr1jMGKRqOe+6KaGWf8lOXFq278ltUoLiuVStXNflf5HVWy6zqOYzY2NjzXsTPmVM6i4xezsAWL+gfQCc3mC8YczLyqlb9vya16P7eKxWImk8k0LKvTuVUr51+QQsZ0qU9ak9bX1zU5Odm1LnMADobJyUlJ0traWsCRPGCfTe+133ehUEhra2tul20/7N3BixcvNrWtcrnc8qwn7RKJRJRKpQa6rHaYn5/X0NBQ09+x1Jvn30FC/QPohFbyhU7q1byq1b9vya16u6x22E9u1Wvn3154BA8A0DbT09O6efNmVTd2P4JOkLa3tzU3NzfQZbVDNptVNpvV9PR00KEAAHAgkFv1blntcNByKxqgACBAlc/aez3H32/C4bBWVlZ0+fLltjyr3w2bm5t69NFHW576tx/Kaod79+7pypUrWllZCTypBQDAy6DlVRK5Va+W1Q4HMbc6HHQAAHCQjYyMVP2/17qLt2J4eFirq6taWVnR6Oho0OHsqXLAyEEtqx3S6bTefvttDQ8PBx0KAACeBjGvksiterGsdjiIuRUNUAAQoEFJjGqFw+GWnmNH7+L7BAD0ukHNqyRyq0F0EL9PHsEDAAAAAABAR9EABQAAAAAAgI6iAQoAAAAAAAAdRQMUAAAAAAAAOqrnByG/fv160CEAGCD5fF4Sv1v8unPnjh566KGgw8CAuH79us6cORN0GAfa9evX9eqrrwYdBoABQ76wtzt37kgiB8XBFjI9OlXABx98oOeffz7oMAAAQBv95//8n3Xp0qWgwziQYrGY/vzP/zzoMAAAQBvduXNHx44dCzoMX3q2AQrAwREKhbS2tqaJiYmgQwEAAOhr6+vrmpycFH/mAeg1jAEFAAAAAACAjqIBCgCA/8fe/cfGUd/5H38tSaA9BBvRygkBEoooEVJ75kcPwi/lSKIiUmahd3XwD0zayonWp8K1wjqpubUiZF/KSWuB6FciWlvtRZa9K8IfxStAJ2HrEnHE4dqT91p0SlSF2rS0Xh3qTivdXYnC5/uHbyb7095d7+zsj+dDWiU7M/uZ93521vPez3zm8wEAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAPkTZV4AACAASURBVOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4KmNfgcAoL0sLCzon//5nwuWJ5NJffjhh+7zW2+9VX/9139dz9AAAACazquvvqoPPvjAfb6wsCBJ+sd//Mec7b72ta/pS1/6Ul1jA4BsAWOM8TsIAO3jb//2b/Xyyy/rqquuKrnNn/70J0kSf54AAABWFwgEJGnN3Orv/u7vChqlAKCeuAUPQF391V/9laSVRKjU48orr9R3vvMdnyMFAABofN/5znd05ZVXrppbSdL+/ft9jhRAu6MHFIC6+vTTT3XDDTfod7/73arbvfPOO3rggQfqFBUAAEBz+td//Vc9+OCDq26zdetW/eY3v9EVV9D/AIB/+AsEoK6uuOIK9fX16corryy5zbZt23T//ffXMSoAAIDmdP/992vbtm0l11955ZXq6+uj8QmA7/grBKDuenp69MknnxRdt2nTJj399NPueAYAAAAoLRAI6Omnn9amTZuKrv/kk0/U09NT56gAoBC34AHwxS233JIzY0u2//iP/9CXv/zlOkcEAADQnH7+85/rz//8z4uu+8IXvqALFy7UOSIAKEQPKAC++OY3v1n0St0Xv/hFGp8AAAAq8OUvf1lf/OIXC5Zv2rRJ3/zmN+sfEAAUQQMUAF/09PTo4sWLOcs2bdqkgwcP+hQRAABA8zp48GDBxb2LFy9y+x2AhsEteAB809nZqZ///Ody/gwFAgH98pe/1C233OJzZAAAAM3lwoULuvXWW3Pyqi9/+ctKpVI+RwYAK+gBBcA3Bw8e1IYNGyStJEl33XUXjU8AAABVuOWWW3TXXXe5E7ls2LCBnuUAGgoNUAB8093drUuXLklaSZL6+/t9jggAAKB59ff3uxf3Ll26pO7ubp8jAoDLaIAC4Jtt27bpoYcekiR9+umnevLJJ32OCAAAoHk9+eST+vTTTyVJDz30kLZt2+ZzRABwGQ1QAHzV19cnSbr77ru1detWn6MBAABoXlu3btXdd98t6XKOBQCNomAQ8vfee0/33nuvX/EAAAB47u///u81Ojpal31FIhH9wz/8Q132BQAA4IezZ8/qnnvuWXWbjfkLfvnLX0qSXn31VW+iAoA8tm3r2muvdQfNbAcvv/yyJOnZZ5/1OZLGd+DAAT377LN68MEH/Q4FLaKvr08ffPBB3fb3wQcfaNOmTZqamqrbPgG0L2OM/vCHPygYDPodSl2RL5TnnXfe0csvv8zvfdTUgQMH9Mtf/rLyBihHV1dXzYMCAKz4yU9+Iom/teW69957qSvUjPP9q6euri6OYQDwGPnC2i5evCiJHBT+YAwoAAAAAAAAeIoGKAAAAAAAAHiKBigAAAAAAAB4igYoAAAAAAAAeIoGKAAAAAAAAHiKBigAaHLDw8MaHh72O4yGlE6nNTY25ncYqKGxsTHZtu13GACAFkVetTpyq9ZTz9yKBigAwLrYtq1AIOB3GAXS6bSOHj2qO++8U4FAQIFAoGRC6azPfjSqpaUlDQ4OKhAIaHBwUHNzczUre3x8vOr3btu25ufnNT4+rlAoVHSbdDqt4eFht44TiUTR7ZLJpEKhkEKhkJLJZM66ffv2qb+/X+l0uqo4AQBoZI2aV0nkVtXwOreSpFQqlVPPg4ODBds0Sm5FAxQANLmRkRGNjIz4tv/Tp0/7tu9SbNvWwMCADh48qD179iiTySgej2t0dLRoomSM0fLysiRpeXlZxph6h1wW27aVSqX0yiuvKJPJaPfu3dq7d29BIlGNVCqlw4cPV/36aDSqN954Q4cPHy4aTzqd1oULFzQyMiJjjOLxuHp6egquoiYSCY2Pj2tyclKTk5N68803NT4+7q7v7OzUkSNHNDAwQE8oAEDNkVcVR25VOa9zK8d7772X83z//v05zxspt6IBCgBQNdu2c05gjWJiYkKdnZ3atWuXJCkYDKq7u1uSNDo6WrTnTUdHR86/jej06dOyLEtS7nta7apYOWzb1muvvbauMtZK2C9cuOB+HpLc2IeGhtxlS0tL6unp0ZEjRxQMBhUMBhUOh3X48GGlUil3u127dumGG27QxMTEumIGAKCRNGpeJZFbVaoeuZVj69atMsa4D+f9SI2XW9EABQBNLJ1OK5FIuCfJ/OfJZFKBQEChUEhLS0vuNk43XOly1+DBwUGdP3/eLbtYl+n8ZdFo1L0ik73cz/ET0um0hoaG9PDDDxddH41G1dPTU/L2r3y2bSuRSLjvb3x8PKeLcjl1nr3t2NiYu77SLt7ZCUW2cDhcUTn5JiYm9Mwzz6yrjLVkNz5Jcq+wRSIRd9m7774rSdq2bZu77Prrr5dUeHWvq6tLQ0ND3IoHAKgZ8qriyK0qV4/cSlppYAqFQhoeHtb8/HzB+obLrUyeqakpU2QxAKCGent7TW9v77rLsSzLSHL/bmc/P3PmjDHGmMXFRSPJhMNhY4xx12dvk8lkTDgcNpLMuXPnjDHGLC8v55SdXVb2svznxhgTiURMJBJZ9/tzyp+amip7+5mZGSPJLC4uFi3LiU+SWVhYKLo+m2VZJhaLGWNW6sSyLGNZlslkMu76teo8+7XxeNwYY8zs7GzRGCqRyWSMJDMzM1N1GbOzs27cxT7LSpVTxuLiovsZOMebMcY9BouVaVlWQRnVvvdaff8adX8A0I4qzReKaYe8qprf++RWlalnbuV8Ns7DsiyzvLzsrq9XblXu948GKADwQS1/kJaTuJSzzcLCgpFkotHousuqpUoTSicBKlWWMSvJhZPcZDeA5L/OSWSyT+Rnzpwxktxkx3ndWvUUj8eLbrOehHJ2djYnYavU8vKymwA68XjdAJWdbJdzvJVa7iSI2a8vFw1QANB6atEA5ZTTynlVNb/3ya3K50dulclkzMLCgvs5lbP/WudW5X7/uAUPACBpZQBCKXdMnmY0Ojq65jbBYNC9x321rsYnT56UlDt2we233y5Jmp6eriguZ/v87vblxFvKSy+95N7TX43XX39dhw4dqnr/1di+fbuMMVpYWFAkEtHQ0FBV410477nZj1cAQGtqlbxKIreqhB+5VTAYVGdnp0ZGRhSLxaoeQL0euRUNUACAttTR0aGFhQUlk8mSs34cP368YJlzcq705O5sb7IGiXQe1UgkErIsq2BspUrieeSRR6p6bS10dnaqv79fktwZYkqNwyCtfywGAADgLXIrf3MrSTpw4EBOPTZabkUDFAAgRzv90O/s7NTMzIySyaSi0WjBeuekXewqXrX1lD0gabVSqZTef//9dV1hC4VC2rFjR8lBUevhtttuy3lerL6dwUbvuuuuusQEAEAttVNeJZFb+Z1bObPcORott6IBCgAg6fLJe//+/T5Hsj5OslPsqlsxlmUpHo8X7a7d29srSbpw4YK7zCm3q6urorhisZgkaXJy0i3DmbmlEul0Wm+//XbOtLypVEqDg4MVlbPa1cJqrxxWyqmHeDwuSe5Vw+z6/uijj3LW5cueRQ8AgEbRKnmVRG5VrkbJrbLrsdFyKxqgAKCJ5U9Zm/3cORFnJwv5V5uc6XJt29bk5KQsy8rpqutcQXGSqOzpXZ2TcvaVFeeE7+d0wU6vmvwkyXnvxa64dXd3Fz3ZPvroo7IsS8eOHXNf99ZbbykcDmvPnj0F5a1W548//riklXEJNm/erEAgoC1btrhJgjOFcCqVKvne0um0BgYGNDQ0lHN17Y477shJcMspq1yVlJX9vvPrPxQKaWxszL3qZtu2otGoIpGIuru7Ja2MDxWLxXTixAnZti3btnXixAnFYjFt3749pzynnHvuuWdd7w8AAAd5VXHkVo2ZWyUSCc3NzbnPl5aWdPr0abcepcbLrWiAAoAmtmXLlpz/Zz/fvHlzzr/520srgz6GQiFt3rxZ27dv1+TkZM7673//+7IsSzt37lQymdSuXbvcq1rPP/+8JLlXi374wx+6Y/r46d5775V0+eqOJDchkVbqoFg36JGRkYL75J0BNS3LynndCy+84G5Tbp13dHRocXHRTcbC4bAWFxfdk38mk1E4HF41wTx69GjJ8RF27tzp/r+csspVblmBQCDnfTuJoOPQoUMaGhpyu6ZPTEzoa1/7Ws7VRme7/fv3a/Pmzerv71dXV1fR7vDO5+t83gAArBd5VXHkVo2ZW1199dXau3evAoGAhoeH9fvf/77omE+NlFsFTF5fsOnpafX19dWtixgAtKO+vj5J0tTUlC/7d05ezfC3PhAIaGpqyu2yXQ7niuFzzz1X0b5s26561pNaCYVCmpmZaemyamF4eFibN2+u+DOW6v/98/v7DgDtoJp8oZb7lpojr6r29z65VWOXVQvrya3K/f7RAwoA0HIGBgZ06tSpnK7t5fA7QZqfn9eRI0dauqxaSKVSSqVSGhgY8DsUAADaArlV45ZVC/XKrdbdAJVOp5VIJBQKhWoRT9Ptv1EUq4d63Cvs9/3IrabdjmeOW3/kj2/Qipzu3ceOHavJvfr1MDc3p+uuu67qqX+boaxaOH/+vI4fP66JiQnfk1qv+H0u8Hv/jYJzVGtot+OZ47b+2iGvksitGrWsWqhnbrXuBqijR4+qp6en5H2TlbBtu+LpCWu5/2ZWj3qo5vOpt1QqpfHxcYVCoYpizR5wLv8xNjam8fHximPx4niem5tz4yp1ki/2HhoVx60/8sc3aFUdHR2anJzU22+/7XcoZdmzZ487yGerllULyWRSzz//vDo6OvwOxTPkVo2hnc9R6XRa4+Pjbh7hDKxcCXIrf7TzceuXdsmrJHKrRiyrFuqaW5k8U1NTpsjiVUmq+DXFzMzMVFVOrfbf7Lyuh2o/n3qJRqPGsiwzMzNjFhcXK3798vJy0TqcnZ01kkw8Hq+oPK+O50wmY+LxuJFkIpFI0W2c97K8vFzx/uutXY/b3t5e09vb63cYTUGSmZqa8jsMtJB6f/+q2R+5VWNox3NUJpMxlmWZWCxmjFnJKSzLKplzrIbcyh/teNwaQ75Qrmp+7wNrKff71zBjQNm2XdWVENRHo38+g4ODymQy7nSn+VNKlqNUi68zjeX09HTZZXlZX8Fg0J2yfHR0tOhVSee9tHIPgXI0+nELAF7ib2Bja9TP56233lIymdSBAwckreQSIyMjGh0dzZnuuxzkVq2nUY9bAM2hpg1Q6XRaY2NjCgQCGhwc1NLSUs565w9WdjdX5z7ZaDTqdhXN79pq27YSiYS7fLU/eslk0t1/pffg5t8z7ZQVCoWKvpf8mPLv/00mkwqFQrJtW4ODg+77LbaP7Ppyys2vw9Xqb633IpXuCu1sU+nnU+qe+nLqptx6LofTXXpkZKTkPau1uC89vytzIxzP0WhUPT09ZXeN57htnOMWAMpBbsU5qty6qdU5ymkUys6pbr75ZknSyZMn3WXkVqX3zXFbft2QWwFtJr9L1HpuwTtz5owx5nJXXeV1Uw2Hw+6yxcVFI8mEw+GCcvLld/sNh8M5z/P3f+7cuYKyy+HEnF1WsTidbfO7JluWZTKZTNGyFhYWTDgczlm+sLBgjDHmzJkz7j5W228l9Ze9n+z12Z+H033WuV2t0s+n2D6qqZvV6nktCwsLRpKZmZkxsVjMSDKWZZnZ2dmc7SKRSFldx0sdgyrSTdzv49kpOxKJ5BxP+evz981x6/9xawy34FVCdKlHjTXTLXjkVpyjqqmb1ep5LavlQtnLya0u75vj1v/j1omLfGFt3IIHL5T7/fNsDCjnD7vzR8eYlT/mq/3xKlaOcz929h/JM2fOGMuyVn1dqRNUNe8lf5lz33p+TPknUud1zh/ZSuMtdqKvpP5WqwPn88luqKnm81lv3axVB2uJRqM5J+5MJuOeNJ0TWSWcGPIfkUik4HP0+3h2njtjNUgy586dK1jv4LgtHWO9j1tjaICqBAklaq2ZGqCykVtxjnJ4eY5y8qjsnKLasrJfR27Fcet1bkW+UB4aoOCFcr9/G+URZ1T3w4cP69ChQ5JWbpGSpKWlpZwuvKtxugFn32+9a9cuzczM1DLcijixZ8d0++23S1qJ17mH3FGrqQyrqb9i0um0hoaGFI1G3Xvwa1V+pXWzXkNDQ5Kkzs5OSSt1HQ6Hdfz4cZ04caLqqS1XvkMr0um0fvjDH6q/v18TExPue2uU49mZEnXLli0aGhrKiTEbx21p9T5uHeutk3Zy9uxZbdq0ye8w0CKWlpaqGivQb+RWl3GO8u4cdfDgQR0/flwvvviiXnjhBQWDQXfK9Wg0WnW55FYct/XIrcgX1nb27FlJIgeFP/JbpGo5C16x5bFYzFiW5ba2q8LW9HL3X87rqi2r3PdbSb2Uu6yS+iu1/0gkknNVqNry1/Oea/WZVVLH6ynPmfkkv6u5n8dz/nPndkSna3O5++a4rf9xa8xKjwjntTx48Kj/oxl7QJVaTm7FOaqS91uO2dlZtwdQLBZze7Pk35JWjlIxkFtx3FbyfstR7/MIDx48ch++3oLnLM/uvul0kXXuL85/XbFynJPfaie8Yq8rFVM17yV/WbExGJztyrlPvdx485dVWn/FynTGSXLKyFbN51PLuqnmM3O6iRfr0lzqhLqa1WJY7+dhTG2P52JxOvfwO2MXFNs3x63/x60x3IJXiXJPaEC5mvUWPGc5uRXnKGc7r85R+aLRaFnjPRVDbsVxm7+dV8ct+UJ5uAUPXij3+1fTWfCyOV11d+/e7S7r6emRpIq6vVuWJUk6fvy4bNuWtNIddHBwsFahVqy3t1eSdOHCBXeZE1tXV5dn+62m/rLNz8/r8OHDmp2dLVrGesuX6l83Tpm/+tWvCvbnxFILzkwc4XDYXdaIx7NlWYrH4xodHS1Yx3Fbml91AwCVILeqPc5Ra0skEjp16pQ77EGtkFtVj+MWQNPKb5GqpkXUaeF2Bq9zZjmIRqNFt1tcXMzpzum0jGe3lDuvzZ71xXmEw2F3QECn+252OU4XWRVpdV9NdllOj5piZTkDE1qW5S6Lx+M5rfnZZa21j2Lvodiy1eovf/v8586MEvmfibNdNZ9PqbqvpG5Wq+dyOd2Hndc5XYbzt1nryl2xuIxZGZzRueqVPRCln8ezs12puip2lY7jtrGOW3pAlU9c0USNNUMPKHIrzlF+nqMymYw7W1v++3OQW3HcNtpxS75QHnpAwQvlfv9q0gBlTO694uFwOGcmBYdzH3UkEjHLy8vuDAtO98789Q5nW2dd/mwU2Y9Sy8pRSVnLy8tu11VpZTaH7BNr9muKzcKx1j6KLVut/vK3z3/kn5grLb/Y+lrUzXo/M0f2/mKxWNFZVVZLktaqu1gsVtBF2a/judTnl6/YLYgct41z3NIAVT6JhBK11QwNUMaQW3GO8ucc5Wwfi8VWva2N3Oryvjlu/T9undeSL6yNBih4odzvX+D/NnZNT0+rr69PeYsBNBnbtms22wpqr6+vT5I0NTXlcySNLxAIaGpqqqa31aK91fv7x/cdaA3kVo2NfKE8/N6HF8r9/nk2BhQAf5EgAQAA1A65FQCsDw1QAAAAAAAA8FRbNEAFAoGyHmgcfGYAaiGdTmtsbMzvMFBDY2Nj7kxK8A/n6ebDZwagFsitWk89c6u2aIAyK4Otr/lA4+AzA7xl27anPzS8Lr8c6XRaR48e1Z133un+sBoeHi66bTP9CHOmFw8EAhocHNTc3FzNyh4fH6/6vdu2rfn5eY2PjysUChXdJp1Oa3h42K3jRCJRdLtkMqlQKKRQKKRkMpmzbt++ferv71c6na4qTtQG5+nmw2cGeKcd8iqJ3KoaXudWkpRKpXLqeXBwsGCbRsmt2qIBCgCQ6/Tp001d/lps29bAwIAOHjyoPXv2KJPJKB6Pa3R0tGiiZIzR8vKyJGl5eblhf4TZtq1UKqVXXnlFmUxGu3fv1t69ewsSiWqkUikdPny46tdHo1G98cYbOnz4cNF40um0Lly4oJGRERljFI/H1dPTU3AVNZFIaHx8XJOTk5qcnNSbb76p8fFxd31nZ6eOHDmigYEBekIBABpCq+dVErlVNbzOrRzvvfdezvP9+/fnPG+k3IoGKABoM7Zt55x0mq38ckxMTKizs1O7du2StDJwbHd3tyRpdHS0aM+bjo6OnH8b0enTp2VZlqTc97TaVbFy2Lat1157bV1ljIyMaGRkpOT6CxcuuJ+HJDf2oaEhd9nS0pJ6enp05MgRBYNBBYNBhcNhHT58WKlUyt1u165duuGGGzQxMbGumAEAWK92yKskcqtK1SO3cmzdujWnJ6vzfqTGy61ogAKAJmLbthKJhNvFdnx8PKe7bLFuzvnLotGoexXFWZ5Op92uudLl7sKDg4M6f/78usuXpOHh4ZLdtGspnU5raGhIDz/8cNH10WhUPT09JW//yrdWnafTaSUSCbfuksmkAoGAQqGQlpaWCmIbGxtz11faxTs7ocgWDocrKiffxMSEnnnmmXWVsZbsxidJ7hW2SCTiLnv33XclSdu2bXOXXX/99ZIKr+51dXVpaGiIW/EAAFUjryoPuVXl6pFbSSsNTKFQSMPDw5qfny9Y32i5FQ1QANBE+vv79cc//tHt1pxMJnO6yzpdnbMtLi7mPM++kuJcKdmyZYt7T/j8/LwOHTqkTCYjSdq5c6ebLFVbfj2dPXtWknTrrbcWXf/cc88pEomop6cn58pPKWvV+cDAgHp6ety6syxLi4uLSiaT+sEPfuCWk06nNTAwoBtuuEHGGH33u9/V3r17y4qhFCeG/K7WlZibm9MDDzxQ16uTS0tLikajklbq13Hq1ClJ0vbt291lTlz5Xc+dz9f5vAEAqBR5VXnIrSpTz9zKea+jo6O67777FAqFchqQGi63MnmmpqZMkcUAgBrq7e01vb29Fb1mdnbWSDLLy8vusjNnzhhJJh6Pu8skFfwdz19WzjbGGLOwsGAkmWg0uu7yqyXJTE1Nlb19JBIpuW9neSaTMZZlGUnm3LlzBesdtazzeDxedJtIJFL2e8s3OztrLMsymUymqtcvLy+bWCyWE896P7e1ylhcXHS3KefYKrU8k8kUvL5c1Xz/1qPe+wOAdlRpvtCueVU1v/fJrcrnR26VyWTMwsKC+zmVs/9a51blfv/oAQUATeLkyZOScu+jv/322yVJ09PTnuyzs7NTUu44PY1udHR0zW2CwaB7j/tqXY1rWefO9vld68uJt5SXXnrJvae/Gq+//roOHTpU9f6rsX37dhljtLCwoEgkoqGhoarGtnDeczMdmwCAxkFeVT5yq/L5kVsFg0F1dnZqZGREsVis6gHU65Fb0QAFAE3i+PHjBcucE0UtZupoNx0dHVpYWCjo9p2tlnXubG9qNO15IpGQZVkFYytVEs8jjzxS1WtrobOz0739zpkhptQ4DNL6x2IAACAbeVXtkVv5m1tJ0oEDB3LqsdFyKxqgAKBJOCeQYleUvD6BtOqP/87OTs3MzCiZTLpjEmXzos6zBx+tViqV0vvvv7+uK2yhUEg7duwoOQBqPdx22205z4vVtzPY6F133VWXmAAA7YG8yhvkVv7mVs4sd45Gy61ogAKAJtHb2ytpZTp7h3Nlqaury5N9Oif09QzEWG9OslPsqlsxlmUpHo8X7a5dyzqPxWKSpMnJSbcMZ+aWSqTTab399ts5g5KmUikNDg5WVM5qVwurvXJYKace4vG4JLlXDbPr+6OPPspZly97Fj0AAMpFXlU+cqvyNEpulV2PjZZb0QAFAE3i0UcflWVZOnbsmHsV46233lI4HNaePXvc7ZyrHk6Skz0lq3Mizb4akn+SdqbQtW1bk5OTsiwrp/tuteXXa7pgp1dNfpLk1FmxK27d3d1FT7bl1Hl2ec4+s/ftrH/88cclrYxLsHnzZgUCAW3ZssVNEpwphFebucWZ7WVoaCjn6todd9yRk8yWU1a5Kikr+33n138oFNLY2Jh71c22bUWjUUUiEXV3d0taGR8qFovpxIkTsm1btm3rxIkTisViObO3SJev3t1zzz3ren8AgPZEXlU+cqvGzK0SiYTm5ubc50tLSzp9+nTO8dtwuVX+qOTMggcA3qt2VixnZg3938wV8Xi8YJaOxcVFdxaSmZkZY4wxlmWZeDzuzjjizMISiUTcZU6ZCwsL7utjsVjNyo9EIlXNSqIKZ7VZXl42ksyZM2dyysh/FGNZVtHyVqvzYuWW2tfi4qI7Q0k4HDaLi4vuukgkYsLhcNEYHOFwuOh7Ud6MM+WUVUyxuim3rFJxOWZmZgpmv8v+jLI521qWZWZnZ4tu48yYkz2LTrmYBQ8AWk+l+YIx7ZlXVfN7n9yq8XOrSCRiFhYWSpbldW5V7vcv8H8bu6anp9XX11e3LmIA0I76+vokSVNTUz5Hcplzb3qj/f0PBAKamppyu2yXw7k6+Nxzz1W0L9u2q571pFZCoZBmZmZauqxaGB4e1ubNmyv+jKX6f/8a8fsOAK2mmnzBS42aV1X7e5/cqrHLqoX15Fblfv+4BQ8A0HIGBgZ06tSpnG7s5fA7QZqfn9eRI0dauqxaSKVSSqVSGhgY8DsUAADaArlV45ZVC/XKrWiAAgDk3Gtf7D7+ZhMMBjUxMaFjx47V5F79epibm9N1111X9dS/zVBWLZw/f17Hjx/XxMSE70ktAADFtFpeJZFbNWpZtVDP3Gqjp6UDAJrCli1bcv7faN3Fq9HR0aHJyUlNTEyos7PT73DWlD1gZKuWVQvJZFLPP/+8Ojo6/A4FAICiWjGvksitGrGsWqhnbkUDFACgZRKjfMFgsKr72NG4+DwBAI2uVfMqidyqFdXz8+QWPAAAAAAAAHiKBigAAAAAAAB4igYoAAAAAAAAeIoGKAAAAAAAAHiq5CDkBw4cqGccANBWzp49K4m/teV6+eWX9ZOf/MTvMNAiTp48qd7e3rruc3p6WhcvXqzrPgGgDLp5kwAAIABJREFU3ZAvrG1paUkSOSj8ETB5Q/T/7ne/0/e+9z1dunTJr5gAtJm3335bX/rSl7R161a/QwHQJvr7+2VZVl32lUwmNTk5WZd9AcDvfvc7/eIXv9C+ffv8DgVAm9iwYYNefPHFNX/PFTRAAUC9BQIBTU1N1b1HAgAAQKuZnp5WX1+f+JkHoNEwBhQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADxFAxQAAAAAAAA8RQMUAAAAAAAAPEUDFAAAAAAAADwVMMYYv4MA0D4mJib0N3/zN9q5c6e77MMPP9TnPvc5/dmf/Zkk6be//a0eeOABvf76636FCQAA0BT27dunhYUFXX/99ZKk//7v/9bHH3+sm266yd3m3Llz+qd/+if19vb6FSYAaKPfAQBoL8vLy7p48aJ+8Ytf5Cy3bTvneTKZrGdYAAAATWlubk7GGH388cc5y/Nzq1/96ld1jAoACnELHoC66unpUSAQWHWbjRs36oUXXqhTRAAAAM3rhRde0MaNq/crCAQC6u7urlNEAFAct+ABqLu/+Iu/0M9+9jOV+vMTCAT0wQcfaMeOHXWODAAAoLksLi7qC1/4wqp51d13361/+7d/q3NkAJCLHlAA6u6pp57Shg0biq674oordM8999D4BAAAUIYdO3bonnvu0RVXFP9pt2HDBj311FN1jgoACtEABaDunnzySX366adF1wUCAR08eLDOEQEAADSvgwcPlhzi4NNPP9WTTz5Z54gAoBANUADqbuvWrdq9e3fJXlBdXV11jggAAKB5lcqdNmzYoN27d2vr1q11jggACtEABcAXTz/9dMFYBRs2bNDDDz+sz3/+8z5FBQAA0Hw+//nP6+GHHy64uGeM0dNPP+1TVACQiwYoAL74+te/TpIEAABQI6Uu7n3961/3KSIAyEUDFABfBINBPfrooznTBm/atElPPPGEj1EBAAA0pyeeeEKbNm1yn2/cuFGPPvqogsGgj1EBwGU0QAHwTX9/vy5duiRpJUl67LHHdM011/gcFQAAQPO55ppr9Nhjj7kX9y5duqT+/n6fowKAy2iAAuCbxx57TJ/97GclrSRJfX19PkcEAADQvPr6+tyLe5/97Gf12GOP+RwRAFxGAxQA33zmM5/RN77xDUnS1Vdfrf379/scEQAAQPPav3+/rr76aknSN77xDX3mM5/xOSIAuGzj2pugVj788EPNz8/7HQbQUG688UZJ0o4dOzQzM+NzNEBjufHGG3Xffff5HQbQsM6cOaNf//rXfocBNJQdO3bo/fff14033qiTJ0/6HQ7QUHbt2qWbbrrJ7zDaVsDkT5UAz3z729/Wj3/8Y7/DAAA0EU7TQGmBQMDvEAAATeRb3/qWfvSjH/kdRtuiB1Qd/elPf1Jvb6+mpqb8DgUoy/T0tPr6+vgBXAZn/Cq+36gV5/sHYHVTU1Pq7e31OwygLOQL5QsEAny/UVN9fX3605/+5HcYbY0xoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoADUxfDwsIaHh/0Oo2Gl02mNjY35HQZqaGxsTLZt+x0GAKBFkVuVRl7VesirWgMNUADagm3bCgQCfodRVDqd1tGjR3XnnXcqEAgoEAiUTCid9dmPRrW0tKTBwUEFAgENDg5qbm6uZmWPj49X/d5t29b8/LzGx8cVCoWKbpNOpzU8POzWcSKRKLpdMplUKBRSKBRSMpnMWbdv3z719/crnU5XFScAAI2sUXMr8qrKeZ1XSVIqlcqp58HBwYJtyKtaHw1QAOpiZGREIyMjvu3/9OnTvu17NbZta2BgQAcPHtSePXuUyWQUj8c1OjpaNFkyxmh5eVmStLy8LGNMvUMui23bSqVSeuWVV5TJZLR7927t3bu3IJmoRiqV0uHDh6t+fTQa1RtvvKHDhw8XjSedTuvChQsaGRmRMUbxeFw9PT0FV1ITiYTGx8c1OTmpyclJvfnmmxofH3fXd3Z26siRIxoYGOCKHQCg5sitCpFXVc7rvMrx3nvv5Tzfv39/znPyqvZAAxSAlmfbds4JrJFMTEyos7NTu3btkiQFg0F1d3dLkkZHR4v2vOno6Mj5txGdPn1almVJyn1Pq10ZK4dt23rttdfWVcZaCfuFCxfcz0OSG/vQ0JC7bGlpST09PTpy5IiCwaCCwaDC4bAOHz6sVCrlbrdr1y7dcMMNmpiYWFfMAAA0kkbNrcirKlOPvMqxdetWGWPch/N+JPKqdkIDFADPpdNpJRIJ9ySZ/zyZTCoQCCgUCmlpacndxumGK13uGjw4OKjz58+7ZRfrMp2/LBqNuldkspf7PXZCOp3W0NCQHn744aLro9Goenp6St7+lc+2bSUSCfc9jo+P53RTLqfes7cdGxtz11fazTs7qcgWDocrKiffxMSEnnnmmXWVsZbsxidJ7lW2SCTiLnv33XclSdu2bXOXXX/99ZIKr/B1dXVpaGiILuMAgJohtypEXlW5euRV0koDUygU0vDwsObn5wvWk1e1EYO66e3tNb29vX6HAZRtamrK1OLPhGVZRpJbVvbzM2fOGGOMWVxcNJJMOBw2xhh3ffY2mUzGhMNhI8mcO3fOGGPM8vJyTtnZZWUvy39ujDGRSMREIpF1vz9jqvt+z8zMGElmcXGxYJ0TayQSMZLMwsJC0fXZLMsysVjMGLNSL5ZlGcuyTCaTcdevVe/Zr43H48YYY2ZnZ4vGUIlMJmMkmZmZmarLmJ2ddeMu9nlWqpwyFhcX3c/AOeaMMe5xWKxMy7IKyqjmvdfq+we0MklmamrK7zCAstXq90A75FaVfr/JqypTz7zK+Wych2VZZnl52V1fj7zKGH6PNwIy2zrigEezqeUP4HKSlnK2WVhYMJJMNBpdd1m1VM3320mCinGWZzIZN8HJbgDJf52TzGSfzM+cOWMkuQmP87q16ioejxfdZj0J5ezsbE7SVqnl5WU3CXTi8boBKjvZLueYK7XcSRKzX18OGqCAtdEAhWZTy98DrZ5bVfr9Jq8qnx95VSaTMQsLC+7nVM7+a5lXGcPv8UbALXgAmkpnZ6ek3PF4mtXo6Oia2wSDQfc+99W6G588eVJS7vgFt99+uyRpenq6oric7fO725cTbykvvfSSe19/NV5//XUdOnSo6v1XY/v27TLGaGFhQZFIRENDQ1WNd+G851Y4ZgEAradVcivyqvL5kVcFg0F1dnZqZGREsVis6gHUyauaGw1QANDgOjo6tLCwoGQyWXLmj+PHjxcsc07QlZ7gne1N1kCRzqMaiURClmUVjK1USTyPPPJIVa+thc7OTvX390uSO0tMqbEYpPWPxwAAALxDXuVvXiVJBw4cyKlH8qr2QQMUgKbUbiejzs5OzczMKJlMKhqNFqx3TtzFruRVW1fZA5JWK5VK6f3331/XVbZQKKQdO3aUHBS1Hm677bac58Xq2xlw9K677qpLTAAA1FI75VbkVf7mVc4sdw7yqvZBAxSApuKcvPfv3+9zJOvnJDzFrrwVY1mW4vF40S7bvb29kqQLFy64y5xyu7q6KoorFotJkiYnJ90ynNlbKpFOp/X222/nTM2bSqU0ODhYUTmrXTGs9uphpZx6iMfjkuReOcyu748++ihnXb7sWfQAAGgUrZJbkVeVp1Hyqux6JK9qHzRAAfBc/pS12c+dE3F2spB/tcmZLte2bU1OTsqyrJyuus4VFCeByp7e1TkpZ19ZcU74fk4VLF3uVZOfKDnvv9hVt+7u7qIn3EcffVSWZenYsWPu69566y2Fw2Ht2bOnoLzV6v3xxx+XtDI2webNmxUIBLRlyxY3UXCmEU6lUiXfWzqd1sDAgIaGhnKusN1xxx05CW45ZZWrkrKy33d+/YdCIY2NjblX3mzbVjQaVSQSUXd3t6SV8aFisZhOnDgh27Zl27ZOnDihWCym7du355TnlHPPPfes6/0BAOAgtypEXtWYeVUikdDc3Jz7fGlpSadPn3brUSKvaic0QAHw3JYtW3L+n/188+bNOf/mby+tDPoYCoW0efNmbd++XZOTkznrv//978uyLO3cuVPJZFK7du1yr2o9//zzkuReLfrhD3/ojufjt3vvvVfS5Ss8ktykRFqph2JdoUdGRgrulXcG1bQsK+d1L7zwgrtNufXe0dGhxcVFNyELh8NaXFx0E4BMJqNwOLxqgnn06NGSYyTs3LnT/X85ZZWr3LICgUDO+3aSQcehQ4c0NDTkdk+fmJjQ1772tZwrjs52+/fv1+bNm9Xf36+urq6iXeKdz9f5vAEAWC9yq0LkVY2ZV1199dXau3evAoGAhoeH9fvf/77omE/kVe0hYOrVzw7q6+uTJE1NTfkcCVCe6elp9fX11a07bj7n5NUMf6aq/X47Vwyfe+65il5n23bVM5/USigU0szMTEuXVQvDw8PavHlzxZ+x398/oBkEAgFNTU25t8sAjc7v3wPNlFtV8/0mr2rssmqh2rxK8v/7B3pAAYCvBgYGdOrUqZyu7eXwO0man5/XkSNHWrqsWkilUkqlUhoYGPA7FAAAWh55VeOWVQvkVc2PBqgmlE6nlUgkFAqF/A4F8Ez+2AatyunifezYsZrcr18Pc3Nzuu6666qe/rcZyqqF8+fP6/jx45qYmPA9sQVQGnkV2kU75FbkVY1ZVi2QV7UGGqCa0NGjR9XT01PyPuBmYdt2Tab6TKVSGh8fVygUqqi87AH88h9jY2NKJpNlz6LR6GpV1/WUP7ZBK+vo6NDk5KTefvttv0Mpy549e9yBPlu1rFpIJpN6/vnn1dHR4XcoAFZBXrXSGDE+Pu7mQc4A1ZUgr2p87ZJbkVc1Xlm1QF7VGmiAakKvvPKK3yHUxOnTp9ddxtjYmIaHh7V161b9v//3/yq6n90Yo+XlZfd5JpNxpyLdt2+fxsfH1d/f3xJXiGpR1/VWanrYVhUMBqu6lx2N67nnniNJAppAu+dVtm27t7M4udH09HTFgxiTVzW+dsqtyKtaD3lVa6ABCr6wbVvj4+PrKmNwcFCZTMadOjZ/is5yZP8Ry+7K2dnZqYmJCUkr95I38xW7WtQ1AABoXOs517/11ltKJpM6cOCApJXcaGRkRKOjozlTp5eDvAoAsBoaoJqAbdtKJBIKBAIKhUI6f/58zvp0Oq1kMqlQKCTbtjU4OJhz1Sr79YFAQOPj4wX3gDuvl+R2wR4cHCzYVznlZXe5LrUsGo26Xd3zty2H8/5GRkZK3gM8PDy8rilIOzo69N3vflfJZNK90tWOdQ0AQCshr8o1PT0tKbfB6Oabb5YknTx50l1GXkVeBQDrRQNUE+jv79epU6eUyWQ0MzOjf//3f89ZPzAwoFAopGQyqf/8z/9UOBzWf/3Xf+W8/o9//KPbNTqZTOZcfdqyZYv7+vn5eR06dEiZTEaStHPnzoIT+FrlZXe/diwuLuY8HxkZcf9faTfgVCql0dFR7d+/3000QqFQxVfpynH33XdLkt58801J7VfXAAC0GvKqXMXGvnIao44fP152OeUgrwKANmdQN729vaa3t7ei18zMzBhJ5ty5c+6yTCZjJJnsj895nslkcl4/OztrJJnl5WV32ZkzZ4wkE4/HC16fbWFhwUgy0Wi0JuWVirlS0WjUSDILCwvGmJX6CIfDRpI5c+ZMxeWtFUc71/XU1FRVr2tH1Xy/gdXw/QPWJslMTU2VvT15VSEnh8quk/WUR161OvKF8lX6/QbWwvfPfxsrbK9CnTlXiLJnIFht2sn8dU7X6ex78m+//XZJK12uu7u7S5bV2dkpSRoaGnIH8VtPebUyNDSUE18wGFQ4HNbx48d14sSJuk0V2g517XDGhUBpZ8+elURdoXaWlpb8DgFoOeRVhQ4ePKjjx4/rxRdf1AsvvKBgMOhOXx+NRj3fv6Md6trxzjvvkC+U6eWXX9ZPfvITv8NAizh79qwefPBBv8Noa9yC1+DW2/W52OudE3w10w3XurxacRKNWncVd7pkRyKRNbdtl7oGAKBZkVcV2rVrl2ZnZ/Wb3/xGmzdv1vj4uD7++GNJ0r59+2q6L/IqAGhv9IBqcZZlKZlMKp1OF0xbGQ6Hyyoje7talLdeTm8n27YLrpZZllXTff3sZz+TJD388MNrbtuKde149dVX67q/ZtTX1ydJmpqa8jkStIrp6Wn3uALQGFr1XL9nzx7t2bPHfT42NqZIJOJe4KsV8qoVDz74IPlCGQKBgJ599ln19vb6HQpaBHmV/+gB1eBisZgkuV2hK+X8wb5w4YK7zLn61NXVteprnYEb9+/fX5PyasXZz69+9auCGGp5gkqn03rppZdkWVZOUlZKK9Y1AACthLxqbYlEQqdOnXKHPKgV8ioAAA1QDe6RRx6RtDL1rTMeSPZsb4ODgznTx+Z79NFHZVmWjh075m731ltvKRwOFz35JxIJSSsn5MnJSVmWldOrqNzynCtJTgIwPz+fE7N0ubdSOp3W2NhYWfUhrVyli0QiGh4edmN49dVXZVlWzv375UwX7CQe+f9PpVIaGBiQJE1MTLjL262uAQBoJeRVxdm2rVQqpcHBQf3mN7/RzMxMQS9z8iryKgBYN79HQW8n1Y66v7i46M5QEg6HzfLysrEsy8TjcbO8vOzOxCHJWJZV8Prl5WUTi8XcbeLxeMFMI866hYUFY1mWkWRisVjBduWWt7i46JYzMzNjjDE5MRtzeYaSSCSSMyNJubJjKBZrJBIxkUik5Ouz6y3/EY1Gi86o1251zSxc5WNWDdQa3z9gbapilizyqlzZuZQzw3Ax5FW1yWHJF8pXzfcbWA3fP/8FjDGm+uYrVKKRx4gJBAKSJA4H7zVTXTtj0DRDrH5r5O83mhPfP2BtgUBAU1NTDTdGTDOd65tds9U1+UL5GvX7jebF989/3IIHAAAAAAAAT9EAhZz771e7Fx/rR12jFMaRaD1jY2M5Y6AAaA+c6+uHukYp5FWth7yqNdAABW3ZsqXo/+stEAiU9WhmjVLXzcK2bU8/c6/LL1c6ndbRo0d15513usd5qYFem+k7sbS0pMHBQQUCAQ0ODuYM9Lte4+PjVb9327Y1Pz+v8fFxhUKhotuk02kNDw+7dewMbpsvmUwqFAopFAopmUzmrNu3b5/6+/v5UQS0mUY515NXoZh2yK3IqyrndV4lrUxGkF3PzqD+2cirWh8NUJAxJufRKHGUejSzVnov9XD69OmmLr8ctm1rYGBABw8e1J49e5TJZBSPxzU6Olo0WTLGaHl5WZK0vLzcsMeRM6PSK6+8okwmo927d2vv3r0FyUQ1UqmUDh8+XPXro9Go3njjDR0+fLhoPOl0WhcuXNDIyIiMMYrH4+rp6Sm4kppIJDQ+Pq7JyUlNTk7qzTff1Pj4uLu+s7NTR44c0cDAAFfsgDbSKOd68ioU0+q5FXlV5bzOqxzvvfdezvP9+/fnPCevag80QAFoSLZt55x0mq38ck1MTKizs1O7du2SJAWDQXV3d0uSRkdHi/a86ejoyPm3EZ0+fdqdpjr7Pa12Zawctm3rtddeW1cZIyMjGhkZKbn+woUL7uchyY19aGjIXba0tKSenh4dOXJEwWBQwWBQ4XBYhw8fViqVcrfbtWuXbrjhhpxpxwEA8EM75FbkVZWpR17l2Lp1a06DsfN+JPKqdkIDFICas21biUTC7WI7Pj6e0122WDfn/GXRaNS9iuIsT6fTbtdc6XJ34cHBQZ0/f37d5UvS8PBwyW7atZZOpzU0NKSHH3646PpoNKqenp6St3/lW6ve0+m0EomEW3/JZFKBQEChUEhLS0sFsY2NjbnrK+3mnZ1UZAuHwxWVk29iYkLPPPPMuspYS3bjkyT3KlskEnGXvfvuu5Kkbdu2ucuuv/56SYVX+Lq6ujQ0NESXcQBA1cit1kZeVbl65FXSSgNTKBTS8PCw5ufnC9aTV7UPGqAA1Fx/f7/++Mc/ut2ak8lkTndZp6tztsXFxZzn2VdSnCslW7Zsce8Jn5+f16FDh5TJZCRJO3fudBOlasuvt7Nnz0qSbr311qLrn3vuOUUiEfX09ORc/SllrXofGBhQT0+PW3+WZWlxcVHJZFI/+MEP3HLS6bQGBgZ0ww03yBij7373u9q7d29ZMZTixJDf3boSc3NzeuCBB+p6hXJpaUnRaFTSSv06Tp06JUnavn27u8yJK7/7ufP5Op83AACVIrdaG3lVZeqZVznvdXR0VPfdd59CoVBOAxJ5VRsxqJve3l7T29vrdxhA2aampkylfyZmZ2eNJLO8vOwuO3PmjJFk4vG4u0xSQdn5y8rZxhhjFhYWjCQTjUbXXX61qvl+RyKRkvt3lmcyGWNZlpFkzp07V7DeUct6j8fjRbeJRCIVvb/8+CzLMplMpqrXLy8vm1gslhPPej+7tcpYXFx0tynn+Cq1PJPJFLy+HNV8/4B2I8lMTU35HQZQtmryhXbNrSr9fpNXlc+PvCqTyZiFhQX3cypn/7XMq4zh93gjoAcUgJo6efKkpNz76G+//XZJ0vT0tCf77OzslJQ7Rk8zGB0dXXObYDDo3ue+WnfjWta7s31+1/py4i3lpZdecu/rr8brr7+uQ4cOVb3/amzfvl3GGC0sLCgSiWhoaKiqsS2c99xsxycAoDGQW5WHvKp8fuRVwWBQnZ2dGhkZUSwWq3oAdfKq5kYDFICaOn78eMEy50RRi5k62lFHR4cWFhYKun5nq2W9O9ubGs2WlEgkZFlWwdhKlcTzyCOPVPXaWujs7HRvv3NmiSk1FoO0/vEYAADIRm5VW+RV/uZVknTgwIGceiSvah80QAGoKecEUuyKktcnkFY+QXV2dmpmZkbJZNIdkyibF/WePfhotVKplN5///11XWULhULasWNHyQFQ6+G2227LeV6svp0BR++66666xAQAaA/kVrVHXuVvXuXMcucgr2ofNEABqKne3l5JK1PZO5wrS11dXZ7s0zmhr2cgRj84CU+xK2/FWJaleDxetMt2Les9FotJkiYnJ90ynNlbKpFOp/X222/nDEqaSqU0ODhYUTmrXTGs9uphpZx6iMfjkuReOcyu748++ihnXb7sWfQAACgXuVV5yKvK0yh5VXY9kle1DxqgANTUo48+KsuydOzYMfcqxltvvaVwOKw9e/a42zlXPZwEJ3tKVudEmn01JP8k7Uyha9u2JicnZVlWTvfdasuv11TB0uVeNfmJklNvxa66dXd3Fz3hllPv2eU5+8zet7P+8ccfl7QyNsHmzZsVCAS0ZcsWN1FwphFebfYWZ8aXoaGhnCtsd9xxR04yW05Z5aqkrOz3nV//oVBIY2Nj7pU327YVjUYViUTU3d0taWV8qFgsphMnTsi2bdm2rRMnTigWi+XM4CJdvoJ3zz33rOv9AQDaE7lVecirGjOvSiQSmpubc58vLS3p9OnTOccueVUbqeOA522PUffRbKqdhcuZWUP/N3NFPB4vmKVjcXHRnYVkZmbGGGOMZVkmHo+7M444M7BEIhF3mVPmwsKC+/pYLFaz8iORSFWzklTz/V5eXjaSzJkzZ9xlzvvLfhRjWVbR8lar92LlltrX4uKiO0tJOBw2i4uL7rpIJGLC4XDRGBzhcLjoe1HerDPllFVMsbopt6xScTlmZmYKZr/L/oyyOdtalmVmZ2eLbuPMmpM9k045mAUPWJuYBQ9NptrfA+2YW1X6/Savavy8KhKJmIWFhZJleZlXGcPv8UYQMKZO/eygvr4+SdLU1JTPkQDlmZ6eVl9fX92645bDuTe9kWKSqv9+O1cHn3vuuYpeZ9t21TOf1EooFNLMzExLl1ULw8PD2rx5c8WfcSN+/4BGEwgENDU15d4uAzS6Rvw90Ki5VTXfb/Kqxi6rFqrNq6TG/P61G27BAwAfDQwM6NSpUznd2Mvhd5I0Pz+vI0eOtHRZtZBKpZRKpTQwMOB3KAAAtDzyqsYtqxbIq5ofDVAAmkb2vfbF7uNvRsFgUBMTEzp27FhN7tevh7m5OV133XVVT//bDGXVwvnz53X8+HFNTEz4ntgCAFBMq+VW5FWNWVYtkFe1ho1+BwAA5dqyZUvO/xutq3i1Ojo6NDk5qYmJCXV2dvodzpqyB41s1bJqIZlM6vnnn1dHR4ffoQAAUFQr5lbkVY1XVi2QV7UGGqAANI1WSIpKCQaDVd3LjsbF5wkAaHStmluRV7UePs/WwC14AAAAAAAA8BQNUAAAAAAAAPAUDVAAAAAAAADwFA1QAAAAAAAA8BQNUAAAAAAAAPBUwLTq1AcN6Nvf/rZ+/OMf+x0GAKCJcJoGSgsEAn6HAABoIt/61rf0ox/9yO8w2hYNUHX04Ycfan5+3u8wgIZz4MABPfvss3rwwQf9DgVoKDfeeKPuu+8+v8MAGtaZM2f061//2u8wgIbyzjvv6OWXX9arr77qdyhAw9m1a5duuukmv8NoWxv9DqCd3HTTTRzsQAn33nuvurq6/A4DANBEaKAFCl28eFGSyKsANBzGgAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIBWndrjAAAgAElEQVQCAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACAp2iAAgAAAAAAgKdogAIAAAAAAICnaIACAAAAAACApzb6HQCA9vI///M/+u1vf1uwPJ1O68KFC+7zYDCoz33uc/UMDQAAoOl8/PHHsm3bfZ5OpyUpJ6+SpOuvv16f/exn6xobAGQLGGOM30EAaB/f+9739NJLL5W1LX+eAAAAVhcIBMraLhKJaGRkxONoAKA0bsEDUFd33XXXmtsEAgHdf//9dYgGAACgud1///1lNULddtttdYgGAEqjAQpAXT3xxBO66qqr1tzumWeeqUM0AAAAza2cnOmqq67SE088UYdoAKA0GqAA1NU111wjy7K0cWPpIeiuuuoqWZZVx6gAAACak2VZq17c27hxoyzL0jXXXFPHqACgEA1QAOqut7dXly5dKrpu06ZNeuKJJ3T11VfXOSoAAIDmc/XVV+uJJ57Qpk2biq6/dOmSent76xwVABSiAQpA3e3fv79kA9PFixf11FNP1TkiAACA5vXUU0/p4sWLRdddffXV2r9/f50jAoBCNEABqLurrrpKBw4cKHql7tprr9VXv/pVH6ICAABoTl/96ld17bXXFizftGmTDhw4UNb4mwDgNRqgAPiir6+v4Erdpk2b9OSTT5bsQg4AAIBCpXKoixcvqq+vz6eoACBXwBhj/A4CQPu5dOmStmzZoo8//jhn+b/8y79o9+7dPkUFAADQnE6dOqW//Mu/zFn2uc99TsvLy9qwYYM/QQFAFnpAAfDFhg0b9NRTT+nKK690l23dulUPPfSQj1EBAAA0p4ceekhbt251n1955ZV66qmnaHwC0DBogALgm97eXn3yySeSVpKk3t5eXXEFf5YAAAAqdcUVV6i3t9e9uPfJJ58w+x2AhsIteAB8tWPHDi0tLUmSfvrTn+ruu+/2OSIAAIDm9LOf/Uxf+cpXJEnbt2/X4uKizxEBwGV0NQDgq/7+fknSzTffTOMTAADAOtx99926+eabJV3OsQCgUWz0O4B2lEwmNTk56XcYQEP4wx/+IEn63//9Xx04cMDnaIDGcOutt+rYsWN+hwE0JPIoYHXODS4//elPya2AEjZs2KAXX3wxZ9w0eI8eUD5IJBI6efKk32EAJS0tLdXtGL322mv1la98Rffee29d9ldrZ8+e1dmzZ/0OAy3k5MmT+sEPfuB3GEDDIo9Cqzh58qQ7DEEtdXZ26itf+Yquvfbampfth3rmpWgfiURCc3NzfofRdhgDygd9fX2SpKmpKZ8jAYqbnp5WX1+f+POwNr7PqDW+f8Dq+LuLVhEIBDQ1NcVA4WvgvAgv8P3zBz2gAAAAAAAA4CkaoAAAAAAAAOApGqAAAAAAAADgKRqgAAAAAAAA4CkaoAAAAAAAAOApGqAAeGp4eFjDw8N+h9Gw0um0xsbG/A4DNTQ2Nibbtv0OAwDQJsi1Vkeu1XrItZoXDVAAWppt2woEAn6HUVQ6ndbRo0d15513KhAIKBAIlEwgnfXZj0a1tLSkwcFBBQIBDQ4Oam5urmZlj4+PV/3ebdvW/Py8xsfHFQqFim6TTqc1PDzs1nEikSi6XTKZVCgUUigUUjKZzFm3b98+9ff3K51OVxUnAADNhFyr/po515KkVCqVU8+Dg4MF25BrtSYaoAB4amRkRCMjI77t//Tp077tezW2bWtgYEAHDx7Unj17lMlkFI/HNTo6WjQxMsb8f/buP7aN+77/+ItNnLULOgpZITtO62BBGiNAO6YJ5jj9scB2gCBOjmm3ytWPqukG2aCGJstgYVg1CkYg18kACg3SLxCDErYZgkSiDrBURBIMsATY6Gw5Qwtyrf+wUTilumQhgaLkAhRIs+Tz/UO9M38cpSNF8kjq+QAIm3fHz33uwzvdm5/P5z4f5XI5SVIul5Mxpt1Z9qRYLCqTyejll19WoVDQww8/rEOHDlUFDo3IZDI6duxYw5+PxWJ67bXXdOzYMdf85PN5Xb9+XdPT0zLGKJFIaGhoqKrVNJlManZ2VvPz85qfn9frr7+u2dlZZ30oFNLk5KTGxsZonQMAtByxljtirfq1Otayvfnmm2XvDx8+XPaeWKt3UQEFoGcVi8Wym1UnmZubUygU0v79+yVJwWBQg4ODkqSTJ0+69rzp7+8v+7cTXbhwQZZlSSo/po1awbwoFot65ZVXtpTGZgH69evXne9DkpP3iYkJZ9na2pqGhoY0OTmpYDCoYDCoSCSiY8eOKZPJONvt379fd9xxh+bm5raUZwAAOhmxVvt1c6xl27Vrl4wxzss+HolYq9dRAQWgZfL5vJLJpHNDrHyfSqUUCAQUDoe1trbmbGN3uZVudAMeHx/XtWvXnLTdukdXLovFYk7rS+lyv8dKyOfzmpiY0IEDB1zXx2IxDQ0N1Xz8q1KxWFQymXSOcXZ2tqxLspdyL912ZmbGWV9vl+7SAKJUJBKpK51Kc3Nzevrpp7eUxmZKK58kOS1q0WjUWXbx4kVJ0u7du51lt99+u6Tq1ryBgQFNTEzQPRwA0DLEWu6IterXjlhLWq9gCofDmpqa0urqatV6Yq0eZ9B2w8PDZnh42O9sADUtLCyYZvx5sCzLSHLSKn1/6dIlY4wx2WzWSDKRSMQYY5z1pdsUCgUTiUSMJHP16lVjjDG5XK4s7dK0SpdVvjfGmGg0aqLR6JaPz5jGruelpSUjyWSz2ap1dl6j0aiRZNLptOv6UpZlmXg8boxZLxfLsoxlWaZQKDjrNyv30s8mEgljjDHLy8uueahHoVAwkszS0lLDaSwvLzv5dvs+6+UljWw263wH9jlnjHHOQ7c0LcuqSqORY2/W9Qf0KuIo9ApJZmFhYUtpbIdYq5H7IrFWfdoZa9nfjf2yLMvkcjlnfTtiLTu9rV5/qB8Rrg8InNDpmvkD2EuQ4mWbdDptJJlYLLbltJqpkevZDnjc2MsLhYITzJRWgFR+zg5cSm/cly5dMpKc4Mb+3GZllUgkXLfZSgC5vLxcFqDVK5fLOQGfnZ9WV0CVBtdezrlay+2AsPTzXlABBWyMOAq9olk/gHs91mrkvkis5Z0fsVahUDDpdNr5nrzsv5mxlp0eFVDtxyN4ALpCKBSSVD4eT7c6efLkptsEg0HnmfaNuhafPXtWUvlYBffee68kaXFxsa582dtXdq/3kt9aXnzxRecZ/kb86Ec/0tGjRxvefyP27NkjY4zS6bSi0agmJiYaGt/CPuZeOGcBAL2PWItYq12CwaBCoZCmp6cVj8cbHkCdWKv7UAEFAB2qv79f6XRaqVSq5iwfp0+frlpm34zrvZnb25uSQSHtVyOSyaQsy6oaW6me/Dz66KMNfbYZQqGQRkdHJcmZEabWuAvS1sdeAAAA7UWs5W+sJUlHjhwpK0dird5GBRSArrLdbjyhUEhLS0tKpVKKxWJV6+2btFurXaNlVToAaaMymYyuXLmypRa1cDisO++8s+YgqO1wzz33lL13K297cNH777+/LXkCAKCViLXKEWu1lj3LnY1Yq7dRAQWgK9g36sOHD/uck62zgxu3VjY3lmUpkUi4ds8eHh6WJF2/ft1ZZqc7MDBQV77i8bgkaX5+3knDnqmlHvl8XufOnSubhjeTyWh8fLyudDZqHWy0pbBedjkkEglJcloJS8v7nXfeKVtXqXQWPQAAOhWxFrGWX7FWaTkSa/U2KqAAtEzl9LSl7+2bbmlgUNmyZE+NWywWNT8/L8uyyrrl2q0ldsBUOpWrfQMubUWxb+5+Tw1s96qpDIrs43drYRscHHS9uT722GOyLEunTp1yPvfGG28oEono4MGDVeltVO5PPvmkpPVxCPr6+hQIBLRz504nKLCnDM5kMjWPLZ/Pa2xsTBMTE2Wtaffdd19ZQOslLa/qSav0uCvLPxwOa2ZmxmllKxaLisViikajGhwclLQ+PlQ8HteZM2dULBZVLBZ15swZxeNx7dmzpyw9O519+/Zt6fgAAKiFWMsdsVZnxlrJZFIrKyvO+7W1NV24cMEpR4lYq9dRAQWgZXbu3Fn2/9L3fX19Zf9Wbi+tD/AYDofV19enPXv2aH5+vmz9d7/7XVmWpb179yqVSmn//v1OC9Zzzz0nSU7L0A9+8ANnPB+/Pfjgg5JutOZIcgIQab0c3Lo9T09PVz0Xbw+gaVlW2edeeOEFZxuv5d7f369sNusEX5FIRNls1rnZFwoFRSKRDQPKEydO1BwPYe/evc7/vaTllde0AoFA2XHbgZ/t6NGjmpiYcLqiz83N6fHHHy9rXbS3O3z4sPr6+jQ6OqqBgQHX7u/292t/3wAANBuxljtirc6MtW699VYdOnRIgUBAU1NT+s1vfuM65hOxVu8KmHb1rYNjZGREkrSwsOBzTgB3i4uLGhkZaVvX20r2jaob/jw1ej3bLYTHjx+v63PFYrHhWU6aJRwOa2lpqafTaoapqSn19fXV/R37ff0BnY44Cr0iEAhoYWHBecSr3fuWuiPWavS+SKzV2Wk1Q6OxluTv9bed0QMKAHwwNjam8+fPl3Vl98LvgGh1dVWTk5M9nVYzZDIZZTIZjY2N+Z0VAAC2JWKtzk2rGYi1uhMVUAA6SuVYBr3K7s596tSppjyb3w4rKyu67bbbGp7qtxvSaoZr167p9OnTmpub8z2IBQCgErFW5+rU+IhYC81CBRTaLp/PK5lMKhwO+50VdKDKsQx6WX9/v+bn53Xu3Dm/s+LJwYMHnUE9ezWtZkilUnruuefU39/vd1YA9CDiKGwVsVbn6tT4iFgLzUIFFBq2tram8fFxBQIBjY+Pl81osJETJ05oaGio5uB5m0mlUgqHwwoEAgqHw87sHY1aXV3V1NSUM4PE1NSUMpmM8vm86+CE7bJZ+ZbOelH5mpmZUSqV8jz1bCepNRVsrwoGgw09t47Odfz4cQIiAJvyI44qFotaXV3V7OxszQosL9vUi1irsxBrodsRa3UvKqDQkGKxqEwmo5dfflmFQkEPP/ywDh065CkYevnllxve78zMjMLhsKanp2WM0fT0tIaGhpxBBus1NTWlM2fOaHR01LkJP/3001pbW/O1RchL+RpjlMvlnPeFQsE5hkceeUSzs7MaHR3t6a7VAAB0I7/iqFgsptdee03Hjh2ruS8v29SDWAsAYGMWPB/0wuwtqVSqasrMembTaHTmDbfPBQIBWZZV94wMdutbrc+trq7qoYce8qVlqJ7yrbU8n887g/LNz8/X9Xw0s3B51wvXMzoL1x+wsV74u+tXHFXP55sxSxqx1saYhcsb7otoBa4/f9ADqosUi0Ulk0mn6+/s7KynbSoHGiwdNyCVSjmPsq2trWl1dbWqi7FtZmbGWRYKhVzzGIlENsxTOBzWtWvXGi6DWCwmSc5sFmtra5Kk6elpZ5upqSlNTU1tmM7q6qpOnjy54UwOboPsdWL51tLf369nn31WqVRKFy5c8Pw5AAB6EXFU8xBrrSPWAoD6UAHVRUZHR3XlyhWn6+9Pf/rTqpv/6Oio3nvvPafLcCqV0tjYmPN8+tjYmDNuwOrqqizLUjabVSqV0vPPP6/9+/dreXlZkhSNRstaGo4fP65oNKp0Oq09e/aU7ddO//Dhw675Pn/+vAqFgpaWlvTTn/604TKw8/DQQw9pdXVVFy9eVC6XqxlE1PLaa69Jku66664Nt6tsaenE8t3IAw88IEl6/fXX6/ocAAC9hjiqvYi1AABVDNpueHjYDA8P1/WZRCJhJJlcLucsu3TpkrEsy3m/vLzsuo0kk0gknGWSTOVXX7ksGo0aSaZQKDjLCoWCiUajrvlbXl42lmWVbW+MMUtLS0aSuXr1alk6bnmoRyQSMZJMNBqt2qcXjey/E8vXy7E0cqwLCwtb+n62k0auZ2AjXH/Axoijth5Hefl8O/ZRqRO/Ay/H0mhZSTILCwt1f2674b6IVuD68wdXsg8aCZwsy9r0D69dKVPKDlJKAywvN+10Ol11s19eXjbpdLpm/i5duuQpT7Xy4FUsFjOJRMIJImoFCxtpZP+dWL5ejmUrFVC8ePHy7wXAHXFUeyqH2rGPSp34HXg5lkbLyu/7DC9e2/1FBVT7MQi5DxoZPHMrg0VWLq81kHflMvvZenvgyKmpqbKxlmzJZFLvvfeejh492nCevEomkxoaGlKhUFAwGNS1a9e0d+9exeNx1/3XMj4+rtOnTzvpeNGJ5btRvqT17uR9fX2KRqOuaddiD/b4wx/+0PNntquXXnpJkvTMM8/4nBP0ih//+Md66aWXGGwVqIE4qjsGISfW2lwgENAzzzyjL3/5y3V9brux74vEpWimI0eOMAi5D272OwPwxrIspVIpZTKZmuMd2dvk83n19/eXratnQEXb8PCwhoaGtLq6qt27d2vfvn1V22QyGV25cqXuG26jhoaGJMkJZOzpe48dO1ZXBdThw4d1+vRp/fKXv/Q8flQ3lu9PfvITSdKBAwca+vzAwEBDn9tOXn31VUmUFZrngw8+8DsLQM8hjmo/Yi1vHnzwQWKITdj3RcoJ6H4MQt4l7GliT58+7QySuLa2pvHxcWcbu/b2+vXrzjJ720b+YB88eFCSdObMGV28eFF//ud/XrY+n8/r3LlzZTfsTCZTlqd4PO4sb4bK6XLtiqjK5V7SsSxLp0+frrnN2tqaZmZmnPedWL4byefzevHFF2VZlrMvAAC2I+Ko9iPWAgBUacuDfijTyNgFuVzOGb/AfkUikapBKS3LMpZlOYM3JhIJE4lEytKxP2+Pm1Q6mGXpoI/G3BjAMRaLbZof+7W0tORsl81mjbT+3H42mzXG3Bhg0j6GetiftZ/ntwemXF5eLstzrQEm3Y6hshztfJeWozGdWb6laZeOg5VOp6vyWg8Ge/SOQcjRbFx/wMaIoxqPoyrzW2sMzc22IdbaeqxlDIMge8V9Ea3A9ecPrmQfNPqDNZfLOTfRaDRadSO3t4nH486N0h6s21Z5g621zGYP4Fi5L3uQSLeXW4Bhbx+JRJwbfiKRaOiGvby8XJZeaeWTMd6DImPWg4qlpaWy47Esy8TjcSfQK9VJ5VtrvR1k1RpI0wtu9N5RAYVm4/oDNkYc1XgcVWuf9W5DrLX1WMtOnx/Am+O+iFbg+vMHg5D7oJHBM4F2sgch58/D5rie0Wxcf8DG+LuLXhEIBBgE2QPui2gFrj9/MAYUAAAAAAAAWooKKADwUT6fLxuAFd1vZmbGGTQXAAD4i1ir9xBrdS8qoOC7QCDg6YXto1gstvQ7b3X6XuXzeZ04cUJf+MIXnPN8amrKddtuuibsmaUCgYDGx8e1srLStLRnZ2cbPvZisajV1VXNzs4qHA67bpPP5zU1NeWUcTKZdN0ulUopHA4rHA4rlUqVrXvkkUc0OjqqfD7fUD4BoB7EUWgEsVa1brpuujnWktZnmywtZ7eZJ4m1ehMVUPCdWR8Mf9MXto8LFy50dfpeFItFjY2N6amnntLBgwdVKBSUSCR08uRJ18DIGKNcLidJyuVyHXtNFItFZTIZvfzyyyoUCnr44Yd16NChqsChEZlMRseOHWv487FYTK+99pqOHTvmmp98Pq/r169renpaxhglEgkNDQ1VtZomk0nNzs5qfn5e8/Pzev311zU7O+usD4VCmpyc1NjYGK1zAFqOOAqNINYi1nLT6ljL9uabb5a9P3z4cNl7Yq3eRQUUgI5SLBbLbjDdlr5Xc3NzCoVC2r9/vyQpGAxqcHBQknTy5EnXnjf9/f1l/3aiCxcuyLIsSeXHtFErmBfFYlGvvPLKltKYnp7W9PR0zfXXr193vg9JTt4nJiacZWtraxoaGtLk5KSCwaCCwaAikYiOHTumTCbjbLd//37dcccdmpub21KeAQBoNmItYi037Yi1bLt27SqrILePRyLW6nVUQAFommKxqGQy6XSnnZ2dLesa69aluXJZLBZzWkzs5fl83umGK93oGjw+Pq5r165tOX1Jmpqaqtklu9ny+bwmJiZ04MAB1/WxWExDQ0M1H/+qtFm55/N5JZNJp/xSqZQCgYDC4bDW1taq8jYzM+Osr7dLd2kAUSoSidSVTqW5uTk9/fTTW0pjM6WVT5KcFrVoNOosu3jxoiRp9+7dzrLbb79dUnVr3sDAgCYmJugeDgBoGmItb4i16teOWEtar2AKh8OamprS6upq1Xpird5GBRSAphkdHdV7773ndGFOpVJlXWPtbs2lstls2fvSVhO7VWTnzp3O89+rq6s6evSoCoWCJGnv3r1OYNRo+u12+fJlSdLdd9/tuv748eOKRqMaGhoqa+mpZbNyHxsb09DQkFN+lmUpm80qlUrp+eefd9LJ5/MaGxvTHXfcIWOMnn32WR06dMhTHmqx81DZtboeKysr+tKXvtTW1si1tTXFYjFJ6+VrO3/+vCRpz549zjI7X5Vdze3v1/6+AQDYKmItb4i16tPOWMs+1pMnT+qhhx5SOBwuq0Ai1upxBm03PDxshoeH/c4GUNPCwoKp98/D8vKykWRyuZyz7NKlS0aSSSQSzjJJVWlXLvOyjTHGpNNpI8nEYrEtp9+oRq7naDRac//28kKhYCzLMpLM1atXq9bbmlnuiUTCdZtoNFrX8VXmz7IsUygUGvp8Lpcz8Xi8LD9b/e42SyObzTrbeDm/ai0vFApVn/eikesP2E6Io9ArJJmFhQXP22/XWKuR+yKxlnd+xFqFQsGk02nne/Ky/2bGWnZ69Vx/aA56QAFoirNnz0oqf2b+3nvvlSQtLi62ZJ+hUEhS+Rg93eDkyZObbhMMBp1n2jfqWtzMcre3r+xK7yW/tbz44ovOM/yN+NGPfqSjR482vP9G7NmzR8YYpdNpRaNRTUxMNDSWhX3M3XZ+AgA6E7GWd8Ra3vkRawWDQYVCIU1PTysejzc8gDqxVvehAgpAU5w+fbpqmX1TaMasHNtRf3+/0ul0VTfvUs0sd3t706TZk5LJpCzLqhpbqZ78PProow19thlCoZDz+J09I0ytcRekrY+9AADARoi1mo9Yy99YS5KOHDlSVo7EWr2NCigATWHfLNxaj1p9s+jlm1EoFNLS0pJSqZQzJlGpVpR76WCjjcpkMrpy5cqWWtTC4bDuvPPOmgOetsM999xT9t6tvO3BRe+///625AkAsD0Ra7UGsZa/sZY9y52NWKu3UQEFoCmGh4clrU9lb7NbkQYGBlqyT/vmvZVBF/1gBzdurWxuLMtSIpFw7Z7dzHKPx+OSpPn5eScNe6aWeuTzeZ07d65sENJMJqPx8fG60tmodbDRlsJ62eWQSCQkyWklLC3vd955p2xdpdJZ9AAAaBSxlnfEWt50SqxVWo7EWr2NCigATfHYY4/JsiydOnXKabF44403FIlEdPDgQWc7u4XDDmhKp1+1b5qlLR+VN2R7utxisaj5+XlZllXWVbfR9Ns5NbDdq6YyKLLLza2FbXBw0PXm6qXcS9Oz91m6b3v9k08+KWl9HIK+vj4FAgHt3LnTCQrsKYM3mqnFnt1lYmKirDXtvvvuKwtevaTlVT1plR53ZfmHw2HNzMw4rWzFYlGxWEzRaFSDg4OS1seHisfjOnPmjIrFoorFos6cOaN4PF42W4t0o7Vu3759Wzo+AAAkYq16EGt1ZqyVTCa1srLivF9bW9OFCxfKzl9irR7XxgHP8XvM3oJO1+gsXPYsGvr9LBWJRKJqRo5sNuvMOLK0tGSMMcayLJNIJJzZRewZV6LRqLPMTjOdTjufj8fjTUs/Go02NANJI9dzLpczksylS5ecZfbxlb7cWJblmt5G5e6Wbq19ZbNZZ0aSSCRistmssy4ajZpIJOKaB1skEnE9FlXMMOMlLTduZeM1rVr5si0tLVXNflf6HZWyt7UsyywvL7tuY8+QUzprjhfMggdsjDgKvUINzMK1HWOtRu6LxFqdH2tFo1GTTqdrptXKWMvOK7PgtV/AmDb1rYNjZGREkrSwsOBzTgB3i4uLGhkZaVvXWy/s59A7KU9S49ez3Rp4/Pjxuj5XLBYbnuWkWcLhsJaWlno6rWaYmppSX19f3d9xJ15/QCchjkKvCAQCWlhYcB7x8lunxlqN3heJtTo7rWZoNNaSOu/62y54BA8AfDA2Nqbz58+XdVv3wu+AaHV1VZOTkz2dVjNkMhllMhmNjY35nRUAALYlYq3OTasZiLW6ExVQADpe6XP1bs/sd6NgMKi5uTmdOnWqKc/mt8PKyopuu+22hqf67Ya0muHatWs6ffq05ubmfA9iAQDwglirM3RqfESshWa52e8MAMBmdu7cWfb/Tusa3qj+/n7Nz89rbm5OoVDI7+xsqnSAyF5NqxlSqZSee+459ff3+50VAAA8IdbqDJ0aHxFroVmogALQ8XolCHITDAYbem4dnYvvEwDQbYi10E34PrsXj+ABAAAAAACgpaiAAgAAAAAAQEtRAQUAAAAAAICWogIKAAAAAAAALcUg5D45e/asvvrVr/qdDcDV5cuXJa2fp9jY2tqaJMoKzcO5BGyOOAq94vLly9qxY4ff2ehoxKVA7wiYXp7yoENFo1F973vf8zsbAIAOdcstt+j999/3OxtARyKOAgA0w+XLl7Vv3z6/s7GtUAEFwHeBQEALCwsaHh72OysAAABdbXFxUSMjI+JnHoBOwxhQAOut758AACAASURBVAAAAAAAaCkqoAAAAAAAANBSVEABAAAAAACgpaiAAgAAAAAAQEtRAQUAAAAAAICWogIKAAAAAAAALUUFFAAAAAAAAFqKCigAAAAAAAC0FBVQAAAAAAAAaCkqoAAAAAAAANBSVEABAAAAAACgpaiAAgAAAAAAQEtRAQUAAAAAAICWogIKAAAAAAAALUUFFAAAAAAAAFqKCigAAAAAAAC0FBVQAAAAAAAAaCkqoAAAAAAAANBSVEABAAAAAACgpaiAAgAAAAAAQEtRAQUAAAAAAICWogIKAAAAAAAALUUFFAAAAAAAAFqKCigAAAAAAAC0FBVQAAAAAAAAaCkqoAAAAAAAANBSVEABAAAAAACgpaiAAgAAAAAAQEtRAQUAAAAAAICWogIKAAAAAAAALUUFFAAAAAAAAFqKCigAAAAAAAC0FBVQAAAAAAAAaKmb/c4AgO0lnU7r3//936uWp1Ip/epXv3Le33333frLv/zLdmYNAACg6/zwhz/UW2+95bxPp9OSpH/6p38q2+7xxx/X5z73ubbmDQBKBYwxxu9MANg+/vZv/1YvvfSS/uAP/qDmNu+//74kiT9PAAAAGwsEApK0aWz193//91WVUgDQTjyCB6Ct/uIv/kLSeiBU63XLLbfoO9/5js85BQAA6Hzf+c53dMstt2wYW0nS4cOHfc4pgO2OHlAA2uqjjz7SHXfcoXfffXfD7X784x/rS1/6UptyBQAA0J3+4z/+Q1/+8pc33GbXrl16++239bGP0f8AgH/4CwSgrT72sY9pZGREt9xyS81tdu/erS9+8YttzBUAAEB3+uIXv6jdu3fXXH/LLbdoZGSEyicAvuOvEIC2Gxoa0u9+9zvXdTt27NC3vvUtZzwDAAAA1BYIBPStb31LO3bscF3/u9/9TkNDQ23OFQBU4xE8AL646667ymZsKfVf//Vf+vznP9/mHAEAAHSnn/3sZ/rTP/1T13V/8id/ouvXr7c5RwBQjR5QAHzx7W9/27Wl7rOf/SyVTwAAAHX4/Oc/r89+9rNVy3fs2KFvf/vb7c8QALigAgqAL4aGhvTBBx+ULduxY4eeeuopn3IEAADQvZ566qmqxr0PPviAx+8AdAwewQPgm1AopJ/97Gey/wwFAgH94he/0F133eVzzgAAALrL9evXdffdd5fFVZ///OeVyWR8zhkArKMHFADfPPXUU7rpppskrQdJ999/P5VPAAAADbjrrrt0//33OxO53HTTTfQsB9BRqIAC4JvBwUF9+OGHktaDpNHRUZ9zBAAA0L1GR0edxr0PP/xQg4ODPucIAG6gAgqAb3bv3q2vfOUrkqSPPvpI3/jGN3zOEQAAQPf6xje+oY8++kiS9JWvfEW7d+/2OUcAcAMVUAB8NTIyIkl64IEHtGvXLp9zAwAA0L127dqlBx54QNKNGAsAOkXHDkL+5ptv6sEHH/Q7GwAAoIn+8R//USdPnvQ7G9tCNBrV9773Pb+zAQAA2ujy5cvat2+f39lwdbPfGajlF7/4hSTphz/8oc85AdBqxWJRf/RHf+QMmtlKL730kiTpmWeeafm+ut2RI0f0zDPP6Mtf/rLfWUGPGBkZ0VtvveV3NraNt956Szt27NDCwoLfWQHQRsYY/e///q+CwaAv+yd+8ObHP/6xXnrpJX7voqmOHDmiX/ziF1RANWpgYMDvLADoIa+++qok/rZ49eCDD1JWaBr7+kP7DAwMcA0DaDvih8198MEHkohJsb0wBhQAAAAAAABaigooAAAAAAAAtBQVUAAAAAAAAGgpKqAAAAAAAADQUlRAAQAAAAAAoKWogAKABk1NTWlqasrvbHSkfD6vmZkZv7OBJpqZmVGxWPQ7GwCAbYI4a2PEWr1nO8RaVEABQJcqFosKBAJ+Z6NKPp/XiRMn9IUvfEGBQECBQKBmAGmvL311qrW1NY2PjysQCGh8fFwrKytNS3t2drbhYy8Wi1pdXdXs7KzC4bDrNvl8XlNTU04ZJ5NJ1+1SqZTC4bDC4bBSqVTZukceeUSjo6PK5/MN5RMAgG7SqXGWRKzViFbHWpKUyWTKynl8fLxqm+0ea1EBBQANmp6e1vT0tG/7v3Dhgm/7rqVYLGpsbExPPfWUDh48qEKhoEQioZMnT7oGRsYY5XI5SVIul5Mxpt1Z9qRYLCqTyejll19WoVDQww8/rEOHDlUFDo3IZDI6duxYw5+PxWJ67bXXdOzYMdf85PN5Xb9+XdPT0zLGKJFIaGhoqKrVNJlManZ2VvPz85qfn9frr7+u2dlZZ30oFNLk5KTGxsZ6vnUOAOA/4ix3xFr1a3WsZXvzzTfL3h8+fLjsPbEWFVAA0JWKxWLZDatTzM3NKRQKaf/+/ZKkYDCowcFBSdLJkydde9709/eX/duJLly4IMuyJJUf00atYF4Ui0W98sorW0pjswD9+vXrzvchycn7xMSEs2xtbU1DQ0OanJxUMBhUMBhUJBLRsWPHlMlknO3279+vO+64Q3Nzc1vKMwAAnaxT4yyJWKte7Yi1bLt27ZIxxnnZxyMRa9mogAKABuTzeSWTSeemWPk+lUopEAgoHA5rbW3N2cbudivd6Ao8Pj6ua9euOWm7dZGuXBaLxZwWmNLlfo6XkM/nNTExoQMHDriuj8ViGhoaqvn4V6VisahkMukc3+zsbFmXZC9lXrrtzMyMs77eLt2lAUSpSCRSVzqV5ubm9PTTT28pjc2UVj5JclrUotGos+zixYuSpN27dzvLbr/9dknVrXkDAwOamJjo6e7hAAB/EWe5I9aqXztiLWm9gikcDmtqakqrq6tV64m1fs90qIWFBdPB2QPQpYaHh83w8PCW07Esy0hy/k6Vvr906ZIxxphsNmskmUgkYowxzvrSbQqFgolEIkaSuXr1qjHGmFwuV5Z2aVqlyyrfG2NMNBo10Wh0y8dnp7+wsOB5+6WlJSPJZLNZ17Ts/Eky6XTadX0py7JMPB43xqyXiWVZxrIsUygUnPWblXnpZxOJhDHGmOXlZdc81KNQKBhJZmlpqeE0lpeXnXy7fZf18pJGNpt1vgP7fDPGOOegW5qWZVWl0eixN+v6gzeUNwA/1Bs/uNkOcVYjv3eJterTzljL/m7sl2VZJpfLOevbFWs14/prpY6t4aECCkArNPMHmZdAxcs26XTaSDKxWGzLaTVTvTcwO+CplZYx68GEHcyUVoBUfs4OXEpv3JcuXTKSnODG/txm5ZRIJFy32UoAuby8XBag1SuXyzkBn52fVldAlQbXXs63WsvtgLD0815RIdJelDcAPzTrB3Cvx1mN/N4l1vLOj1irUCiYdDrtfE9e9t/sWKvTK6B4BA8AfBYKhSSVj8nTjU6ePLnpNsFg0HmmfaOuxWfPnpVUPlbBvffeK0laXFysK1/29pXd673kt5YXX3zReYa/ET/60Y909OjRhvffiD179sgYo3Q6rWg0qomJiYbGt7CPudvPVwDA9tArcZZErFUPP2KtYDCoUCik6elpxePxhgdQ7+VYiwooAEBb9ff3K51OK5VK1Zzl4/Tp01XL7JtxvTdze3tTMiik/WpEMpmUZVlVYyvVk59HH320oc82QygU0ujoqCQ5M8LUGndB2vrYCwAAoL2ItfyNtSTpyJEjZeVIrLWOCigA6BDb6eYTCoW0tLSkVCqlWCxWtd6+Sbu12jVaTqUDkDYqk8noypUrW2pRC4fDuvPOO2sOgtoO99xzT9l7t/K2Bxe9//7725InAABaaTvFWRKxlt+xlj3LnY1Yax0VUADgM/tmffjwYZ9zsjV2cOPWyubGsiwlEgnX7tnDw8OSpOvXrzvL7HQHBgbqylc8Hpckzc/PO2nYM7XUI5/P69y5c2XT8GYyGY2Pj9eVzkatg422FNbLLodEIiFJTithaXm/8847Zesqlc6iBwBAp+qVOEsi1vKqU2Kt0nIk1lpHBRQANKByitrS9/aNtzQ4qGxdsqfHLRaLmp+fl2VZZV1z7RYTO2gqnc7VvgmXtqTYN3g/pwe2e9VUBkX2sbu1sA0ODrreXB977DFZlqVTp045n3vjjTcUiUR08ODBqvQ2KvMnn3xS0vo4BH19fQoEAtq5c6cTFNhTBmcymZrHls/nNTY2pomJibLWtPvuu68soPWSllf1pFV63JXlHw6HNTMz47SyFYtFxWIxRaNRDQ4OSlofHyoej+vMmTMqFosqFos6c+aM4vG49uzZU5aenc6+ffu2dHwAANRCnOWOWKszY61kMqmVlRXn/drami5cuOCUo0SsZaMCCgAasHPnzrL/l77v6+sr+7dye2l9kMdwOKy+vj7t2bNH8/PzZeu/+93vyrIs7d27V6lUSvv373dasZ577jlJclqHfvCDHzhj+vjpwQcflHSjNUeSE4BI62Xg1u15enq66rl4ewBNy7LKPvfCCy8423gt8/7+fmWzWSf4ikQiymazzs2+UCgoEolsGFCeOHGi5ngIe/fudf7vJS2vvKYVCATKjtsO/GxHjx7VxMSE0xV9bm5Ojz/+eFnror3d4cOH1dfXp9HRUQ0MDLh2f7e/X/v7BgCg2Yiz3BFrdWasdeutt+rQoUMKBAKamprSb37zG9cxn4i1pIBpVx+0Oi0uLmpkZKRtXeQAbA8jIyOSpIWFBV/2b9+suuFvWyAQ0MLCgtNF2wu7hfD48eN17atYLDY8y0mzhMNhLS0t9XRazTA1NaW+vr66v2PJ/+tvu6G8AfihkfihmfuWuiPOavT3LrFWZ6fVDFuJtfy8/rygBxQAoGnGxsZ0/vz5sq7sXvgdEK2urmpycrKn02qGTCajTCajsbExv7MCAMC2RKzVuWk1Q6/HWlRAbQP5fF7JZFLhcNhZ5vfzy5Xc8oj26YZzpBdUjmfQi+zu3KdOnWrKs/ntsLKyottuu63hqX67Ia1muHbtmk6fPq25uTnfg1ig3brhPkks1fm64TzqZtshzpKItTo1rWbYDrEWFVDbwIkTJzQ0NFTzmdpmWltb0/j4uAKBgMbHx8sGY9tIo3ksFotlg9SVvuzBB72o/OxGLQqrq6tV2zdDreMIh8OanZ1t6Y20k86RWuUQCAQ0MzOjVCrleeaPTlM5nkGv6u/v1/z8vM6dO+d3Vjw5ePCgM6hnr6bVDKlUSs8995z6+/v9zgrQdp10n6xlK3ksFotaXV3V7OzshhVYqVRK4XBY4XC4of0Qb3XOedSL8dZ2ibMkYq1OTKsZtkWsZTrUwsKC6eDsdR1JLS/PQqFglpaWnP8nEgkjyVm2mUbyeOnSJedzla9cLldXWtls1vlsJBKpuV0kEml4H5vJ5XJV5ZDNZk00GjWSzNWrV5u6v1KddI6UlkOhUHCWp9NpY1mWsSyr4bIfHh42w8PDjR/ENiLJLCws+J0N9BCuv/aivJurk+6TtTSax2g06sQatT6fSCSMZVmmUCiYQqFgIpGIicfjde+LeKtzzqNWxVvED97wexet0OnXHz2g0DQXLlxwRvsPBoPO9N6t7Ar+y1/+UtlsVsYY55XL5RSNRuuuObZnaYjFYjp9+rQz/WWptbU13X333c77ZtdOu6W3Z88ePf3005Kk73//+03dX7t5PUdKy6G0+2koFNLc3Jyk9effu61lDgCAjfgRS0nrM2RVzoxZam1tTUNDQ5qcnFQwGFQwGFQkEtGxY8fqfgSIeKv1iLcAdKqeqYCqfKY6lUo5XU7tG1symaxaJq13O56dnXW6nU5NTTndb926/jbaHTifzztdlyU5+xwfH9e1a9eqti8Wi06eA4FAzW7BXrerVVa1yi4cDlcFBSsrKwqHw0733NL9uE01Ka1Pw7lRnsPhsOvxe3Hw4EEnkCnN49e//vWyZfU8X//II49Iki5evFi17uLFi876Sq08j+wA4fTp01X77NVzpJb+/n49++yzSqVSunDhgufPAQA2RizVXffJZsVSXtgx0e7du51lt99+uyTpzTffdJYRb3XfeVQL8RaAlvC5B1ZN9XZJtCzL6UKaTqeNMTcez4pEIubSpUvGmBvdfku7/NpdfHO5nOv6eDxe1v03l8sZy7Kc/Xhl50+Skx+7C7NcuvtaluV0bbb3aXd9rnc7lXT3LS2ryvcbldPS0lLZNnZ3XtXoSlwoFGp2G7csy0QiESePpWltlVt3brtr+Wbs/dvfSa203fLarPPILW27LCuPrZfPkY3Oh1rl4QWPpHinDu/Ci+7D9dde9ZY3sVR33SdbEUvV+nytuEiSsSzLeU+81X3nUSviLeIHb3gED63Q6ddfx57xjVyQbn9AvSyLRqNlf1g3u9nFYrGGn0V3SzudThtJJhaLOcuWl5fLbp7G3AgCE4lE3dtV7nez9/VsU5rvUsvLy643Z/umWRok2je4rf4RTqfTZcddL3v/drnaN3Y77eXlZWe7yrw26zyqDP4LhYIzJkFpfnr5HKmVVj3ra+EHsHedfgND9+H6a69GyptYqvZ2nXSfbFUsVevz9S73sh9jiLe8vK9nm06Jt4gfvKECCq3Q6dffzYLzzPva2prOnj3rus1zzz2n06dPa2xsTLFYrKnPoodCIUnSxMSEjh8/LklOPkr3c++990qSFhcXnWe5vW7XDJFIpKpLcmW+S7344ovOWAGlXn/9dUkqm3GgWdNMvvLKK87z+1tx8OBBSdKZM2ecaTlfeeWVDcdHaPZ5dN999zn/j0ajSqfTzrki9fY50mobfUcod/nyZe3YscPvbKBHrK2tVT02jd5ALOVNN8RS7US81ZhuiLeIHzZ3+fJlSSImxfbidw1YLe3sAWXMenddy7LM1atXa9b0291bS1tF6lUr7crlrd7OSzqVy+zWRbsVx6210ZZIJGrOjOI1z/XK5XKeun1vpHT/9vedzWZNLpfbsPXK1ozzyGs59PI5slG+jbnRytvI9z08POykzYsXr/a/6AHVPu3sAWUMsZSXcur0WKrW5+1Hw9y2b+Rx+NK0iLf8O482yrcxjcdbft9nePHa7q9O7gHVM4OQb0UymdSxY8f0//7f/ytrSSqVz+f19ttvKxaL6aGHHtpwQMFGlQ4MaA8e6LafRrZrhlAopKWlJb399tvOoI+JRKKqpSWTyejKlSs6evRoU/e/GbfBx7fii1/8oqT1gTBXVlac97W0+zzazufIT37yE0nSgQMHGvr88PCwjDG8NnlJ0sLCgu/54NU7r+Hh4YauWXQ+YilvOj2WqsWtjOxBse+///4tpU28Vb9uiLeIHzZ/LSwsSJLv+eDVW6+OZzpUO3tAbfbeGOO0KBQKBWfQx0a4pW233pQODOjWamO3QtjPxdezXSPHXLlsaWnJ9dnxUvaz9qXS6bTr4JBeBoOsR6PfSWUeStljAVQeUyPnlTHeziOv5dDL50it/dmftwf1bARj0HinDm9BQffh+muvdvaAIpbqjViq1uftgbBLy8ge3yibzTa0n1LEW70VbxE/eMMYUGiFTr/+OvaMr/eCzOVyzh9Q+w9y6bLSmTAql9ndirPZbFlX3lwu5wxIWPpHfiuP/9hp211m7fQr/7jbN0zLspx8JhKJqpuGl+0qj3mj9/Zxlg5kaadrv698RSIRJ53S2TtKX6UBoR3EWJblBC32wI12evXabPBxL7Oy2OVQOnik3aW5NMBzO4eMac555FbutfTyOVKadmmZpdPpqmOpFz+Avev0Gxi6D9dfe9Vb3sRS3XOfbEUsVevea4vH486se/bMg5WPdhFvddd51Kp4i/jBGyqg0Aqdfv117Blf7wVZ+ce1nmX2TS8ajTrjCEUiEefmXrptrTTqzaf9h12Sicfjrjf6XC7ntHDZgVYj29W6SdV6bVROtW5mkUjEmXHE7VU5LXI2m3W2t2+WlmWZRCLR0I3O/u42Wr9RQFSrHIwp71m1UZlt9TzaKO1aevEc2Wi/sVhsS+OGGMMP4Hp0+g0M3Yfrr73qLW9iqe64T9qaGUt5jUHs2fcsyyrr1WMj3uqe86iV8RbxgzdUQKEVOv36CxjTmQ8KLi4uamRkpDueY6xDIBCQpK48rmvXrunjH/941QxG165d0969e7vymNBc3XCOjIyMSJLz3D1qCwQCWlhYYNweNA3XX3v1ankTS2G76/TziPjBm179vQt/dfr1xyDk8CSZTOqee+5xnT57586dSiQSPuQKnYRzBACA2rhPohk4jwB0s5v9zsB2UjorRj6fV39/v4+5qc/i4qLee+89Pfroo2U3vGvXrun8+fMdM0sL/MM5AgBoNWIpbHecRwC6GT2gmiAQCHh67dy50/lM6f+7wfz8vD75yU/q+eefd45nampK//3f/92SG53XMkXnaPc5gs6Wz+c1MzPjdzbQRDMzMyoWi35nAz2KWIpYCt4Qb8FGrNV7tkOsRQVUE5j1wdzrfnWTYDCowcFBvfzyy07+p6endfDgwZbsrxfLsNe1+xzpVsVisaUBf6vT9yKfz+vEiRP6whe+UBYcu+mmH0PFYlGrq6uanZ1VOBzeUlpra2saHx9XIBDQ+Pi4VlZWWpqvfD6vqakpp4yTyaTrdqlUSuFwWOFwWKlUqmzdI488otHR0bIeKECzEEs1Xy+WIYi3NrMd4iyJWMuLdsdakpTJZMrKeXx8vGqb7R5rUQEFAG104cKFrk5/M8ViUWNjY3rqqad08OBBFQoFJRIJnTx50jUwMsYol8tJknK5XEf/GIrFYnrttdd07NixqoChHsViUZlMRi+//LIKhYIefvhhHTp0qOE0N8tXPp/X9evXNT09LWOMEomEhoaGqlpNk8mkZmdnNT8/r/n5eb3++uuanZ111odCIU1OTmpsbKznW+cAAN2p1+MsiVjLi3bHWrY333yz7P3hw4fL3hNrqXPnfWRaSgCt4Oc08IVCwZk2uRvSVwPTuMZiMdcpuFUyJXWtfXUL+1gatbS01PQ0N0rDbSrtym3t6ctLt7WnOk+n02WfjUQiJhaLNZRHP6+/7YjyBuCHRuKHZui2OKvR37vEWptrd6y10X5t7Yq1/Lr+vKIHFAB4UCwWlUwmnS61s7OzZd1j3bo1Vy6LxWJOq4m9PJ/PO11xJWl2dtbpsnvt2rUtpy9JU1NTNbtlN1M+n9fExIQOHDjguj4Wi2loaKjm41+VNivzfD6vZDLplF0qlVIgEFA4HNba2lpV3mZmZpz1W+mGvVWWZbkuj0QiLdnf/v37y97bLWrRaNRZdvHiRUnS7t27nWW33367pOrWvIGBAU1MTPR093AAQHsRZ3lDrOVNu2Mtaf2Rv3A4rKmpKa2urlatJ9ZaRwUUAHgwOjqq9957z+nGnEqlyrrH2l2bS2Wz2bL309PTzv/N78dt2Llzp/MM+Orqqo4ePapCoSBJ2rt3rxMcNZp+O12+fFmSdPfdd7uuP378uKLRqIaGhpTJZDZNb7MyHxsb09DQkFN2lmUpm80qlUrp+eefd9LJ5/MaGxvTHXfcIWOMnn32WR06dMhTHtrBPp7KbtqtsLa2plgsJmm9fG3nz5+XpLIZlezZxSq7mtvfr/19AwCwVcRZ3hBrNaYdsZZ9rCdPntRDDz2kcDhcVoFErPV7fnS78oJH8AC0QiOPpCwvLxtJJpfLOcsuXbpU1c1ZLt1yK5d52caYG11yS7vfNpp+o1RnF95oNFpz3/by0u7rV69erVpva2aZJxIJ123cuq970cwyNmb9WC3LMoVCYUvpbJYvu+u3/drs3Kq1vFAoVH3eKx4Jay/KG4Af6o0ftmuc1cjvXWKtxrQr1ioUCiadTjvfUzwe3/SzzY616r3+2o0eUACwibNnz0q60UohSffee68kaXFxsSX7DIVCkqSJiYmWpN8KJ0+e3HSbYDCoubk5Sdqwa3Ezy9zevrIrvZf8tsOLL76oyclJBYPBlu5nz549MsYonU4rGo1qYmKibOBLr+x8dtO5CQDoXMRZ3hFrNaZdsVYwGFQoFNL09LTi8XjDg573cqxFBRQAbOL06dNVy+wbw1Zm6Niu+vv7lU6nq7p5l2pmmdvbmw6cfjyZTMqyrKpxmlopFAo5j98dO3ZMUu2xEqTWjpcAAABxVvMRa93gR6wlSUeOHCkrR2KtdVRAAcAm7BuGWwtSq28YvXpDCoVCWlpaUiqVcsYkKtWKMi8dbLQTZDIZXblyRUePHm37vu+5556y927lbQ8uev/997cvYwCAbYc4qzWItfyNtYLBYFk5EmutowIKADYxPDwsSbp+/bqzzG5JGhgYaMk+7Rt4OwambhY7uHFrZXNjWZYSiYRr9+xmlnk8Hpckzc/PO2nYM7X4JZ/P69y5c2UDmmYyGY2Pj7dl/3Y5JBIJSdKjjz4qqby833nnnbJ1lUpn0QMAoFHEWd4Ra3nXCbFWaTkSa62jAgoANvHYY4/JsiydOnXKabV44403FIlEdPDgQWc7u5XDDmpKp2C1b3alrR+VN2V7ytxisaj5+XlZllXWXbfR9Ns1PbDdq6YyKLLLzK2FbXBw0PXm6qXMS9Oz91m6b3v9k08+KWl9HIK+vj4FAgHt3LnTCQrsKYO9zNRSmr5b8OclLXummImJibKxEu67776yQLhZ+QqHw5qZmXFa2YrFomKxmKLRqAYHByWtjw8Vj8d15swZFYtFFYtFnTlzRvF4vGy2FulGa92+ffs2zRcAAJshzvKOWKszY61kMqmVlRXn/drami5cuFB2/hJr/Z5Pg59vilnwALRCo7NC5XI5E4/HnZkqEolE1Uwa2WzWmXVkaWnJGGOMZVkmkUg4M4zYs65Eo1FnmZ1mOp12Ph+Px5uWfjQabWgWEtU5i0YulzOSzKVLl8rSqHy5sSzLNb2Nytwt3Vr7ymazzowkkUjEZLNZZ100GjWRSMQ1D6XcjqXyeLykFYlEaqZVOltNs/K1tLRUNftd6XdUyt7WsiyzvLzsuo09Q07prDleMStbe1HeAPxQb/xgzPaMsxr5vUus1fmxVjQaNel0qIpkhgAAIABJREFUumZarY61Grn+2ilgTAeMDOZicXFRIyMjHTFwGYDeMTIyIklaWFjwOSc32DOFdNrfu0AgoIWFBaeLthd2a+Dx48fr2lexWGz5zCSbCYfDWlpa6um0mmFqakp9fX11f8dSZ15/vYzyBuCHRuKHVurUOKvR37vEWp2dVjNsJdbqtOuvEo/gAQCaZmxsTOfPny/rtu6F3wHR6uqqJicnezqtZshkMspkMhobG/M7KwAAbEvEWp2bVjP0eqxFBRQA+Kj02Xq35/a7TTAY1NzcnE6dOuXpefpOsLKyottuu60p0/N2alrNcO3aNZ0+fVpzc3O+B7EAAHjRa3GWRKzVqWk1w3aItW72OwMAsJ3t3Lmz7P+d1j28Ef39/Zqfn9fc3JxCoZDf2dlU6QCRvZpWM6RSKT333HPq7+/3OysAAHjSi3GWRKzViWk1w3aItaiAAgAf9UogVCkYDDb03Do6F98nAKDb9GqcJRFr9aLt8H3yCB4AAAAAAABaigooAAAAAAAAtBQVUAAAAAAAAGgpKqAAAAAAAADQUh0/CPmRI0f8zgKAHnL58mVJ/G3x6qWXXtKrr77qdzbQI86ePavh4WG/s7GtLC4u6oMPPvA7GwC2GeKHza2trUkiJsX2EjAdOjXAu+++q7/7u7/Thx9+6HdWALTYuXPn9LnPfU67du3yOysAWmx0dFSWZfmdjW0hlUppfn7e72wAaLN3331XP//5z/XII4/4nRUAbXbTTTfp+9//fsf+rurYCigA20cgENDCwgI9IwAAALZocXFRIyMj4mcegE7DGFAAAAAAAABoKSqgAAAAAAAA0FJUQAEAAAAAAKClqIACAAAAAABAS1EBBQAAAAAAgJaiAgoAAAAAAAAtRQUUAAAAAAAAWooKKAAAAAAAALQUFVAAAAAAAABoKSqgAAAAAAAA0FJUQAEAAAAAAKClqIACAAAAAABAS1EBBQAAAAAAgJaiAgoAAAAAAAAtRQUUAAAAAAAAWooKKAAAAAAAALQUFVAAAAAAAABoKSqgAAAAAAAA0FJUQAEAAAAAAKClqIACAAAAAABAS1EBBQAAAAAAgJaiAgoAAAAAAAAtRQUUAAAAAAAAWooKKAAAAAAAALQUFVAAAAAAAABoKSqgAAAAAAAA0FJUQAEAAAAAAKClqIACAAAAAABAS1EBBQAAAAAAgJaiAgoAAAAAAAAtRQUUAAAAAAAAWooKKAAAAAAAALQUFVAAAAAAAABoqYAxxvidCQDbx9zcnP7mb/5Ge/fudZb96le/0h//8R/rD//wDyVJ//M//6MvfelL+tGPfuRXNgEAALrCI488onQ6rdtvv12S9Nvf/la//vWv9ZnPfMbZ5urVq/rXf/1XDQ8P+5VNANDNfmcAwPaSy+X0wQcf6Oc//3nZ8mKxWPY+lUq1M1sAAABdaWVlRcYY/frXvy5bXhlb/fKXv2xjrgCgGo/gAWiroaEhBQKBDbe5+eab9cILL7QpRwAAAN3rhRde0M03b9yvIBAIaHBwsE05AgB3PIIHoO3+7M/+TD/5yU9U689PIBDQW2+9pTvvvLPNOQMAAOgu2WxWf/Inf7JhXPXAAw/oP//zP9ucMwAoRw8oAG33zW9+UzfddJPruo997GPat28flU8AAAAe3Hnnndq3b58+9jH3n3Y33XSTvvnNb7Y5VwBQjQooAG33jW98Qx999JHrukAgoKeeeqrNOQIAAOheTz31VM0hDj766CN94xvfaHOOAKAaFVAA2m7Xrl16+OGHa/aCGhgYaHOOAAAAulet2Ommm27Sww8/rF27drU5RwBQjQooAL741re+VTVWwU033aQDBw7oU5/6lE+5AgAA6D6f+tSndODAgarGPWOMvvWtb/mUKwAoRwUUAF987WtfI0gCAABoklqNe1/72td8yhEAlKMCCoAvgsGgHnvssbJpg3fs2KGvfvWrPuYKAACgO331q1/Vjh07nPc333yzHnvsMQWDQR9zBQA3UAEFwDejo6P68MMPJa0HSU888YQ++clP+pwrAACA7vPJT35STzzxhNO49+GHH2p0dNTnXAHADVRAAfDNE088oU984hOS1oOkkZERn3MEAADQvUZGRpzGvU984hN64oknfM4RANxABRQA33z84x/X17/+dUnSrbfeqsOHD/ucIwAAgO51+PBh3XrrrZKkr3/96/r4xz/uc44A4IabKxf83//9n5aWlpyacwBopU9/+tOSpDvvvFNLS0s+5wbAdrF//3595jOfaUnav/rVr7S6utqStAFgM3feeaeuXLmiT3/60zp79qzf2QGwDd10000Kh8Nl4/1KUsBUTJXw6quvMlMCAADoaX/1V3+lf/7nf25J2n/913+tf/mXf2lJ2gAAAN3g3/7t36ommKrqAfXb3/5Wkqqm8AQANI893tXCwoLPOel8gUBACwsLGh4e9jsr6BEjIyN6//33W5b++++/r+HhYa5vAPAR8YM3i4uLGhkZ4fc/mioQCDh1S6UYAwoAAAAAAAAtRQUUAAAAAAAAWooKKAAAAAAAALQUFVAAAAAAAABoKSqgAAAAAAAA0FJUQAEAAAAAAKClqIACgC43NTWlqakpv7PRkfL5vGZmZvzOBppoZmZGxWLR72wAALYJ4qyNEWv1nlbGWlRAAQC2pFgsKhAI+J2NKvl8XidOnNAXvvAFBQIBBQKBmgGkvb701amKxaJWV1c1OzurcDi8pbTW1tY0Pj6uQCCg8fFxraystDRf+XxeU1NTThknk0nX7VKplMLhsMLhsFKpVNm6Rx55RKOjo8rn8w3nFQCAbtGpcZZErOVFu2MtScpkMmXlPD4+XrWNX7HWzU1PEQDQVtPT077u/8KFC77u302xWNTY2JgmJye1f/9+FQoFvfHGGxoaGpJUXWbGGOXzee3cuVO5XE79/f1+ZNuTWCwmSTp58uSW0ikWi8pkMnr55Zf1wgsv6I033tChQ4e0tLQky7Kanq98Pq/r169renpa09PTSiaTGhoa0ttvv63jx4872yWTSS0uLmp+fl6S9A//8A969913dfToUUlSKBTS5OSkxsbGND8/r2AwWHdeAQDwijjLHbHW5toda9nefPPNsveHDx8ue+9rrGUqLCwsGJfFAIAmGh4eNsPDw35nY8sKhYKxLKul9w1JZmFhoa7PxGIxE41GXdOSZBKJRM19dQv7WBq1tLTU9DQ3SuPSpUubbpvNZo2ksm3T6bSRZNLpdNlnI5GIicViDeWx1ddfr1zfANDNGokfOk074qxGf/8Ta22u3bHWRvu1tSvWqnX98QgeAHSxfD6vZDLpdMOtfJ9KpRQIBBQOh7W2tuZsY3e7laTZ2Vmne+61a9ectN26SFcui8ViTrfd0uV+jpeQz+c1MTGhAwcOuK6PxWIaGhqq+fhXpWKxqGQy6Rzf7OxsWZdkL2Veuu3MzIyzfivdsLeqVstbJBJpyf72799f9t4eWyAajTrLLl68KEnavXu3s+z222+XVN2aNzAwoImJCR7FAwC0DHGWO2Itb9oda0nrj/yFw2FNTU1pdXW1ar3vsVZljRQ9oACg9ZrVQ8JuFbP/bpe+t1s27JaOSCRijLnRalK6TaFQMJFIxEgyV69eNcYYk8vlavZQKV1W+d4YY6LRqGurWCNUZwvm0tKSkWSy2axrWnb+5NLS43b/syzLxONxY8x6mViWZSzLMoVCwVm/WZmXftZuEVxeXnbNg1du5b4VhULBSNqw1cwLL/nKZrPOd2Cfb8YY5xx0S9OyrKo0Gs0vPaAAoPfVGz+42Q5xViO//4m1GtOOWMv+buyXZVkml8s569sVa9W6/qiAAgAfNPMHqpdAxcs2dvfb0q62jabVTPUGkHbAUystY8q7tJdWgFR+zg5cSm/cly5dqupa7qWcEomE6zaNBpDNLvfl5eWyYK9Rm+WrNLj2cr7VWm4HcY10DacCCgB6XzMqoOx0ejnOauT3P7FWY9oVaxUKBZNOp53vya7c2+izzY61al1/PIIHAJC0PuCgJE1MTPick63xMmBkMBjU3NycJG3Ytfjs2bOSVDZQ5r333itJWlxcrCtf9vaV3eu3OsBls7z44ouanJxs+aDee/bskTFG6XRa0WhUExMTmp2drTsdO5/dfr4CALaHXomzJGKtRrUr1goGgwqFQpqenlY8Hq+a5a6edKTmnrNUQAEAtqX+/n6l02mlUimNjY05YxKVOn36dNUy+2Zc783c3t6s9z4ue/ktmUzKsqyqcZpaKRQKaXR0VJJ07NgxSbXHSpBaO14CAABoPmKtG/yItSTpyJEjZeXod6xFBRQAoMx2+qEfCoW0tLSkVCrlTG1byr5Ju7XaNVpOpQOQdoJMJqMrV644U++20z333FP23q287cFF77///vZlDACAFtlOcZZErCX5G2sFg8GycvQ71qICCgAg6cbN+vDhwz7nZGvs4Matlc2NZVlKJBKu3bOHh4clSdevX3eW2ekODAzUla94PC5Jmp+fd9KwZ2rxSz6f17lz5zQ9Pe0sy2QyGh8fb8v+7XJIJBKSpEcffVRSeXm/8847Zesqlc6iBwBAp+qVOEsi1qpHJ8RapeXod6xFBRQAdLHKKWpL39s33tLgoLJ1yZ4et1gsan5+XpZllXXNtVtM7KCpdDpX+8ZZ2pJi3+D9nB7Y7lVTGRTZx+7WwjY4OOh6c33sscdkWZZOnTrlfO6NN95QJBLRwYMHq9LbqMyffPJJSevjEPT19SkQCGjnzp1OUGBPGZzJZDY9xtL03YI/L2nl83mNjY1pYmKibKyE++67ryw4bla+wuGwZmZmnFa2YrGoWCymaDSqwcFBSevjQ8XjcZ05c0bFYlHFYlFnzpxRPB7Xnj17ytKz09m3b9+m+QIAoBHEWe6ItToz1komk1pZWXHer62t6cKFC045Sv7HWlRAAUAX27lzZ9n/S9/39fWV/Vu5vbQ+yGM4HFZfX5/27Nmj+fn5svXf/e53ZVmW9u7dq1Qqpf379zutWM8995wkOS06P/jBD5wxffz04IMPSrrRmiPJCUCk9TKwB6UsNT09XfVcvD2ApmVZZZ974YUXnG28lnl/f7+y2awTfEUiEWWzWedmXygUFIlENg0oA4FAWfp2gFXKS1onTpyoObbC3r1760rLS76OHj2qiYkJ3XnnnQoEApqbm9Pjjz9e1iJob3f48GH19fVpdHRUAwMDrl3W7e/X/r4BAGg24ix3xFqdGWvdeuutOnTokAKBgKampvSb3/zGdcwnP2OtgKkYkWtxcVEjIyMdMVAXAPSqkZERSdLCwoIv+7dvVt3wtz4QCGhhYcHpou2F3UJ4/PjxuvZVLBZbPjPJZsLhsJaWlno6rWaYmppSX19f3d+x1Prrz+/rGwDQWPzQzH1L3RFnNfr7n1irs9Nqhq3EWrWuP3pAAQB6ztjYmM6fP1/Wld0LvwOi1dVVTU5O9nRazZDJZJTJZDQ2NuZ3VgAA2JaItTo3rWZoVay15QqofD6vZDKpcDjcjPx03f47hVs5tOPZYL+fP+412+185rz1R+V4Br3o/7N3//Ft1fe9x9/KL1q21intbJK1sNJcKCvFhJ8ppEACbSBFAraaxg5pe5lJZDZSWBTYzeQxliyFG5lkSUuCbFqCsaUb0kCtEeijxGsCxE6gVH4ATZOygE2bYTWjUtLyI7/O/cM7B0mWbVmWdPTj9Xw89HhY5xx9z0dfHen79fd8f5jduVeuXJnWePpC0NnZqVNOOSUry/MWalrZsG/fPm3YsEEtLS22V2Kzxe7ffrvPXygok8pHuV3zXNv5VQ71LIm6VqGmlQ25rGtNGGsC99xzjzZs2JCNWBSLxTR58uRRdf/L5vmLWT7yIZPPJ9fMmFIJBALWxLYjSTVG2eTz+fTxj3981Mtm5uJ67uzs1FVXXSVpYDWC5LlTpNTvpZA+s3jlet3aLXk+g1LNm8rKSrW2tqqlpUXV1dV2hzOi+AkiSzWtbAiFQrr33ntVWVlpdyhZQ12qMJRzmRSLxbRnzx698sorCoVCQw4BCYVCam5uljQwh0iquUWGQ33LHuV8bduhXOpZEnWtQkwrG3Ja1zKStLW1GSk2D0vSqF+TSkdHR0bpZOv8xS7X+ZDp55NLXV1d1vtOfvT3948qrf7+/pR5uG3bNkOSEQgERpVerq7naDRqBAIBQ5Lh9XpTHmO+l9HmgR3K8bo1DMOoq6sz6urq7A6jKEgy2tra7A4DJSTX379M0qcuVRjKtUzyer2G1+sd9v0HAgHD6XQa0WjUiEajhtvtNvx+/6jPRX3LHuV4bVN/SE8m//8DIxnq+1cwc0DFYjHrjgoKT6F+Pm+++aZ6e3tlGIb16O/vl9frHXWL7VDHmy3S7e3taaeVy/yqqKiwenatWLHCWt41nvleSqmHQCYK9boFgFzgN6+wFfLns3z58pS9fEx9fX2qra3VsmXLVFFRoYqKCrndbi1cuHDUQ2+ob5WeQr62ARSWrDZARSIRNTU1yeFwqKGhQX19fQn7zR8nh8NhLQ1ojov1+XzWEoXm/vjXBYNBa/twP3ChUMg6/2jH3CaPjzbTcrlcKd9LckzJ431DoZBcLpdisZgaGhqs95vqHPH5ZaabnIfD5d9I70X6MF+TH+Yxo/18hho/n07epJvPI5k9e7a1rKaps7NTX//61xO2ZWMMevISmoVwPft8PtXW1qasFKXCdVsY1y0ADIW6FGVSunmTzzJp586dkqSpU6da26ZMmSJJ2r17t7WN+tbQ5+baTj9vqG8BJSy5S9RYhuB1dXUZhjHQHdXpdA7qkup2u61tvb29hiTD7XYPSieZ0+lM6PrqdrsTnieff+/evYPSTocZc3xaqeI0jzW7HZvv1+yWnCqtcDhsuN3uhO3hcNgwjA+Hkbnd7mHPO5r8iz9P/P74z8PsKtvb2zvq9Ic6RyZ5M1w+ZyJVGmbX8pEMdQ0qRZdwu69nM22zy7x5PSXvTz43121hXLcMwUuf6EKPLCvkIXjUpSiTMsmb4fJ5NIa6fsz3lep4p9NpPae+9eG5ubbtv7apP6SHIXjIhaG+fzmbA8r8EY8fG+71eof9oUqVjjn2Ov4HsaurK6GwS/W6oQqjTN5L8jZzjHpyTMmFpvk68wd1tPEmbxtt/g2XB+bns23btozTT7VttHkzUh6MVjgcHvXcAaliSH54vd5Bn6Pd17P5PBqNWoX13r17B+03cd0OHaMd1y0NUOmjAolsK+QGqHjUpSiTTPmqSw31+tFuT/c81Le4tnN5bVN/SA8NUMiFob5/Y14FbyhnnnmmJGnhwoXWahbm2PK+vj49/vjjaaVjjgOPH1s9Y8aMIVfnyAcz9viYzj77bEkD8SavvJatpQszyb9UIpGIPB6PfD5fwoz72Uh/tHmTbZs3b9btt98+5nQGvjMDIpGI1q1bpwULFqilpcV6b4VyPZtLoFZVVcnj8STEGI/rdmh2XbfPP/+8brrpppykXWrWrl2rJ5980u4wUCJ27dqlmTNn2h3GiKhLfYgyKX91qXyivsW1netrm/rDyMyhjdRJkQ95n4S8ublZf/d3f5f2sq3J48ALQaplTM3CI9fxjjb/Ulm3bp0kacmSJVlP3868McePZ3siyMrKSt1+++0KhUJW3pkK5XqurKxUOBxWKBRSfX29YrHYoGO4bodmZ94AwGgVStkzFpRJQyvEMmm49+J2u7NyDupbY8e1DaDgJXeJytYQPHN7fFdNszusOZY4+XWp0jG7uiaPtx7p/EPFlMl7Sd6Wak4G87h0xqSnG2/yttHmX6o0/X5/QhrxMvl8spk3mX5m8fEPd52kY7gYxvp5GEZ2r+dUcZrj9c15ClKdm+u2MK5bhuClT3ShR5YVyxA8czt1Kcok87hc16WGer35nuNjMufliR8iOtbzpNpn9zVPfSuz9FNty/e1Tf0hPQzBQy4M9f3LWQ8oc0nWK664wtpWW1srSYNWLRuO2cK+YcMG6y5DX1+fGhoashXqqNXV1UmS9u/fb20zY6upqcnZeTPJv3jd3d1auHChtm3bljKNsaYv2Zc3krR9+3ZVV1fnJG2za2r8Xb5CvJ6dTqcCgYBWrFgxaB/X7dDsvG4BYCjUpbKPMikzc+bMGRTTgQMHEvZlA/WtzHFtAygKyS1SmbSAmq3Z5kR15ooGPp8v5XG9vb3W5HaKawWPbxU3Xxu/Coz5cLvd1uR//f39g9KJRqODtqUjPi1zQsBUaZmTEDqdTmtbIBBIaLmPT2ukc6R6D6m2DZd/yccnPzfvUiV/JuZxmXw+Q+X9aPJmuHwejZEmH09nVZZUcRnGwESM5h2u+Ekn7byezeOGyqtUd+S4bgvruqUHVPrEHUxkWSH2gKIuRZlkd10q/vXJE2MbxkDvGLfbbUSjUSMajRput3tQ7yfqW1zbhXRtU39IDz2gkAtDff+y0gBlGAOrGpg/PG63O2HVBFM4HDakge6q/f391moKZlfO5P0m81hzX/LKE/GPobalYzRp9ff3W91UpYGVG+IL0fjXpFpxY6RzpNo2XP4lH5/8SC6ER5t+qv3ZyJuxfmam5Gsm1f7hKkQj5Z3f7x/UHdmu63mozy9Z/HUXf26u28K4bmmASp9EBRLZVYgNUIZBXYoyyb4yKd36hTn0zOl0prw+qW99eG6ubfuvbYn6QzpogEIuDPX9c/zPTkt7e7vmz5+vpM0AikwsFsvayirIvvnz50uS2trabI6k8DkcDrW1tVld94GxyvX3j+83UD6obxUu6g/p4f9/5MJQ37+8r4IHID+oDAEAAOQW9S0ASB8NUACAkhWJRNTU1GR3GMiipqamlEufAwCA/KOuVXpyWdcqiwYoh8OR1gOFg88MyK1YLJbT71Cu009HJBLRPffco+nTp1u/GY2NjSmPLabfl1gspu7ubjU3N8vlco0pLXNVKIfDoYaGBnV2duY0rkgkosbGRiuPg8FgyuNCoZBcLpdcLpdCoVDCvquvvloLFixQJBLJOFaMHuVy8eEzA+xTDvUsibpWOvJd15IGVtGNz+dUK4DaVdcqiwYoY2Cy9REfKBx8ZkBu7dixo6jTH0ksFlN9fb2+9a1vafbs2YpGo9aS2akqRoZhqL+/X5LU399f0L8vPp9PTz31lBYuXDiowjAasVhMPT09Wr9+vaLRqK644gpdddVVGac5UlyRSET79+/X8uXLZRiGAoGAamtrB901DQaDam5uVmtrq1pbW7V161Y1Nzdb+6urq7Vs2TLV19fTEyqPKJeLD58ZYJ9Sr2dJ1LXSke+6lmn37t0Jz+fOnZvw3Na6VvKs5MyCDwC5Z+cqeOYyyLn6rc92+spgFRufz5dyJSbFrboz1LmKhUa5GlCyjo6OrKc5XBpdXV0jHmuu1hR/rLmyUjgcTnit2+0etGR4ugp1FTwAQPZkUn/IhmKrZ2X6/z91rZHlu6413HlN+aprDfX9K4seUABQKmKxmILBoNWltrm5OaF7bKpuzcnbfD6fddfE3B6JRKyuuJLU3Nxsddndt2/fmNOXpMbGxiG7ZWdTJBKRx+PRrFmzUu73+Xyqra0dcvhXspHyPBKJKBgMWnkXCoXkcDjkcrnU19c3KLampiZr/1i6YY+V0+lMud3tdufkfDNmzEh4bt5R83q91radO3dKkqZOnWptmzJliqTBd/Nqamrk8XgYigcAyBrqWemhrpWefNe1pIEhfy6XS42Njeru7h603+66Fg1QAFBEFixYoMOHD1vdmEOhUEL3WLNrc7ze3t6E58uXL7f+Nv5nCEZVVZU1Bry7u1u33nqrotGoJOmss86yKkeZpp9Pu3btkiRNmzYt5f4lS5bI6/WqtrZWPT09I6Y3Up7X19ertrbWyjun06ne3l6FQiF997vftdKJRCKqr6/Xn//5n8swDN1xxx266qqr0oohH8z3k9xNOxf6+vrk8/kkDeSvafv27ZKk0047zdpWWVkpSYO6mpufr/l5AwAwVtSz0kNdKzP5qGuZ73XFihX60pe+JJfLldCAZHtdK7lLFEPwACD3Mhmis23bNkOS0d/fb23r6uoa1M1ZKbrlJm9L5xjD+LBLbnz320zTz5RG2YXe6/UOeW5ze3z39b179w7ab8pmngcCgZTHpOq+no5s5rFhDLxXp9NpRKPRMaUzUlxm12/zMdK1NdT2aDQ66PXpYggeAJS+0dYfyrWelcn//9S1MpOvulY0GjXC4bD1Ofn9/hFfm+261lDfP3pAAUCRePzxxyV9eJdCks4++2xJUnt7e07OWV1dLUnyeDw5ST8XVqxYMeIxFRUVamlpkaRhuxZnM8/N45O70qcTbz6sWbNGy5YtU0VFRU7Pc9ppp8kwDIXDYXm9Xnk8noSJL9NlxllM1yYAoHBRz0ofda3M5KuuVVFRoerqai1fvlx+vz/jSc9zUdeiAQoAisSGDRsGbTMLhrGs0FGuKisrFQ6HB3XzjpfNPDePNwpwFapgMCin0zlonqZcqq6utobfLVy4UNLQcyVIuZ0vAQAA6lnZR13rQ3bUtSTppptuSshHu+taNEABQJEwC4xUd5ByXWCU6j//1dXV6ujoUCgUsuYkipeLPI+fbLQQ9PT06LXXXtOtt96a93OfeeaZCc9T5bc5uej555+fv8AAAGWHelZuUNeyt65VUVGRkI9217VogAKAIlFXVydJ2r9/v7XNvJNUU1OTk3OaBXg+JqbOFrNyk+ouWypOp1OBQCBl9+xs5rnf75cktba2WmmYK7XYJRKJ6Nlnn02Y0LSnp0cNDQ15Ob+ZD4FAQJI0Z84cSYn5feDAgYR9yeJX0QMAIFPUs9JHXSt9hVDXis9Hu+taNEABQJG49tpr5XQ6tXLlSuuuxdNPPy23263Zs2dbx5l3OcxKTfwSrGZhF3/3I7lQNpfMjcViam1tldPpTOium2n6+Voe2OxVk1wpMvMs1R22efN4oYsyAAAgAElEQVTmpSxc08nz+PTMc8af29x//fXXSxqYh2Dy5MlyOByqqqqyKgXmksHprNQSn36qyl86aZkrxXg8noS5Es4777yEinC24nK5XGpqarLussViMfl8Pnm9Xs2bN0/SwPxQfr9fGzduVCwWUywW08aNG+X3+xNWa5E+vFt38cUXjxgXAAAjoZ6VPupahVnXCgaD6uzstJ739fVpx44dCdev7XWt5FnJWQUPAHIv01Wy+vv7Db/fb61UEQgEBq2k0dvba6060tHRYRiGYTidTiMQCFgrjJirrni9XmubmWY4HLZe7/f7s5a+1+vNaBUSjXIVm/7+fkOS0dXVlZBG8iMVp9OZMr3h8jxVukOdq7e311qRxO12G729vdY+r9druN3ulDHES/Vekt9POmm53e4h04pfrSZbcXV0dAxa/S7+M4pnHut0Oo1t27alPMZcISd+1Zx0sQoeAJS+0dYfDKM861mZ/P9PXavw61per9cIh8NDppXrutZQ3z/H/+y0tLe3a/78+QUxURcAlKr58+dLktra2myO5EPmSiGF9vvvcDjU1tZmddFOh3k3cMmSJaM6VywWy/nKJCNxuVzq6Ogo6bSyobGxUZMnTx71Zyzl/vtXiN9vACg3mdQfcqlQ61mZ/v9PXauw08qGsdS1hvr+MQQPAFBy6uvrtX379oRu6+mwu0LU3d2tZcuWlXRa2dDT06Oenh7V19fbHQoAAGWJulbhppUNuapr0QAFAEgYW59q3H6xqaioUEtLi1auXJnWePpC0NnZqVNOOSUry/MWalrZsG/fPm3YsEEtLS22V2IBAEhHqdWzJOpahZpWNuSyrjUhq6kBAIpSVVVVwt+F1j08E5WVlWptbVVLS4uqq6vtDmdE8RNElmpa2RAKhXTvvfeqsrLS7lAAAEhLKdazJOpahZhWNuSyrkUDFACgZCpCySoqKjIat47CxecJACg2pVrPkqhrlaJcfp4MwQMAAAAAAEBO0QAFAAAAAACAnKIBCgAAAAAAADlFAxQAAAAAAAByigYoAAAAAAAA5JTDSJqS/8knn9SNN95oVzwAAAA597//9//WD37wg5ykfcstt+iHP/xhTtIGAAAoBk888YRuuOGGhG2DGqCOHTumjo4OHT9+PK/BAUAuhMNhrVy5UosXL9bMmTPtDgdAgZgxY4Y+85nP5CTtt956S93d3TlJG0D5aWpq0h/+8Afdc889docCAGkZP368XC6XJkyYkLB9UAMUAJSav//7v9dDDz2knTt3qrq62u5wAAAA0nLgwAGdfvrp2rhxo+rq6uwOBwDGhAYoACXv2LFjuvrqq/XWW2/ppZde0ic+8Qm7QwIAABjRv/zLv2jdunX67W9/q0mTJtkdDgCMCZOQAyh5EyZM0KZNm3TkyBHV1dUxxBgAABS8Y8eOqaWlRbfccguNTwBKAg1QAMpCZWWlfvSjH6mzs1P33nuv3eEAAAAMa+vWrfrtb3+rhQsX2h0KAGQFQ/AAlJWWlhYtXLhQTzzxhK6//nq7wwEAAEhp7ty5On78uH7yk5/YHQoAZAUNUADKTn19vTZv3qzdu3frzDPPtDscAACABG+88YamTZumzZs368Ybb7Q7HADIChqgAJSdI0eOaObMmfrjH/+o7u5ufexjH7M7JAAAAMv/+T//R62trXrzzTcHLWMOAMWKOaAAlJ1JkyZpy5Yt+t3vfqdbbrlFtMMDAIBCceTIEf3gBz9QfX09jU8ASgoNUADK0qc//Wlt2rRJTzzxhFatWmV3OAAAAJKkLVu26J133lF9fb3doQBAVjEED0BZW716tZYuXaqnn35aX/nKV+wOBwAAlLkrr7xSn/jEJ/TEE0/YHQoAZBUNUADK3rx589TZ2akXX3xRp59+ut3hAACAMrVnzx594Qtf0NatW3XNNdfYHQ4AZBUNUADK3rvvvqsZM2Zo4sSJev755/XRj37U7pAAAEAZuuOOOxQKhfTrX/9a48YxWwqA0sKvGoCyd/LJJ2vLli36z//8T9122212hwMAAMrQe++9p40bN2rhwoU0PgEoSfyyAYCkadOmqb29XY8++qgefPBBu8MBAABlJhgM6t1339Utt9xidygAkBMMwQOAOPfee69Wrlyp//iP/9Cll15qdzgAAKBMXHLJJfrc5z6n9vZ2u0MBgJygAQoA4hiGIZfLpZdfflkvvfSSpkyZYndIAACgxL388su64IILtH37dl1++eV2hwMAOUEDFAAkicViuuiii1RVVaVt27Zp0qRJdocEAABK2KJFi/T888/rtddeszsUAMgZ5oACgCQVFRXasmWLfvGLX8jj8dgdDgAAKGGHDh1Se3u7Fi1aZHcoAJBTNEABQArnnHOOfvjDH+p73/ueHn30UbvDAQAAJeqxxx7TiRMn9M1vftPuUAAgpxiCBwDD8Hg8Wr9+vZ5//nlNnz7d7nAAAECJOffcc3XRRRfp4YcftjsUAMgpGqAAYBjHjx/XV7/6Vb3xxht68cUX9clPftLukAAAQIl44YUXNHPmTO3evVsXXXSR3eEAQE7RAAUAI/jd736nCy64QGeffba2bt2q8ePH2x0SAAAoAQsWLNCePXv00ksv2R0KAOQcc0ABwAj+7M/+TFu2bNH27dvV2NhodzgAAKAEHDx4UJs3b5bb7bY7FADICxqgACANF154oR588EHdd9992rJli93hAACAIrdx40Z95CMf0bx58+wOBQDygiF4ADAKbrdbgUBAu3bt0uc//3m7wwEAAEXIMAydddZZmjNnjtatW2d3OACQFzRAAcAoHDlyRFdeeaV+//vfq7u7WxUVFXaHBAAAisyzzz6rr3zlK3r11Vf1hS98we5wACAvaIACgFH6zW9+owsvvFCXXXaZNm/eLIfDYXdIAACgiHz9619XJBLRjh077A4FAPKGOaAAYJQ+/elPa9OmTQqFQvrud79rdzgAAKCI/Nd//Zd+/OMfM/k4gLJDAxQAZODyyy/XqlWr9E//9E/6yU9+Ync4AACgSLS0tGjy5Mn6+te/bncoAJBXDMEDgDG4+eab9cwzz+jFF1/UZz/7WbvDAQAABez48eM644wzNG/ePN1///12hwMAeUUDFACMwbvvvqtLL71UDodDL7zwgk4++WS7QwIAAAUqFArphhtu0L59+/S5z33O7nAAIK9ogAKAMdq/f78uvPBCXXfddXr00UftDgcAABSor33tazp27BjD9wGUJeaAAoAxOuOMM9Te3q729natXbvW7nAAAEABeuONN/TMM88w+TiAskUDFABkwTXXXKN//ud/1tKlS/X8888P2n/06FG9//77NkQGAADy7Xvf+55OOeUU/fCHP9R7770nSfL7/ZoyZYqcTqfN0QGAPWiAAoAs+cd//EfNnTtXNTU1OnDggLX99ddf16RJk1RVVWVjdAAAIF9+8pOf6Pe//73q6+tVVVWlxYsXq6WlRfX19ZowYYLd4QGALWiAAoAscTgceuSRR1RRUaGamhodOXJE//7v/67p06dLkg4dOqS9e/faHCUAAMi1Y8eOSZJOnDihw4cPa8OGDTp48KCeeeYZBYNBHTlyxOYIASD/aIACgCyqqKjQli1b9Morr+hrX/uaXC6X3n33XUnSxIkTFQwGbY4QAADkWiwWS3h+9OhRSdJLL72kuro6nXrqqfr7v/97q44AAOWAVfAAIMui0ai+9rWvqbu7WydOnEjY99nPflb79++3KTIAAJAP55xzjl577bURj+vq6tKMGTPyEBEA2I8eUACQRa+88oqmT5+uF198cVDjkzSwAk44HLYhMgAAkC+HDx8edr/D4dB3v/tdGp8AlBUaoAAgS5544gmde+65+s1vfmN1tU82adIk/b//9//yHBkAAMinP/7xj0PuGz9+vBYtWqR/+Id/yGNEAGA/huABQJace+65euWVV0Y8burUqfrNb34jh8ORh6gAAEC+nXzyyXrvvfcGbZ84caK++tWv6sc//rHGjx9vQ2QAYB96QAFAluzatUt33323xo0bN+wSywcOHFB3d3ceIwMAAPk0VOPTF7/4RW3atInGJwBliQYoAMiSj370o7rvvvv08ssv65xzztG4cal/YhmGBwBA6UpeAU8aaHyaOnWqnnnmGZ188sk2RAUA9qMBCgCyrLq6Wi+99JIeeOABffSjH9XEiRMT9h85ckSPPfZYyknKAQBAcfvDH/6Q8Hz8+PH6kz/5Ez377LP6sz/7M5uiAgD70QAFADkwfvx4fec739GePXt01VVXSVJCj6j//u//1s9+9jObogMAALkS3wDlcDg0YcIEPfPMM5o2bZqNUQGA/WiAAoAcOv300/X0008rGAxq8uTJVm+oiRMnKhgM2hwdAADItsOHD1t/OxwObdq0SZdccomNEQFAYaABCgDy4Bvf+IZef/113XzzzXI4HDp69Kiam5t19OhRu0MDAABZFN8D6nvf+55cLpeN0QBA4XAYhmHYHQSQqbffflt33nmnjh8/bncoQNp+97vfaceOHTpx4oQuv/xyVVVV2R0SUDbGjx+v1atX69RTT7U7FNuFQiG1trbaHQZQcn77299q586dOvPMM1VdXW13OEBeUc5iOPSAQlHr7OxkGBOKzs9+9jNdcMEFuvDCC/WJT3zC7nAKVl9fnx5//HG7w0CJCQaD6uzstDuMghAMBvmOoSQ9/vjj6uvrs+38VVVVOv/88wu+8YlyFrlAOYvhTLA7ACAbNm3aZHcIQNocDofuuOMO1dXV2R1KQWtvb9f8+fP5fiOrHA6H3SEUlLq6OrW1tdkdBpBVDodDixcvppwdAeUscoFyFsOhBxQAAAAAAAByigYoAAAAAAAA5BQNUAAAAAAAAMgpGqAAAAAAAACQUzRAAQAAAAAAIKdogAKAItXY2KjGxka7wyhYkUhETU1NdoeBLGpqalIsFrM7DABlgnJ2eJSzpYdyFrlGAxQAICOxWKxgl9qNRCK65557NH36dDkcDjkcjiH/iTD3xz8KVSwWU3d3t5qbm+VyucaUVl9fnxoaGuRwONTQ0KDOzs6cxhWJRNTY2GjlcTAYTHlcKBSSy+WSy+VSKBRK2Hf11VdrwYIFikQiGccKAMWCcjb/irmclaSenp6EfG5oaBh0DOUs7DTB7gAAAJlZvny5reffsWOHrecfSiwWU319vZYtW6YZM2YoGo3q6aefVm1traTB+WYYhiKRiKqqqtTf36/Kyko7wk6Lz+eTJK1YsWJM6cRiMfX09Gj9+vW677779PTTT+uqq65SR0eHnE5n1uOKRCLav3+/li9fruXLlysYDKq2tla//e1vtWTJEuu4YDCo9vZ2tba2SpL+4R/+QW+//bZuvfVWSVJ1dbWWLVum+vp6tba2qqKiYtSxAkC6KGdTo5wdWb7LWdPu3bsTns+dOzfhOeUsbGcARaytrc3gMkaxkWS0tbXZHcaYRKNRw+l05vT7l+n32+fzGV6vd9B2SYYkIxAIpHxdMf2WmO8lUx0dHVlPc7g0urq6Rjy2t7fXkJRwbDgcNiQZ4XA44bVut9vw+XwZx1js379sqaurM+rq6uwOA8i6UvieU87aq9jK2eHOa6KcRSFgCB4AFKFIJKJgMGh1w05+HgqF5HA45HK51NfXZx1jdruWpObmZqt79r59+6y0U3WRT97m8/msbtvx2+2eLyMSicjj8WjWrFkp9/t8PtXW1g45/CtZLBZTMBi03mNzc3NCt/R08j3+2KamJmv/WLrij9VQd1/dbndOzjdjxoyE5+b8El6v19q2c+dOSdLUqVOtbVOmTJE0+I5uTU2NPB4PQwQA5AzlbGqUs+nJdzkrDQz5c7lcamxsVHd396D9lLMoCHa3gAFjQQ8oFCNl4c6QeVfUvP7jn5t3tsw7XW632zpv8jHRaNRwu92GJGPv3r2GYRhGf3//kL1T4rclPzcMw/B6vSnvimYik+93R0eHIcno7e0dtM9My+v1przbl+pcTqfT8Pv9hmEM5IvT6TScTqcRjUat/SPle/xrzbvC27ZtSxlDulLl/VhEo1FD0rB3TtORTly9vb3WZ2Bec4ZhWNdhqjSdTuegNDKNNxvfv1JBDyiUKsrZ9FDODq0Yy1nzszEfTqfT6O/vt/ZTzqIQ8J87ihoNUChG2SqY06mopnOM2f06vqt1pmllUybfb7PSm4q5PX5YQ3wDSPLrzMprfOWtq6tr0PCCdPIqEAikPCbTfyKynffbtm1LqPBnaqS44v/BSueaG2q7WZHPZHgAFeMP0QCFUkU5mx7K2aEVazkbjUaNcDhsfU5m495wr6WcRT4xBA8Aylx1dbUkyePx2BzJ2KUzaWhFRYVaWlokadju5Y8//rgkJUyWevbZZ0uS2tvbRxWXeXzyEIuxTnKaLWvWrNGyZctyPtnoaaedJsMwFA6H5fV65fF41NzcPOp0zDhL4ZoFUPooZyln81XOVlRUqLq6WsuXL5ff7x+0yt1o0pFK45pFYaEBCgBQdiorKxUOhxUKhVRfX2/NSRRvw4YNg7aZFbLRVujM442BnscJD7sFg0E5nc5B8zTlUnV1tRYsWCBJWrhwoaSh58uQcjtnBgAg+yhnP2RHOStJN910U0I+Us6iENAABQCQVH6Vj+rqanV0dCgUClnLG8czK2qp7txmmlfxk9AWgp6eHr322mvW8sv5dOaZZyY8T5Xf5gSz559/fv4CA4AcoZxNRDmbWxUVFQn5SDmLQkADFACUObOyNnfuXJsjGTuzgpvqTmsqTqdTgUAgZRf9uro6SdL+/futbWa6NTU1o4rL7/dLklpbW600zNV67BKJRPTss89q+fLl1raenh41NDTk5fxmPgQCAUnSnDlzJCXm94EDBxL2JYtfRQ8AChXlLOWsKd/lbHw+Us6iENAABQBFKHmJ4vjnZsUrvnKYfHfRXB45FouptbVVTqczoWu2ecfMrDTHL+drVpzi76SZFTy7l4c2e9UkV4zN95/qLuu8efNSVrCuvfZaOZ1OrVy50nrd008/LbfbrdmzZw9Kb7h8v/766yUNzEUxefJkORwOVVVVWRVDc9nonp6eEd9jfPqp/gFIJ61IJKL6+np5PJ6E+TLOO++8hH+QshWXy+VSU1OTdac1FovJ5/PJ6/Vq3rx5kgbmh/L7/dq4caNisZhisZg2btwov9+v0047LSE9M52LL754xLgAIBOUs6lRzhZmORsMBtXZ2Wk97+vr044dO6x8lChnUSBsmvwcyApWwUMxUhZWB1HcSmKpHqmOid8WDoetFWr8fv+gVVl6e3ut/eYSvObyxuZqNeaqPl6v19pm9/LQ5tLW5lLNhpE6r1JJXoLYTM/v91uvCwQCCXmVbr4bxkCemqvSuN3uhCWsvV6v4Xa7U8YQb7jPezRpmUsxp3rEr1iUrbiSl4b2+XwJn1E881in02ls27Yt5THmKknxKyelKxvfv1LBKngoVZSz6aGcHawUylmv12uEw+Eh06KchZ0chlEAM7MBGWpvb9f8+fMLYoJBIF0Oh0NtbW1W1/N8n1tSUXxnMv1+m3eJlyxZMqrXxWKxnK9OMxKXy6WOjo6STisbGhsbNXny5FF/xpK9379CM3/+fElSW1ubzZEA2UU5mx7K2dJMKxsoZ5ErDMEDAJSU+vp6bd++PWE4QzrsrhR3d3dr2bJlJZ1WNvT09Kinp0f19fV2hwIAZYlytnDTygbKWeQSDVAAUCaS57MoVRUVFWppadHKlSvTmlOhEHR2duqUU07JyhLNhZpWNuzbt08bNmxQS0uL7f/IAEAyytnCVahlI+Usyg0NUIAGKgnBYFAul8vuUICcqaqqSvl3KaqsrFRra6ueffZZu0NJy+zZs62JXUs1rWwIhUK69957VVlZaXcoiEMZCgygnC1chVo2Us6i3NAABUi65557VFtbq1AoZHcoYxKLxay5BzJ5XaqHuYpLOoZKw+FwqKmpSaFQKO1lewtdpnltJ8MwEh6lrqKiIqO5C1C4lixZQqW4AJV7GWq+tru7W83NzcM2xIVCIblcLrlcrozyi3K2sFHOothRziLXaIACJK1fv97uELJix44dGb1uz549Q+6LX751JIZhqL+/33oejUatStjVV1+t5uZmLViwoCS6pWea1wBQasq9DJUkn8+np556SgsXLhyyYSkYDKq5uVmtra1qbW3V1q1b1dzcPKrzUM4CAIoZDVBAiYjFYqOuyJrefPNN9fb2Jty16+/vl9frHfVdkPjj48eOV1dXq6WlRdLA5JXFfId2LHkNACg8Y/1dX758uZYvXz7k/r6+PtXW1mrZsmWqqKhQRUWF3G63Fi5cOOo5dChnAQDFigYolKVYLKZgMCiHwyGXy6V9+/Yl7I9EIlY3+VgspoaGBjU2NqZ8vcPhUHNz86CJJ83XS1Jzc7McDocaGhoGnSud9OK72A+1zefzWXddk48dyezZs3XaaaclbOvs7NTXv/71hG2NjY0J+TBalZWVuuOOOxQKhaw7m+WW1wBQ7ChDR2/nzp2SpKlTp1rbpkyZIknavXu3tY1ylnIWAEoZDVAoSwsWLND27dsVjUbV0dGhl19+OWF/fX29NT/Dnj175Ha7dfDgwYTXHz582OopFAqFEu42VlVVWa/v7u7Wrbfeqmg0Kkk666yzBlXYRkovvru9qbe3N+F5/J3X0c49kKqX0/bt21VdXZ12Gum64IILJElbt26VVH55DQDFjjJ09LZv3y5JCTd7zLI323NnUc4CAAqWARSxtrY2Y7SXcUdHhyHJ2Lt3r7UtGo0akhLSMp9Ho9GE12/bts2QZPT391vburq6DElGIBAY9Pp44XDYkGT4fL6spDdUzGMVDocTzj1aI8VR7nktyWhra8voteUkk+83MBK+fx+qq6sz6urqRvUaytDhDZXGaLdnep6h9pfb58H3PD2Us8gFvn8YDr84KGqZFJxutzutSuBQFZ9Urzcr306nc8TXJ28fS3q5aoDyer0JlcfRyrRinKxU89p8LQ8ePOx5UDEekEkDFGXo8NKNe6znHOl15f552P0bw4NHuT8oZzEUh2HQpxXFq729XfPnzx9V12xzHoHk1yRvT/e4sb5+LMelm9ZoRCIRrVu3btjJVEcyXByxWEyTJ0+W1+u1zlFuee1wOLR48WLNnDlz1K8tJ88//7zWrl2rTZs22R0KSshNN92ktrY21dXV2R2K7ebPny9JamtrS/s1lKHDGyoNcwhbqjjdbveoVxKknB0e5Wx6KGeRC5SzGM4EuwMAio3T6VQoFFIkEhk0d5Lb7U4rjfjjspFeNqWafDybfv7zn0uSZs2aNeKxpZzXl1xyiWpqavJ6zmJz9OhRSSKfgBJSyr/rw0kVZ19fnyTp/PPPz+q5KGcHUM6OjHIWQL4xCTnKjt/vl6RRL3tsMlvz9+/fb20zJ9YcqQA3J+qcO3duVtLLhVxNPi4N9K5as2aNnE6nZs+ePeLxpZ7XAFBsKEMzM2fOHEmJcR44cCBhXzZQzgIAChkNUCg7ZkWvsbHRuvvY2dlp7W9oaEhYLjjZtddeK6fTqZUrV1rHPf3003K73Skre8FgUNJABay1tVVOp1NOp3PU6Zl3Ds0KX3d3d0LMkqx0I5GImpqa0sqPeD09PbriiiuG3J/O8tBmRTP5756eHtXX10uSWlparO3lmtcAUIwoQ4c2VPknDax+5/f7tXHjRsViMcViMW3cuFF+vz9hZTzKWcpZAChpWZ5TCsirTFfv6O3ttSbKdLvdRn9/v+F0Oo1AIGD09/cnTKIXP2mmqb+/3/D7/dYxgUBg0Moy5r5wOGw4nU5DkuH3+wcdl256vb29VjodHR2GYRgJMRvGhyvSZDqJ+Eiv83q9htfrHXJ/fL4lP3w+n9HV1TXsa8olr8XkjGlhdR7kAt+/D2UyCblhUIamMlTZl8xcRdDpdBrbtm0btJ9ylnI2nyhnkQt8/zAcJiFHUctkEvJ8ycZkpkhPseW1w+FgcsY0FPL3G8WL79+HMpmEPF+K7Xe91BXb58H3PD2Us8gFvn8YDkPwAAAAAAAAkFM0QAE5ED/fwnBzL2DsyGsMhXlDSk9TU9OguXVQevhdLyx8HhgK5WzpoZxFrtEABeRAVVVVyr/zzeFwpPUoZoWS18UiFovl9DPPdfrpikQiuueeezR9+nTrOh9qYt9i+k7EYjF1d3erublZLpdrTGn19fWpoaFBDodDDQ0NCRNJ5yKuSCSixsZGK4/NyYyThUIhuVwuuVwuhUKhhH1XX321FixYwD/BJa5QftfLoQxNR6F8HsWCcnawYvreFHM5Kw0sRhCfz+ak/vEoZ2EnGqCAHDAMI+FRKHEM9ShmpfRe8mHHjh1FnX46YrGY6uvr9a1vfUuzZ89WNBpVIBDQihUrUlaODcNQf3+/JKm/v7+gryOfz6ennnpKCxcuHFRpHI1YLKaenh6tX79e0WhUV1xxha666qqM0xwprkgkov3792v58uUyDEOBQEC1tbWD7pwHg0E1NzertbVVra2t2rp1q5qbm6391dXVWrZsmerr67lDW8IK5Xe9HMrQdJTb+x0rylnKWSn/5axp9+7dCc/nzp2b8JxyFrbL9qzmQD6xegeKkWxaHSQajVqrDhVD+pl+v30+X8pVpBS3IlMqxfRbYr6XTJmrTmUzzeHSGG5lLlNvb68hKeFYcxWscDic8Fq32234fL6MY2R1ngGZroIHFDrK2fRQzg6t2MrZ4c5ropxFIaAHFAAUgVgspmAwaHWpbm5uTugenapbe/I2n89n3TUzt0ciEasrtiQ1NzdbXbb37ds35vQlqbGxcchu+dkWiUTk8Xg0a9aslPt9Pp9qa2uHHP6VbKR8j0QiCgaDVv6FQiE5HA65XC719fUNiq2pqcnaP5au+GPldDpTbne73Tk534wZMxKem3dVvV6vtW3nzp2SpKlTp1rbpkyZImnwHV4QSGoAACAASURBVN2amhp5PB6GCADIGsrZ9FDOpiff5aw0MOTP5XKpsbFR3d3dg/ZTzqIQ0AAFAEVgwYIFOnz4sNWNPRQKJXSPNru2x+vt7U14vnz5cutv43+GUlRVVVlzAHR3d+vWW29VNBqVJJ111llW5TjT9PNt165dkqRp06al3L9kyRJ5vV7V1taqp6dnxPRGyvf6+nrV1tZa+ed0OtXb26tQKKTvfve7VjqRSET19fX68z//cxmGoTvuuENXXXVVWjHkg/l+krvq50JfX598Pp+kgfw1bd++XZJ02mmnWdsqKysladBwA/PzNT9vABgrytn0UM5mJh/lrPleV6xYoS996UtyuVwJDUiUsygIdnS7ArKFIXgoRhpl1+Rt27YZkoz+/n5rW1dX16Bu7krRLTt5WzrHGMaHXbLju19nmn6mMvl+e73eIV9jbo8fwrB3795B+03ZzPdAIJDymFRDGNKRzXw2jIH36nQ6jWg0OqZ0RorL7P5vPka6vobaHo1GB71+NDEyNGAAQ/BQqihn00M5O7RiLWej0agRDoetz8nv94/4WspZ5BM9oACgwD3++OOSPrxLJUlnn322JKm9vT0n56yurpYkeTyenKSfKytWrBjxmIqKCrW0tEjSsN3Ls5nv5vHJwynSiTcf1qxZo2XLlqmioiKn5znttNNkGIbC4bC8Xq88Hk/C5KfpMuMstusTQGGinE0f5Wxm8lXOVlRUqLq6WsuXL5ff78940nPKWeQKDVAAUOA2bNgwaJtZMRjLCi3lrLKyUuFweFBX/3jZzHfzeKMAV9AKBoNyOp2D5mnKperqamv43cKFCyUNPV+GlNs5MwCAcjb7KGc/ZEc5K0k33XRTQj5SzqIQ0AAFAAXOrDCkuoOY6wpDKVdIqqur1dHRoVAoZM1JFC8X+R4/4Wwh6Onp0WuvvaZbb7017+c+88wzE56nym9zgtnzzz8/f4EBKDuUs7lBOWtvOVtRUZGQj5SzKAQ0QAFAgaurq5Mk7d+/39pm3kmsqanJyTnNClw+JqXOJrOCm+pOaypOp1OBQCBlF/1s5rvf75cktba2WmmYq/XYJRKJ6Nlnn02Y1Lanp0cNDQ15Ob+ZD4FAQJI0Z84cSYn5feDAgYR9yeJX0QOATFHOpo9yNn2FUM7G5yPlLAoBDVAAUOCuvfZaOZ1OrVy50rpr9fTTT8vtdmv27NnWceZdLrNSG78Er1nZib/7lVwpM5dMjsViam1tldPpTOiunWn6+Vwe2uxVk1wxNvMt1V3WefPmpaxgpZPv8emZ54w/t7n/+uuvlzQwF8XkyZPlcDhUVVVlVQzNZaPTWa0nPv1U/wCkk5a5WpDH40mYL+O8885L+GcoW3G5XC41NTVZd1pjsZh8Pp+8Xq/mzZsnaWB+KL/fr40bNyoWiykWi2njxo3y+/0JK/ZIH96xvfjii0eMCwBGQjmbPsrZwixng8GgOjs7red9fX3asWNHwvVLOYuCYNPk50BWsAoeipEyWB2kv7/f8Pv91kolgUBg0Eoqvb291qozHR0dhmEYhtPpNAKBgLXCjLnqjtfrtbaZaYbDYev1fr8/a+l7vd6MVqHJ5Pvd399vSDK6urqsbeb7i3+k4nQ6U6Y3XL6nSneoc/X29lqr0rjdbqO3t9fa5/V6DbfbnTKGeKneS/L7SSctt9s9ZFrxKxZlK66Ojo5Bq9/Ff0bxzGOdTqexbdu2lMeYqyTFr5yUrky+f6WKVfBQqihn00M5O1gplLNer9cIh8NDpkU5Czs5DKMAZmYDMtTe3q758+cXxASDQLocDofa2tqsrud2M1eKKbTvUabfb/OO8JIlS0b1ulgslvPVaUbicrnU0dFR0mllQ2NjoyZPnjzqz1gqvO+fnebPny9JamtrszkSILsK7XtOOTuAcjY/aWUD5SxyhSF4AICSUl9fr+3btycMXUiH3ZXi7u5uLVu2rKTTyoaenh719PSovr7e7lAAoCxRzhZuWtlAOYtcogEKAMpY/NwKqeZtKEYVFRVqaWnRypUr05pToRB0dnbqlFNOycoSzYWaVjbs27dPGzZsUEtLi+3/yABAOihnC0Ohlo2Usyg3E+wOAABgn6qqqoS/C214QKYqKyvV2tqqlpYWVVdX2x3OiOInCS3VtLIhFArp3nvvVWVlpd2hAEBaKGcLQ6GWjZSzKDc0QAFAGSuVinAqFRUVGc1dgMLF5wmg2FDOopjweSLXGIIHAAAAAACAnKIBCgAAAAAAADlFAxQAAAAAAAByigYoAAAAAAAA5BSTkKMkPP7443aHAIzKrl27NHHiRLvDKGi7du2SlPn3+8SJExo3jvsswHAef/xx3XDDDXaHgQJnGIYcDofdYYwK5ezIxlrOAsBoOYxSXpoBJW/37t265JJL7A4DAFBEdu3apYsvvtjuMGzn9Xr1r//6r3aHAQAoMZSzGAoNUACAknP06FE1NzerqalJvb29+qu/+istXbpUF110kd2hAUDBi0QiWrdunR588EF98MEHuuWWW3TXXXfp05/+tN2hAQCKGA1QAICSdfz4cf3oRz/S//2//1c///nPNWvWLC1dulTXXHNN0Q0nAYBce/3119XU1KRHHnlEH//4x/W3f/u3+tu//Vt98pOftDs0AEAJYHIMAEDJGj9+vG666Sa99NJL6uzs1KRJk/S1r31N1dXVevTRR3X06FG7QwQA2+3evVs1NTU666yz9NOf/lQPPPCA3njjDf3TP/0TjU8AgKyhAQoAUBZmzZqlZ555RuFwWNXV1aqvr9fnPvc5PfDAAzp8+LDd4QFAXhmGoaeeekpXXnmlLrnkEvX29ioYDGrv3r1qaGjQySefbHeIAIASwxA8AEBZ6uvr0+rVq/Xwww9r/Pjxamho0OLFi3XqqafaHRoA5MyRI0cUCATk8/n02muv6ZprrtHSpUs1a9Ysu0MDAJQ4GqAAAGXtnXfe0YYNG7R27VpFo1F985vf1JIlS3TWWWfZHRoAZM3hw4f10EMPae3atXr77bc1b948LV26VF/84hftDg0AUCZogAIAQNL777+vRx99VD6fT//5n/+p66+/Xh6PR5deeqndoQFAxt5++22tWbNGDz30kI4fP676+nrdeeed+sxnPmN3aACAMkMDFAAAcU6cOKEnn3xSq1atUnd3t2bOnKmlS5fquuuu07hxTJ0IoDj86le/ks/n02OPPaZPfOITuv3223Xbbbdp8uTJdocGAChT1KQBAIgzbtw4/dVf/ZW6urq0Y8cOTZ48WTfccIO++MUv6uGHH9aRI0fsDhEAhrRz505df/31+sIXvqDnn39e69at05tvvqlly5bR+AQAsBUNUAAADOHLX/6yQqGQXn31VV188cW67bbb9NnPflb333+/Dh06ZHd4ACBpoOdmR0eHLrvsMl122WU6ePCgNm/erF/+8pe69dZbddJJJ9kdIgAADMEDACBdBw4c0OrVq+X3+yVJCxcu1J133qmpU6faHBmAcvTBBx/osccek8/n0969e+V0OuXxePTlL3/Z7tAAABiEBigAAEbp0KFDWr9+vdauXauDBw+qrq5OS5cu1V/+5V/aHRqAMhCNRvXQQw/p3/7t3/Tf//3fuvnmm7VkyRJ+gwAABY0GKAAAMnTkyBE99thjampq0p49ezR37lzdfffd9D4AkBNvvfWW1q5dq4ceekgOh0MNDQ1avHgxvTABAEWBBigAAMbIMAyFQiH5fD4999xzuuSSS3TXXXfphhtuYOU8AGP26quvyufzKRAI6FOf+pTuvPNOLVy4UB//+MftDg0AgLRRKwYAYIwcDodcLpd27NihF154QVOnTlVNTY0+//nPy+/36/3337c7RABFaPv27bruuut07rnn6sUXX9RDDz2kN954Qx6Ph8YnAEDRoQEKAIAsuvTSS7Vlyxb98pe/1JVXXqnFixfrL/7iL7Ry5Uq98847docHoMCdOHFCP/rRj3TJJZfoyiuv1KFDh9TR0aFXX31V3/72tzVp0iS7QwQAICMMwQMAIIfefvttrV27VuvXr9fx48f1N3/zN7rjjjt0+umn2x0agALy/vvv65FHHlFTU5P279+vG264QUuXLtWMGTPsDg0AgKygAQoAgDw4fPiwWlpatHr1ar399tv6xje+IY/Ho+rqartDA2Cjd955Rw8++KDWrVunQ4cO6Zvf/KY8Ho/+1//6X3aHBgBAVtEABQBAHh09elTBYFCrVq3Sq6++qq9+9au66667NHv2bLtDA5BHvb29Wr16tVpaWnTSSSfJ7XZr8eLFqqqqsjs0AABygjmgAADIo4kTJ2rBggXq6enRU089pSNHjuiqq67ShRdeqE2bNun48eN2hwggh8LhsG6++WZNmzZNTzzxhFasWKE333xT//qv/0rjEwCgpNEABQCADRwOh6699lp1dnZq9+7dOuOMM1RXV6czzzxTDz74oN599127QwSQRc8++6yuueYaTZ8+Xa+88op++MMf6vXXX9cdd9yhj33sY3aHBwBAztEABQCAzS666CJt2rRJv/rVrzRnzhx5PB6dfvrp+pd/+RcdPHjQ7vAAZOjYsWMKBoO64IIL9JWvfEVHjx7VM888Y/WCmjhxot0hAgCQN8wBBQBAgYlEIvr+97+v73//+3rvvfd0yy236M4779QZZ5xhd2gA0vDuu+/q4Ycf1po1a9Tb26u//uu/1l133aULLrjA7tAAALANDVAAABSoP/7xj/rBD36gBx54QG+99ZZqamrk8Xj4JxYoUAcPHtS6deusYbTf/va3tWTJEhqPAQAQDVAAABS8Y8eOafPmzbr//vsVDod19dVXy+PxaM6cOXaHBkDS/v371dTUpEceeUQnn3yybrvtNt1+++361Kc+ZXdoAAAUDOaAAgCgwE2YMEHz5s3TL37xC/30pz+VJGsy47a2Nh07dszmCIHy9POf/1w33XSTzjzzTD3zzDO677771Nvbq3vvvZfGJwAAktAABQBAEbn66qv105/+VL/4xS/0+c9/Xt/+9rc1bdo0rVmzRn/84x/tDg8oeYZh6Cc/+Ylmz56tCy+8UPv379djjz2mffv26fbbb9fJJ59sd4gAABQkGqAAAChC5513ngKBgF5//XW5XC55vV6ddtpp8nq9ikQidocHlJyjR4/qscce03nnnadrrrlGkyZN0k9/+lO99NJLmjdvnsaPH293iAAAFDTmgAIAoAS88847+t73vqfvf//7OnTokL71rW/J4/Fo2rRpdocGFLXDhw/r4Ycf1urVq3XgwAF94xvfkMfj0XnnnWd3aAAAFBUaoAAAKCHvvfeeHnnkET3wwAPav3+/brzxRt111126+OKL7Q4NKCpvv/221q1bpw0bNuiDDz5QfX297rzzTp1++ul2hwYAQFGiAQoAgBJ0/PhxbdmyRatWrdKLL76oK664QkuXLtXcuXPlcDjsDg8oWPv27VNTU5MeffRRffzjH9fixYvV0NCgU045xe7QAAAoaswBBQBACRo/frxqamq0e/dudXZ26uSTT5bT6dS5556rjRs36siRI3aHCBSU7u5u/fVf/7XOPvtsdXZ2as2aNert7dU//uM/0vgEAEAW0AAFAECJmzVrlrZu3aqenh6df/75uvXWW/W5z31OPp9Phw4dsjs8wDaGYejf//3fdfnll+tLX/qSfvOb32jTpk3au3evFi1apI985CN2hwgAQMlgCB4AAGXmrbfe0po1a9Tc3Kzx48dr0aJF+s53vqMpU6bYHRqQF0eOHFF7e7tWrVqlPXv2aO7cuVq6dKmuuOIKu0MDAKBk0QAFAECZikajWr9+vdauXavf//73uvnmm+XxePT5z3/e7tCAnDh06JAeeughrVmzRgcPHlRtba08Ho/OOeccu0MDAKDk0QAFAECZ++CDD/Too4+qqalJv/71r3Xdddfp7rvv1qWXXmp3aEBWHDhwQP/2b/+mDRs2SJIWLlyo73znO/r0pz9tc2QAAJQPGqAAAIAk6cSJE/rxj38sn8+nnTt36tJLL9Xdd9+t6667TuPGMW0kis8vf/lLNTU16bHHHtMnP/lJfec739GiRYs0efJku0MDAKDsUJsEAACSpHHjxunGG2/UCy+8oOeee06f+tSndOONN+ov//Iv9fDDD+uDDz6wO0QgLc8995yuv/56nXPOOdq5c6cefPBBvfHGG7r77rtpfAIAwCY0QAEAgEFmzpypH//4x3rllVc0c+ZM3XbbbfqLv/gL3X///YpGo8O+9rnnnpPD4dDPfvaz/ASLkvbOO+/ojDPO0KpVq4Y97sSJE9qyZYsuu+wyXX755Tp48KCefPJJvfbaa/qbv/kbnXTSSXmKGAAApMIQPAAAMKIDBw5o7dq1Wr9+vQzD0KJFi7R48WJ95jOfGXTsN7/5TbW2tmrSpEnq6OjQnDlzbIgYpSASiejKK6/Unj17JEmHDx/Wn/7pnyYc88EHH2jjxo164IEH9Otf/1pOp1N33XUXc5gBAFBgaIACAABpO3TokPx+v1avXp1yFbG+vj6dccYZOn78uMaNG6fx48dr8+bNcrlcNkeOYnPgwAFdccUV6u3t1dGjRzV+/HitXr1at99+u6SBVRwffPBBrVu3zlrFcenSpTrrrLNsjhwAAKRCAxQAABi1I0eOqL29XatWrdKePXs0d+5ceTwehUIhrVu3TkePHpUkORwOjR8/Xu3t7aqpqbE5ahSLvr4+XX755Tpw4IB1LUnSlClT9MILL2jdunVqaWnR+PHjtWjRIt1xxx069dRTbYwYAACMhAYoAACQMcMwtHXrVt1///167rnnNHHixIQGA2mgEcrhcGjjxo26+eabbYoUxWL//v368pe/rN/97ncpr6UJEybo1FNP1eLFi7Vo0SJ97GMfsylSAAAwGjRAAQCArGhoaFBLS4uOHTuWcv+4ceP00EMPqb6+Ps+RoVj86le/0hVXXKHf//73gxqfpIFr6PTTT9evfvUrTZo0yYYIAQBAplgFDwAAjNn777+vzZs3D9n4JA2sUrZw4UJ9//vfz2NkKBavvvqqLrvsMr3zzjspG5+kgWvojTfe0H/8x3/kOToAADBWNEABAIAxa21t1TvvvDPicYZh6Pbbb9eqVavyEBWKxcsvv6yZM2fq0KFDwzZiStKECRN033335SkyAACQLQzBAwAAY2IYhsaNG/09reXLl8vr9eYgIhST7u5ufeUrX9H7778/YuNTvJdfflnTp0/PYWQAACCb6AEFAADGxOFw6DOf+UzKfRMnTtRJJ52kCRMmDNrX2Nio+fPn5zo8FLCtW7fqS1/6kv7whz8kND6NGzdOkyZN0qRJkwY1bqa6lgAAQOGjBxSAsnPSSSfpyJEjdocBAABQUHbt2qWLL77Y7jAAlChuIQEoO0eOHNENN9yguro6u0MBitbzzz+vtWvXatOmTRm93jAMHTp0SO+++66mTJmS5egKy9q1ayVJixcvtjmSwvPWW2/pIx/5iCZPnqyJEyfaHQ5Q1m666Sa9/vrrNEAByBkaoACUpZqaGtXU1NgdBlC0zFXK+B6N7Mknn5REXgEAgPLGHFAAAAAAAADIKRqgAAAAAAAAkFM0QAEAAAAAACCnaIACAAAAAABATtEABQAAAAAAgJyiAQoAANiqsbFRjY2NdodRUBwOR8IjlUgkoqampjxHhlxqampSLBbLWnpcI6VnuGsknd8NALATDVAAAKCsxWKxgv1nzTAMGYYxaHskEtE999yj6dOnW/9sDtWIl/xPaaG+V2ngs+ju7lZzc7NcLteY0urr61NDQ4McDocaGhrU2dmZ07gikYgaGxutPA4GgymPC4VCcrlccrlcCoVCCfuuvvpqLViwQJFIJONY4+PhGhlevq8RSerp6UnI54aGhkHHZHqNDPV7AQAFwwCAMiPJaGtrszsMoKi1tbUZpVKN6OjoyOl7qaurM+rq6kb1GklDxhSNRg2n02l0dXVZzwOBgCHJ8Hq9KV/T399vSDL6+/tHF3yeeb1ew+v1Dvv+0xGNRo2Ojg7rbzN/zG3Zjqu/v9/6PAzDsM7n8/kSjgsEAobT6TSi0agRjUYNt9tt+P3+hGO6urqsYzLFNTKyfF8jJr/fbx2T6nzZuEYyzRvqRwByzWEYNJMDKC8Oh0NtbW2qq6uzOxSgaLW3t2v+/PlFf7c9FotpwYIFCoVCOXsv8+fPlyS1tbWl/RqzB0qqmJqamhSNRrV8+fKUrwkEApo3b17KNIvl8xru/acjFArJ6XRmNc3h0uju7taMGTOGPbavr0+nn366urq6rGN7enp03nnnKRwOq7q62nptQ0ODpk2bpiVLlmQUJ9fIyPJ9jQx3XlO2rpFM3wf1IwC5xhA8AABgm0gkomAwaA1ZSX4eCoXkcDjkcrnU19dnHWMOUZGk5uZmayjLvn37rLRTDSdK3ubz+awhLvHbC3VeqkgkIo/Ho1mzZqXc7/P5VFtbO+Twr2SxWEzBYNB6783NzQlDe9L5POKPbWpqsvaPZTjTWA31D77b7c7J+ZIbn8w5erxer7Vt586dkqSpU6da26ZMmSJJ2r17d8Lra2pq5PF4MhqKxzWSnnxfI9JAA5PL5VJjY6O6u7sH7c/XNQIAtrGh1xUA2Ep0MQfGLFtD8JxOZ8Jwkfjn5vCh3t5eQ5LhdrsNwzAShq/EDzFyu92GJGPv3r2GYXw4pCg+TjOt+G3Jzw3jw+E02ZDNIXjmcMHe3t6UrzEMwxoGFA6HU+6P53Q6reE9/f39hvP/s3f/oY3cd/7HX3NJmrsLPenSQ97EV+/3j2XNQsGblMt6uV5D7HBhTUfbwnm7tuuGgrzIxzVsWf9xMRLLIrPJHxYXmkCMZQpGyBbZ/NF6aPafXXNeSuINl8OCyx81xzbyQYpFj2oucNe7NDffP3wzK8myLdkaS7KfDxC2Zkaf+cyPTa1XP5/3mGbZ1J5arkfpZxcXFx3HcZy7d+9W7UOtdjr+/SoWiweaXuWqpV/5fN67Bu696DiOd39Wa9M0zW1t7Le/3CP7cxj3iHtt3JdpmmVTHht1j+z33PD3EQC/EUABOHb4Aws4uEbWgKolEKplm7W1tW11d/bbViM1MoByg4OdPuM4D+v/VAYglZ9zA4DSL8AffPCBI8kLCXbqS+Uyt35O5Tb7DfEafU3u3r174LpKjrN3v0oDzlruxZ2Wu2FIZQ2pWnCP7M9h3SPFYtFZW1vzrlNpfadG3SMEUABaFVPwAADAkeDWR5mYmGhyT/wzNTW15zaBQEBzc3OStOsUnVu3bkmSQqGQt+zMmTOStmp81cPdvnKKYy39PQxvvPGGJicnFQgEfN1PV1eXHMfR2tqaYrGYJiYmlEql6m7H7ed+7mXukf05rHskEAiop6dHiURCs7Oz255yV0870tH+7x2Ao4cACgAA4IgJhUJaW1uTZVmKRCJeTaJSMzMz25a5X2rr/VLsbu/832PgS1/Nls1mZZrmtjpNfurp6dHo6Kgk6cqVK5J2rjkk+Vt3aCfcIw814x6RpEuXLpWdx1a7RwCg0QigAADAkcIXtS09PT1aWlqSZVmanp7ett79sltt9Mt+z2FpEfhWkMvl9PHHH2tsbOzQ93369Omy99XOt1uk+9lnnz28jpXgHmnuPRIIBMrOYyveIwDQSARQAADgSHC/2A4MDDS5J/5xQ4Jqo1WqMU1Ti4uLVac5uY9af/DggbfMbXdwcLCufs3OzkqS0um014b7xLNmKRQKunPnjhKJhLcsl8tpfHz8UPbvnofFxUVJ0ksvvSSp/Hx/+umnZesqlT5Fr1bcI7VrhXuk9Dwe1j0CAM1CAAUAAJqm8nHupe/dL6mlX6QrR2K4j5K3bVvpdFqmaZZNY3FHF7jhVOmjz90vmaWjDtwvw/F4XPF4/IBH13juqJrKcME9L9VGqly+fLnql9QLFy7INE3dvHnT+9zt27cVjUbV19e3rb3drsfFixclbdXzCQaDMgxDHR0d3pfrZDIpwzCUy+X2PMbS9quFKLW0VSgUFIlENDExUVZz6OzZs2UBZaP6FQ6HlUwmvdEqtm1renpasVhMly9flrRVH2p2dlbz8/OybVu2bWt+fl6zs7Pq6uoqa89t57nnnqu7r9wjrXmPZLNZLS8ve+83NjZ079497zxKB79HAKDlNan4OQA0jXjKC3BgjXoKnkqeGFbtVW2b0mVra2ve07xmZ2e3PcEqn897693HlbuPgnef7OU+PS8Wi3nLYrHYvp/OVamRT8Hb3Nwse9x96baV56hS5WPc3fZmZ2e9zy0uLpadw1qvh+NsnWv3yV7RaNTJ5/Peulgs5kSj0ap9qHbcux1PLW25j7Ov9ip96luj+rW0tLTt6Xel16iUu61pms7du3erbuM+aa706XO19pV7pPXvkVgs5qytre3Y1n7vkcq+1Iu/jwD4zXCcFqj8BwCHyDAMZTIZb2oBgPotLCxoZGSkaQWE3SdotcOfMSMjI5KkTCZT82d2Oz53lNa1a9fq6odt274/4Wsv4XBYS0tLR7qtRojH4woGg1WvcS195R5p7bYaYbd7ZL//feTvIwB+YwoeAABAG4lEIlpZWSmbTliLZgcLq6urmpycPNJtNUIul1Mul1MkEtm2rta+co+0bluNsNs9AgCtjAAKAAC0lcq6UcdNIBDQ3Nycbt68WVNdmlawvLysJ598siGPuW/VthphfX1dMzMzmpub2xYG1dNX7pHWbKsRdrtHAKDVEUABwBFk27Y3BP+o7Xd1dVXxeNwrGBuPx5XL5VQoFJpyzLU6ytfksHV0dFT9/Shy7/NKoVBI6XRad+7caUKv6tfX1+cVxz6qbTWCZVm6ceOGQqHQtnX19pV7pPXaaoTd7pGd/nsBAK3i0WZ3AADQePfu3TuS+43H4/rNb36jH/3oR95jswuFgu7fv6+zZ8/6uu+DOqrXpBnaoe7TQdVyH0GH4gAAIABJREFUjIFAoO4aP2htjb6e3CNHz27X8zj8txFAeyOAAoAjxrZtpVKpI7dfd6RTZRHYUCgk0zT1wQcf6Pz5877t/yCO6jUBAAAAasUUPACogW3bymaz3vD2al/qq21TWasmm80qHA5L2hpGbxiGwuGwNjY26tqfGyyUTkNz9zU9PS3LsiRtH45fKBSUTCa9/S4vL9fVt0bvV9oKluLx+K7nf3V1VVNTU7sWga1Wn4Nrsr9rAgAAADScAwDHjCQnk8nU9RnTNJ1YLOa9j0ajZe/dbWZnZx3HcZzNzU3HNE3HNE2nWCx66yU5kpwPPvjAcRzHyefzjiQnGo3Wtb9oNOpIcjY3N6u24e6nlNunxcVFx3Ec5+7du44kZ21trea+NXq/juM4sVhs27msFIvFvP3Wg2uyv2tSi0wms61dVDc8POwMDw83uxsAsKv9/H0EAPXgL0cAx069f2AtLi5uCz8++OADxzRN7737Bb5yG0nel3x335Vf2iuX1bK/WCy2a8hQbT9uu5X7dkOUWvrmx35rUa3dvXBN9r/fWhBA1Y4ACkA7IIAC4DfDcahWB+B4MQxDmUxGw8PDNW0fDodlWdauxT3Hx8c1MzNTto1t2woGgzJN06tb5E6BKt2uclkt+3NtbGzo1q1bmpiYKGuj2n7cdqtxHKemvvmx31rs1I/dcE38vSYLCwsaGRnRO++8U9P2x9mPf/xjSdIrr7zS5J4AwM4uXbpU199HAFAvAigAx069AVQt4cdO21QuryVQqDVsSaVSsixL09PT6u7urns/tRxDtWWN3m8t3DCpWCwqEAjU9Bmuib/XxA2gAABHBwEUAD9RhBwA9mCapiQpl8vtuU1pgWtXNBpt+P6y2ayuXLmit956S6dPn66r/fX19bq2b4X9DgwMSJI++eSTmj/DNfF3vy5nazo/r11ew8PDGh4ebno/ePHixWu3FwD4jQAKAPbghg8zMzOybVvS1nSn8fFxbxv3/y188OCBt8zddnBwsOH7GxoakiR1dXXV3O7s7KwkKZ1Oe+26T0KrVbP2a5qmTNPUzMzMjttsbGyUtck18Xe/AAAAQF0cADhmVGeRTfeJYfq/Ys76v6eM/fKXv/S2KRaL3hPW3ELVi4uLZcWhNzc3vc+7T2ErFoveMvdztezPXZ/P551f/vKX29pw129ubjrT09Pb9l/6yufzNfet0ft1nNqegld6XirPheNsPR2u9NxzTQ52TWpBEfLaUYQcQDuo9+8jAKgXfzkCOHb28wfW5uamE4vFvCeFVQYg7jazs7Pel/nFxUUvOHD3W/raaVkt+1tbW/PWudtGo1EvQKhc78rn8167pdvX2rdG79dxag+gHGcrgFlaWnKi0ajXL9M0ndnZ2arhCddkf9ekFgRQtSOAAtAOCKAA+I0i5ACOnXqLkAPYzi1Czp8Re3OLtWcymSb3BAB2xt9HAPxGDSgAAAAAAAD4igAKAAAAAAAAviKAAgAAQNvjSY5HTzKZ9J7UCQBofwRQAACg7di2LcMw2rZ9NFahUND169f1zDPPyDAMGYaheDxedVt3femrVdm2rdXVVaVSKYXD4QO1tbGxofHxcRmGofHxcS0vL/velmVZCofDMgxD4XBY2Wx2x23C4bAsyypb9+KLL2p0dFSFQmHffQUAtA4CKAAA0Hbu3bvX1u2jcWzbViQS0csvv6y+vj4Vi0UtLi5qamqqagjlOI42NzclSZubmy1dSH96elo///nPdeXKlW3hTD1s21Yul9Pbb7+tYrGo559/Xv39/ftqs9a2ksmkwuGwEomEHMdRIpHQ0NBQ2Si1bDarVCqldDqtdDqt9957T6lUylvf09OjyclJRSIRRkIBwBHAU/AAHDs85QU4uGY+Bc+2bY2OjsqyLF/23+j2eQqev5LJpIrFohKJRNlyd2TT4uKiLl++vO1zhmG0dPhUyj2W/fbXsiyZptmQNmtta6dlpmlqaWlJGxsbOnnypD744AP19vZKknK5nM6ePau1tTX19PR4nxsfH9epU6d07dq1uvqK+vD3EQC/MQIKAAAcGtu2lc1mvalPqVSqbHpNtWlRlcump6e90Rbu8kKh4E3lkaRUKuVND1pfXz9w+5IUj8d3nNaF5igUCpqYmNALL7xQdf309LSGhoaqTv2qZq/7s1AoKJvNeveZZVne9LKNjY1tfUsmk976g0x5O6jKwMgVjUZ9a2t6elqStLq6Kkne+XGDwvfff1+S9PTTT3ufeeqppyRJH374YVlbg4ODmpiYYCoeALQ5AigAAHBoRkdH9dlnn3nToCzLKpte406NKpXP58vel450cRxHjuOoo6PDqyGzurqqsbExFYtFSVJ3d7cXQu23fbSm+/fvS5JOnTpVdf21a9cUi8U0NDSkXC63Z3t73Z+RSERDQ0PefWaapvL5vCzL0muvvea1UygUFIlE1NnZKcdxdPXqVfX399fUh8PgHs/AwIBvbbnn/vz581pdXdX777+vzc1Nb2TTysqKJKmrq8v7TCgUkqRt0/nc6+tebwBAeyKAAgAAh2J5eVmWZenixYuStr5sTk5OyrIs3b5921tWqfQL6k5KQyJ3Ok8gEPBGZbhfaPfbvrQVTFVO80JzuSNldruGExMTMk1TZ8+eLRsNV6mW+3Npacnb3r3P3H3PzMxsa8ud+tfX1ydJevfdd+s+Rj989NFHMk1T3/zmN31tK5FIKBqN6vz58/r444/1+OOPe+tKz1elygAqEAhI0q7XDwDQ+gigAADAobh165ak8hDozJkzkrZqSvnBHW0xMTHhS/torqmpqT23CQQCmpubk6Rdp3E18v50t6+c3llLfw/DG2+8ocnJSS/Y8autZDKp559/3huNODo6uq9i4m7b/DsGgPZGAAUAAA5FtREP7hfLgzzhC9hLKBTS2tratil1pRp5f7rbu1M4S1/Nls1mZZqmN4LLr7ay2awmJiZ04cIFBQIBr7D/O++8I2nnWlLS/mpTAQBaHwEUAAA4FO4XzmojUPz+wskXWvT09GhpaUmWZXkFskv5cX+22pSxXC6njz/+WGNjY763NTQ0JOlhiNfR0SFJunLliqTq59stVP7ss88euH8AgNZDAAUAAA6F+2jvBw8eeMvckSiDg4O+7NMNABpRbBmtxw2Sap3WZZqmFhcXq06Fa+T9OTs7K0lKp9NeG+5T8ZqlUCjozp07ZXXMcrmcxsfHfWmrcoSTG0S5y1966SVJ5ef7008/LVtXKRaL1d1XAEDrIIACAACH4sKFCzJNUzdv3vRGPdy+fVvRaNQr0iw9HG3ihkfuY9wleV9wS0dPVH6pz2azkrbCg3Q6LdM0y74M77f9eDyueDy+/xOAhjt9+rSk7QGUe39VG810+fLlqkFGLfdnaXvuPkv37a53C5lPTU0pGAzKMAx1dHR4QVYymZRhGDU9Fa+0/WpBWy1tuU/lm5iYKKtLdfbs2bJwtpFtXb16VdLDf4/uvzN3eVdXl2ZnZzU/Py/btmXbtubn5zU7O7utqLw7Muq5557bsV8AgNZHAAUAAA6FWwzaNE11dHR4hZlff/31su1effVVmaap7u5uWZal3t5eb+TKjRs3JMkbefHmm29qdHS07PNnzpxROBxWMBhUV1eX0ul0Q9tH6zh37pykhyNnJHlhj6Sy+6xUIpGoOkJnr/vTbVeSgsFg2c/S9aFQSPl83gu6otGo8vm8F6wUi0VFo9E9A03DMMrad8OsUrW0df369R3rWHV3d/vSVl9fn+7evauVlRUZhqH5+XndvXu3LGweGxvTwMCAgsGgRkdHNTg4WHVKn3t93esNAGhPhtMK1RAB4BAZhqFMJuNNtwBQv4WFBY2MjLREUWWX+8W8lfokSSMjI5KkTCbT5J4cTe4ItWvXrtX1Odu2G/IUuIMIh8NaWlo60m01QjweVzAYrPsaoz78fQTAb4yAAgAAQNuKRCJaWVkpm0pZi2aHT6urq5qcnDzSbTVCLpdTLpdTJBJpdlcAAAdEAAUAANpeaW2eanV/cHS5U+du3rxZU02lVrC8vKwnn3xSvb29R7atRlhfX9fMzIzm5uaaHhgCAA7u0WZ3AAAA4KBKa/N0dHS03DQ8+CsUCimdTmtubk49PT3N7s6eSusgHdW2GsGyLN24cUOhUKjZXQEANAABFAAAaHsETggEAtQIOmK4ngBwtDAFDwAAAAAAAL4igAIAAAAAAICvCKAAAAAAAADgKwIoAAAAAAAA+Ioi5ACOpZGREf30pz9tdjeAtrWxsSFJunTpUpN70vru378viXMFAACON8PhsTEAjpnJyUn967/+a7O7AQC6c+eOvva1r+nEiRPN7gqAY+6RRx7RP/zDP/DfIwC+IYACAABoEsMwlMlkNDw83OyuAAAA+IoaUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPCV4TiO0+xOAAAAHHVzc3P627/9W3V3d3vL/u3f/k1f+cpX9Md//MeSpF//+tf6y7/8S/3sZz9rVjcBAAB88WizOwAAAHAcbG5u6vPPP9e//Mu/lC23bbvsvWVZh9ktAACAQ8EUPAAAgEMwNDQkwzB23ebRRx/V66+/fkg9AgAAODxMwQMAADgkf/EXf6GPPvpIO/35ZRiGfvWrX+nkyZOH3DMAAAB/MQIKAADgkHzve9/TI488UnXdH/zBH+i5554jfAIAAEcSARQAAMAh+e53v6v//d//rbrOMAy9/PLLh9wjAACAw0EABQAAcEhOnDih559/fsdRUIODg4fcIwAAgMNBAAUAAHCIvv/972+rAfXII4/ohRde0J/92Z81qVcAAAD+IoACAAA4RN/5zne2jYByHEff//73m9QjAAAA/xFAAQAAHKJAIKALFy7o0Ucf9ZY99thj+va3v93EXgEAAPiLAAoAAOCQjY6O6osvvpAkPfroo/rWt76lL3/5y03uFQAAgH8IoAAAAA7Zt771Lf3RH/2RJOmLL77QyMhIk3sEAADgLwIoAACAQ/aHf/iH+pu/+RtJ0hNPPKGBgYEm9wgAAMBfj+69CQAArev3v/+9lpaWvOlMQLv48z//c0nSyZMntbS01OTeAPXr7e3VV7/61WZ3AwDQJgyn8jnAAAC0kZ/+9Kf6zne+0+xuAMCx84Mf/EA/+clPmt0NAECbYAQUAKCt/ed//qekrcfY43hx6yZlMpkm96T1GYahTCaj4eHhZncFR8TIyIj++7//u9ndAAC0EWpAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAADj24vG44vF4s7vRkgqFgpLJZLO7gQZKJpOybbvZ3QAAHDMEUAAAAE1m27YMw2h2N7YpFAq6fv26nnnmGRmGIcMwdgzq3PWlr1Zl27ZWV1eVSqUUDocP1NbGxobGx8dlGIbGx8e1vLzse1uWZSkcDsswDIXDYWWz2R23CYfDsiyrbN2LL76o0dFRFQqFffcVAIB6EUABAIBjL5FIKJFING3/9+7da9q+d2LbtiKRiF5++WX19fWpWCxqcXFRU1NTVUMox3G0ubkpSdrc3JTjOIfd5ZpNT0/r5z//ua5cubItnKmHbdvK5XJ6++23VSwW9fzzz6u/v39fbdbaVjKZVDgcViKRkOM4SiQSGhoaKhulls1mlUqllE6nlU6n9d577ymVSnnre3p6NDk5qUgkwkgoAMChMZxW/usAAIA9LCwsaGRkpKW/7MIfIyMjkqRMJtPknhyMbdsaHR2VZVm+3ceGYSiTyWh4eLjmzySTSRWLxW3BnDuyaXFxUZcvX666r3b59+gey377a1mWTNNsSJu1trXTMtM0tbS0pI2NDZ08eVIffPCBent7JUm5XE5nz57V2tqaenp6vM+Nj4/r1KlTunbtWl19lY7Ovz8AwOFhBBQAADjWCoWCstmsNxWr8r1lWd5Up42NDW8bd4qTJKVSKW/a1Pr6utd2telolcump6e9US6ly5tZl6pQKGhiYkIvvPBC1fXT09MaGhqqOvWrGtu2lc1mveNLpVJl079qOeel2yaTSW/9Qaa8HVRlYOSKRqO+tTU9PS1JWl1dlSTv/LhB4fvvvy9Jevrpp73PPPXUU5KkDz/8sKytwcFBTUxMMBUPAHAoCKAAAMCxFolENDQ05IVApe9XV1dlmqby+bwsy9Jrr70mSero6PBq66yurmpsbEzFYlGS1N3d7YVQ7pS0Uvl8vux96Qgjx3FaYvTQ/fv3JUmnTp2quv7atWuKxWIaGhpSLpfbs73R0VF99tln3jQ9y7LKpn/Vcs6lrfApEomos7NTjuPo6tWr6u/vr6kPh8E9noGBAd/acs/9+fPntbq6qvfff1+bm5veyKaVlRVJUldXl/eZUCgkSdum87nX173eAAD4iQAKAAAca0tLSzu+d6cwuV/mZ2ZmJJVPf3K3CQQC3mgV94u++8W/VGkwsJtm1qVyR8rs1teJiQmZpqmzZ8+WjfqqtLy8LMuydPHiRUlb52RyclKWZen27duSajvnpW25U//6+vokSe+++27dx+iHjz76SKZp6pvf/KavbSUSCUWjUZ0/f14ff/yxHn/8cW9d6fmqVBlABQIBSdr1+gEA0CgEUAAAAA3ijkKZmJhock8OZmpqas9tAoGA5ubmJGnXaVy3bt2SVB7GnTlzRtJWDbd6uNtXTmOspb+H4Y033tDk5KQX7PjVVjKZ1PPPP++NuhsdHd1XMXG37Xa/XwEA7YEACgAAAPsSCoW0tra2bUpdqWojctzgo96nxbnbu1MVS1/Nls1mZZqmN4LLr7ay2awmJiZ04cIFBQIBr4D9O++8I2nnWlLS/mpTAQDQKARQAAAADXacvuj39PRoaWlJlmV5BbJLuYFItRFS+z1PrTZlLJfL6eOPP9bY2JjvbQ0NDUl6GOJ1dHRIkq5cuSKp+vl2C5U/++yzB+4fAAD7RQAFAADQIG4w0ogi1M3kBkm1TusyTVOLi4tVp8INDw9Lkh48eOAtc9sdHBysq1+zs7OSpHQ67bXhPhWvWQqFgu7cuVNWryuXy2l8fNyXtipHOLlBlLv8pZdeklR+vj/99NOydZVisVjdfQUAoF4EUAAA4FgrHSlSKBTK3rshR2kQUzmSJ5vNetuk02mZplkWErijfNxwanV11VvnBgulo1bcMCUejysejx/w6Pbn9OnTkrYHUO6xVxvNdPny5apBxoULF2Sapm7evOl97vbt24pGo14R8VrPuVvIfGpqSsFgUIZhqKOjwwuyksmkDMOo6al4pe1XC9pqact9Kt/ExERZXaqzZ8+WhZCNbOvq1auSHt537v3kLu/q6tLs7Kzm5+dl27Zs29b8/LxmZ2e3FZV3R0Y999xzO/YLAIBGIYACAADHmjuFyf299H0wGCz7Wbm9tFVQOxwOKxgMqqurS+l0umz9q6++KtM01d3dLcuy1Nvb640YunHjhiR5I17efPNNjY6ONvYA9+HcuXOSHo6ckeSFPdLWOXALgJdKJBJVR+jMzc3JNM2yz73++uveNrWe81AopHw+7wVd0WhU+XzeC1aKxaKi0eiewZ1hGGXtu2FWqVraun79+o51rLq7u31pq6+vT3fv3tXKyooMw9D8/Lzu3r3rhXmSNDY2poGBAQWDQY2OjmpwcLDqlD73+rrXGwAAPxlOK1RtBABgnxYWFjQyMtISRYhxuEZGRiRJmUymKft3A4t2uPcMw1Amk/Gmw9XCHYl17dq1uvZl23ZDngJ3EOFwWEtLS0e6rUaIx+MKBoN1X2Op+f/+AADthxFQAAAA2CYSiWhlZaVsymAtmh0+ra6uanJy8ki31Qi5XE65XE6RSKTZXQEAHBMEUACAY2t1dVXj4+NerZXx8XGFw+Fmd8t3hUJB2Wz2WByrXyrrRh1F7tS5mzdv1lRTqRUsLy/rySefVG9v75FtqxHW19c1MzOjubm5pgeGAIDj49FmdwAAgGZYXl5Wf3+/8vm83n77bY2Pj2tmZqauNmzbVjAYLJuCVW3ZYahWj6cax3F0/fr1tj7WVlBZN+qonoNQKKR0Oq25uTn19PQ0uzt7Kq2DdFTbagTLsnTjxg2FQqFmdwUAcIwwAgoAcCzdunVLkrzixW+//Xbdbdy7d6+mZYfBcRwVi8Wy96Wvu3fveuva/VhbQeX5PcoCgcC+agShdV27do3wCQBw6AigAADHUr0jgCrZtq1UKrXnssO021Sag4zAaMVjBQAAQHshgAIAHCtuvaed3pdyQxZ3m3g87tX7mZ6e9h6Z7q6vtsxVKBSUTCZlGIbC4bCWl5e95aX1mCzL8rbZ2NjwPh+Px/d8tPxuxyzt/rS2VjpWAAAAHD3UgAIAHCtuCFNLKPP3f//3mpmZ0ebmpn73u9/p5MmT+s1vfqO3335biURCU1NT29qotqxQKCgSiWh4eFiO43j1p9bW1hSPx70gZ3V1VaZpKp/P6+TJk+rs7NzXdLlStQY7R+FYAQAA0LoM56gXLgAAHGkLCwsaGRmpuw5PtQCqclk8HvdCmGrra2lDkrLZrIaGhrZtF4vFlEgkam6nnuOqVNnOUTjWkZERSVImk6nrc8eRYRjKZDIaHh5udldwRPDvDwBQLwIoAEBb8zOAcm1sbOjWrVuamJgoW19rG+Fw2Bv5U8lxHF8CKPdzGxsbOnny5J4BlKudjnVkZES/+MUvdO7cubo+dxzdunVL586d84ruAwd1//59feMb3yCAAgDUjBpQAADsIpVK6e/+7u9kmua+23ADmconpx3G/wdUT+DQ7scKAACA1kUNKAAAdpDNZnXlyhXl8/mGjBxZX1/X6dOnG9Cz+tQS/rTrsTICozaGYeiVV15hCh4axp2CBwBArRgBBQDADoaGhiTVN4qomtnZWUlSOp2WbduSHj4prlUcp2MFAADA4SOAAgAcO7lczvt9fX1d0lZI4nJ/d6eibWxseNtVW18asFRbdvHiRUlbT40LBoMyDEMdHR0aHBws268b2Lg/S/cVj8cVj8d3Pa7Sz5X+XqnVjxUAAABHDwEUAOBYMQxDZ8+e9d53d3d7IYnL/T2RSEjaqo0UDAYVi8UUjUb1u9/9rmz9m2++qdHR0R2XhUIh5fN5xWIxSVI0GvWmupXuNxgMlv0s7Ustx1X6OTf8qabdjxUAAADth6fgAQDa2n6fgof2x2Pga2cYhjKZDDWg0DD8+wMA1IsRUAAAAAAAAPAVARQAAAAODUXp9y+ZTO5a3w0AgFZGAAUAALAPtm3vWGerHdpvhkKhoOvXr+uZZ56RYRgyDGPH4vru+tJXK8vlcmV9HR8f33XbVCqlcDi87bgsy1I4HFY4HJZlWWXrXnzxRY2OjlKwHwDQlgigAAAA9uHevXtt3f5hs21bkUhEL7/8svr6+lQsFrW4uKipqamqIZTjONrc3JQkbW5utnydtw8//LDs/cDAQNXtksmk4vG4Tpw4obfeeqvsuLLZrFKplNLptNLptN577z2lUilvfU9PjyYnJxWJRBgJBQBoOwRQAAAAdbJtuywYaLf2m2Fubk49PT3q7e2VJAUCAV2+fFmSNDU1pWw2u+0zoVCo7GcrO3HihBzH8V6maW7bZnx8XMViUel0WqZpqqury1u3sbGhoaEhTU5OKhAIKBAIKBqN6sqVK8rlct52vb296uzs1Nzc3KEcFwAAjUIABQAAjhXbtpXNZr2pUqlUqmxKU7UpX5XLpqenvelR7vJCoeBNn5KkVCrlTcVaX18/cPuSFI/Hd5yy1soKhYImJib0wgsvVF0/PT2toaGhqiFUNXtdw0KhoGw2610Ly7JkGIbC4bA2Nja29S2ZTHrrl5eX6z6+jY0NhcNhxeNxra6uVt3GvW6JREKBQGDb+vfff1+S9PTTT3vLnnrqKUnbR1cNDg5qYmKCqXgAgLZCAAUAAI6V0dFRffbZZ94UL8uyyqY0udO+SuXz+bL3iUTC+90d8dLR0eHV7VldXdXY2JiKxaIkqbu72wuh9tt+O7t//74k6dSpU1XXX7t2TbFYTENDQ2WjfXay1zWMRCIaGhryroVpmsrn87IsS6+99prXTqFQUCQSUWdnpxzH0dWrV9Xf319TH0q5209NTen8+fMKh8Nl4VAul9PU1JQGBga8YLIy7FpZWZGkslFR7sivylpQ7nl0zysAAO2AAAoAABwby8vLsixLFy9elLT1BX9yclKWZen27dveskqlocBOSkOi0mlm0WhU0sMQYb/tS1vBVGk41S7cETy7HefExIRM09TZs2fLRoxVquUaLi0tedu718Ld98zMzLa23KmAfX19kqR33323ruMzTVPFYlFra2uKxWKyLEs/+9nPvPV37tzx+uAGk52dnerv7/dGTJX2q1JlAOWOoNrtPAEA0GoIoAAAwLFx69YtSeUh0JkzZyRJCwsLvuyzp6dH0lbAclxNTU3tuU0gEPDqGu02vayR19DdvnIKZC39rRQIBNTT06NEIqHZ2dmy0Mi99u69UBpMzs/P72tfpe0CANAOCKAAAMCxUW2UiftlvnKUCQ5fKBTS2trLBNpCAAAgAElEQVTatil1pRp5Dd3tS4uHN2LK46VLl/bsixtGucdTrWi5yw2rAABoZwRQAADg2HC/5FcbXeP3l3xChNr09PRoaWlJlmVpenp623o/rmGjp7KVjnCSHvarWqDmHk+143ILpj/77LMN7R8AAM1AAAUAAI6N4eFhSdKDBw+8ZW4oMDg46Ms+3XBjYGDAl/bbgRskVQtgqjFNU4uLi1WnwjXyGs7OzkqS0um014b7VLyDsG27rC/u75988knZNtLD43nppZcklR/Xp59+WrauUiwWO1A/AQA4TARQAADg2Lhw4YJM09TNmze9kSa3b99WNBr1ClBLD0esuOGRWyhaksbHxyWVj1ipDCyy2aykrZAhnU7LNM2yKVb7bT8ejysej+//BDTJ6dOnJW0PoNxrUG000+XLl6sGLLVcw9L23H2W7ttd7xYyn5qaUjAYlGEY6ujo8AKjZDIpwzB2fSpeNpste5rdxsaG7t27V3Y/9fX1KRaLKR6Pe/t+5513ZJqmVwC9q6tLs7Ozmp+fl23bsm1b8/Pzmp2d3Va83R0Z9dxzz+3YLwAAWg0BFAAAODbcQtemaaqjo8MrOv3666+Xbffqq6/KNE11d3fLsiz19vZ6o3Ju3LghSd7T6N58802Njo6Wff7MmTMKh8MKBoPq6upSOp1uaPvt5ty5c5IejuiR5IU9ksquRalEIrGtNlIt19BtV5KCwWDZz9L1oVBI+XzeC7qi0ajy+bwX+BSLRUWj0V1DvyeeeEL9/f0yDEPxeFy//e1vq9Zzco+ltM+V98XY2JgGBgYUDAY1OjqqwcFBjY2NbWvLPY/ueQUAoB0YzkGrLAIA0EQLCwsaGRk5cNFgtJ+RkRFJUiaTaXJPHnKDhVa7Hw3DUCaT8aZ7NYM7iuvatWt1fc62ba/IeLOEw2EtLS01tQ+l4vG4gsFg3eeykVrx3x8AoLUxAgoAAAC+i0QiWllZKZtuWItmh0+rq6uanJxsah9K5XI55XI5RSKRZncFAIC6EEABAAA0QGndoWo1jY47d+rczZs3d62p1EqWl5f15JNPqre3t9ldkbRVM2xmZkZzc3NND+YAAKgXARQAAEADlNYdKv0dD4VCIaXTad25c6fZXalJX1+fV0C9FViWpRs3bigUCjW7KwAA1O3RZncAAADgKGi1uk+tKhAINLV2UTvjvAEA2hkjoAAAAAAAAOArAigAAAAAAAD4igAKAAAAAAAAviKAAgAAAAAAgK8IoAAAAAAAAOArw+GRLQCANvbTn/5U3/nOd5rdDQA4dn7wgx/oJz/5SbO7AQBoEwRQAIC29vvf/15LS0v64osvmt0VoG6XLl3SK6+8om984xvN7gpQt97eXn31q19tdjcAAG2CAAoAAKBJDMNQJpPR8PBws7sCAADgK2pAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXz3a7A4AAAAcB//1X/+lX//619uWFwoFPXjwwHsfCAT0la985TC7BgAA4DvDcRyn2Z0AAAA46n70ox/pjTfeqGlb/jwDAABHDSOgAAAADsGzzz675zaGYej8+fOH0BsAAIDDRQ0oAACAQ/Dtb39bjz/++J7b/fCHPzyE3gAAABwuAigAAIBD8OUvf1mmaerRR3cegP7444/LNM1D7BUAAMDhIIACAAA4JMPDw/riiy+qrnvsscf07W9/W0888cQh9woAAMB/BFAAAACHZGBgYMeA6fPPP9f3vve9Q+4RAADA4SCAAgAAOCSPP/64Ll26pMcee2zbuj/5kz/RX//1XzehVwAAAP4jgAIAADhEIyMj+vzzz8uWPfbYY/rud79bNZgCAAA4CgzHcZxmdwIAAOC4+OKLL9TR0aF///d/L1v+j//4j3r++eeb1CsAAAB/MQIKAADgED3yyCP63ve+py996UveshMnTuiv/uqvmtgrAAAAfxFAAQAAHLLh4WH9z//8jyTpS1/6koaHh/UHf8CfZQAA4OhiCh4AAEATnDx5UhsbG5Kkf/qnf9LXv/71JvcIAADAP/xfbQAAAE0wOjoqSfp//+//ET4BAIAj79FmdwAAgFZlWZbS6XSzu4Ej6j/+4z8kSb/73e906dKlJvcGR9WpU6d08+bNZncDAACm4AEAsJORkREtLCxocHCw2V2BD+7fvy9JOnfuXNP68Ktf/UqdnZ1lBclb0a1bt3Tu3Dl1dXU1uyuow61btyRJ/LkPAGgFBFAAAOxgZGREkpTJZJrcE/iB61s7wzCUyWQ0PDzc7K6gDgsLCxoZGSGAAgC0BGpAAQAAAAAAwFcEUAAAAAAAAPAVARQAAAAAAAB8RQAFAAAAAAAAXxFAAQAAAAAAwFcEUAAAAAcUj8cVj8eb3Y2WVCgUlEwmm92NtpRMJmXbdrO7AQBAQxBAAQAAtDnbtmUYRrO7sU2hUND169f1zDPPyDAMGYaxY1Dnri99tbJcLlfW1/Hx8V23TaVSCofD247LsiyFw2GFw2FZllW27sUXX9To6KgKhYIvxwAAwGEigAIAADigRCKhRCLRtP3fu3evafveiW3bikQievnll9XX16disajFxUVNTU1VDaEcx9Hm5qYkaXNzU47jHHaX6/Lhhx+WvR8YGKi6XTKZVDwe14kTJ/TWW2+VHVc2m1UqlVI6nVY6ndZ7772nVCrlre/p6dHk5KQikQgjoQAAbY8ACgAAoI3Ztl0WWrSKubk59fT0qLe3V5IUCAR0+fJlSdLU1JSy2ey2z4RCobKfrezEiRNyHMd7maa5bZvx8XEVi0Wl02mZpqmuri5v3cbGhoaGhjQ5OalAIKBAIKBoNKorV64ol8t52/X29qqzs1Nzc3OHclwAAPiFAAoAAOAACoWCstmswuFw1feWZckwDIXDYW1sbHjbuFOvJCmVSnnTuNbX1722q01Hq1w2PT3tTd0qXd7MulSFQkETExN64YUXqq6fnp7W0NBQ1RCqGtu2lc1mveNLpVJl09JqOeel2yaTSW/98vJy3ce3sbGhcDiseDyu1dXVqtu45z6RSCgQCGxb//7770uSnn76aW/ZU089JWn76KrBwUFNTEwwFQ8A0NYIoAAAAA4gEoloaGjIC4FK36+urso0TeXzeVmWpddee02S1NHR4dX8WV1d1djYmIrFoiSpu7vbC6HcKWml8vl82fvSqX/uaJxmu3//viTp1KlTVddfu3ZNsVhMQ0NDZaN9djI6OqrPPvvMm6ZnWVbZtLRazrm0FT5FIhF1dnbKcRxdvXpV/f39NfWhlLv91NSUzp8/r3A4XBYO5XI5TU1NaWBgwAsXK8OulZUVSSobFeWO/KqsBeWeR/e8AgDQjgigAAAADmBpaWnH9+70MzdkmJmZkaSykKh0ilo0GpX0MICoNhWtNLDYTTPrUrkjeHbr68TEhEzT1NmzZ8tGfVVaXl6WZVm6ePGipK1zMjk5KcuydPv2bUm1nfPSttypgH19fZKkd999t67jM01TxWJRa2trisVisixLP/vZz7z1d+7c8frghoudnZ3q7+/3RkyV9qtSZQDljqDa7TwBANDqCKAAAABaRE9Pj6StcKadTU1N7blNIBDw6hrtNr3s1q1bksrDuDNnzkiSFhYW6uqXu33lNMZa+lspEAiop6dHiURCs7OzZaGRe/3c61kaLs7Pz+9rX6XtAgDQjgigAAAA0BShUEhra2vbptSVqjZSyA1kKkcK7cXdvrR4eCOmLV66dGnPvrhhlHs81YqWu9ywCgCAo4QACgAAoMUcpwCip6dHS0tLsixL09PT29a7QU21EVL7PU+NnspWOsJJetivaoGaezzVjsstmP7ss882tH8AALQCAigAAIAW4QYjAwMDTe7JwbhBUrUAphrTNLW4uFh1Ktzw8LAk6cGDB94yt93BwcG6+jU7OytJSqfTXhvuU/EOwrbtsr64v3/yySdl20gPj+ell16SVH5cn376adm6SrFY7ED9BACgmQigAAAADqB0BEuhUCh774YOpUFM5UiebDbrbZNOp2WaZtn0LHc0jRtOuUWsJWl8fFxS+WgaN0yJx+OKx+MHPLr9OX36tKTtAZR77NVGM12+fLlqwHLhwgWZpqmbN296n7t9+7ai0ahXRLzWc+4WMp+amlIwGJRhGOro6PACo2QyKcMwdn0qXjabLXua3cbGhu7du+f1Rdoqbh6LxRSPx719v/POOzJN0yuA3tXVpdnZWc3Pz8u2bdm2rfn5ec3Ozm4r3u6OjHruued27BcAAK2OAAoAAOAAOjo6yn4vfR8MBst+Vm4vbRXUDofDCgaD6urqUjqdLlv/6quvyjRNdXd3y7Is9fb2eiOGbty4IUne0+7efPNNjY6ONvYA9+HcuXOSHo7okeSFPdLWOXALgJdKJBLbaiO5xcpN0yz73Ouvv+5tU+s5D4VCyufzXtAVjUaVz+e9wKdYLCoaje4a3D3xxBPq7++XYRiKx+P67W9/W7Wek3sspX2uvLZjY2MaGBhQMBjU6OioBgcHNTY2tq0t9zy65xUAgHZkOAetuggAwBE1MjIiScpkMk3uCfzQ7OvrhhLt8KeYYRjKZDLe9LFauCOxrl27Vte+bNv2iow3Szgc1tLSUlP7UCoejysYDNZ9LhcWFjQyMtIW9xgA4OhjBBQAAAAaLhKJaGVlpWzKYC2aHT6trq5qcnKyqX0olcvllMvlFIlEmt0VAAAOhAAKAADgkFXWjTqK3KlzN2/e3LWmUitZXl7Wk08+qd7e3mZ3RdJW3a+ZmRnNzc01PZgDAOCgCKAAAPBZoVBQNptVOBxudlfQIirrRh1VoVBI6XRad+7caXZXatLX1+cVUG8FlmXpxo0bCoVCze4KAAAH9mizOwAAwFF3/fp1zczMNLsbdatWJNo1PT2t06dP65vf/CYjM/bhONXkCQQCddcuwhbOGwDgKGEEFAAAPnv77beb3YV9cRxHm5ub3vtisSjHceQ4jl588UWlUimNjo4e2SlkAAAAaBwCKAAAsKPSqT+lI516eno0NzcnaavYtG3bh943AAAAtA8CKAAAGsy2bWWzWRmGoXA4rPX19arbFQoFJZNJb7vl5WVveWnNKMuyvG02NjbK2nA/n0qlVCgUtk2b22kf0taj3ePx+L6PMxQK6erVq7IsS/fu3WupYwMAAEBrIYACAKDBRkdHtbKyomKxqKWlJf3zP//ztm0KhYIikYg6OzvlOI6uXr2q/v5+73HrQ0NDsixLq6urMk1T+XxelmXptdde89pIJpMaHByU4zi6dOmS3nzzzZr30Shf//rXJUnvvffekTs2AAAANI7hHKcqmAAA1GFkZESSlMlkav6MZVkKh8P65S9/6T1Ny7ZtBYNBSQ+LT2ezWQ0NDZUVozYMQ7FYTIlEwhvtU7m+dJlhGNrc3PSmyRUKBXV0dNS8j1pV68tu69vl2PZzfY8rwzCUyWQ0PDzc7K6gDgsLCxoZGTlWRe8BAK2LEVAAADSQOxKo9FHu1Z4St7CwIGnri737kqSpqama9xWNRtXR0aFsNivbthUKhcq+aDZiH/vRTse2sLBQ1gav6i9pK7Brdj941fdyQ1YAAFoBI6AAANjBfkbIuF/WK//ntXL5Ttvt1k7lsvX1dU1MTMiyLEnS9PR02WPb99pHrXZrxx3dVTryqF2ObWRkRBsbG3rllVf23cZxcenSJb3yyiv6xje+0eyuoA6/+MUv9OMf/5gRUACAlvBoszsAAMBxtr6+XjZaqh6nT5/W0tKScrmcZmZmNDExIUllQc1B97GXjz76SJL0wgsvbFvXDsfW1dWlwcHBfX/+ODl37hznqs18/vnnze4CAAAepuABANBAs7OzkrRnMWx3u3Q6Ldu2JT18qlutDMOQbdvq6enR22+/rbW1NS+oadQ+dlMoFPTGG2/INE319fU1dL/NPjYAAAA0FgEUAAAN9NJLL0mS4vG4NjY2JEnLy8ve+vHxcUnSxYsXJW3VLAoGgzIMQx0dHRocHFShUPC2d8MV96eksvXT09Pefv70T/9U09PT3rrd9uH2MR6P73o8pfst/d19op0kzc3NlX2mFY4NAAAArYUACgCABurq6lI+n1dnZ6dOnjyp8fFxfe1rX5NpmlpcXNSNGzckSaFQSPl8XrFYTNJW0e18Pq+uri51dHR47blPz3N/Sipb/8Mf/lC3bt2SYRi6detW2RS13fZRC8MwyvbrBj2GYejOnTuanJzU0tKS96S6WvbbKscGAACAw0URcgAAdrCfIuRoH1zf2hmGoUwmo+Hh4WZ3BXVYWFjQyMgIRcgBAC2BEVAAAAAAAADwFQEUAAAAfENx+P1LJpNlNdIAAGhnBFAAAABNYNu2DMNo2/ZrUSgUdP36dT3zzDNe/bCdCt+760tf7SCXyymVSikcDm/rcy6XKzse9yEEpSzL8j4bDoeVzWa9dS+++KJGR0fLivMDANCuCKAAAACa4N69e23d/l5s21YkEtHLL7+svr4+FYtFLS4uampqqmoI5TiONjc3JUmbm5ttUbcomUwqHo/rxIkTeuutt7b1+cMPPyx7PzAwsO3z4XBYiURCjuMokUhoaGjIGzHW09OjyclJRSIRRkIBANoeARQAAMAhs21bqVSqbduvxdzcnHp6etTb2ytJCgQCunz5siRpamqqbKSPy32iYuWTFVvR+Pi4isWi0um0TNOs+gTGEydOyHEc72WaZtn6iYkJSVtBU+nPlZUVb5ve3l51dnZqbm7Or0MBAOBQEEABAADUwbZtZbNZb1pVKpUqmyJVbQpZ5bLp6WlZllW2rlAoeNOxJCmVSnnTttbX1w/cviTF4/Edp8A1UqFQ0MTEhF544YWq66enpzU0NFQ1hKpmr3NeKBSUzWa9c2dZljelbWNjY1vfksmkt355ebnu43PPYSKRUCAQqLrNxsaGwuGw4vG4VldXq24zPT0tSd56t6+JRKJsu8HBQU1MTDAVDwDQ1gigAAAA6jA6OqrPPvvMmzJmWVbZFCl3GlmpfD5f9r40YHBHx3R0dCgcDsuyLK2urmpsbEzFYlGS1N3d7YVQ+23/MN2/f1+SdOrUqarrr127plgspqGhIeVyuT3b2+ucRyIRDQ0NeefONE3l83lZlqXXXnvNa6dQKCgSiaizs1OO4+jq1avq7++vqQ+uXC6nqakpDQwMeCFhtSDLbXNqakrnz59XOBzeFiC55+H8+fNaXV3V+++/r83NTW8klMs9j+55BQCgHRFAAQAA1Gh5eVmWZenixYuStqaKTU5OyrIs3b5921tWqdr0rEqlIVHptLVoNCpJ3oim/bYvbQVTlaNr/ODWPtqtXxMTEzJNU2fPni0b4VWplnO+tLTkbe+eO3ffMzMz29pypwL29fVJkt59992aj+3OnTte+25I2NnZqf7+/rKRTqZpqlgsam1tTbFYTJZl6Wc/+9m29hKJhKLRqM6fP6+PP/5Yjz/++LZt3FFWu50nAABaHQEUAABAjW7duiWpPAQ6c+aMJGlhYcGXfbqjYdx6Qe1gampqz20CgYBX12i36WWNPOfu9pVTFmvpr6uyblNpSDg/P1+2bSAQUE9PjxKJhGZnZ70QsVQymdTzzz/vjXYbHR3dVnDcDaDa6R4AAKCS4bTDI0YAAGiCkZERSVImk2lyT+CH/VxfN7Co/POpcnm17fazTaPb3y/DMJTJZDQ8PFzz9jvt2zCMsuW5XE5nz56VaZpKp9MKBoMtfU5q7U8l27a3HVs2m9XQ0JCKxaICgYDW19fV3d2t2dlZjY2N1dV+NQsLCxoZGWmLJwoCAI4+RkABAADUyH2KWbXROu4oGL/43X6z9PT0aGlpSZZleUW5S/lxzg8ylc3dZ+UoJUnbnnJXqnSklGtoaMhbJ0kdHR2SpCtXruy7fwAAtCoCKAAAgBq5I4AePHjgLXODiMHBQV/26YYlAwMDvrTvBzdIqhbSVGOaphYXF6tOhWvkOZ+dnZUkpdNprw33qXi1cvf5ySefbOvPbiPEbNve1t/KwMoNonYKsmKxWM39BACg1RBAAQAA1OjChQsyTVM3b970RuTcvn1b0WjUK2gtPRwl44ZHpcWpx8fHJZWP7KkMQLLZrKSt0CKdTss0zbJQYr/tx+NxxePx/Z+AGp0+fdrrfyn3nFUbzXT58uWqAUst57y0PXefpft217uFzKemphQMBmUYhjo6OrxgKJlMyjCMXZ+K19fXp1gspng87rX7zjvvyDRNr7h5NpsteyrexsaG7t27V3aPSNLVq1e97aWH19FdXvp5SXruued27BcAAK2OAAoAAKBGbuFs0zTV0dHh1eV5/fXXy7Z79dVXZZqmuru7ZVmWent7vVE+N27ckCTvaXRvvvmmRkdHyz5/5swZhcNhBYNBdXV1KZ1ON7R9v507d06S9Omnn3rL3LBHUtm5K5VIJKqOCtrrnLvtSlIwGCz7Wbo+FAopn897QVc0GlU+n/eemFcsFhWNRvcM6dx+lvan9Bo98cQT6u/vl2EYisfj+u1vf1t1VFNfX5/u3r2rlZUVGYah+fl53b17d1tQ5Z5H97wCANCOKEIOAMAOKEJ+tLXi9W1k4fBGqrcIuSRv1NW1a9fq2pdt295UtGYJh8NaWlpqah9KxeNxBYPBus8lRcgBAK2EEVAAAABouEgkopWVlbLpgbVodvi0urqqycnJpvahVC6XUy6XUyQSaXZXAAA4EAIoAACAFlBax6hajaR2406du3nz5q41lVrJ8vKynnzySfX29ja7K5K2anzNzMxobm6u6cEcAAAHRQAFAADQAkrrGJX+3s5CoZDS6bTu3LnT7K7UpK+vzyug3gosy9KNGzcUCoWa3RUAAA7s0WZ3AAAAAK1X96lRAoFA3bWLsIXzBgA4ShgBBQAAAAAAAF8RQAEAAAAAAMBXBFAAAAAAAADwFQEUAAAAAAAAfEUR8v/f3v2FxnXn9/9/nbWdtF36ndm0jLxRUWBJLQwp4yTUUbo0xpIhWOSM0z/yaqRO3ItxGJVu6/1JNxEahJBq+0JDTVywkHRjBklDFNhdHRpT0AokQioHvHja+mJNyHa027Sam84QKGxCMr8LcU5mNCN5JM2ZMyM9HyCkOecz57znI2Gyr/183gcAgF0sLi7qzTff9LoMuGBjY0PS1u8YT3b//n2dOHHC6zKwB/xtAwAaiVE4rI9cAQDggEZGRvQP//APXpcBAPv21FNP6Te/+Y3XZQAAQAAFAADgFcMwNDc3p76+Pq9LAQAAcBU9oAAAAAAAAOAqAigAAAAAAAC4igAKAAAAAAAAriKAAgAAAAAAgKsIoAAAAAAAAOAqAigAAAAAAAC4igAKAAAAAAAAriKAAgAAAAAAgKsIoAAAAAAAAOAqAigAAAAAAAC4igAKAAAAAAAAriKAAgAAAAAAgKsIoAAAAAAAAOAqAigAAAAAAAC4igAKAAAAAAAAriKAAgAAAAAAgKsIoAAAAAAAAOAqAigAAAAAAAC4igAKAAAAAAAAriKAAgAAAAAAgKsIoAAAAAAAAOAqAigAAAAAAAC4igAKAAAAAAAAriKAAgAAAAAAgKsIoAAAAAAAAOAqAigAAAAAAAC4igAKAAAAAAAAriKAAgAAAAAAgKsIoAAAAAAAAOAqAigAAAAAAAC4igAKAAAAAAAAriKAAgAAAAAAgKsIoAAAAAAAAOCq414XAAAAcBQ8fPhQ//Iv/1J23LIs/epXv3JeP//88/qLv/iLepYGAADgOqNQKBS8LgIAAOCw+/u//3u9++67evrpp3cc85vf/EaSxH+eAQCAw4YteAAAAHXw53/+55K2Qqadvp566in97d/+rceVAgAA1B4roAAAAOrg66+/Vmtrq/7nf/5n13Effvihvv/979epKgAAgPpgBRQAAEAdfOtb31J/f7+eeuqpHcc8++yz+pM/+ZM6VgUAAFAfBFAAAAB1Eg6H9cUXX1Q8d+LECb311lsyDKPOVQEAALiPLXgAAAB19L3vfU+//OUvK577t3/7N/3RH/1RnSsCAABwHyugAAAA6uiv//qvdeLEibLjf/iHf0j4BAAADi0CKAAAgDoKh8P68ssvS46dOHFCV65c8agiAAAA97EFDwAAoM6CwaD+/d//XfZ/hhmGoU8++UTf+973PK4MAADAHayAAgAAqLMrV67o2LFjkrbCp5deeonwCQAAHGoEUAAAAHXW29urr776SpJ07NgxRSIRjysCAABwFwEUAABAnT377LP60z/9U0nS119/rR/84AceVwQAAOAuAigAAAAP9Pf3S5JefvllnTx50uNqAAAA3EUTcgDAkfT000/riy++8LoMAGgI9+/f19mzZ70uAwBwiB33ugAAALzwxRdf6M0331RfX5/XpaCBffjhh3r33Xf13nvvuXL9fD6v//f//p8Mw3Dl+vX07rvvSpL+7u/+zuNKsFeXL1/WJ598QgAFAHAVARQA4Mjq6elRT0+P12WggX355ZeSxN9JFX7yk59IYq4AAEBl9IACAAAAAACAqwigAAAAAAAA4CoCKAAAAAAAALiKAAoAAAAAAACuIoACAAAAAACAqwigAAAA6iAejysej3tdRsPKZmDC64oAACAASURBVLNKJBJel9GUEomE8vm812UAALArAigAAIAjIJ/PyzAMr8uoKJvNanR0VC+++KIMw5BhGDuGdfb54q9mkE6nNTMzo1AoVFZzOp0u+TwDAwNl77csy3lvKBRSKpVyzl24cEGRSETZbNb1zwEAwH4RQAEAANTB+Pi4xsfHPbv/2tqaZ/feTT6fVzQa1ZUrV9TZ2alcLqeFhQVNTExUDKEKhYI2NzclSZubmyoUCvUuec8SiYTi8bhOnjypf/qnfyqr+eOPPy553d3dXfb+UCik8fFxFQoFjY+PKxwOOyvGgsGghoeHFY1GWQkFAGhYBFAAAACHXD6f18zMjNdlVDQ7O6tgMKiOjg5Jks/nU29vryRpYmKiZKWPLRAIlHxvZAMDA8rlckomkzJNU21tbWVjTp48qUKh4HyZpllyfmhoSNJW0FT8fXV11RnT0dGh1tZWzc7OuvVRAAA4EAIoAAAAl2WzWaVSKYVCoYqvLctytlZtbGw4Y+xtV5I0MzPjbM96/Pixc+1KW9G2H5ucnJRlWSXnJO/7UmWzWQ0NDen8+fMVz09OTiocDlcMoSrJ5/NKpVLOZ5yZmSnZllbNvBePTSQSzvmVlZU9fz57bsfHx+Xz+SqO2djYUCgUUjwe1/r6esUxk5OTkuSct2vdvqKup6dHQ0NDbMUDADQkAigAAACXRaNRhcNhJwQqfr2+vi7TNJXJZGRZlm7cuCFJamlpUSgUcsZcvXpVuVxOktTe3u6EUPZ2tGKZTKbkdXFQYa+yaQT379+XJD3//PMVzw8ODmpkZEThcFjpdPqJ14tEIvr888+dbXqWZZVsS6tm3qWt8Ckajaq1tVWFQkHXrl1TV1dXVTXY0um0JiYm1N3d7YSHlYIs+5oTExN69dVXFQqFygIkex5effVVra+v66OPPtLm5qazEspmz6M9rwAANBICKAAAAJctLS3t+NreemZvzZqampKkkpCoeHtaLBaTJCfMqrQNrdI2r0q87ktl9z7ard6hoSGZpqkzZ86UrPzabmVlRZZl6dKlS5K25mV4eFiWZenevXuSqpv34mvZWwE7OzslSe+//37Vn215edm5vh0etra2qqurq2Slk2mayuVyevjwoUZGRmRZln7605+WXW98fFyxWEyvvvqqHj16pKeffrpsjL3Kard5AgDAKwRQAAAATcRe9WL3BWpmExMTTxzj8/mcvka7bS9bXFyUVBrInT59WpI0Pz+/p7rs8du3MlZTr21736bi8PDu3bslY30+n4LBoMbHxzU9Pe2Ei8USiYTOnTvnrIKLRCJlDcftAOow/G0AAA4fAigAAAA0tEAgoIcPH5ZtqStWvILJZgcylQKd3djjixuD12Lroh1GVarVdvny5bJ6U6mUhoaGdPHiRfl8PkUiEVmWpffee+9A9QAAUE8EUAAAAE3IXk1zVASDQS0tLcmyLKcpdzH7yXGVVkjtd64OspXNvmelsGz7U+6KFa+UsoXDYeectNUfTJLefvvtfdcHAEC9EUABAAA0ETsU6e7u9riSg7ODpEohTSWmaWphYaHiVri+vj5J0qeffuocs6/b09Ozp7qmp6clSclk0rmG/VS8atn3/M///M+yeuxaK8nn82X1bg+s7CBqpyBrZGSk6joBAKgXAigAAACXFa/KyWazJa/tUKI4hNm+iieVSjljksmkTNMsCR/sFTN2OFXc5HpgYEBS6QohO0iJx+OKx+MH/HT7d+rUKUnlAZT9+SutZurt7a0YsFy8eFGmaer69evO++7du6dYLOY0Ea923u1G5hMTE/L7/TIMQy0tLU4wlEgkZBjGrk/F6+zs1MjIiOLxuHPd9957T6ZpOs3NU6lUyVPxNjY2tLa25tRru3btmjNe+ub3ax8vfr8knT17dse6AADwCgEUAACAy+wtU/bPxa/9fn/J9+3jpa1m2qFQSH6/X21tbUomkyXn33nnHZmmqfb2dlmWpY6ODme10NjYmCQ5T7u7ffu2IpFIbT/gPr3yyiuSpM8++8w5Zoc90tY82A3Ai42Pj1dcFTQ7OyvTNEved/PmTWdMtfMeCASUyWScoCsWiymTyThPzMvlcorFYk8M7+w6i+sp/t19+9vfVldXlwzDUDwe1//+7/9WXNXU2dmpn/3sZ1pdXZVhGLp7965+9rOflQVV9jza8woAQCMxCgftpggAQBMyDENzc3O7boUB5ufn1d/ff+Dm0/tlhxbN8J9r/f39kqS5ubk9vc9ejTU4OLin9+XzeWcrmldCoZCWlpY8raFYPB6X3+/f81zy7yEAoB5YAQUAAADPRKNRra6ulmwbrIbX4dP6+rqGh4c9raFYOp1WOp1WNBr1uhQAACoigAIAYJ+y2axSqZRCoZDXpeAQ2t436rCyt85dv359155KjWRlZUXPPPOMOjo6vC5F0lbvr6mpKc3OznoezAEAsBMCKAAA9ml0dFThcFiWZXldyr7k83mtr69rZmZm3yGaYRg7fiUSCVmWVfUTzlBqe9+owywQCCiZTGp5ednrUqrS2dnpNFBvBJZlaWxsTIFAwOtSAADYEQEUAAD7dOfOHa9LOJDJyUn98z//s95+++19h2iFQkGbm5vO61wup0KhoEKhoAsXLmhmZkaRSORQr+Bxiz2P9tdh5/P59ty7CFsGBwcJnwAADY8ACgCAI2p8fNx5MtpBFP8P3+LtP8FgULOzs5K2+vywEgoAAODoIoACAKBK+XxeqVRKhmEoFArp8ePHFcdls1klEgln3MrKinO8uGeUZVnOmI2NjZJr2O+fmZlRNpstexT9TveotXg8/sRHze8mEAjo2rVrsixLa2trJecO0zwBAABgdwRQAABUKRKJaHV1VblcTktLS/r5z39eNiabzSoajaq1tVWFQkHXrl1TV1eX83Qqu2fU+vq6TNNUJpORZVm6ceOGc41EIqGenh4VCgVdvnxZt2/frvoejejll1+WJH3wwQfOMeYJAADgaDEKR6GpAAAA2xiGobm5OfX19VU13rIshUIh/eIXv3CaD+fzefn9fklyevSkUimFw+GSnj2GYWhkZETj4+POCp3t54uPGYahzc1NZ2tbNptVS0tL1ffYz1xsr6nW19h+vlnmaX5+Xv39/UeiB9NB9ff3S5Lm5uY8rgR7tdd/DwEA2I/jXhcAAEAzsFfvFD/5qtLjzufn5yWpbCvYxMRE1aFHLBZTS0uLFhYWdPHiRQUCgZIApBb38FqzzdPi4uKexh9F9vZI5goAAFRCAAUAQBWmpqaqGmc/Te4gK2Z+9KMf6b/+678UDoclbT2trvjpYLW4Rz3ZzcdHRkacY802T5cvXz7wNY6KDz/80OsSAABAAyKAAgDABY8fPy5ZLbUXp06d0tLSktLptKampjQ0NCRJZY+oP8g96unBgweSpPPnz5eda5Z5apawz0tswWte21cJAgDgBpqQAwBQhenpaUl6YgNre1wymXRW/thPYquWYRjK5/MKBoO6c+eOHj586IQrtbpHvWSzWd26dUumaaqzs9M5zjwBAAAcLQRQAABU4fXXX5ckxeNxp9fNysqKc35gYECSdOnSJUlbfYb8fr8Mw1BLS4t6enqUzWad8XYgYn+XVHJ+cnLSuc93vvMdTU5OOud2u8deFd+/+GdbPB5XPB7f1zXsJ9pJ0uzsbMl7mm2eAAAAcDAEUAAAVKGtrU2ZTEatra167rnnNDAwoBdeeEGmaWphYUFjY2OSpEAgoEwm4/Q7isViymQyamtrU0tLi3M9++l59ndJJed/+MMfanFxUYZhaHFxsWRb2W732AvDMErub4c0tbiGYRhaXl7W8PCwlpaWnCfVVfMZGm2eAAAAcHBGgaYGAIAjiMeOoxrz8/Pq7++nB1QV6AHVvPj3EABQD6yAAgAAAAAAgKsIoAAAAAAAAOAqAigAAA4Ru//Sk76ARsMTCvcvkUhUfIgAAACNhAAKAIBDpFAoVPWF5pDP510NDN2+frWy2axGR0f14osvOiHpTk9fbKZAdWNjQwMDAzIMQwMDAyVPzixmWZZCoZBCoZAsy9rzmAsXLigSiZQ8IRIAgEZDAAUAANCg1tbWmvr61cjn84pGo7py5Yo6OzuVy+W0sLCgiYmJiiFUoVDQ5uamJGlzc7NhA9V8Pq90Oq07d+4ol8vp3Llz6urqKguPUqmUZmZmlEwmlUwm9cEHH2hmZmZPY4LBoIaHhxWNRlkJBQBoWARQAAAADSifz5cFEc10/WrNzs4qGAyqo6NDkuTz+dTb2ytJmpiYUCqVKntPIBAo+d6I1tbWZJqmpNLPFAqFnDEbGxsKh8MaHh6Wz+eTz+dTLBbT22+/rXQ6XfUYSero6FBra6tmZ2fr+CkBAKgeARQAAECN5fN5pVIpZ4vYzMxMyfaoStvHth+bnJx0VsvYx7PZrLMVS5JmZmac7V2PHz8+8PUlKR6P77j9rday2ayGhoZ0/vz5iucnJycVDocrhlCVPGnes9msUqmUM3+WZckwDIVCIW1sbJTVlkgknPM7bZ/biR0+bReLxZyfP/roI0nSs88+6xz77ne/K0n6+OOPqx5j6+np0dDQEFvxAAANiQAKAACgxiKRiD7//HNnu5hlWSXbo+wtZMUymUzJ6/Hxcednu3dXS0uL0wNofX1dV69eVS6XkyS1t7c7IdR+r19v9+/flyQ9//zzFc8PDg5qZGRE4XC4ZLXPTp4079FoVOFw2Jk/0zSVyWRkWZZu3LjhXCebzSoajaq1tVWFQkHXrl1TV1dXVTXsxK6hu7vbOba6uipJamtrc47Zq7rscLCaMTZ7Hu15BQCgkRBAAQAA1NDKyoosy9KlS5ckbYUFw8PDsixL9+7dc45tVxww7KQ4JCresmavqrEDif1eX9oKporDKTfZK3h2q21oaEimaerMmTMlq7y2q2bel5aWnPH2/Nn3npqaKruWvW2us7NTkvT+++/v+TPaHjx4INM09dprrznHiu+5nf27rGaMzefzSdKu8wQAgFcIoAAAAGpocXFRUmkIdPr0aUnS/Py8K/cMBoOStsKaZjIxMfHEMT6fz+lrtNv2slrOuz1++7bFaurdya1bt5w+Tm6xr91sfwcAgKOBAAoAAKCGKq1YsYOB7StWUJ1AIKCHDx+WbakrVst5t8fbWxOLv/YjlUrJNE1n1ZVtpz5R0je9oqoZAwBAMyCAAgAAqCE7MKi0UsftwOAwBxLBYFBLS0uyLEuTk5Nl592Y91psZUun03r06JGuXr1adq5SzXYz9JdeeqnqMQAANAMCKAAAgBrq6+uTJH366afOMXvFTk9Pjyv3tIOS4gbXzcAOkiqtaKrENE0tLCxU3ApXy3mfnp6WJCWTSeca9lPx9iKbzWp5ebmkp1Y6ndbAwIAk6fXXXy+r+bPPPis5V82Y7UZGRvZUJwAA9UAABQAAUEMXL16UaZq6fv26s2rl3r17isViTjNr6ZtVOXZ4tL6+7pyzA4ri1S/bw49UKiVpK2RJJpMyTbNku9Z+rx+PxxWPx/c/AXtw6tQpSeUBlD1vlVYz9fb2VgxYqpn34uvZ9yy+t33ebmQ+MTEhv98vwzDU0tLiBFmJREKGYez6VDz7SXpDQ0MlvaTOnDnjBIVtbW2anp7W3bt3lc/nlc/ndffuXU1PTzvN0asZY7NXRp09e3bHugAA8AoBFAAAQA3ZTbNN01RLS4vTwPrmzZsl49555x2Zpqn29nZZlqWOjg5nhc/Y2JgkOStnbt++rUgkUvL+06dPKxQKye/3q62tTclksqbXr4dXXnlF0jcreiQ5YY+kkvkrNj4+XtYbqZp5t68rSX6/v+R78flAIKBMJuMEXbFYTJlMxgl8crmcYrHYrkHd6Ojojr2n2tvbnZ+vXr2q7u5u+f1+RSIR9fT0lG3Xq2aM9M082vMKAEAjMQr77aYIAEATMwxDc3NzzrYdoJL5+Xn19/fvu/m0G+xgpZFqkqT+/n5J0tzc3J7eZ6+8Ghwc3NP78vm8q0+Uq0YoFNLS0pKnNRSLx+Py+/17nkv+PQQA1AMroAAAAOCZaDSq1dXVki2C1fA6fFpfX9fw8LCnNRRLp9NKp9OKRqNelwIAQEUEUAAAAE2iuIdRpf5IzcjeOnf9+vVdeyo1kpWVFT3zzDPq6OjwuhRJW32+pqamNDs763kwBwDATgigAAAAmkRxD6Pin5tdIBBQMpnU8vKy16VUpbOz02mg3ggsy9LY2JgCgYDXpQAAsKPjXhcAAACA6jRa36da8vl8e+5dhC3MGwCgGbACCgAAAAAAAK4igAIAAAAAAICrCKAAAAAAAADgKgIoAAAAAAAAuIom5ACAI6u/v18/+clPvC4DDWxjY0OSdPnyZY8raXz379+XxFwBAIDKjMJhfpwKAAA7GB4e1ieffOJ1GTjilpeX9cILL+jkyZNel4Ij7NixY/rHf/xH/g4BAK4igAIAAPCIYRiam5tTX1+f16UAAAC4ih5QAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcJVRKBQKXhcBAABw2M3Ozupv/uZv1N7e7hz71a9+pd/7vd/T7/zO70iS/vu//1vf//739dOf/tSrMgEAAFxx3OsCAAAAjoLNzU19+eWX+o//+I+S4/l8vuS1ZVn1LAsAAKAu2IIHAABQB+FwWIZh7Drm+PHjunnzZp0qAgAAqB+24AEAANTJH//xH+vBgwfa6T+/DMPQL3/5Sz333HN1rgwAAMBdrIACAACok7/6q7/SsWPHKp771re+pbNnzxI+AQCAQ4kACgAAoE5+8IMf6Ouvv654zjAMXblypc4VAQAA1AcBFAAAQJ2cPHlS586d23EVVE9PT50rAgAAqA8CKAAAgDp66623ynpAHTt2TOfPn9fv//7ve1QVAACAuwigAAAA6ujP/uzPylZAFQoFvfXWWx5VBAAA4D4CKAAAgDry+Xy6ePGijh8/7hw7ceKE3nzzTQ+rAgAAcBcBFAAAQJ1FIhF99dVXkqTjx4/rjTfe0O/+7u96XBUAAIB7CKAAAADq7I033tBv//ZvS5K++uor9ff3e1wRAACAuwigAAAA6uy3fuu39Jd/+ZeSpG9/+9vq7u72uCIAAAB3HX/yEAAADp9//dd/1a9//Wuvy8AR9gd/8AeSpOeee05LS0seV4Oj7NixYwqFQiV9yQAAqDWjsP05wAAAHAGGYXhdAgA0jB//+Mc0wgcAuIr/mwMAcGTNzc2pr6/P6zLQwObn59Xf3y/+/7ons/tYzc3NeVwJ9sowDP3f//2f12UAAA45ekABAAAAAADAVQRQAAAAAAAAcBUBFAAAAAAAAFxFAAUAAAAAAABXEUABAAAAAADAVQRQAAAAAAAAcBUBFAAAQB3E43HF43Gvy2hY2WxWiUTC6zKaUiKRUD6f97oMAAB2RQAFAABwBOTzeRmG4XUZFWWzWY2OjurFF1+UYRgyDGPHsM4+X/zVqDY2NjQwMCDDMDQwMKCVlZWK4yzLUigUUigUkmVZex5z4cIFRSIRZbPZmn8GAABqhQAKAACgDsbHxzU+Pu7Z/dfW1jy7927y+byi0aiuXLmizs5O5XI5LSwsaGJiomIIVSgUtLm5KUna3NxUoVCod8lVyefzSqfTunPnjnK5nM6dO6eurq6y8CiVSmlmZkbJZFLJZFIffPCBZmZm9jQmGAxqeHhY0WiUlVAAgIZFAAUAAHDI5fP5slCjUczOzioYDKqjo0OS5PP51NvbK0mamJhQKpUqe08gECj53ojW1tZkmqak0s8UCoWcMRsbGwqHwxoeHpbP55PP51MsFtPbb7+tdDpd9RhJ6ujoUGtrq2ZnZ+v4KQEAqB4BFAAAgMuy2axSqZQTPmx/bVmWDMNQKBTSxsaGM8bediVJMzMzzlaux48fO9eutBVt+7HJyUln5U3xca/7UmWzWQ0NDen8+fMVz09OTiocDlcMoSrJ5/NKpVLOZ5yZmSnZllbNvBePTSQSzvmdts/txA6ftovFYs7PH330kSTp2WefdY5997vflSR9/PHHVY+x9fT0aGhoiK14AICGRAAFAADgsmg0qnA47IRAxa/X19dlmqYymYwsy9KNGzckSS0tLU6/n/X1dV29elW5XE6S1N7e7oRQ9na0YplMpuR18da/QqHQMNvW7t+/L0l6/vnnK54fHBzUyMiIwuFwyWqfnUQiEX3++efONj3Lskq2pVUz79JW+BSNRtXa2qpCoaBr166pq6urqhp2YtfQ3d3tHFtdXZUktbW1OcfsVV3230o1Y2z2PNrzCgBAIyGAAgAAcNnS0tKOr+2tZ3bAMDU1JUklIVHx9jR7BY0dPlTahlYcVuzG675U9gqe3eodGhqSaZo6c+ZMycqv7VZWVmRZli5duiRpa16Gh4dlWZbu3bsnqbp5L76WvW2us7NTkvT+++/v+TPaHjx4INM09dprrznHiu+5nf37rWaMzefzSdKu8wQAgFcIoAAAAJpIMBiUtBXMNLuJiYknjvH5fE5fo922ly0uLkoqDeROnz4tSZqfn99TXfb47VsZq6l3J7du3XL6OLnFvvZh+NsAABw+BFAAAABoaIFAQA8fPizbUles0kohO5DZvlLoSezx9nbF4q/9SKVSMk3TWXVl26lPlPRNr6hqxgAA0AwIoAAAAJrQUQsfgsGglpaWZFmWJicny87bQU2lFVL7natabGVLp9N69OiRrl69WnauUs12M/SXXnqp6jEAADQDAigAAIAmYocixc2sm5UdJFVa0VSJaZpaWFiouBWur69PkvTpp586x+zr9vT07Kmu6elpSVIymXSuYT8Vby+y2ayWl5dL+myl02kNDAxIkl5//fWymj/77LOSc9WM2W5kZGRPdQIAUA8EUAAAAC4rXr2SzWZLXtsBR3EIs30VTyqVcsYkk0mZplmyNcte4WOHU+vr6845O+woXkljBynxeFzxePyAn27/Tp06Jak8gLI/f6XVTL29vRUDlosXL8o0TV2/ft1537179xSLxZwm4tXOu93IfGJiQn6/X4ZhqKWlxQmyEomEDMPY9al49pP0hoaGSnpJnTlzxgkP29raND09rbt37yqfzyufz+vu3buanp52mqNXM8Zmr4w6e/bsjnUBAOAVAigAAACXtbS0lPxc/Nrv95d83z5e2mqmHQqF5Pf71dbWpmQyWXL+nXfekWmaam9vl2VZ6ujocFYLjY2NSZKzCuf27duKRCK1/YD79Morr0j6ZkWPJCfskbbmwW4AXmx8fLysN5LdrNw0zZL33bx50xlT7bwHAgFlMhkn6IrFYspkMk7gk8vlFIvFdg3vRkdHd+w91d7e7vx89epVdXd3y+/3KxKJqKenp2y7XjVjpG/m0Z5XAAAaiVHYbzdFAACamGEYmpubc7btAJXMz8+rv79/382nD8oOUZrhP9f6+/slSXNzc3t6n70aa3BwcE/vy+fzrj5RrhqhUEhLS0ue1lAsHo/L7/fveS759xAAUA+sgAIAAIBnotGoVldXS7YNVsPr8Gl9fV3Dw8Oe1lAsnU4rnU4rGo16XQoAABURQAEAsE/ZbFapVEqhUMjrUnAIbe8bdVjZW+euX7++a0+lRrKysqJnnnlGHR0dXpciaav319TUlGZnZz0P5gAA2AkBFAAA+zQ6OqpwOLxjn5dGt7GxoYGBARmGoYGBAa2srOz5GsXNlbd/JRIJWZZV9RPOUGp736jDLBAIKJlManl52etSqtLZ2ek0UG8ElmVpbGxMgUDA61IAANgRARQAAPt0584dr0vYt3w+r3Q6rTt37iiXy+ncuXPq6urac5hWKBS0ubnpvM7lcioUCioUCrpw4YJmZmYUiUQO9Qoet9jzaH8ddj6fb8+9i7BlcHCQ8AkA0PAIoAAAOILW1tacp4j5fD719vZK0r62Exb/D9/i7T/BYFCzs7OStvr8sBIKAADg6CKAAgCgSvl8XqlUSoZhKBQK6fHjxxXHZbNZJRIJZ5y9tW17zyjLspwxGxsbJdew3z8zM6NsNlv2KPqd7lGt7Y+wt8VisZLX8Xh810fNP0kgENC1a9dkWZbW1tZKzjXDPAEAAKA2CKAAAKhSJBLR6uqqcrmclpaW9POf/7xsTDabVTQaVWtrqwqFgq5du6auri7n6VR2z6j19XWZpqlMJiPLsnTjxg3nGolEQj09PSoUCrp8+bJu375d9T32y16d1N3dve9r7OTll1+WJH3wwQfOsWadJwAAAOyPUTgKTQUAANjGMAzNzc2pr6+vqvGWZSkUCukXv/iF03w4n8/L7/dLktOjJ5VKKRwOl/TsMQxDIyMjGh8fd1bobD9ffMwwDG1ubjpb27LZrFpaWqq+x36srKzo1q1bSiaT+3qKVqXPtdv5Zpmn+fl59ff3H4keTAfV398vSZqbm/O4EuzVXv89BABgP457XQAAAM3AXr1T/OSrSkHN/Py8JJVtBZuYmKg69IjFYmppadHCwoIuXryoQCBQEoDU4h7b3bp1S8PDw3V7hHuzzdPly5f3NP4oun//viTmCgAAVMYWPAAAqjA1NVXVOPspctufYLaXFTQ/+tGPUPQFrgAACOFJREFUZJqmwuGw/H6/EolEze9RLJVKyTRNdXR07Ov9T2Jv7xsZGXGONeM8AQAAYP9YAQUAgAseP35cslpqL06dOqWlpSWl02lNTU1paGhIksoeUX+Qe9jS6bQePXq075VT1Xjw4IEk6fz582XnmmWe3nvvvQO9/yhgC17z2r5KEAAAN7ACCgCAKkxPT0vSExtY2+OSyaSz8sd+Elu1DMNQPp9XMBjUnTt39PDhQydcqdU97PcsLy+XhE/pdFoDAwN7us6T7nHr1i2ZpqnOzk7neDPNEwAAAA6OAAoAgCq8/vrrkqR4PK6NjQ1JW427bXZoc+nSJUlbfYb8fr8Mw1BLS4t6enqUzWad8XYgYn+XVHJ+cnLSuc93vvMdTU5OOud2u0e17CfEDQ0NyTAM5+vMmTMlT8KLx+OKx+O7Xqv4MxT/bD/RTpJmZ2dL3tMs8wQAAIDaIIACAKAKbW1tymQyam1t1XPPPaeBgQG98MILMk1TCwsLGhsbkyQFAgFlMhmn31EsFlMmk1FbW5taWlqc69lPz7O/Syo5/8Mf/lCLi4syDEOLi4sl28p2u0e1RkdHnR5J27W3t1d9HcMwSj6DHfQYhqHl5WUNDw9raWnJeVJdNZ+hkeYJAAAAtWEU6MQJADiCeOw4qjE/P6/+/n4al1eBHlDNi38PAQD1wAooAAAAAAAAuIoACgAAAA3vKDaQTyQSJf3PAABoZgRQAAAcIsUNxXf7QnPI5/Ou/r7cvn6tZLNZjY6O6sUXX3T+hndqjt9Mf+8bGxsaGBiQYRgaGBgoebCBJF24cEGRSKSk8T4AAM2KAAoAgEOkUChU9YXmsLa21tTXr4V8Pq9oNKorV66os7NTuVxOCwsLmpiYqBhCFQoFbW5uSpI2Nzcb9u89n88rnU7rzp07yuVyOnfunLq6ukoeDhAMBjU8PKxoNMpKKABA0yOAAgAAaED5fF4zMzNNe/1amZ2dVTAYVEdHhyTJ5/Opt7dXkjQxMaFUKlX2Hvupi9ufvthI1tbWZJqmpNLPFAqFSsZ1dHSotbVVs7Ozda8RAIBaIoACAACosXw+r1Qq5WwBm5mZKdlGVWl72PZjk5OTzmoY+3g2m5VlWU5IMTMz42zfevz48YGvL0nxeHzH7W31ls1mNTQ0pPPnz1c8Pzk5qXA4XDGEquRJv5dsNqtUKuXMr2VZMgxDoVBIGxsbZbUlEgnn/Pbtc09ih0/bxWKxsmM9PT0aGhpiKx4AoKkRQAEAANRYJBLR559/7mwHsyyrZBuVvUWsWCaTKXk9Pj7u/GxvnWxpaVEoFJJlWVpfX9fVq1eVy+UkSe3t7U4Itd/rN5r79+9Lkp5//vmK5wcHBzUyMqJwOKx0Ov3E6z3p9xKNRhUOh535NU1TmUxGlmXpxo0bznWy2ayi0ahaW1tVKBR07do1dXV1VVXDTuwauru7y87Zn9+eDwAAmhEBFAAAQA2trKzIsixdunRJ0tY2sOHhYVmWpXv37jnHtmtra3vitYtDouItafaqGXtF036vL20FU8XhlJc+/vhjSbvXPjQ0JNM0debMmZJVYNtV83tZWlpyxtvza997amqq7Fr2trnOzk5J0vvvv7/nz2h78OCBTNPUa6+9VnbO5/NJ0q6fDwCARkcABQAAUEOLi4uSSkOg06dPS5Lm5+dduWcwGJS0FcYcJhMTE08c4/P5nP5Iu21Tq+XvxR6/fVtjNfXu5NatWxoeHnbCpmL2scP2+wUAHC0EUAAAADVUvFLGZgcIxU84Q+0EAgE9fPiwbEtdsVr+XuzxtXrCZCqVkmmazqorAAAOIwIoAACAGrKbS1daiVOpwXQtuX39RhYMBrW0tCTLsjQ5OVl23o3fSy22xKXTaT169EhXr1498LUAAGhkBFAAAAA11NfXJ0n69NNPnWP2ipyenh5X7mkHIZUaWDczO0iqtKKpEtM0tbCwUHErXC1/L9PT05KkZDLpXMN+Kt5eZLNZLS8vl/TcSqfTGhgYqDh+ZGRkT9cHAKCREEABAADU0MWLF2Wapq5fv+6strl3755isZjTrFr6ZtWNHR6tr6875+wAonjVzvZwI5VKSdoKUZLJpEzTdMYf5PrxeFzxeHz/E1BDp06dklQeQNnzWmk1U29vb8WgpprfS/H17HsW39s+bzcyn5iYkN/vl2EYamlpcYKsRCIhwzB2fSqe/SS9oaGhkl5SZ86cKQsSNzY2JElnz57d8XoAADQ6AigAAIAasptim6aplpYWp0H1zZs3S8a98847Mk1T7e3tsixLHR0dzgqesbExSXJWxty+fVuRSKTk/adPn1YoFJLf71dbW5uSyWRNr98IXnnlFUnSZ5995hyzwx5JJfNbbHx8vCSMk6r7vdjXlSS/31/yvfh8IBBQJpNxgq5YLKZMJuM8MS+XyykWi+0a5I2Oju7Ye6q9vb3ktf357fkAAKAZGYX9dksEAKCJGYahubk5Z1sOUMn8/Lz6+/v33VzaDXZw0kg1SVJ/f78kaW5urqbXtVdmDQ4O7ul9+Xy+4hPl6ikUCmlpaenA14nH4/L7/Xueg2rx7yEAoB5YAQUAAICGFY1Gtbq6WrKFsBpeh0/r6+saHh4+8HXS6bTS6bSi0WgNqgIAwDsEUAAAAE2iuEdRpf5Hh5G9de769eu79lRqJCsrK3rmmWfU0dFxoOs8fvxYU1NTmp2d9TxQAwDgoAigAAAAmkRxj6Linw+7QCCgZDKp5eVlr0upSmdnp9NA/SAsy9LY2JgCgUANqgIAwFvHvS4AAAAA1Wm0vk/15PP5XOuB1KiO2ucFABxurIACAAAAAACAqwigAAAAAAAA4CoCKAAAAAAAALiKAAoAAAAAAACuIoACAAAAAACAq4zCUX6cCgDgyDIMw+sSAKBh/PjHP9abb77pdRkAgEPsuNcFAADghY8++ki//vWvvS4DADx37NgxvfHGG16XAQA45FgBBQAAAAAAAFfRAwoAAAAAAACuIoACAAAAAACAqwigAAAAAAAA4Krjkv4/r4sAAAAAAADA4fX/A4vx9ZlwI8iOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import pydot_ng as pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "keras.utils.plot_model(model, 'lschen.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,10,16,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,10,16,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,10,16,1)\n",
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,10,16,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,10,16,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,10,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YTraining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.6488 - acc: 0.7450\n",
      "Epoch 00001: val_acc improved from -inf to 0.53760, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.54.model\n",
      "17000/17000 [==============================] - 8s 469us/sample - loss: 0.6333 - acc: 0.7503 - val_loss: 0.6912 - val_acc: 0.5376\n",
      "Epoch 2/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.2997 - acc: 0.8688\n",
      "Epoch 00002: val_acc improved from 0.53760 to 0.55120, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.55.model\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2995 - acc: 0.8684 - val_loss: 0.6887 - val_acc: 0.5512\n",
      "Epoch 3/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.2441 - acc: 0.8951\n",
      "Epoch 00003: val_acc improved from 0.55120 to 0.60520, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.61.model\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2426 - acc: 0.8956 - val_loss: 0.6861 - val_acc: 0.6052\n",
      "Epoch 4/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.2036 - acc: 0.9169\n",
      "Epoch 00004: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2051 - acc: 0.9162 - val_loss: 0.6888 - val_acc: 0.5484\n",
      "Epoch 5/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.1647 - acc: 0.9350\n",
      "Epoch 00005: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1639 - acc: 0.9352 - val_loss: 0.7154 - val_acc: 0.4964\n",
      "Epoch 6/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.1439 - acc: 0.9424\n",
      "Epoch 00006: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1455 - acc: 0.9418 - val_loss: 0.7490 - val_acc: 0.4964\n",
      "Epoch 7/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.1092 - acc: 0.9574\n",
      "Epoch 00007: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1119 - acc: 0.9562 - val_loss: 0.7427 - val_acc: 0.4964\n",
      "Epoch 8/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0932 - acc: 0.9638\n",
      "Epoch 00008: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0927 - acc: 0.9639 - val_loss: 0.8897 - val_acc: 0.4964\n",
      "Epoch 9/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0731 - acc: 0.9727\n",
      "Epoch 00009: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0747 - acc: 0.9724 - val_loss: 1.4374 - val_acc: 0.4964\n",
      "Epoch 10/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0591 - acc: 0.9777\n",
      "Epoch 00010: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0593 - acc: 0.9776 - val_loss: 1.0150 - val_acc: 0.4964\n",
      "Epoch 11/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0490 - acc: 0.9820\n",
      "Epoch 00011: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0488 - acc: 0.9817 - val_loss: 2.0333 - val_acc: 0.4964\n",
      "Epoch 12/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0414 - acc: 0.9849\n",
      "Epoch 00012: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0417 - acc: 0.9848 - val_loss: 1.6743 - val_acc: 0.4964\n",
      "Epoch 13/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0320 - acc: 0.9885\n",
      "Epoch 00013: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0315 - acc: 0.9889 - val_loss: 1.5115 - val_acc: 0.4964\n",
      "Epoch 14/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0255 - acc: 0.9906\n",
      "Epoch 00014: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0260 - acc: 0.9905 - val_loss: 2.2506 - val_acc: 0.4964\n",
      "Epoch 15/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0302 - acc: 0.9891\n",
      "Epoch 00015: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0307 - acc: 0.9891 - val_loss: 1.3982 - val_acc: 0.4964\n",
      "Epoch 16/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0266 - acc: 0.9901\n",
      "Epoch 00016: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0270 - acc: 0.9901 - val_loss: 2.4448 - val_acc: 0.4964\n",
      "Epoch 17/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0225 - acc: 0.9930\n",
      "Epoch 00017: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0223 - acc: 0.9931 - val_loss: 2.6110 - val_acc: 0.4964\n",
      "Epoch 18/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0194 - acc: 0.9929\n",
      "Epoch 00018: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0201 - acc: 0.9928 - val_loss: 2.7838 - val_acc: 0.4964\n",
      "Epoch 19/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0188 - acc: 0.9935\n",
      "Epoch 00019: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0195 - acc: 0.9932 - val_loss: 3.5276 - val_acc: 0.4964\n",
      "Epoch 20/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0152 - acc: 0.9945\n",
      "Epoch 00020: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0151 - acc: 0.9945 - val_loss: 2.7743 - val_acc: 0.4964\n",
      "Epoch 21/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0141 - acc: 0.9955\n",
      "Epoch 00021: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0139 - acc: 0.9955 - val_loss: 2.7525 - val_acc: 0.4964\n",
      "Epoch 22/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0154 - acc: 0.9947\n",
      "Epoch 00022: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0148 - acc: 0.9950 - val_loss: 2.8210 - val_acc: 0.4964\n",
      "Epoch 23/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0150 - acc: 0.9951\n",
      "Epoch 00023: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0147 - acc: 0.9952 - val_loss: 1.4982 - val_acc: 0.5372\n",
      "Epoch 24/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0130 - acc: 0.9959\n",
      "Epoch 00024: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0133 - acc: 0.9958 - val_loss: 1.6692 - val_acc: 0.5092\n",
      "Epoch 25/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0142 - acc: 0.9947\n",
      "Epoch 00025: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0143 - acc: 0.9949 - val_loss: 3.5216 - val_acc: 0.4964\n",
      "Epoch 26/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0117 - acc: 0.9963\n",
      "Epoch 00026: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0122 - acc: 0.9962 - val_loss: 3.0481 - val_acc: 0.4964\n",
      "Epoch 27/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0075 - acc: 0.9981\n",
      "Epoch 00027: val_acc did not improve from 0.60520\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0079 - acc: 0.9979 - val_loss: 2.0770 - val_acc: 0.5224\n",
      "Epoch 28/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0124 - acc: 0.9953\n",
      "Epoch 00028: val_acc improved from 0.60520 to 0.69800, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.70.model\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.0125 - acc: 0.9952 - val_loss: 0.8040 - val_acc: 0.6980\n",
      "Epoch 29/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0160 - acc: 0.9945\n",
      "Epoch 00029: val_acc did not improve from 0.69800\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0155 - acc: 0.9948 - val_loss: 1.0843 - val_acc: 0.6476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0151 - acc: 0.9944\n",
      "Epoch 00030: val_acc did not improve from 0.69800\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0152 - acc: 0.9943 - val_loss: 1.6034 - val_acc: 0.6440\n",
      "Epoch 31/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0118 - acc: 0.9957\n",
      "Epoch 00031: val_acc did not improve from 0.69800\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0116 - acc: 0.9958 - val_loss: 1.6602 - val_acc: 0.6144\n",
      "Epoch 32/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0128 - acc: 0.9956\n",
      "Epoch 00032: val_acc improved from 0.69800 to 0.74320, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.74.model\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0126 - acc: 0.9958 - val_loss: 0.9063 - val_acc: 0.7432\n",
      "Epoch 33/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0132 - acc: 0.9956\n",
      "Epoch 00033: val_acc did not improve from 0.74320\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0128 - acc: 0.9958 - val_loss: 1.3198 - val_acc: 0.7008\n",
      "Epoch 34/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0114 - acc: 0.9961\n",
      "Epoch 00034: val_acc did not improve from 0.74320\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0112 - acc: 0.9963 - val_loss: 1.4446 - val_acc: 0.6900\n",
      "Epoch 35/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0129 - acc: 0.9946\n",
      "Epoch 00035: val_acc did not improve from 0.74320\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0128 - acc: 0.9947 - val_loss: 1.8553 - val_acc: 0.6416\n",
      "Epoch 36/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0131 - acc: 0.9951\n",
      "Epoch 00036: val_acc improved from 0.74320 to 0.79240, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.79.model\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0135 - acc: 0.9950 - val_loss: 0.9855 - val_acc: 0.7924\n",
      "Epoch 37/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0112 - acc: 0.9963\n",
      "Epoch 00037: val_acc did not improve from 0.79240\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0118 - acc: 0.9961 - val_loss: 1.0350 - val_acc: 0.7844\n",
      "Epoch 38/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0116 - acc: 0.9957\n",
      "Epoch 00038: val_acc improved from 0.79240 to 0.82440, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.82.model\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0119 - acc: 0.9955 - val_loss: 0.9200 - val_acc: 0.8244\n",
      "Epoch 39/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0134 - acc: 0.9958\n",
      "Epoch 00039: val_acc improved from 0.82440 to 0.85160, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.85.model\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.0131 - acc: 0.9958 - val_loss: 0.6527 - val_acc: 0.8516\n",
      "Epoch 40/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0112 - acc: 0.9963\n",
      "Epoch 00040: val_acc improved from 0.85160 to 0.88120, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.88.model\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0114 - acc: 0.9962 - val_loss: 0.5376 - val_acc: 0.8812\n",
      "Epoch 41/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 00041: val_acc did not improve from 0.88120\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0070 - acc: 0.9976 - val_loss: 0.6657 - val_acc: 0.8584\n",
      "Epoch 42/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0064 - acc: 0.9976\n",
      "Epoch 00042: val_acc did not improve from 0.88120\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0066 - acc: 0.9974 - val_loss: 0.5379 - val_acc: 0.8736\n",
      "Epoch 43/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0075 - acc: 0.9976\n",
      "Epoch 00043: val_acc did not improve from 0.88120\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0075 - acc: 0.9976 - val_loss: 0.8819 - val_acc: 0.8232\n",
      "Epoch 44/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0055 - acc: 0.9977\n",
      "Epoch 00044: val_acc did not improve from 0.88120\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0059 - acc: 0.9976 - val_loss: 1.3919 - val_acc: 0.7584\n",
      "Epoch 45/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0062 - acc: 0.9981\n",
      "Epoch 00045: val_acc did not improve from 0.88120\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0062 - acc: 0.9981 - val_loss: 0.6284 - val_acc: 0.8772\n",
      "Epoch 46/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0061 - acc: 0.9979\n",
      "Epoch 00046: val_acc improved from 0.88120 to 0.88640, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.89.model\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0059 - acc: 0.9980 - val_loss: 0.5658 - val_acc: 0.8864\n",
      "Epoch 47/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0058 - acc: 0.9981\n",
      "Epoch 00047: val_acc did not improve from 0.88640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0060 - acc: 0.9982 - val_loss: 0.7650 - val_acc: 0.8564\n",
      "Epoch 48/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0057 - acc: 0.9981\n",
      "Epoch 00048: val_acc did not improve from 0.88640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0056 - acc: 0.9982 - val_loss: 0.6763 - val_acc: 0.8740\n",
      "Epoch 49/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0049 - acc: 0.9983\n",
      "Epoch 00049: val_acc improved from 0.88640 to 0.89960, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.90.model\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0056 - acc: 0.9980 - val_loss: 0.4830 - val_acc: 0.8996\n",
      "Epoch 50/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 00050: val_acc improved from 0.89960 to 0.90120, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.90.model\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.5330 - val_acc: 0.9012\n",
      "Epoch 51/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9986\n",
      "Epoch 00051: val_acc improved from 0.90120 to 0.91320, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.91.model\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.4650 - val_acc: 0.9132\n",
      "Epoch 52/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9986\n",
      "Epoch 00052: val_acc did not improve from 0.91320\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.5756 - val_acc: 0.8980\n",
      "Epoch 53/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0048 - acc: 0.9984\n",
      "Epoch 00053: val_acc did not improve from 0.91320\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0049 - acc: 0.9984 - val_loss: 0.4598 - val_acc: 0.9088\n",
      "Epoch 54/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0068 - acc: 0.9975\n",
      "Epoch 00054: val_acc did not improve from 0.91320\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0076 - acc: 0.9974 - val_loss: 0.5196 - val_acc: 0.9056\n",
      "Epoch 55/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0060 - acc: 0.9982\n",
      "Epoch 00055: val_acc did not improve from 0.91320\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0062 - acc: 0.9982 - val_loss: 0.5066 - val_acc: 0.9048\n",
      "Epoch 56/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0067 - acc: 0.9976\n",
      "Epoch 00056: val_acc did not improve from 0.91320\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0066 - acc: 0.9976 - val_loss: 0.4924 - val_acc: 0.9124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0073 - acc: 0.9971\n",
      "Epoch 00057: val_acc did not improve from 0.91320\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0074 - acc: 0.9971 - val_loss: 0.4784 - val_acc: 0.9132\n",
      "Epoch 58/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0062 - acc: 0.9979\n",
      "Epoch 00058: val_acc did not improve from 0.91320\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0061 - acc: 0.9980 - val_loss: 1.2232 - val_acc: 0.7964\n",
      "Epoch 59/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 00059: val_acc improved from 0.91320 to 0.91640, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.92.model\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0056 - acc: 0.9982 - val_loss: 0.4490 - val_acc: 0.9164\n",
      "Epoch 60/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00060: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 0.4718 - val_acc: 0.9140\n",
      "Epoch 61/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9984\n",
      "Epoch 00061: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0050 - acc: 0.9981 - val_loss: 0.4454 - val_acc: 0.9084\n",
      "Epoch 62/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0070 - acc: 0.9975\n",
      "Epoch 00062: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0072 - acc: 0.9974 - val_loss: 0.5027 - val_acc: 0.9072\n",
      "Epoch 63/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0084 - acc: 0.9970\n",
      "Epoch 00063: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0082 - acc: 0.9971 - val_loss: 0.6143 - val_acc: 0.9044\n",
      "Epoch 64/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0064 - acc: 0.9983\n",
      "Epoch 00064: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.6232 - val_acc: 0.9012\n",
      "Epoch 65/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0059 - acc: 0.9976\n",
      "Epoch 00065: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0059 - acc: 0.9976 - val_loss: 0.5480 - val_acc: 0.9028\n",
      "Epoch 66/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
      "Epoch 00066: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0069 - acc: 0.9978 - val_loss: 0.5372 - val_acc: 0.9040\n",
      "Epoch 67/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0049 - acc: 0.9985\n",
      "Epoch 00067: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0056 - acc: 0.9982 - val_loss: 0.5013 - val_acc: 0.9032\n",
      "Epoch 68/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0075 - acc: 0.9972\n",
      "Epoch 00068: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0077 - acc: 0.9972 - val_loss: 0.5489 - val_acc: 0.9028\n",
      "Epoch 69/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0104 - acc: 0.9962\n",
      "Epoch 00069: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0103 - acc: 0.9962 - val_loss: 0.4961 - val_acc: 0.9040\n",
      "Epoch 70/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0086 - acc: 0.9976\n",
      "Epoch 00070: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.4905 - val_acc: 0.9164\n",
      "Epoch 71/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0092 - acc: 0.9966\n",
      "Epoch 00071: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0091 - acc: 0.9966 - val_loss: 0.5403 - val_acc: 0.9064\n",
      "Epoch 72/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0084 - acc: 0.9969\n",
      "Epoch 00072: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0082 - acc: 0.9969 - val_loss: 0.4587 - val_acc: 0.9116\n",
      "Epoch 73/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
      "Epoch 00073: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0063 - acc: 0.9978 - val_loss: 0.5449 - val_acc: 0.9044\n",
      "Epoch 74/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0050 - acc: 0.9979\n",
      "Epoch 00074: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0048 - acc: 0.9981 - val_loss: 0.5348 - val_acc: 0.9116\n",
      "Epoch 75/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9987\n",
      "Epoch 00075: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0036 - acc: 0.9986 - val_loss: 0.4958 - val_acc: 0.9116\n",
      "Epoch 76/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 00076: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0043 - acc: 0.9988 - val_loss: 0.7659 - val_acc: 0.8884\n",
      "Epoch 77/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00077: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.5766 - val_acc: 0.8860\n",
      "Epoch 78/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 00078: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.4720 - val_acc: 0.9128\n",
      "Epoch 79/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00079: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.5358 - val_acc: 0.8968\n",
      "Epoch 80/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9991\n",
      "Epoch 00080: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0024 - acc: 0.9991 - val_loss: 0.4673 - val_acc: 0.9124\n",
      "Epoch 81/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 00081: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0044 - acc: 0.9987 - val_loss: 0.4996 - val_acc: 0.9096\n",
      "Epoch 82/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0035 - acc: 0.9987\n",
      "Epoch 00082: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0036 - acc: 0.9988 - val_loss: 0.6786 - val_acc: 0.9012\n",
      "Epoch 83/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9986\n",
      "Epoch 00083: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0037 - acc: 0.9986 - val_loss: 0.5513 - val_acc: 0.9072\n",
      "Epoch 84/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 00084: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.5625 - val_acc: 0.8960\n",
      "Epoch 85/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0031 - acc: 0.9989\n",
      "Epoch 00085: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0033 - acc: 0.9988 - val_loss: 0.4939 - val_acc: 0.9096\n",
      "Epoch 86/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9984\n",
      "Epoch 00086: val_acc did not improve from 0.91640\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0043 - acc: 0.9985 - val_loss: 0.5058 - val_acc: 0.9088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9989\n",
      "Epoch 00087: val_acc improved from 0.91640 to 0.91720, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.92.model\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.5130 - val_acc: 0.9172\n",
      "Epoch 88/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0029 - acc: 0.9989\n",
      "Epoch 00088: val_acc did not improve from 0.91720\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0028 - acc: 0.9989 - val_loss: 0.6954 - val_acc: 0.8800\n",
      "Epoch 89/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9986\n",
      "Epoch 00089: val_acc did not improve from 0.91720\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0035 - acc: 0.9986 - val_loss: 0.5157 - val_acc: 0.9144\n",
      "Epoch 90/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9988\n",
      "Epoch 00090: val_acc did not improve from 0.91720\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0030 - acc: 0.9989 - val_loss: 0.5560 - val_acc: 0.9028\n",
      "Epoch 91/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0035 - acc: 0.9986\n",
      "Epoch 00091: val_acc improved from 0.91720 to 0.92360, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.92.model\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0041 - acc: 0.9984 - val_loss: 0.4812 - val_acc: 0.9236\n",
      "Epoch 92/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9983\n",
      "Epoch 00092: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0045 - acc: 0.9983 - val_loss: 0.5801 - val_acc: 0.9036\n",
      "Epoch 93/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0055 - acc: 0.9981\n",
      "Epoch 00093: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0051 - acc: 0.9983 - val_loss: 0.5521 - val_acc: 0.9120\n",
      "Epoch 94/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0054 - acc: 0.9983\n",
      "Epoch 00094: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0053 - acc: 0.9984 - val_loss: 0.5924 - val_acc: 0.9084\n",
      "Epoch 95/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0073 - acc: 0.9975\n",
      "Epoch 00095: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0072 - acc: 0.9975 - val_loss: 0.5215 - val_acc: 0.9084\n",
      "Epoch 96/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0059 - acc: 0.9980\n",
      "Epoch 00096: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0059 - acc: 0.9981 - val_loss: 0.6886 - val_acc: 0.9108\n",
      "Epoch 97/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0067 - acc: 0.9979\n",
      "Epoch 00097: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.5550 - val_acc: 0.9080\n",
      "Epoch 98/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0065 - acc: 0.9974\n",
      "Epoch 00098: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0068 - acc: 0.9973 - val_loss: 0.6119 - val_acc: 0.9008\n",
      "Epoch 99/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0108 - acc: 0.9956\n",
      "Epoch 00099: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0110 - acc: 0.9956 - val_loss: 0.5675 - val_acc: 0.9052\n",
      "Epoch 100/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0083 - acc: 0.9969\n",
      "Epoch 00100: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0083 - acc: 0.9969 - val_loss: 0.6063 - val_acc: 0.9040\n",
      "Epoch 101/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0080 - acc: 0.9972\n",
      "Epoch 00101: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0082 - acc: 0.9972 - val_loss: 0.6984 - val_acc: 0.8988\n",
      "Epoch 102/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0064 - acc: 0.9975\n",
      "Epoch 00102: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0061 - acc: 0.9977 - val_loss: 0.7026 - val_acc: 0.8952\n",
      "Epoch 103/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0078 - acc: 0.9969\n",
      "Epoch 00103: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0079 - acc: 0.9969 - val_loss: 0.5797 - val_acc: 0.9040\n",
      "Epoch 104/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 00104: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0064 - acc: 0.9978 - val_loss: 0.4994 - val_acc: 0.9100\n",
      "Epoch 105/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0051 - acc: 0.9983\n",
      "Epoch 00105: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0051 - acc: 0.9982 - val_loss: 0.6540 - val_acc: 0.9056\n",
      "Epoch 106/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0055 - acc: 0.9981\n",
      "Epoch 00106: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0056 - acc: 0.9981 - val_loss: 0.5743 - val_acc: 0.9108\n",
      "Epoch 107/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0060 - acc: 0.9974\n",
      "Epoch 00107: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0057 - acc: 0.9976 - val_loss: 0.5050 - val_acc: 0.9108\n",
      "Epoch 108/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9981\n",
      "Epoch 00108: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0046 - acc: 0.9982 - val_loss: 0.7475 - val_acc: 0.8940\n",
      "Epoch 109/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0036 - acc: 0.9985\n",
      "Epoch 00109: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0034 - acc: 0.9985 - val_loss: 0.5366 - val_acc: 0.9108\n",
      "Epoch 110/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9987\n",
      "Epoch 00110: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0035 - acc: 0.9988 - val_loss: 0.4685 - val_acc: 0.9172\n",
      "Epoch 111/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0026 - acc: 0.9988\n",
      "Epoch 00111: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0025 - acc: 0.9989 - val_loss: 0.4704 - val_acc: 0.9192\n",
      "Epoch 112/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 00112: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0013 - acc: 0.9995 - val_loss: 0.5213 - val_acc: 0.9140\n",
      "Epoch 113/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 00113: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0020 - acc: 0.9994 - val_loss: 0.4728 - val_acc: 0.9184\n",
      "Epoch 114/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 00114: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0016 - acc: 0.9995 - val_loss: 0.5233 - val_acc: 0.9148\n",
      "Epoch 115/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0022 - acc: 0.9989\n",
      "Epoch 00115: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0022 - acc: 0.9988 - val_loss: 0.5630 - val_acc: 0.9132\n",
      "Epoch 116/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9991\n",
      "Epoch 00116: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0023 - acc: 0.9991 - val_loss: 0.5002 - val_acc: 0.9176\n",
      "Epoch 117/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00117: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0020 - acc: 0.9994 - val_loss: 0.5332 - val_acc: 0.9148\n",
      "Epoch 118/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0017 - acc: 0.9994\n",
      "Epoch 00118: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0018 - acc: 0.9994 - val_loss: 0.5479 - val_acc: 0.9088\n",
      "Epoch 119/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9994\n",
      "Epoch 00119: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 0.5263 - val_acc: 0.9112\n",
      "Epoch 120/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0038 - acc: 0.9986\n",
      "Epoch 00120: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0039 - acc: 0.9985 - val_loss: 0.5005 - val_acc: 0.9164\n",
      "Epoch 121/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0030 - acc: 0.9990\n",
      "Epoch 00121: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.5313 - val_acc: 0.9208\n",
      "Epoch 122/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0045 - acc: 0.9986\n",
      "Epoch 00122: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0044 - acc: 0.9987 - val_loss: 0.5482 - val_acc: 0.9072\n",
      "Epoch 123/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 00123: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0035 - acc: 0.9990 - val_loss: 0.5564 - val_acc: 0.9068\n",
      "Epoch 124/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00124: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.4740 - val_acc: 0.9136\n",
      "Epoch 125/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0021 - acc: 0.9993\n",
      "Epoch 00125: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0021 - acc: 0.9992 - val_loss: 0.5072 - val_acc: 0.9220\n",
      "Epoch 126/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0035 - acc: 0.9989\n",
      "Epoch 00126: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0045 - acc: 0.9987 - val_loss: 0.9712 - val_acc: 0.8516\n",
      "Epoch 127/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0045 - acc: 0.9985\n",
      "Epoch 00127: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0048 - acc: 0.9984 - val_loss: 0.5572 - val_acc: 0.9144\n",
      "Epoch 128/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0041 - acc: 0.9987\n",
      "Epoch 00128: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.5578 - val_acc: 0.9136\n",
      "Epoch 129/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9991\n",
      "Epoch 00129: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 0.5766 - val_acc: 0.9076\n",
      "Epoch 130/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0042 - acc: 0.9985\n",
      "Epoch 00130: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0040 - acc: 0.9986 - val_loss: 0.6935 - val_acc: 0.8856\n",
      "Epoch 131/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9983\n",
      "Epoch 00131: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0047 - acc: 0.9983 - val_loss: 0.5525 - val_acc: 0.9116\n",
      "Epoch 132/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0068 - acc: 0.9976\n",
      "Epoch 00132: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0074 - acc: 0.9972 - val_loss: 0.5810 - val_acc: 0.9108\n",
      "Epoch 133/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 00133: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0071 - acc: 0.9978 - val_loss: 0.5974 - val_acc: 0.9024\n",
      "Epoch 134/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9982\n",
      "Epoch 00134: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.0047 - acc: 0.9982 - val_loss: 0.6584 - val_acc: 0.8912\n",
      "Epoch 135/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9987\n",
      "Epoch 00135: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.5086 - val_acc: 0.9156\n",
      "Epoch 136/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0029 - acc: 0.9991\n",
      "Epoch 00136: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0032 - acc: 0.9989 - val_loss: 0.5656 - val_acc: 0.9088\n",
      "Epoch 137/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0034 - acc: 0.9988\n",
      "Epoch 00137: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0033 - acc: 0.9988 - val_loss: 0.5162 - val_acc: 0.9188\n",
      "Epoch 138/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0031 - acc: 0.9991\n",
      "Epoch 00138: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0031 - acc: 0.9991 - val_loss: 0.5523 - val_acc: 0.9068\n",
      "Epoch 139/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9989\n",
      "Epoch 00139: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0032 - acc: 0.9989 - val_loss: 0.5164 - val_acc: 0.9184\n",
      "Epoch 140/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9984\n",
      "Epoch 00140: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0044 - acc: 0.9985 - val_loss: 0.9015 - val_acc: 0.8620\n",
      "Epoch 141/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9991\n",
      "Epoch 00141: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0030 - acc: 0.9991 - val_loss: 0.4878 - val_acc: 0.9140\n",
      "Epoch 142/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 00142: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.5533 - val_acc: 0.9112\n",
      "Epoch 143/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0029 - acc: 0.9990\n",
      "Epoch 00143: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0031 - acc: 0.9990 - val_loss: 0.5045 - val_acc: 0.9184\n",
      "Epoch 144/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0027 - acc: 0.9990\n",
      "Epoch 00144: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0028 - acc: 0.9990 - val_loss: 0.5232 - val_acc: 0.9116\n",
      "Epoch 145/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 00145: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.4974 - val_acc: 0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9994\n",
      "Epoch 00146: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.7435 - val_acc: 0.9064\n",
      "Epoch 147/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9991\n",
      "Epoch 00147: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0025 - acc: 0.9992 - val_loss: 0.5525 - val_acc: 0.9200\n",
      "Epoch 148/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9990\n",
      "Epoch 00148: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0028 - acc: 0.9989 - val_loss: 0.5533 - val_acc: 0.9156\n",
      "Epoch 149/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9992\n",
      "Epoch 00149: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 0.6494 - val_acc: 0.9088\n",
      "Epoch 150/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9991\n",
      "Epoch 00150: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0024 - acc: 0.9991 - val_loss: 0.5949 - val_acc: 0.9060\n",
      "Epoch 151/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9991\n",
      "Epoch 00151: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0027 - acc: 0.9991 - val_loss: 0.5462 - val_acc: 0.9144\n",
      "Epoch 152/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 00152: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0033 - acc: 0.9991 - val_loss: 0.4884 - val_acc: 0.9184\n",
      "Epoch 153/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0031 - acc: 0.9989\n",
      "Epoch 00153: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6496 - val_acc: 0.9016\n",
      "Epoch 154/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9987\n",
      "Epoch 00154: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0030 - acc: 0.9988 - val_loss: 0.6382 - val_acc: 0.9072\n",
      "Epoch 155/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9987\n",
      "Epoch 00155: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0038 - acc: 0.9988 - val_loss: 0.5298 - val_acc: 0.9180\n",
      "Epoch 156/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 00156: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0036 - acc: 0.9989 - val_loss: 0.6181 - val_acc: 0.9096\n",
      "Epoch 157/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00157: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.7312 - val_acc: 0.9000\n",
      "Epoch 158/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 00158: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0013 - acc: 0.9996 - val_loss: 0.6126 - val_acc: 0.9088\n",
      "Epoch 159/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0024 - acc: 0.9993\n",
      "Epoch 00159: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0023 - acc: 0.9994 - val_loss: 0.7611 - val_acc: 0.9004\n",
      "Epoch 160/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9992\n",
      "Epoch 00160: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0021 - acc: 0.9992 - val_loss: 0.7443 - val_acc: 0.9016\n",
      "Epoch 161/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00161: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0020 - acc: 0.9994 - val_loss: 0.6970 - val_acc: 0.9100\n",
      "Epoch 162/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9991\n",
      "Epoch 00162: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0027 - acc: 0.9991 - val_loss: 0.5851 - val_acc: 0.9136\n",
      "Epoch 163/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9991\n",
      "Epoch 00163: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0024 - acc: 0.9991 - val_loss: 0.5779 - val_acc: 0.9108\n",
      "Epoch 164/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
      "Epoch 00164: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0030 - acc: 0.9988 - val_loss: 0.5307 - val_acc: 0.9176\n",
      "Epoch 165/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 00165: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0075 - acc: 0.9976 - val_loss: 0.6434 - val_acc: 0.9080\n",
      "Epoch 166/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 00166: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0064 - acc: 0.9975 - val_loss: 0.5521 - val_acc: 0.9172\n",
      "Epoch 167/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9984\n",
      "Epoch 00167: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0043 - acc: 0.9985 - val_loss: 0.5860 - val_acc: 0.9036\n",
      "Epoch 168/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9985\n",
      "Epoch 00168: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0044 - acc: 0.9985 - val_loss: 1.0622 - val_acc: 0.8464\n",
      "Epoch 169/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9987\n",
      "Epoch 00169: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0033 - acc: 0.9988 - val_loss: 0.5456 - val_acc: 0.9100\n",
      "Epoch 170/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0029 - acc: 0.9991\n",
      "Epoch 00170: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6353 - val_acc: 0.9012\n",
      "Epoch 171/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9983\n",
      "Epoch 00171: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0055 - acc: 0.9981 - val_loss: 0.8497 - val_acc: 0.8772\n",
      "Epoch 172/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0072 - acc: 0.9976\n",
      "Epoch 00172: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.5613 - val_acc: 0.9076\n",
      "Epoch 173/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9981\n",
      "Epoch 00173: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0051 - acc: 0.9980 - val_loss: 0.6086 - val_acc: 0.9096\n",
      "Epoch 174/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0040 - acc: 0.9984\n",
      "Epoch 00174: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0040 - acc: 0.9983 - val_loss: 0.5515 - val_acc: 0.9112\n",
      "Epoch 175/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0054 - acc: 0.9980\n",
      "Epoch 00175: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0053 - acc: 0.9981 - val_loss: 0.5891 - val_acc: 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0039 - acc: 0.9987\n",
      "Epoch 00176: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0036 - acc: 0.9988 - val_loss: 0.5477 - val_acc: 0.9188\n",
      "Epoch 177/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9989\n",
      "Epoch 00177: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0034 - acc: 0.9989 - val_loss: 0.5898 - val_acc: 0.9132\n",
      "Epoch 178/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0032 - acc: 0.9986\n",
      "Epoch 00178: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0034 - acc: 0.9986 - val_loss: 0.5120 - val_acc: 0.9128\n",
      "Epoch 179/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 00179: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.5111 - val_acc: 0.9136\n",
      "Epoch 180/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 00180: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0033 - acc: 0.9989 - val_loss: 0.6789 - val_acc: 0.9052\n",
      "Epoch 181/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0023 - acc: 0.9991\n",
      "Epoch 00181: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.5823 - val_acc: 0.9064\n",
      "Epoch 182/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9990\n",
      "Epoch 00182: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0030 - acc: 0.9991 - val_loss: 0.6314 - val_acc: 0.9060\n",
      "Epoch 183/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 00183: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.5203 - val_acc: 0.9152\n",
      "Epoch 184/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9993\n",
      "Epoch 00184: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.5664 - val_acc: 0.9104\n",
      "Epoch 185/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0023 - acc: 0.9990\n",
      "Epoch 00185: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.0024 - acc: 0.9991 - val_loss: 0.5776 - val_acc: 0.9124\n",
      "Epoch 186/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9997    \n",
      "Epoch 00186: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.5042 - val_acc: 0.9228\n",
      "Epoch 187/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 00187: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0014 - acc: 0.9995 - val_loss: 0.5504 - val_acc: 0.9120\n",
      "Epoch 188/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9992\n",
      "Epoch 00188: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0020 - acc: 0.9993 - val_loss: 0.5496 - val_acc: 0.9172\n",
      "Epoch 189/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9994\n",
      "Epoch 00189: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0014 - acc: 0.9994 - val_loss: 0.6119 - val_acc: 0.9152\n",
      "Epoch 190/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0010 - acc: 0.9996   \n",
      "Epoch 00190: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0010 - acc: 0.9996 - val_loss: 0.6164 - val_acc: 0.9128\n",
      "Epoch 191/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 5.6285e-04 - acc: 0.9999\n",
      "Epoch 00191: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 6.0491e-04 - acc: 0.9999 - val_loss: 0.5426 - val_acc: 0.9140\n",
      "Epoch 192/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00192: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.5232 - val_acc: 0.9168\n",
      "Epoch 193/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0013 - acc: 0.9994  \n",
      "Epoch 00193: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0014 - acc: 0.9994 - val_loss: 0.5078 - val_acc: 0.9192\n",
      "Epoch 194/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00194: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.5237 - val_acc: 0.9128\n",
      "Epoch 195/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9993\n",
      "Epoch 00195: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.6161 - val_acc: 0.9104\n",
      "Epoch 196/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0015 - acc: 0.9992\n",
      "Epoch 00196: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0015 - acc: 0.9992 - val_loss: 0.5811 - val_acc: 0.9176\n",
      "Epoch 197/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
      "Epoch 00197: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0026 - acc: 0.9989 - val_loss: 0.6133 - val_acc: 0.9140\n",
      "Epoch 198/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9993\n",
      "Epoch 00198: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0021 - acc: 0.9994 - val_loss: 0.6277 - val_acc: 0.9012\n",
      "Epoch 199/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9991\n",
      "Epoch 00199: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.7072 - val_acc: 0.9028\n",
      "Epoch 200/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00200: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0013 - acc: 0.9996 - val_loss: 0.6929 - val_acc: 0.9120\n",
      "Epoch 201/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0032 - acc: 0.9987\n",
      "Epoch 00201: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0032 - acc: 0.9988 - val_loss: 0.7909 - val_acc: 0.8916\n",
      "Epoch 202/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0027 - acc: 0.9987\n",
      "Epoch 00202: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0027 - acc: 0.9988 - val_loss: 0.6128 - val_acc: 0.9084\n",
      "Epoch 203/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 00203: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0046 - acc: 0.9985 - val_loss: 1.1668 - val_acc: 0.8604\n",
      "Epoch 204/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9985\n",
      "Epoch 00204: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0045 - acc: 0.9985 - val_loss: 0.5537 - val_acc: 0.9132\n",
      "Epoch 205/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0049 - acc: 0.9985\n",
      "Epoch 00205: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0051 - acc: 0.9983 - val_loss: 0.5908 - val_acc: 0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0063 - acc: 0.9979\n",
      "Epoch 00206: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0061 - acc: 0.9981 - val_loss: 0.7461 - val_acc: 0.9044\n",
      "Epoch 207/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0069 - acc: 0.9973\n",
      "Epoch 00207: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0071 - acc: 0.9974 - val_loss: 0.6451 - val_acc: 0.8948\n",
      "Epoch 208/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0077 - acc: 0.9973\n",
      "Epoch 00208: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0074 - acc: 0.9974 - val_loss: 0.7058 - val_acc: 0.9008\n",
      "Epoch 209/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0059 - acc: 0.9984\n",
      "Epoch 00209: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0056 - acc: 0.9985 - val_loss: 0.5925 - val_acc: 0.9144\n",
      "Epoch 210/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0047 - acc: 0.9984\n",
      "Epoch 00210: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0051 - acc: 0.9982 - val_loss: 0.6656 - val_acc: 0.9004\n",
      "Epoch 211/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0059 - acc: 0.9978\n",
      "Epoch 00211: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0057 - acc: 0.9978 - val_loss: 0.6216 - val_acc: 0.9124\n",
      "Epoch 212/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0036 - acc: 0.9989\n",
      "Epoch 00212: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 1.0607 - val_acc: 0.8536\n",
      "Epoch 213/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0051 - acc: 0.9981\n",
      "Epoch 00213: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0049 - acc: 0.9982 - val_loss: 0.6314 - val_acc: 0.9068\n",
      "Epoch 214/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0048 - acc: 0.9984\n",
      "Epoch 00214: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0047 - acc: 0.9985 - val_loss: 0.5945 - val_acc: 0.9144\n",
      "Epoch 215/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00215: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0042 - acc: 0.9989 - val_loss: 0.5855 - val_acc: 0.9148\n",
      "Epoch 216/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0018 - acc: 0.9993\n",
      "Epoch 00216: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0019 - acc: 0.9992 - val_loss: 0.5351 - val_acc: 0.9200\n",
      "Epoch 217/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00217: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0013 - acc: 0.9996 - val_loss: 0.4838 - val_acc: 0.9228\n",
      "Epoch 218/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0012 - acc: 0.9997    \n",
      "Epoch 00218: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0011 - acc: 0.9996 - val_loss: 0.5663 - val_acc: 0.9176\n",
      "Epoch 219/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00219: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0017 - acc: 0.9994 - val_loss: 0.6198 - val_acc: 0.9080\n",
      "Epoch 220/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0012 - acc: 0.9997 \n",
      "Epoch 00220: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.6097 - val_acc: 0.9124\n",
      "Epoch 221/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 8.2885e-04 - acc: 0.9998\n",
      "Epoch 00221: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 7.9206e-04 - acc: 0.9998 - val_loss: 0.6104 - val_acc: 0.9148\n",
      "Epoch 222/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 8.9951e-04 - acc: 0.9996\n",
      "Epoch 00222: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 8.7537e-04 - acc: 0.9996 - val_loss: 0.5061 - val_acc: 0.9216\n",
      "Epoch 223/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 9.9144e-04 - acc: 0.9996\n",
      "Epoch 00223: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0011 - acc: 0.9995 - val_loss: 0.6114 - val_acc: 0.9132\n",
      "Epoch 224/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 00224: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0013 - acc: 0.9994 - val_loss: 0.6270 - val_acc: 0.9136\n",
      "Epoch 225/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9997   \n",
      "Epoch 00225: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0010 - acc: 0.9997 - val_loss: 0.5672 - val_acc: 0.9172\n",
      "Epoch 226/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 7.0552e-04 - acc: 0.9999\n",
      "Epoch 00226: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 6.9244e-04 - acc: 0.9999 - val_loss: 0.6088 - val_acc: 0.9120\n",
      "Epoch 227/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0010 - acc: 0.9996    \n",
      "Epoch 00227: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 9.9728e-04 - acc: 0.9996 - val_loss: 0.6633 - val_acc: 0.9068\n",
      "Epoch 228/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9994  \n",
      "Epoch 00228: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0014 - acc: 0.9994 - val_loss: 0.6236 - val_acc: 0.9116\n",
      "Epoch 229/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9992\n",
      "Epoch 00229: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0021 - acc: 0.9992 - val_loss: 0.6486 - val_acc: 0.9100\n",
      "Epoch 230/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0021 - acc: 0.9993\n",
      "Epoch 00230: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0020 - acc: 0.9994 - val_loss: 0.6434 - val_acc: 0.9104\n",
      "Epoch 231/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9993\n",
      "Epoch 00231: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0023 - acc: 0.9993 - val_loss: 0.6840 - val_acc: 0.8936\n",
      "Epoch 232/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0019 - acc: 0.9991\n",
      "Epoch 00232: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0018 - acc: 0.9992 - val_loss: 0.6267 - val_acc: 0.9188\n",
      "Epoch 233/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0023 - acc: 0.9991\n",
      "Epoch 00233: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.0022 - acc: 0.9992 - val_loss: 0.5737 - val_acc: 0.9100\n",
      "Epoch 234/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9992\n",
      "Epoch 00234: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0025 - acc: 0.9992 - val_loss: 0.6950 - val_acc: 0.8852\n",
      "Epoch 235/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9993\n",
      "Epoch 00235: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0017 - acc: 0.9994 - val_loss: 0.5515 - val_acc: 0.9120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 00236: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0013 - acc: 0.9995 - val_loss: 0.6037 - val_acc: 0.9116\n",
      "Epoch 237/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0020 - acc: 0.9993\n",
      "Epoch 00237: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.7097 - val_acc: 0.9060\n",
      "Epoch 238/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0024 - acc: 0.9992\n",
      "Epoch 00238: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0022 - acc: 0.9992 - val_loss: 0.6595 - val_acc: 0.9152\n",
      "Epoch 239/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00239: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0018 - acc: 0.9994 - val_loss: 0.5803 - val_acc: 0.9128\n",
      "Epoch 240/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 00240: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0015 - acc: 0.9996 - val_loss: 0.6094 - val_acc: 0.9132\n",
      "Epoch 241/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9996 \n",
      "Epoch 00241: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.6158 - val_acc: 0.9056\n",
      "Epoch 242/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 00242: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6138 - val_acc: 0.9108\n",
      "Epoch 243/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9988\n",
      "Epoch 00243: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0026 - acc: 0.9989 - val_loss: 0.7080 - val_acc: 0.9040\n",
      "Epoch 244/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
      "Epoch 00244: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0028 - acc: 0.9988 - val_loss: 0.7802 - val_acc: 0.8912\n",
      "Epoch 245/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0051 - acc: 0.9982\n",
      "Epoch 00245: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0049 - acc: 0.9982 - val_loss: 0.6616 - val_acc: 0.9000\n",
      "Epoch 246/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0055 - acc: 0.9979\n",
      "Epoch 00246: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0052 - acc: 0.9980 - val_loss: 0.8059 - val_acc: 0.8828\n",
      "Epoch 247/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0064 - acc: 0.9978\n",
      "Epoch 00247: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0065 - acc: 0.9978 - val_loss: 0.6061 - val_acc: 0.9140\n",
      "Epoch 248/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9985\n",
      "Epoch 00248: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0038 - acc: 0.9985 - val_loss: 1.1851 - val_acc: 0.8360\n",
      "Epoch 249/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0041 - acc: 0.9985\n",
      "Epoch 00249: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0040 - acc: 0.9985 - val_loss: 0.7522 - val_acc: 0.8860\n",
      "Epoch 250/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0054 - acc: 0.9983\n",
      "Epoch 00250: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0052 - acc: 0.9984 - val_loss: 0.6260 - val_acc: 0.9140\n",
      "Epoch 251/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9984\n",
      "Epoch 00251: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0049 - acc: 0.9984 - val_loss: 1.0136 - val_acc: 0.8708\n",
      "Epoch 252/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0035 - acc: 0.9990\n",
      "Epoch 00252: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0034 - acc: 0.9989 - val_loss: 0.6190 - val_acc: 0.9092\n",
      "Epoch 253/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9991\n",
      "Epoch 00253: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0032 - acc: 0.9991 - val_loss: 0.6484 - val_acc: 0.9164\n",
      "Epoch 254/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9991\n",
      "Epoch 00254: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0023 - acc: 0.9991 - val_loss: 0.9071 - val_acc: 0.8932\n",
      "Epoch 255/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
      "Epoch 00255: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0033 - acc: 0.9988 - val_loss: 0.6336 - val_acc: 0.9108\n",
      "Epoch 256/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9990\n",
      "Epoch 00256: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0021 - acc: 0.9990 - val_loss: 0.5449 - val_acc: 0.9164\n",
      "Epoch 257/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00257: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0015 - acc: 0.9995 - val_loss: 0.6106 - val_acc: 0.9172\n",
      "Epoch 258/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0017 - acc: 0.9994\n",
      "Epoch 00258: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 0.7158 - val_acc: 0.9068\n",
      "Epoch 259/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0018 - acc: 0.9995   \n",
      "Epoch 00259: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.6267 - val_acc: 0.9132\n",
      "Epoch 260/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00260: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6076 - val_acc: 0.9084\n",
      "Epoch 261/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00261: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.6124 - val_acc: 0.9100\n",
      "Epoch 262/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9996   \n",
      "Epoch 00262: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0013 - acc: 0.9995 - val_loss: 0.6296 - val_acc: 0.9052\n",
      "Epoch 263/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00263: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0020 - acc: 0.9994 - val_loss: 0.5420 - val_acc: 0.9160\n",
      "Epoch 264/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9989\n",
      "Epoch 00264: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0038 - acc: 0.9989 - val_loss: 0.5375 - val_acc: 0.9164\n",
      "Epoch 265/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00265: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0038 - acc: 0.9991 - val_loss: 0.6103 - val_acc: 0.9124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0027 - acc: 0.9989\n",
      "Epoch 00266: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0028 - acc: 0.9989 - val_loss: 1.3299 - val_acc: 0.8532\n",
      "Epoch 267/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 00267: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0017 - acc: 0.9994 - val_loss: 0.8255 - val_acc: 0.9008\n",
      "Epoch 268/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0014 - acc: 0.9995 \n",
      "Epoch 00268: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0015 - acc: 0.9995 - val_loss: 0.6141 - val_acc: 0.9180\n",
      "Epoch 269/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0016 - acc: 0.9994\n",
      "Epoch 00269: val_acc did not improve from 0.92360\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0016 - acc: 0.9994 - val_loss: 0.6900 - val_acc: 0.9168\n",
      "Epoch 270/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 00270: val_acc improved from 0.92360 to 0.92680, saving model to CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_0.93.model\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.5569 - val_acc: 0.9268\n",
      "Epoch 271/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00271: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.5925 - val_acc: 0.9188\n",
      "Epoch 272/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9994\n",
      "Epoch 00272: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0017 - acc: 0.9994 - val_loss: 0.6617 - val_acc: 0.9144\n",
      "Epoch 273/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0020 - acc: 0.9992\n",
      "Epoch 00273: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0020 - acc: 0.9992 - val_loss: 0.6287 - val_acc: 0.9132\n",
      "Epoch 274/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0016 - acc: 0.9993\n",
      "Epoch 00274: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0015 - acc: 0.9994 - val_loss: 0.6408 - val_acc: 0.9104\n",
      "Epoch 275/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0018 - acc: 0.9991\n",
      "Epoch 00275: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0017 - acc: 0.9992 - val_loss: 0.5877 - val_acc: 0.9156\n",
      "Epoch 276/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0018 - acc: 0.9993\n",
      "Epoch 00276: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.5942 - val_acc: 0.9112\n",
      "Epoch 277/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00277: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.5631 - val_acc: 0.9216\n",
      "Epoch 278/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0015 - acc: 0.9995  \n",
      "Epoch 00278: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0015 - acc: 0.9995 - val_loss: 0.6900 - val_acc: 0.9120\n",
      "Epoch 279/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0028 - acc: 0.9992\n",
      "Epoch 00279: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 0.5711 - val_acc: 0.9176\n",
      "Epoch 280/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00280: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0031 - acc: 0.9992 - val_loss: 0.6126 - val_acc: 0.9120\n",
      "Epoch 281/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0019 - acc: 0.9993\n",
      "Epoch 00281: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0017 - acc: 0.9994 - val_loss: 0.6904 - val_acc: 0.9104\n",
      "Epoch 282/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 6.9407e-04 - acc: 0.9998\n",
      "Epoch 00282: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 6.7736e-04 - acc: 0.9998 - val_loss: 0.5817 - val_acc: 0.9172\n",
      "Epoch 283/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 00283: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0015 - acc: 0.9995 - val_loss: 0.6138 - val_acc: 0.9176\n",
      "Epoch 284/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0013 - acc: 0.9994\n",
      "Epoch 00284: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0013 - acc: 0.9995 - val_loss: 0.5804 - val_acc: 0.9144\n",
      "Epoch 285/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 00285: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.5690 - val_acc: 0.9172\n",
      "Epoch 286/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0021 - acc: 0.9993\n",
      "Epoch 00286: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0020 - acc: 0.9993 - val_loss: 0.5047 - val_acc: 0.9160\n",
      "Epoch 287/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0026 - acc: 0.9991\n",
      "Epoch 00287: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.5964 - val_acc: 0.9132\n",
      "Epoch 288/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9991\n",
      "Epoch 00288: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0024 - acc: 0.9992 - val_loss: 0.8518 - val_acc: 0.8976\n",
      "Epoch 289/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 00289: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0012 - acc: 0.9997 - val_loss: 0.9157 - val_acc: 0.8972\n",
      "Epoch 290/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9992\n",
      "Epoch 00290: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0020 - acc: 0.9993 - val_loss: 0.6688 - val_acc: 0.9116\n",
      "Epoch 291/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0012 - acc: 0.9997  \n",
      "Epoch 00291: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.6586 - val_acc: 0.9148\n",
      "Epoch 292/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 9.1759e-04 - acc: 0.9996\n",
      "Epoch 00292: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 8.7013e-04 - acc: 0.9996 - val_loss: 0.6414 - val_acc: 0.9192\n",
      "Epoch 293/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9993\n",
      "Epoch 00293: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0017 - acc: 0.9994 - val_loss: 0.6375 - val_acc: 0.9172\n",
      "Epoch 294/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 00294: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0011 - acc: 0.9997 - val_loss: 0.6057 - val_acc: 0.9104\n",
      "Epoch 295/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 00295: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.8798 - val_acc: 0.8972\n",
      "Epoch 296/300\n",
      "15000/17000 [=========================>....] - ETA: 0s - loss: 0.0043 - acc: 0.9982\n",
      "Epoch 00296: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0043 - acc: 0.9983 - val_loss: 0.7482 - val_acc: 0.9120\n",
      "Epoch 297/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9988\n",
      "Epoch 00297: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.8201 - val_acc: 0.8964\n",
      "Epoch 298/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
      "Epoch 00298: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0028 - acc: 0.9989 - val_loss: 0.7756 - val_acc: 0.9100\n",
      "Epoch 299/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00299: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0011 - acc: 0.9997 - val_loss: 0.8204 - val_acc: 0.9064\n",
      "Epoch 300/300\n",
      "16000/17000 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00300: val_acc did not improve from 0.92680\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0016 - acc: 0.9995 - val_loss: 0.6539 - val_acc: 0.9136\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Model compile\n",
    "model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "filepath=\"CNN_MutiNetwork_PI_PMT_TandC_BIGGER-improvement-val-acc_{val_acc:.2f}.model\" \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "\n",
    "\n",
    "#model Fit\n",
    "\n",
    "#print(X.shape)\n",
    "history = model.fit([XTrainingC,XTrainingT],\n",
    "                    YTraining,\n",
    "                    batch_size=1000,\n",
    "                    #validation_split=0.2,\n",
    "                    \n",
    "                    validation_data=([XValC,XValT],Yval),\n",
    "                    epochs= 300,\n",
    "                    \n",
    "                    shuffle=True,\n",
    "                    class_weight='balanced',\n",
    "                    callbacks=[\n",
    "                                #monitor,\n",
    "                                checkpoint,\n",
    "                                #tensorboard \n",
    "                    ],\n",
    "                    verbose=1)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.2766162261223422\n",
      "Test accuracy:  0.91880554\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "model = tf.keras.models.load_model(\"CNN_MutiNetwork_PI_PMT_TandC-improvement-val-acc_0.92.model\")\n",
    "score = model.evaluate([XTestC,XTestT], YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17000, 10, 16), (4052, 10, 16, 2), (2500, 10, 16, 2))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTraining[:,:,:,0].shape, XTest.shape,XVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,10,16,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,10,16,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,10,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6416 - acc: 0.6194 - val_loss: 0.6116 - val_acc: 0.6476\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5502 - acc: 0.7118 - val_loss: 0.5195 - val_acc: 0.7136\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4385 - acc: 0.8027 - val_loss: 0.3809 - val_acc: 0.8400\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3805 - acc: 0.8348 - val_loss: 0.3446 - val_acc: 0.8576\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3469 - acc: 0.8519 - val_loss: 0.3179 - val_acc: 0.8644\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3201 - acc: 0.8678 - val_loss: 0.2920 - val_acc: 0.8848\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3101 - acc: 0.8715 - val_loss: 0.2792 - val_acc: 0.8892\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2944 - acc: 0.8792 - val_loss: 0.2729 - val_acc: 0.8904\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.2912 - acc: 0.8783 - val_loss: 0.2711 - val_acc: 0.8900\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2775 - acc: 0.8832 - val_loss: 0.2912 - val_acc: 0.8792\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2667 - acc: 0.8894 - val_loss: 0.2614 - val_acc: 0.8952\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2714 - acc: 0.8846 - val_loss: 0.2781 - val_acc: 0.8856\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2589 - acc: 0.8918 - val_loss: 0.2453 - val_acc: 0.9032\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2457 - acc: 0.8982 - val_loss: 0.2614 - val_acc: 0.8976\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2462 - acc: 0.8992 - val_loss: 0.2313 - val_acc: 0.9072\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2421 - acc: 0.8965 - val_loss: 0.2353 - val_acc: 0.9044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2395 - acc: 0.8998 - val_loss: 0.2276 - val_acc: 0.9072\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2315 - acc: 0.9038 - val_loss: 0.2235 - val_acc: 0.9124\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2267 - acc: 0.9041 - val_loss: 0.2255 - val_acc: 0.9064\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.2203 - acc: 0.9076 - val_loss: 0.2217 - val_acc: 0.9076\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2166 - acc: 0.9111 - val_loss: 0.2269 - val_acc: 0.9068\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2132 - acc: 0.9095 - val_loss: 0.2398 - val_acc: 0.9016\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2097 - acc: 0.9128 - val_loss: 0.2150 - val_acc: 0.9120\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2110 - acc: 0.9108 - val_loss: 0.2150 - val_acc: 0.9136\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2083 - acc: 0.9124 - val_loss: 0.2182 - val_acc: 0.9140\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.1983 - acc: 0.9176 - val_loss: 0.2173 - val_acc: 0.9140\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1950 - acc: 0.9197 - val_loss: 0.2111 - val_acc: 0.9124\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1977 - acc: 0.9173 - val_loss: 0.2150 - val_acc: 0.9156\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.1897 - acc: 0.9208 - val_loss: 0.2056 - val_acc: 0.9164\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1859 - acc: 0.9226 - val_loss: 0.2181 - val_acc: 0.9136\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6570 - acc: 0.5957 - val_loss: 0.6369 - val_acc: 0.6172\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.5818 - acc: 0.6862 - val_loss: 0.4646 - val_acc: 0.7776\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4406 - acc: 0.7942 - val_loss: 0.3547 - val_acc: 0.8512\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3560 - acc: 0.8438 - val_loss: 0.3197 - val_acc: 0.8644\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3219 - acc: 0.8620 - val_loss: 0.2726 - val_acc: 0.8952\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3062 - acc: 0.8679 - val_loss: 0.2627 - val_acc: 0.8884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2893 - acc: 0.8768 - val_loss: 0.2734 - val_acc: 0.8912\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2714 - acc: 0.8854 - val_loss: 0.2423 - val_acc: 0.9044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2589 - acc: 0.8935 - val_loss: 0.2373 - val_acc: 0.9032\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2610 - acc: 0.8924 - val_loss: 0.2401 - val_acc: 0.8968\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2455 - acc: 0.8974 - val_loss: 0.2376 - val_acc: 0.8984\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2394 - acc: 0.9014 - val_loss: 0.2326 - val_acc: 0.9012\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2320 - acc: 0.9052 - val_loss: 0.2059 - val_acc: 0.9176\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2330 - acc: 0.9028 - val_loss: 0.2097 - val_acc: 0.9136\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2206 - acc: 0.9070 - val_loss: 0.2284 - val_acc: 0.9040\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2152 - acc: 0.9108 - val_loss: 0.1990 - val_acc: 0.9184\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2149 - acc: 0.9072 - val_loss: 0.2173 - val_acc: 0.9096\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2076 - acc: 0.9125 - val_loss: 0.2021 - val_acc: 0.9144\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2029 - acc: 0.9162 - val_loss: 0.2256 - val_acc: 0.9020\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2096 - acc: 0.9135 - val_loss: 0.1914 - val_acc: 0.9188\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1952 - acc: 0.9190 - val_loss: 0.1892 - val_acc: 0.9232\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2118 - acc: 0.9114 - val_loss: 0.2217 - val_acc: 0.9064\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1863 - acc: 0.9212 - val_loss: 0.1955 - val_acc: 0.9140\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1921 - acc: 0.9198 - val_loss: 0.1903 - val_acc: 0.9236\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1862 - acc: 0.9213 - val_loss: 0.1921 - val_acc: 0.9180\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1865 - acc: 0.9238 - val_loss: 0.1924 - val_acc: 0.9176\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.1876 - acc: 0.9219 - val_loss: 0.1878 - val_acc: 0.9172\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1799 - acc: 0.9259 - val_loss: 0.1885 - val_acc: 0.9164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1753 - acc: 0.9270 - val_loss: 0.1799 - val_acc: 0.9212\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1720 - acc: 0.9285 - val_loss: 0.3119 - val_acc: 0.8708\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6654 - acc: 0.5852 - val_loss: 0.6193 - val_acc: 0.6560\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.5297 - acc: 0.7282 - val_loss: 0.3622 - val_acc: 0.8456\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3809 - acc: 0.8320 - val_loss: 0.3184 - val_acc: 0.8728\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3368 - acc: 0.8558 - val_loss: 0.3321 - val_acc: 0.8560\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3090 - acc: 0.8673 - val_loss: 0.2685 - val_acc: 0.8840\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2852 - acc: 0.8772 - val_loss: 0.2863 - val_acc: 0.8764\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2681 - acc: 0.8859 - val_loss: 0.2557 - val_acc: 0.8912\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2647 - acc: 0.8896 - val_loss: 0.2238 - val_acc: 0.9136\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2608 - acc: 0.8912 - val_loss: 0.2235 - val_acc: 0.9068\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2438 - acc: 0.8984 - val_loss: 0.2279 - val_acc: 0.9096\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2377 - acc: 0.9006 - val_loss: 0.2096 - val_acc: 0.9140\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2332 - acc: 0.9030 - val_loss: 0.2716 - val_acc: 0.8852\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2340 - acc: 0.9024 - val_loss: 0.2168 - val_acc: 0.9064\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2218 - acc: 0.9080 - val_loss: 0.2161 - val_acc: 0.9116\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2208 - acc: 0.9085 - val_loss: 0.2317 - val_acc: 0.9016\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2188 - acc: 0.9070 - val_loss: 0.2036 - val_acc: 0.9204\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2081 - acc: 0.9131 - val_loss: 0.2442 - val_acc: 0.8976\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2109 - acc: 0.9111 - val_loss: 0.2811 - val_acc: 0.8844\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2012 - acc: 0.9156 - val_loss: 0.2291 - val_acc: 0.9016\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1980 - acc: 0.9186 - val_loss: 0.2451 - val_acc: 0.8916\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2031 - acc: 0.9150 - val_loss: 0.2096 - val_acc: 0.9124\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1978 - acc: 0.9214 - val_loss: 0.2086 - val_acc: 0.9080\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1893 - acc: 0.9221 - val_loss: 0.1881 - val_acc: 0.9168\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1997 - acc: 0.9188 - val_loss: 0.2018 - val_acc: 0.9116\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1820 - acc: 0.9233 - val_loss: 0.2217 - val_acc: 0.9112\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1802 - acc: 0.9269 - val_loss: 0.1969 - val_acc: 0.9152\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1923 - acc: 0.9198 - val_loss: 0.2140 - val_acc: 0.9100\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1902 - acc: 0.9207 - val_loss: 0.2120 - val_acc: 0.9068\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1762 - acc: 0.9286 - val_loss: 0.1879 - val_acc: 0.9168\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1846 - acc: 0.9232 - val_loss: 0.2082 - val_acc: 0.9100\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6411 - acc: 0.6201 - val_loss: 0.5993 - val_acc: 0.6624\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5438 - acc: 0.7214 - val_loss: 0.4563 - val_acc: 0.8004\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4344 - acc: 0.7995 - val_loss: 0.3709 - val_acc: 0.8440\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.3707 - acc: 0.8393 - val_loss: 0.3248 - val_acc: 0.8676\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.3458 - acc: 0.8535 - val_loss: 0.3243 - val_acc: 0.8656\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3170 - acc: 0.8680 - val_loss: 0.3015 - val_acc: 0.8820\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3025 - acc: 0.8719 - val_loss: 0.3140 - val_acc: 0.8696\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.3009 - acc: 0.8715 - val_loss: 0.2775 - val_acc: 0.8888\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2825 - acc: 0.8799 - val_loss: 0.2703 - val_acc: 0.8944\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2756 - acc: 0.8865 - val_loss: 0.2688 - val_acc: 0.8928\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2682 - acc: 0.8881 - val_loss: 0.2482 - val_acc: 0.9020\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2580 - acc: 0.8908 - val_loss: 0.2568 - val_acc: 0.8936\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.2547 - acc: 0.8936 - val_loss: 0.2546 - val_acc: 0.8984\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.2462 - acc: 0.8977 - val_loss: 0.2450 - val_acc: 0.8996\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.2407 - acc: 0.8989 - val_loss: 0.2330 - val_acc: 0.9108\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2347 - acc: 0.9020 - val_loss: 0.2302 - val_acc: 0.9056\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2349 - acc: 0.9018 - val_loss: 0.2628 - val_acc: 0.8924\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2305 - acc: 0.9031 - val_loss: 0.2708 - val_acc: 0.8948\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2170 - acc: 0.9100 - val_loss: 0.2219 - val_acc: 0.9040\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2179 - acc: 0.9087 - val_loss: 0.2909 - val_acc: 0.8816\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2105 - acc: 0.9132 - val_loss: 0.2209 - val_acc: 0.9144\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2075 - acc: 0.9132 - val_loss: 0.2159 - val_acc: 0.9128\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2031 - acc: 0.9159 - val_loss: 0.2122 - val_acc: 0.9136\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.1986 - acc: 0.9155 - val_loss: 0.2390 - val_acc: 0.8968\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.1918 - acc: 0.9176 - val_loss: 0.2124 - val_acc: 0.9144\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.1899 - acc: 0.9209 - val_loss: 0.2079 - val_acc: 0.9104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.1861 - acc: 0.9235 - val_loss: 0.2058 - val_acc: 0.9128\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.1848 - acc: 0.9231 - val_loss: 0.2118 - val_acc: 0.9116\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.1771 - acc: 0.9284 - val_loss: 0.2050 - val_acc: 0.9132\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.1747 - acc: 0.9276 - val_loss: 0.2302 - val_acc: 0.9048\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6510 - acc: 0.6079 - val_loss: 0.5970 - val_acc: 0.6692\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.5189 - acc: 0.7370 - val_loss: 0.4018 - val_acc: 0.8220\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3811 - acc: 0.8308 - val_loss: 0.3931 - val_acc: 0.8268\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3433 - acc: 0.8534 - val_loss: 0.2812 - val_acc: 0.8864\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3128 - acc: 0.8678 - val_loss: 0.2734 - val_acc: 0.8920\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2932 - acc: 0.8749 - val_loss: 0.2699 - val_acc: 0.8868\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2704 - acc: 0.8878 - val_loss: 0.2719 - val_acc: 0.8860\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2684 - acc: 0.8868 - val_loss: 0.2396 - val_acc: 0.8992\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2540 - acc: 0.8944 - val_loss: 0.2169 - val_acc: 0.9124\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2447 - acc: 0.8985 - val_loss: 0.2801 - val_acc: 0.8784\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2364 - acc: 0.8986 - val_loss: 0.2204 - val_acc: 0.9100\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2263 - acc: 0.9065 - val_loss: 0.2056 - val_acc: 0.9128\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2262 - acc: 0.9054 - val_loss: 0.2175 - val_acc: 0.9064\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2182 - acc: 0.9098 - val_loss: 0.2943 - val_acc: 0.8712\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2167 - acc: 0.9095 - val_loss: 0.2050 - val_acc: 0.9156\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2064 - acc: 0.9153 - val_loss: 0.2104 - val_acc: 0.9132\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2092 - acc: 0.9114 - val_loss: 0.1999 - val_acc: 0.9144\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2161 - acc: 0.9101 - val_loss: 0.1952 - val_acc: 0.9184\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1875 - acc: 0.9238 - val_loss: 0.1905 - val_acc: 0.9212\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1899 - acc: 0.9227 - val_loss: 0.1857 - val_acc: 0.9200\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1897 - acc: 0.9216 - val_loss: 0.2021 - val_acc: 0.9144\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1825 - acc: 0.9260 - val_loss: 0.2123 - val_acc: 0.9072\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1812 - acc: 0.9246 - val_loss: 0.1777 - val_acc: 0.9276\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1836 - acc: 0.9244 - val_loss: 0.1964 - val_acc: 0.9184\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1737 - acc: 0.9259 - val_loss: 0.1921 - val_acc: 0.9148\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1765 - acc: 0.9277 - val_loss: 0.2016 - val_acc: 0.9140\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1803 - acc: 0.9267 - val_loss: 0.1923 - val_acc: 0.9148\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1654 - acc: 0.9320 - val_loss: 0.1826 - val_acc: 0.9272\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1628 - acc: 0.9333 - val_loss: 0.1796 - val_acc: 0.9240\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1650 - acc: 0.9314 - val_loss: 0.1764 - val_acc: 0.9232\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6554 - acc: 0.5986 - val_loss: 0.5651 - val_acc: 0.6956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4932 - acc: 0.7567 - val_loss: 0.3549 - val_acc: 0.8528\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3694 - acc: 0.8354 - val_loss: 0.2826 - val_acc: 0.8804\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3249 - acc: 0.8618 - val_loss: 0.2630 - val_acc: 0.8952\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2901 - acc: 0.8767 - val_loss: 0.2512 - val_acc: 0.8908\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2809 - acc: 0.8803 - val_loss: 0.2464 - val_acc: 0.8968\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2655 - acc: 0.8889 - val_loss: 0.2820 - val_acc: 0.8764\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2669 - acc: 0.8872 - val_loss: 0.2786 - val_acc: 0.8836\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2511 - acc: 0.8912 - val_loss: 0.2183 - val_acc: 0.9116\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2521 - acc: 0.8944 - val_loss: 0.2418 - val_acc: 0.8992\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2304 - acc: 0.9055 - val_loss: 0.2952 - val_acc: 0.8692\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2325 - acc: 0.9018 - val_loss: 0.2038 - val_acc: 0.9116\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2208 - acc: 0.9087 - val_loss: 0.2212 - val_acc: 0.9036\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2203 - acc: 0.9059 - val_loss: 0.1987 - val_acc: 0.9160\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2176 - acc: 0.9068 - val_loss: 0.2396 - val_acc: 0.8964\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2159 - acc: 0.9110 - val_loss: 0.2523 - val_acc: 0.8896\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2095 - acc: 0.9122 - val_loss: 0.2111 - val_acc: 0.9080\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1997 - acc: 0.9183 - val_loss: 0.2050 - val_acc: 0.9132\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1962 - acc: 0.9166 - val_loss: 0.2082 - val_acc: 0.9108\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1920 - acc: 0.9214 - val_loss: 0.1876 - val_acc: 0.9168\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1942 - acc: 0.9211 - val_loss: 0.1953 - val_acc: 0.9124\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.1934 - acc: 0.919 - 1s 55us/sample - loss: 0.1929 - acc: 0.9201 - val_loss: 0.2301 - val_acc: 0.8988\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1803 - acc: 0.9251 - val_loss: 0.2116 - val_acc: 0.9116\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1809 - acc: 0.9249 - val_loss: 0.2131 - val_acc: 0.9068\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1803 - acc: 0.9265 - val_loss: 0.1933 - val_acc: 0.9184\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1797 - acc: 0.9251 - val_loss: 0.1847 - val_acc: 0.9268\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1720 - acc: 0.9304 - val_loss: 0.2131 - val_acc: 0.9080\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1785 - acc: 0.9274 - val_loss: 0.2501 - val_acc: 0.8872\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1721 - acc: 0.9288 - val_loss: 0.1980 - val_acc: 0.9168\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1660 - acc: 0.9305 - val_loss: 0.1938 - val_acc: 0.9192\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6453 - acc: 0.6174 - val_loss: 0.6012 - val_acc: 0.6680\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.5477 - acc: 0.7162 - val_loss: 0.5073 - val_acc: 0.7492\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4316 - acc: 0.8066 - val_loss: 0.3657 - val_acc: 0.8460\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3626 - acc: 0.8465 - val_loss: 0.3128 - val_acc: 0.8744\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3326 - acc: 0.8618 - val_loss: 0.3448 - val_acc: 0.8572\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3100 - acc: 0.8704 - val_loss: 0.2775 - val_acc: 0.8868\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.2936 - acc: 0.8788 - val_loss: 0.2649 - val_acc: 0.8928\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2886 - acc: 0.8789 - val_loss: 0.3108 - val_acc: 0.8664\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2696 - acc: 0.8886 - val_loss: 0.2487 - val_acc: 0.9008\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2588 - acc: 0.8939 - val_loss: 0.2446 - val_acc: 0.9056\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2662 - acc: 0.8864 - val_loss: 0.2466 - val_acc: 0.8968\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2513 - acc: 0.8945 - val_loss: 0.2430 - val_acc: 0.8952\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2475 - acc: 0.8951 - val_loss: 0.2323 - val_acc: 0.9072\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2342 - acc: 0.9026 - val_loss: 0.2767 - val_acc: 0.8836\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.2372 - acc: 0.8994 - val_loss: 0.2225 - val_acc: 0.9116\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.2277 - acc: 0.9057 - val_loss: 0.2222 - val_acc: 0.9116\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2238 - acc: 0.9068 - val_loss: 0.2163 - val_acc: 0.9136\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.2175 - acc: 0.9091 - val_loss: 0.2103 - val_acc: 0.9144\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2128 - acc: 0.9108 - val_loss: 0.2064 - val_acc: 0.9144\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2061 - acc: 0.9143 - val_loss: 0.2198 - val_acc: 0.9100\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2027 - acc: 0.9164 - val_loss: 0.2639 - val_acc: 0.8956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2069 - acc: 0.9145 - val_loss: 0.2070 - val_acc: 0.9128\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.2069 - acc: 0.9123 - val_loss: 0.2215 - val_acc: 0.9088\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.1942 - acc: 0.9201 - val_loss: 0.2176 - val_acc: 0.9136\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.1903 - acc: 0.9216 - val_loss: 0.2072 - val_acc: 0.9152\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.1931 - acc: 0.9191 - val_loss: 0.2036 - val_acc: 0.9156\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.1799 - acc: 0.9264 - val_loss: 0.2019 - val_acc: 0.9192\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.1803 - acc: 0.9254 - val_loss: 0.2068 - val_acc: 0.9172\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.1811 - acc: 0.9235 - val_loss: 0.2027 - val_acc: 0.9168\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.1793 - acc: 0.9227 - val_loss: 0.2138 - val_acc: 0.9140\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6463 - acc: 0.6125 - val_loss: 0.6119 - val_acc: 0.6488\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4975 - acc: 0.7538 - val_loss: 0.3668 - val_acc: 0.8516\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3662 - acc: 0.8395 - val_loss: 0.3237 - val_acc: 0.8620\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3287 - acc: 0.8585 - val_loss: 0.3549 - val_acc: 0.8424\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3003 - acc: 0.8731 - val_loss: 0.2911 - val_acc: 0.8824\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2863 - acc: 0.8792 - val_loss: 0.2545 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2777 - acc: 0.8823 - val_loss: 0.2310 - val_acc: 0.9056\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2579 - acc: 0.8898 - val_loss: 0.2240 - val_acc: 0.9084\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2404 - acc: 0.9005 - val_loss: 0.3319 - val_acc: 0.8540\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2374 - acc: 0.8998 - val_loss: 0.2150 - val_acc: 0.9108\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2270 - acc: 0.9064 - val_loss: 0.2116 - val_acc: 0.9080\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2229 - acc: 0.9057 - val_loss: 0.2096 - val_acc: 0.9136\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2107 - acc: 0.9141 - val_loss: 0.2631 - val_acc: 0.8808\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2119 - acc: 0.9114 - val_loss: 0.2052 - val_acc: 0.9132\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1957 - acc: 0.9192 - val_loss: 0.2094 - val_acc: 0.9108\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2001 - acc: 0.9159 - val_loss: 0.2482 - val_acc: 0.8952\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1992 - acc: 0.9174 - val_loss: 0.2150 - val_acc: 0.9108\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1915 - acc: 0.9211 - val_loss: 0.2052 - val_acc: 0.9156\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1890 - acc: 0.9209 - val_loss: 0.2100 - val_acc: 0.9096\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1879 - acc: 0.9214 - val_loss: 0.2067 - val_acc: 0.9116\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1734 - acc: 0.9292 - val_loss: 0.1944 - val_acc: 0.9196\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1822 - acc: 0.9261 - val_loss: 0.1976 - val_acc: 0.9172\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1742 - acc: 0.9306 - val_loss: 0.1850 - val_acc: 0.9192\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1736 - acc: 0.9282 - val_loss: 0.1877 - val_acc: 0.9240\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1735 - acc: 0.9287 - val_loss: 0.1853 - val_acc: 0.9220\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1699 - acc: 0.9309 - val_loss: 0.2025 - val_acc: 0.9132\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1634 - acc: 0.9355 - val_loss: 0.1893 - val_acc: 0.9180\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1574 - acc: 0.9355 - val_loss: 0.1932 - val_acc: 0.9140\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1565 - acc: 0.9344 - val_loss: 0.1889 - val_acc: 0.9188\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1529 - acc: 0.9376 - val_loss: 0.2052 - val_acc: 0.9124\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6641 - acc: 0.5814 - val_loss: 0.6251 - val_acc: 0.6452\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.5092 - acc: 0.7425 - val_loss: 0.3675 - val_acc: 0.8452\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3698 - acc: 0.8378 - val_loss: 0.3403 - val_acc: 0.8544\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3225 - acc: 0.8614 - val_loss: 0.2556 - val_acc: 0.8972\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2995 - acc: 0.8710 - val_loss: 0.2557 - val_acc: 0.8956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2759 - acc: 0.8838 - val_loss: 0.2297 - val_acc: 0.9060\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2603 - acc: 0.8911 - val_loss: 0.2254 - val_acc: 0.9084\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2522 - acc: 0.8934 - val_loss: 0.2349 - val_acc: 0.9052\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2386 - acc: 0.9009 - val_loss: 0.2157 - val_acc: 0.9132\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2423 - acc: 0.8964 - val_loss: 0.2293 - val_acc: 0.9020\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2203 - acc: 0.9061 - val_loss: 0.2105 - val_acc: 0.9160\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2159 - acc: 0.9101 - val_loss: 0.2287 - val_acc: 0.9044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2199 - acc: 0.9071 - val_loss: 0.2159 - val_acc: 0.9084\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2023 - acc: 0.9135 - val_loss: 0.2041 - val_acc: 0.9096\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2045 - acc: 0.9161 - val_loss: 0.1941 - val_acc: 0.9172\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2059 - acc: 0.9146 - val_loss: 0.2082 - val_acc: 0.9116\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1998 - acc: 0.9169 - val_loss: 0.1922 - val_acc: 0.9224\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1900 - acc: 0.9217 - val_loss: 0.1955 - val_acc: 0.9208\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1833 - acc: 0.9259 - val_loss: 0.2447 - val_acc: 0.8928\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1894 - acc: 0.9206 - val_loss: 0.2013 - val_acc: 0.9136\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1833 - acc: 0.9233 - val_loss: 0.1927 - val_acc: 0.9160\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1762 - acc: 0.9276 - val_loss: 0.2411 - val_acc: 0.9012\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1748 - acc: 0.9289 - val_loss: 0.2664 - val_acc: 0.8820\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1740 - acc: 0.9287 - val_loss: 0.2050 - val_acc: 0.9100\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1623 - acc: 0.9344 - val_loss: 0.2413 - val_acc: 0.9004\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1607 - acc: 0.9344 - val_loss: 0.2151 - val_acc: 0.9104\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1614 - acc: 0.9344 - val_loss: 0.2194 - val_acc: 0.9056\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1617 - acc: 0.9355 - val_loss: 0.1868 - val_acc: 0.9256\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1550 - acc: 0.9349 - val_loss: 0.1994 - val_acc: 0.9132\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1561 - acc: 0.9362 - val_loss: 0.2158 - val_acc: 0.9076\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6439 - acc: 0.6153 - val_loss: 0.5949 - val_acc: 0.6680\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5416 - acc: 0.7246 - val_loss: 0.4426 - val_acc: 0.8104\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.4134 - acc: 0.8164 - val_loss: 0.3589 - val_acc: 0.8456\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.3562 - acc: 0.8478 - val_loss: 0.3123 - val_acc: 0.8768\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3228 - acc: 0.8622 - val_loss: 0.3021 - val_acc: 0.8760\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3046 - acc: 0.8739 - val_loss: 0.2826 - val_acc: 0.8868\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2833 - acc: 0.8851 - val_loss: 0.2678 - val_acc: 0.8980\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2770 - acc: 0.8854 - val_loss: 0.2660 - val_acc: 0.8968\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2695 - acc: 0.8886 - val_loss: 0.2526 - val_acc: 0.9032\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2563 - acc: 0.8971 - val_loss: 0.3082 - val_acc: 0.8776\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2545 - acc: 0.8930 - val_loss: 0.2491 - val_acc: 0.9004\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2473 - acc: 0.8964 - val_loss: 0.2453 - val_acc: 0.9052\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2366 - acc: 0.9051 - val_loss: 0.2585 - val_acc: 0.8968\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2238 - acc: 0.9074 - val_loss: 0.2425 - val_acc: 0.8976\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2228 - acc: 0.9086 - val_loss: 0.2554 - val_acc: 0.8896\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2147 - acc: 0.9094 - val_loss: 0.2208 - val_acc: 0.9128\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2313 - acc: 0.9022 - val_loss: 0.2289 - val_acc: 0.9060\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2121 - acc: 0.9111 - val_loss: 0.2402 - val_acc: 0.8980\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2036 - acc: 0.9155 - val_loss: 0.2203 - val_acc: 0.9100\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2004 - acc: 0.9143 - val_loss: 0.2232 - val_acc: 0.9084\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1937 - acc: 0.9198 - val_loss: 0.2624 - val_acc: 0.8892\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1886 - acc: 0.9207 - val_loss: 0.2158 - val_acc: 0.9128\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1911 - acc: 0.9203 - val_loss: 0.2072 - val_acc: 0.9132\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1797 - acc: 0.9247 - val_loss: 0.2141 - val_acc: 0.9160\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1782 - acc: 0.9243 - val_loss: 0.2561 - val_acc: 0.8936\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.1729 - acc: 0.9276 - val_loss: 0.2121 - val_acc: 0.9104\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1822 - acc: 0.9255 - val_loss: 0.2051 - val_acc: 0.9188\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1723 - acc: 0.9275 - val_loss: 0.2150 - val_acc: 0.9128\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1653 - acc: 0.9334 - val_loss: 0.2158 - val_acc: 0.9148\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1626 - acc: 0.9339 - val_loss: 0.2175 - val_acc: 0.9120\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6468 - acc: 0.6058 - val_loss: 0.5757 - val_acc: 0.6904\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4734 - acc: 0.7702 - val_loss: 0.4519 - val_acc: 0.7888\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3648 - acc: 0.8423 - val_loss: 0.3214 - val_acc: 0.8624\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3132 - acc: 0.8654 - val_loss: 0.2586 - val_acc: 0.8984\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2782 - acc: 0.8816 - val_loss: 0.2399 - val_acc: 0.9040\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2736 - acc: 0.8839 - val_loss: 0.2642 - val_acc: 0.8932\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2456 - acc: 0.8996 - val_loss: 0.2142 - val_acc: 0.9132\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2379 - acc: 0.9009 - val_loss: 0.2348 - val_acc: 0.9060\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2262 - acc: 0.9061 - val_loss: 0.2320 - val_acc: 0.9072\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2190 - acc: 0.9111 - val_loss: 0.2280 - val_acc: 0.9052\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2093 - acc: 0.9148 - val_loss: 0.1933 - val_acc: 0.9212\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2052 - acc: 0.9142 - val_loss: 0.1970 - val_acc: 0.9148\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2021 - acc: 0.9179 - val_loss: 0.2147 - val_acc: 0.9108\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1938 - acc: 0.9199 - val_loss: 0.2713 - val_acc: 0.8844\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1792 - acc: 0.9266 - val_loss: 0.1931 - val_acc: 0.9144\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1799 - acc: 0.9257 - val_loss: 0.1968 - val_acc: 0.9172\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1776 - acc: 0.9255 - val_loss: 0.2027 - val_acc: 0.9156\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1699 - acc: 0.9314 - val_loss: 0.1879 - val_acc: 0.9196\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1748 - acc: 0.9280 - val_loss: 0.2460 - val_acc: 0.8940\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1611 - acc: 0.9321 - val_loss: 0.2066 - val_acc: 0.9124\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1705 - acc: 0.9301 - val_loss: 0.2391 - val_acc: 0.8984\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1492 - acc: 0.9399 - val_loss: 0.1894 - val_acc: 0.9188\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1530 - acc: 0.9380 - val_loss: 0.1931 - val_acc: 0.9176\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1447 - acc: 0.9415 - val_loss: 0.1956 - val_acc: 0.9164\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1446 - acc: 0.9396 - val_loss: 0.1940 - val_acc: 0.9184\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1465 - acc: 0.9396 - val_loss: 0.2371 - val_acc: 0.9016\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1386 - acc: 0.9445 - val_loss: 0.1871 - val_acc: 0.9228\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1389 - acc: 0.9428 - val_loss: 0.1979 - val_acc: 0.9156\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1388 - acc: 0.9437 - val_loss: 0.2232 - val_acc: 0.9044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1299 - acc: 0.9475 - val_loss: 0.1884 - val_acc: 0.9244\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6371 - acc: 0.6206 - val_loss: 0.5121 - val_acc: 0.7464\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.4144 - acc: 0.8066 - val_loss: 0.3181 - val_acc: 0.8664\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3346 - acc: 0.8534 - val_loss: 0.2607 - val_acc: 0.8944\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2972 - acc: 0.8733 - val_loss: 0.2966 - val_acc: 0.8772\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2780 - acc: 0.8808 - val_loss: 0.3603 - val_acc: 0.8352\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2653 - acc: 0.8876 - val_loss: 0.3279 - val_acc: 0.8528\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2521 - acc: 0.8921 - val_loss: 0.2182 - val_acc: 0.9104\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2411 - acc: 0.8977 - val_loss: 0.2531 - val_acc: 0.8884\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2325 - acc: 0.9043 - val_loss: 0.2053 - val_acc: 0.9120\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2190 - acc: 0.9086 - val_loss: 0.2227 - val_acc: 0.9080\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2136 - acc: 0.9099 - val_loss: 0.2040 - val_acc: 0.9132\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2033 - acc: 0.9162 - val_loss: 0.1990 - val_acc: 0.9168\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2136 - acc: 0.9103 - val_loss: 0.2868 - val_acc: 0.8744\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1960 - acc: 0.9162 - val_loss: 0.2050 - val_acc: 0.9116\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1832 - acc: 0.9254 - val_loss: 0.2130 - val_acc: 0.9132\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1834 - acc: 0.9265 - val_loss: 0.2146 - val_acc: 0.9072\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1832 - acc: 0.9246 - val_loss: 0.1911 - val_acc: 0.9200\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1766 - acc: 0.9281 - val_loss: 0.1866 - val_acc: 0.9240\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1707 - acc: 0.9296 - val_loss: 0.2657 - val_acc: 0.8908\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1761 - acc: 0.9265 - val_loss: 0.2080 - val_acc: 0.9048\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.1705 - acc: 0.9289 - val_loss: 0.2256 - val_acc: 0.9032\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1599 - acc: 0.9362 - val_loss: 0.1843 - val_acc: 0.9204\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1630 - acc: 0.9315 - val_loss: 0.1991 - val_acc: 0.9148\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1528 - acc: 0.9384 - val_loss: 0.1888 - val_acc: 0.9184\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1592 - acc: 0.9343 - val_loss: 0.2062 - val_acc: 0.9076\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1477 - acc: 0.9399 - val_loss: 0.1982 - val_acc: 0.9128\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1410 - acc: 0.9412 - val_loss: 0.1902 - val_acc: 0.9208\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1505 - acc: 0.9379 - val_loss: 0.2061 - val_acc: 0.9188\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1406 - acc: 0.9439 - val_loss: 0.1873 - val_acc: 0.9208\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1374 - acc: 0.9451 - val_loss: 0.1906 - val_acc: 0.9176\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6346 - acc: 0.6261 - val_loss: 0.5777 - val_acc: 0.6808\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4781 - acc: 0.7685 - val_loss: 0.3576 - val_acc: 0.8476\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3732 - acc: 0.8352 - val_loss: 0.3201 - val_acc: 0.8696\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3183 - acc: 0.8638 - val_loss: 0.2895 - val_acc: 0.8844\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2976 - acc: 0.8711 - val_loss: 0.3416 - val_acc: 0.8412\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2881 - acc: 0.8751 - val_loss: 0.2454 - val_acc: 0.8964\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2545 - acc: 0.8905 - val_loss: 0.2719 - val_acc: 0.8776\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2446 - acc: 0.8951 - val_loss: 0.2378 - val_acc: 0.8968\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2352 - acc: 0.9021 - val_loss: 0.2588 - val_acc: 0.8908\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2272 - acc: 0.9045 - val_loss: 0.2203 - val_acc: 0.9108\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.2098 - acc: 0.9114 - val_loss: 0.2234 - val_acc: 0.9100\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.2015 - acc: 0.9156 - val_loss: 0.2883 - val_acc: 0.8716\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1941 - acc: 0.9204 - val_loss: 0.2470 - val_acc: 0.8920\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.1797 - acc: 0.9258 - val_loss: 0.2556 - val_acc: 0.8936\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1799 - acc: 0.9235 - val_loss: 0.2080 - val_acc: 0.9092\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1682 - acc: 0.9311 - val_loss: 0.2331 - val_acc: 0.9064\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1523 - acc: 0.9372 - val_loss: 0.2063 - val_acc: 0.9132\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1487 - acc: 0.9405 - val_loss: 0.2008 - val_acc: 0.9196\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1451 - acc: 0.9401 - val_loss: 0.2112 - val_acc: 0.9140\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1332 - acc: 0.9455 - val_loss: 0.2285 - val_acc: 0.9076\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1324 - acc: 0.9479 - val_loss: 0.2194 - val_acc: 0.9112\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.1198 - acc: 0.9511 - val_loss: 0.2220 - val_acc: 0.9100\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1242 - acc: 0.9497 - val_loss: 0.2150 - val_acc: 0.9164\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1129 - acc: 0.9551 - val_loss: 0.3030 - val_acc: 0.8876\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.1157 - acc: 0.9553 - val_loss: 0.2138 - val_acc: 0.9140\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1028 - acc: 0.9571 - val_loss: 0.2303 - val_acc: 0.9140\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.1051 - acc: 0.9575 - val_loss: 0.2408 - val_acc: 0.9112\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.1034 - acc: 0.9584 - val_loss: 0.2286 - val_acc: 0.9164\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.0937 - acc: 0.9635 - val_loss: 0.2568 - val_acc: 0.9124\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.0918 - acc: 0.9638 - val_loss: 0.2418 - val_acc: 0.9116\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6541 - acc: 0.6034 - val_loss: 0.6081 - val_acc: 0.6624\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.5227 - acc: 0.7362 - val_loss: 0.4504 - val_acc: 0.7896\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3784 - acc: 0.8309 - val_loss: 0.3049 - val_acc: 0.8768\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3331 - acc: 0.8506 - val_loss: 0.2608 - val_acc: 0.8972\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3074 - acc: 0.8662 - val_loss: 0.2762 - val_acc: 0.8908\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2912 - acc: 0.8738 - val_loss: 0.2466 - val_acc: 0.9056\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2697 - acc: 0.8859 - val_loss: 0.2462 - val_acc: 0.8984\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2585 - acc: 0.8906 - val_loss: 0.2281 - val_acc: 0.9064\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2571 - acc: 0.8912 - val_loss: 0.2618 - val_acc: 0.8868\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2517 - acc: 0.8911 - val_loss: 0.2460 - val_acc: 0.8916\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2381 - acc: 0.9006 - val_loss: 0.2086 - val_acc: 0.9136\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2304 - acc: 0.9034 - val_loss: 0.2126 - val_acc: 0.9152\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2213 - acc: 0.9068 - val_loss: 0.2142 - val_acc: 0.9104\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2156 - acc: 0.9099 - val_loss: 0.2849 - val_acc: 0.8708\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2185 - acc: 0.9088 - val_loss: 0.2147 - val_acc: 0.9152\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2082 - acc: 0.9129 - val_loss: 0.1972 - val_acc: 0.9180\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2096 - acc: 0.9116 - val_loss: 0.2073 - val_acc: 0.9088\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2006 - acc: 0.9174 - val_loss: 0.1915 - val_acc: 0.9148\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2065 - acc: 0.9145 - val_loss: 0.2277 - val_acc: 0.9012\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2016 - acc: 0.9152 - val_loss: 0.2608 - val_acc: 0.8832\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1876 - acc: 0.9219 - val_loss: 0.2561 - val_acc: 0.8884\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1891 - acc: 0.9219 - val_loss: 0.1914 - val_acc: 0.9196\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1876 - acc: 0.9222 - val_loss: 0.2891 - val_acc: 0.8732\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1814 - acc: 0.9229 - val_loss: 0.2150 - val_acc: 0.9080\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1785 - acc: 0.9255 - val_loss: 0.2195 - val_acc: 0.9060\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1718 - acc: 0.9298 - val_loss: 0.1907 - val_acc: 0.9240\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1830 - acc: 0.9231 - val_loss: 0.1868 - val_acc: 0.9204\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1676 - acc: 0.9321 - val_loss: 0.1884 - val_acc: 0.9188\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1663 - acc: 0.9316 - val_loss: 0.1943 - val_acc: 0.9172\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1600 - acc: 0.9316 - val_loss: 0.1871 - val_acc: 0.9204\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6635 - acc: 0.5839 - val_loss: 0.6326 - val_acc: 0.6428\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.5452 - acc: 0.7161 - val_loss: 0.3881 - val_acc: 0.8292\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3901 - acc: 0.8227 - val_loss: 0.3404 - val_acc: 0.8504\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3254 - acc: 0.8566 - val_loss: 0.2764 - val_acc: 0.8780\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3086 - acc: 0.8672 - val_loss: 0.3115 - val_acc: 0.8604\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2909 - acc: 0.8754 - val_loss: 0.2455 - val_acc: 0.8988\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2666 - acc: 0.8892 - val_loss: 0.2549 - val_acc: 0.8948\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2606 - acc: 0.8931 - val_loss: 0.2231 - val_acc: 0.9064\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2611 - acc: 0.8886 - val_loss: 0.3308 - val_acc: 0.8572\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2574 - acc: 0.8909 - val_loss: 0.2227 - val_acc: 0.9072\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2388 - acc: 0.8984 - val_loss: 0.2614 - val_acc: 0.8892\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2310 - acc: 0.9025 - val_loss: 0.2537 - val_acc: 0.8944\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2304 - acc: 0.9032 - val_loss: 0.2412 - val_acc: 0.8968\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2310 - acc: 0.9043 - val_loss: 0.2014 - val_acc: 0.9192\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2149 - acc: 0.9104 - val_loss: 0.2247 - val_acc: 0.9112\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2157 - acc: 0.9076 - val_loss: 0.2500 - val_acc: 0.9008\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2123 - acc: 0.9116 - val_loss: 0.2058 - val_acc: 0.9120\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2028 - acc: 0.9165 - val_loss: 0.2470 - val_acc: 0.8932\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2092 - acc: 0.9114 - val_loss: 0.1941 - val_acc: 0.9180\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1981 - acc: 0.9189 - val_loss: 0.1981 - val_acc: 0.9216\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1962 - acc: 0.9185 - val_loss: 0.1964 - val_acc: 0.9184\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1986 - acc: 0.9185 - val_loss: 0.2695 - val_acc: 0.8820\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1963 - acc: 0.9192 - val_loss: 0.2023 - val_acc: 0.9112\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1971 - acc: 0.9169 - val_loss: 0.1880 - val_acc: 0.9188\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1858 - acc: 0.9228 - val_loss: 0.2319 - val_acc: 0.9064\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1891 - acc: 0.9204 - val_loss: 0.2799 - val_acc: 0.8876\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1829 - acc: 0.9249 - val_loss: 0.2726 - val_acc: 0.8832\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1876 - acc: 0.9202 - val_loss: 0.2053 - val_acc: 0.9112\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1820 - acc: 0.9234 - val_loss: 0.2607 - val_acc: 0.8944\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1765 - acc: 0.9295 - val_loss: 0.2574 - val_acc: 0.9016\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6291 - acc: 0.6325 - val_loss: 0.5343 - val_acc: 0.7300\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4467 - acc: 0.7869 - val_loss: 0.3443 - val_acc: 0.8556\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3597 - acc: 0.8420 - val_loss: 0.3117 - val_acc: 0.8636\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2972 - acc: 0.8742 - val_loss: 0.2746 - val_acc: 0.8852\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2796 - acc: 0.8808 - val_loss: 0.2530 - val_acc: 0.8944\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2628 - acc: 0.8885 - val_loss: 0.2509 - val_acc: 0.8972\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2402 - acc: 0.8985 - val_loss: 0.2594 - val_acc: 0.8840\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2328 - acc: 0.8989 - val_loss: 0.2377 - val_acc: 0.8976\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2203 - acc: 0.9072 - val_loss: 0.2229 - val_acc: 0.9072\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2140 - acc: 0.9091 - val_loss: 0.2236 - val_acc: 0.9076\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1905 - acc: 0.9216 - val_loss: 0.2127 - val_acc: 0.9060\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1866 - acc: 0.9227 - val_loss: 0.2369 - val_acc: 0.9016\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1776 - acc: 0.9269 - val_loss: 0.2381 - val_acc: 0.8980\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.1597 - acc: 0.9352 - val_loss: 0.2249 - val_acc: 0.9052\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1640 - acc: 0.9312 - val_loss: 0.2247 - val_acc: 0.9108\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1487 - acc: 0.9399 - val_loss: 0.2281 - val_acc: 0.9088\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1419 - acc: 0.9427 - val_loss: 0.2180 - val_acc: 0.9132\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1399 - acc: 0.9430 - val_loss: 0.2381 - val_acc: 0.9084\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1306 - acc: 0.9454 - val_loss: 0.2288 - val_acc: 0.9116\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1315 - acc: 0.9471 - val_loss: 0.2568 - val_acc: 0.8964\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1314 - acc: 0.9488 - val_loss: 0.2657 - val_acc: 0.8936\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1140 - acc: 0.9541 - val_loss: 0.2672 - val_acc: 0.9032\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1048 - acc: 0.9585 - val_loss: 0.2442 - val_acc: 0.9160\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.1035 - acc: 0.9578 - val_loss: 0.2773 - val_acc: 0.8984\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.0920 - acc: 0.9640 - val_loss: 0.2480 - val_acc: 0.9076\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.0969 - acc: 0.9605 - val_loss: 0.2537 - val_acc: 0.9084\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.0872 - acc: 0.9655 - val_loss: 0.2651 - val_acc: 0.9056\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.0923 - acc: 0.9635 - val_loss: 0.2570 - val_acc: 0.9140\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.0820 - acc: 0.9692 - val_loss: 0.2397 - val_acc: 0.9164\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.0830 - acc: 0.9672 - val_loss: 0.2910 - val_acc: 0.9040\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6579 - acc: 0.5956 - val_loss: 0.5958 - val_acc: 0.6776\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.5045 - acc: 0.7478 - val_loss: 0.3280 - val_acc: 0.8708\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3654 - acc: 0.8384 - val_loss: 0.2948 - val_acc: 0.8784\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3196 - acc: 0.8605 - val_loss: 0.2853 - val_acc: 0.8836\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2903 - acc: 0.8744 - val_loss: 0.2567 - val_acc: 0.8940\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2883 - acc: 0.8756 - val_loss: 0.2693 - val_acc: 0.8844\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2572 - acc: 0.8906 - val_loss: 0.2230 - val_acc: 0.9072\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2450 - acc: 0.8961 - val_loss: 0.2156 - val_acc: 0.9104\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2378 - acc: 0.8984 - val_loss: 0.2138 - val_acc: 0.9104\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2282 - acc: 0.9047 - val_loss: 0.2088 - val_acc: 0.9100\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2215 - acc: 0.9090 - val_loss: 0.1955 - val_acc: 0.9188\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2159 - acc: 0.9082 - val_loss: 0.3202 - val_acc: 0.8624\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2121 - acc: 0.9129 - val_loss: 0.1953 - val_acc: 0.9204\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2081 - acc: 0.9144 - val_loss: 0.2112 - val_acc: 0.9120\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1974 - acc: 0.9175 - val_loss: 0.2471 - val_acc: 0.8944\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2026 - acc: 0.9141 - val_loss: 0.1907 - val_acc: 0.9168\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1878 - acc: 0.9213 - val_loss: 0.2116 - val_acc: 0.9072\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1877 - acc: 0.9219 - val_loss: 0.2540 - val_acc: 0.8884\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1920 - acc: 0.9212 - val_loss: 0.4015 - val_acc: 0.8380\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1968 - acc: 0.9171 - val_loss: 0.1962 - val_acc: 0.9156\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1736 - acc: 0.9286 - val_loss: 0.1943 - val_acc: 0.9164\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1673 - acc: 0.9304 - val_loss: 0.2282 - val_acc: 0.9068\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1777 - acc: 0.9253 - val_loss: 0.1854 - val_acc: 0.9196\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1691 - acc: 0.9286 - val_loss: 0.1909 - val_acc: 0.9200\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1593 - acc: 0.9341 - val_loss: 0.1866 - val_acc: 0.9204\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1569 - acc: 0.9363 - val_loss: 0.2921 - val_acc: 0.8836\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1642 - acc: 0.9326 - val_loss: 0.1993 - val_acc: 0.9152\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1560 - acc: 0.9344 - val_loss: 0.1894 - val_acc: 0.9228\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1488 - acc: 0.9398 - val_loss: 0.2461 - val_acc: 0.8972\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1487 - acc: 0.9386 - val_loss: 0.1874 - val_acc: 0.9224\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6549 - acc: 0.5982 - val_loss: 0.5637 - val_acc: 0.7056\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4968 - acc: 0.7524 - val_loss: 0.3645 - val_acc: 0.8384\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3657 - acc: 0.8370 - val_loss: 0.3112 - val_acc: 0.8792\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3255 - acc: 0.8604 - val_loss: 0.2885 - val_acc: 0.8816\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3038 - acc: 0.8673 - val_loss: 0.3024 - val_acc: 0.8796\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2819 - acc: 0.8803 - val_loss: 0.2332 - val_acc: 0.9084\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2684 - acc: 0.8876 - val_loss: 0.3293 - val_acc: 0.8484\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.2565 - acc: 0.8910 - val_loss: 0.2463 - val_acc: 0.8960\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2466 - acc: 0.8954 - val_loss: 0.2440 - val_acc: 0.8972\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2399 - acc: 0.8974 - val_loss: 0.2308 - val_acc: 0.9076\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2318 - acc: 0.9025 - val_loss: 0.2104 - val_acc: 0.9164\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2316 - acc: 0.9022 - val_loss: 0.2240 - val_acc: 0.9040\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2200 - acc: 0.9091 - val_loss: 0.2735 - val_acc: 0.8816\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2149 - acc: 0.9097 - val_loss: 0.2549 - val_acc: 0.8924\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2133 - acc: 0.9111 - val_loss: 0.2052 - val_acc: 0.9116\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2121 - acc: 0.9128 - val_loss: 0.2058 - val_acc: 0.9168\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2036 - acc: 0.9145 - val_loss: 0.1997 - val_acc: 0.9220\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2114 - acc: 0.9132 - val_loss: 0.2263 - val_acc: 0.9104\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1972 - acc: 0.9201 - val_loss: 0.1974 - val_acc: 0.9188\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2003 - acc: 0.9187 - val_loss: 0.1943 - val_acc: 0.9172\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1922 - acc: 0.9218 - val_loss: 0.1919 - val_acc: 0.9244\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1873 - acc: 0.9236 - val_loss: 0.1953 - val_acc: 0.9216\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1851 - acc: 0.9247 - val_loss: 0.2033 - val_acc: 0.9220\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1865 - acc: 0.9248 - val_loss: 0.1957 - val_acc: 0.9228\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.1890 - acc: 0.9225 - val_loss: 0.1981 - val_acc: 0.9164\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1800 - acc: 0.9271 - val_loss: 0.2222 - val_acc: 0.9120\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1752 - acc: 0.9276 - val_loss: 0.2141 - val_acc: 0.9128\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1722 - acc: 0.9308 - val_loss: 0.2408 - val_acc: 0.8956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1716 - acc: 0.9308 - val_loss: 0.1973 - val_acc: 0.9180\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.1715 - acc: 0.9296 - val_loss: 0.1803 - val_acc: 0.9260\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6258 - acc: 0.6383 - val_loss: 0.5260 - val_acc: 0.7352\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4685 - acc: 0.7745 - val_loss: 0.3977 - val_acc: 0.8200\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3716 - acc: 0.8346 - val_loss: 0.2940 - val_acc: 0.8836\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3127 - acc: 0.8638 - val_loss: 0.3487 - val_acc: 0.8500\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2769 - acc: 0.8814 - val_loss: 0.2558 - val_acc: 0.8968\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2575 - acc: 0.8889 - val_loss: 0.2619 - val_acc: 0.8892\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2495 - acc: 0.8931 - val_loss: 0.3028 - val_acc: 0.8656\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2230 - acc: 0.9052 - val_loss: 0.2382 - val_acc: 0.9028\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.2194 - acc: 0.9055 - val_loss: 0.2273 - val_acc: 0.9108\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2077 - acc: 0.9132 - val_loss: 0.2414 - val_acc: 0.8980\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1909 - acc: 0.9228 - val_loss: 0.2695 - val_acc: 0.8892\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1895 - acc: 0.9212 - val_loss: 0.2174 - val_acc: 0.9060\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.1687 - acc: 0.9304 - val_loss: 0.2146 - val_acc: 0.9072\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1620 - acc: 0.9334 - val_loss: 0.2183 - val_acc: 0.9056\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1576 - acc: 0.9345 - val_loss: 0.2975 - val_acc: 0.8800\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1528 - acc: 0.9394 - val_loss: 0.2208 - val_acc: 0.9148\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1376 - acc: 0.9449 - val_loss: 0.2266 - val_acc: 0.9136\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1293 - acc: 0.9465 - val_loss: 0.2812 - val_acc: 0.8924\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1328 - acc: 0.9472 - val_loss: 0.2341 - val_acc: 0.9052\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1224 - acc: 0.9514 - val_loss: 0.2342 - val_acc: 0.9104\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1181 - acc: 0.9532 - val_loss: 0.2325 - val_acc: 0.9104\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1110 - acc: 0.9551 - val_loss: 0.2355 - val_acc: 0.9072\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1042 - acc: 0.9585 - val_loss: 0.2432 - val_acc: 0.9092\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0986 - acc: 0.9603 - val_loss: 0.2794 - val_acc: 0.9008\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0977 - acc: 0.9609 - val_loss: 0.2553 - val_acc: 0.9120\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0915 - acc: 0.9632 - val_loss: 0.2554 - val_acc: 0.9132\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.0956 - acc: 0.9628 - val_loss: 0.2851 - val_acc: 0.9016\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0979 - acc: 0.9598 - val_loss: 0.2609 - val_acc: 0.9044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0774 - acc: 0.9702 - val_loss: 0.2643 - val_acc: 0.9212\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0764 - acc: 0.9709 - val_loss: 0.2823 - val_acc: 0.9148\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6395 - acc: 0.6187 - val_loss: 0.5758 - val_acc: 0.7032\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4785 - acc: 0.7682 - val_loss: 0.3235 - val_acc: 0.8704\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3442 - acc: 0.8507 - val_loss: 0.2817 - val_acc: 0.8852\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3053 - acc: 0.8670 - val_loss: 0.3102 - val_acc: 0.8616\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2858 - acc: 0.8794 - val_loss: 0.3264 - val_acc: 0.8528\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2650 - acc: 0.8888 - val_loss: 0.2722 - val_acc: 0.8860\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2518 - acc: 0.8912 - val_loss: 0.2239 - val_acc: 0.9072\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2411 - acc: 0.8998 - val_loss: 0.2380 - val_acc: 0.9056\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2354 - acc: 0.8997 - val_loss: 0.2452 - val_acc: 0.8920\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2225 - acc: 0.9074 - val_loss: 0.2566 - val_acc: 0.8876\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2168 - acc: 0.9078 - val_loss: 0.1965 - val_acc: 0.9204\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2037 - acc: 0.9135 - val_loss: 0.2142 - val_acc: 0.9128\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2022 - acc: 0.9154 - val_loss: 0.1955 - val_acc: 0.9184\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1992 - acc: 0.9168 - val_loss: 0.1988 - val_acc: 0.9148\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1920 - acc: 0.9198 - val_loss: 0.2068 - val_acc: 0.9096\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1968 - acc: 0.9182 - val_loss: 0.2123 - val_acc: 0.9100\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1855 - acc: 0.9239 - val_loss: 0.1837 - val_acc: 0.9232\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1763 - acc: 0.9274 - val_loss: 0.1991 - val_acc: 0.9164\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1741 - acc: 0.9269 - val_loss: 0.1994 - val_acc: 0.9156\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1713 - acc: 0.9300 - val_loss: 0.2106 - val_acc: 0.9092\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1676 - acc: 0.9315 - val_loss: 0.2141 - val_acc: 0.9068\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1609 - acc: 0.9325 - val_loss: 0.1980 - val_acc: 0.9192\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1672 - acc: 0.9331 - val_loss: 0.1943 - val_acc: 0.9200\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1624 - acc: 0.9327 - val_loss: 0.2034 - val_acc: 0.9148\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1489 - acc: 0.9386 - val_loss: 0.1899 - val_acc: 0.9192\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1501 - acc: 0.9374 - val_loss: 0.1886 - val_acc: 0.9212\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1441 - acc: 0.9406 - val_loss: 0.1966 - val_acc: 0.9212\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1456 - acc: 0.9404 - val_loss: 0.2671 - val_acc: 0.8908\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1424 - acc: 0.9409 - val_loss: 0.1918 - val_acc: 0.9212\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1381 - acc: 0.9425 - val_loss: 0.1972 - val_acc: 0.9192\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6620 - acc: 0.5853 - val_loss: 0.5759 - val_acc: 0.6876\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4877 - acc: 0.7541 - val_loss: 0.4490 - val_acc: 0.7884\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3741 - acc: 0.8349 - val_loss: 0.3734 - val_acc: 0.8272\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3171 - acc: 0.8633 - val_loss: 0.2573 - val_acc: 0.8908\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2890 - acc: 0.8766 - val_loss: 0.2593 - val_acc: 0.8984\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2708 - acc: 0.8832 - val_loss: 0.2258 - val_acc: 0.9040\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2667 - acc: 0.8854 - val_loss: 0.2058 - val_acc: 0.9152\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2492 - acc: 0.8929 - val_loss: 0.2350 - val_acc: 0.8968\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2531 - acc: 0.8933 - val_loss: 0.2031 - val_acc: 0.9144\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2293 - acc: 0.9059 - val_loss: 0.2036 - val_acc: 0.9168\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2304 - acc: 0.9028 - val_loss: 0.2067 - val_acc: 0.9180\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2248 - acc: 0.9061 - val_loss: 0.1979 - val_acc: 0.9240\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2212 - acc: 0.9068 - val_loss: 0.1931 - val_acc: 0.9204\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2106 - acc: 0.9121 - val_loss: 0.2015 - val_acc: 0.9208\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1971 - acc: 0.9177 - val_loss: 0.1971 - val_acc: 0.9176\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1970 - acc: 0.9178 - val_loss: 0.2022 - val_acc: 0.9196\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2013 - acc: 0.9156 - val_loss: 0.2176 - val_acc: 0.9116\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1950 - acc: 0.9216 - val_loss: 0.1984 - val_acc: 0.9164\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1956 - acc: 0.9198 - val_loss: 0.1913 - val_acc: 0.9184\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1949 - acc: 0.9205 - val_loss: 0.2163 - val_acc: 0.9108\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1863 - acc: 0.9234 - val_loss: 0.2692 - val_acc: 0.8856\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.1878 - acc: 0.9226 - val_loss: 0.2128 - val_acc: 0.9092\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.1802 - acc: 0.9244 - val_loss: 0.1861 - val_acc: 0.9220\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1739 - acc: 0.9292 - val_loss: 0.1999 - val_acc: 0.9192\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1777 - acc: 0.9268 - val_loss: 0.1939 - val_acc: 0.9224\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1627 - acc: 0.9330 - val_loss: 0.1841 - val_acc: 0.9252\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1670 - acc: 0.9326 - val_loss: 0.2971 - val_acc: 0.8792\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1607 - acc: 0.9341 - val_loss: 0.1987 - val_acc: 0.9176\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1638 - acc: 0.9327 - val_loss: 0.2242 - val_acc: 0.9132\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1695 - acc: 0.9311 - val_loss: 0.1886 - val_acc: 0.9196\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6337 - acc: 0.6254 - val_loss: 0.5828 - val_acc: 0.6664\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4620 - acc: 0.7752 - val_loss: 0.4828 - val_acc: 0.7492\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3493 - acc: 0.8493 - val_loss: 0.3026 - val_acc: 0.8732\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2993 - acc: 0.8720 - val_loss: 0.2652 - val_acc: 0.8924\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2737 - acc: 0.8819 - val_loss: 0.2516 - val_acc: 0.8964\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2596 - acc: 0.8891 - val_loss: 0.2438 - val_acc: 0.9016\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2473 - acc: 0.8952 - val_loss: 0.2314 - val_acc: 0.9068\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2255 - acc: 0.9044 - val_loss: 0.2266 - val_acc: 0.9104\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2114 - acc: 0.9111 - val_loss: 0.2383 - val_acc: 0.9064\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1938 - acc: 0.9201 - val_loss: 0.2136 - val_acc: 0.9124\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1865 - acc: 0.9220 - val_loss: 0.2131 - val_acc: 0.9136\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1746 - acc: 0.9269 - val_loss: 0.2086 - val_acc: 0.9104\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1690 - acc: 0.9301 - val_loss: 0.2151 - val_acc: 0.9112\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1589 - acc: 0.9352 - val_loss: 0.2099 - val_acc: 0.9192\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1535 - acc: 0.9373 - val_loss: 0.2287 - val_acc: 0.9096\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1493 - acc: 0.9385 - val_loss: 0.2370 - val_acc: 0.9120\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1298 - acc: 0.9484 - val_loss: 0.2500 - val_acc: 0.9084\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1290 - acc: 0.9457 - val_loss: 0.2273 - val_acc: 0.9160\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1267 - acc: 0.9501 - val_loss: 0.2254 - val_acc: 0.9156\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1155 - acc: 0.9531 - val_loss: 0.2353 - val_acc: 0.9156\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1091 - acc: 0.9567 - val_loss: 0.2401 - val_acc: 0.9112\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1020 - acc: 0.9580 - val_loss: 0.2266 - val_acc: 0.9196\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1020 - acc: 0.9609 - val_loss: 0.2471 - val_acc: 0.9132\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1006 - acc: 0.9589 - val_loss: 0.2579 - val_acc: 0.9088\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0938 - acc: 0.9643 - val_loss: 0.2452 - val_acc: 0.9144\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0909 - acc: 0.9624 - val_loss: 0.2625 - val_acc: 0.9104\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0878 - acc: 0.9655 - val_loss: 0.2566 - val_acc: 0.9120\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0845 - acc: 0.9671 - val_loss: 0.2545 - val_acc: 0.9180\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0801 - acc: 0.9675 - val_loss: 0.2737 - val_acc: 0.9132\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.0740 - acc: 0.9688 - val_loss: 0.2633 - val_acc: 0.9136\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.6280 - acc: 0.6329 - val_loss: 0.5393 - val_acc: 0.7176\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4168 - acc: 0.8099 - val_loss: 0.3031 - val_acc: 0.8656\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.3286 - acc: 0.8562 - val_loss: 0.2682 - val_acc: 0.8908\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2906 - acc: 0.8756 - val_loss: 0.2336 - val_acc: 0.9056\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2639 - acc: 0.8854 - val_loss: 0.2410 - val_acc: 0.8996\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2578 - acc: 0.8876 - val_loss: 0.2243 - val_acc: 0.9068\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2448 - acc: 0.8953 - val_loss: 0.2287 - val_acc: 0.9008\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2258 - acc: 0.9046 - val_loss: 0.2348 - val_acc: 0.8976\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2247 - acc: 0.9070 - val_loss: 0.1966 - val_acc: 0.9124\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2136 - acc: 0.9122 - val_loss: 0.2150 - val_acc: 0.9084\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2054 - acc: 0.9152 - val_loss: 0.1922 - val_acc: 0.9184\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1979 - acc: 0.9182 - val_loss: 0.2042 - val_acc: 0.9132\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1938 - acc: 0.9177 - val_loss: 0.2022 - val_acc: 0.9084\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1861 - acc: 0.9216 - val_loss: 0.2006 - val_acc: 0.9132\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1862 - acc: 0.9240 - val_loss: 0.2038 - val_acc: 0.9128\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1804 - acc: 0.9275 - val_loss: 0.1901 - val_acc: 0.9208\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1747 - acc: 0.9299 - val_loss: 0.2529 - val_acc: 0.8892\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1692 - acc: 0.9304 - val_loss: 0.2034 - val_acc: 0.9112\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1615 - acc: 0.9349 - val_loss: 0.1988 - val_acc: 0.9136\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1625 - acc: 0.9331 - val_loss: 0.1902 - val_acc: 0.9196\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1504 - acc: 0.9394 - val_loss: 0.1933 - val_acc: 0.9216\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1479 - acc: 0.9400 - val_loss: 0.2131 - val_acc: 0.9072\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1506 - acc: 0.9380 - val_loss: 0.1960 - val_acc: 0.9156\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1342 - acc: 0.9482 - val_loss: 0.2376 - val_acc: 0.9016\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1397 - acc: 0.9417 - val_loss: 0.2028 - val_acc: 0.9184\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1394 - acc: 0.9441 - val_loss: 0.1999 - val_acc: 0.9168\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1362 - acc: 0.9444 - val_loss: 0.2077 - val_acc: 0.9176\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1335 - acc: 0.9453 - val_loss: 0.2121 - val_acc: 0.9160\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1287 - acc: 0.9495 - val_loss: 0.2139 - val_acc: 0.9172\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1240 - acc: 0.9506 - val_loss: 0.1893 - val_acc: 0.9224\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6383 - acc: 0.6156 - val_loss: 0.4991 - val_acc: 0.7568\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.4390 - acc: 0.7952 - val_loss: 0.3604 - val_acc: 0.8472\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.3495 - acc: 0.8459 - val_loss: 0.2622 - val_acc: 0.8844\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.2954 - acc: 0.8719 - val_loss: 0.3009 - val_acc: 0.8676\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.2759 - acc: 0.8814 - val_loss: 0.2327 - val_acc: 0.9028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.2694 - acc: 0.8843 - val_loss: 0.2401 - val_acc: 0.9004\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.2388 - acc: 0.8981 - val_loss: 0.2324 - val_acc: 0.9008\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2371 - acc: 0.9000 - val_loss: 0.2561 - val_acc: 0.8856\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2544 - acc: 0.8952 - val_loss: 0.2733 - val_acc: 0.8816\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2166 - acc: 0.9096 - val_loss: 0.2478 - val_acc: 0.8932\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2158 - acc: 0.9108 - val_loss: 0.2178 - val_acc: 0.9108\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.2085 - acc: 0.9135 - val_loss: 0.2069 - val_acc: 0.9128\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2027 - acc: 0.9150 - val_loss: 0.1943 - val_acc: 0.9204\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1916 - acc: 0.9212 - val_loss: 0.1987 - val_acc: 0.9172\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.1919 - acc: 0.9203 - val_loss: 0.2580 - val_acc: 0.9008\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.1845 - acc: 0.9241 - val_loss: 0.1968 - val_acc: 0.9116\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.1782 - acc: 0.9274 - val_loss: 0.1896 - val_acc: 0.9240\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.1830 - acc: 0.9265 - val_loss: 0.2472 - val_acc: 0.8984\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.1769 - acc: 0.9271 - val_loss: 0.2481 - val_acc: 0.8928\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1721 - acc: 0.9278 - val_loss: 0.3114 - val_acc: 0.8708\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1681 - acc: 0.9305 - val_loss: 0.1813 - val_acc: 0.9224\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1729 - acc: 0.9279 - val_loss: 0.2395 - val_acc: 0.8956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.1611 - acc: 0.9363 - val_loss: 0.1840 - val_acc: 0.9212\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.1547 - acc: 0.9380 - val_loss: 0.1854 - val_acc: 0.9264\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.1535 - acc: 0.9381 - val_loss: 0.1902 - val_acc: 0.9208\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1492 - acc: 0.9409 - val_loss: 0.2582 - val_acc: 0.8900\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.1494 - acc: 0.9384 - val_loss: 0.1812 - val_acc: 0.9320\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1465 - acc: 0.9398 - val_loss: 0.1848 - val_acc: 0.9248\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1444 - acc: 0.9401 - val_loss: 0.1857 - val_acc: 0.9220\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.1446 - acc: 0.9421 - val_loss: 0.1829 - val_acc: 0.9236\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6330 - acc: 0.6281 - val_loss: 0.5472 - val_acc: 0.7156\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4410 - acc: 0.7939 - val_loss: 0.3405 - val_acc: 0.8564\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3408 - acc: 0.8512 - val_loss: 0.2754 - val_acc: 0.8860\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3074 - acc: 0.8638 - val_loss: 0.2742 - val_acc: 0.8864\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2700 - acc: 0.8832 - val_loss: 0.2951 - val_acc: 0.8740\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2478 - acc: 0.8919 - val_loss: 0.2434 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2268 - acc: 0.9045 - val_loss: 0.2395 - val_acc: 0.8980\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2262 - acc: 0.9040 - val_loss: 0.2339 - val_acc: 0.9036\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2024 - acc: 0.9191 - val_loss: 0.2261 - val_acc: 0.9084\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1918 - acc: 0.9224 - val_loss: 0.2190 - val_acc: 0.9104\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1781 - acc: 0.9271 - val_loss: 0.2191 - val_acc: 0.9084\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1648 - acc: 0.9326 - val_loss: 0.2338 - val_acc: 0.9028\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1575 - acc: 0.9361 - val_loss: 0.2755 - val_acc: 0.8884\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1509 - acc: 0.9376 - val_loss: 0.2205 - val_acc: 0.9136\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1465 - acc: 0.9378 - val_loss: 0.2433 - val_acc: 0.9064\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1412 - acc: 0.9427 - val_loss: 0.2251 - val_acc: 0.9132\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1290 - acc: 0.9464 - val_loss: 0.2369 - val_acc: 0.9124\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1223 - acc: 0.9496 - val_loss: 0.2359 - val_acc: 0.9140\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1210 - acc: 0.9514 - val_loss: 0.2951 - val_acc: 0.8876\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1117 - acc: 0.9545 - val_loss: 0.2492 - val_acc: 0.9076\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1069 - acc: 0.9576 - val_loss: 0.2935 - val_acc: 0.8972\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1009 - acc: 0.9585 - val_loss: 0.2466 - val_acc: 0.9088\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1022 - acc: 0.9598 - val_loss: 0.2884 - val_acc: 0.9080\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0944 - acc: 0.9595 - val_loss: 0.2739 - val_acc: 0.9044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0814 - acc: 0.9685 - val_loss: 0.3158 - val_acc: 0.9024\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0883 - acc: 0.9643 - val_loss: 0.2793 - val_acc: 0.9092\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0787 - acc: 0.9684 - val_loss: 0.2905 - val_acc: 0.9076\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0802 - acc: 0.9679 - val_loss: 0.2742 - val_acc: 0.9092\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0886 - acc: 0.9641 - val_loss: 0.2696 - val_acc: 0.9088\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0718 - acc: 0.9704 - val_loss: 0.3595 - val_acc: 0.8948\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6652 - acc: 0.5764 - val_loss: 0.6284 - val_acc: 0.6428\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5171 - acc: 0.7449 - val_loss: 0.3908 - val_acc: 0.8248\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3676 - acc: 0.8386 - val_loss: 0.2961 - val_acc: 0.8732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3337 - acc: 0.8512 - val_loss: 0.2954 - val_acc: 0.8924\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3032 - acc: 0.8697 - val_loss: 0.2465 - val_acc: 0.9040\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2782 - acc: 0.8831 - val_loss: 0.2755 - val_acc: 0.8820\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2739 - acc: 0.8816 - val_loss: 0.2224 - val_acc: 0.9088\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2615 - acc: 0.8887 - val_loss: 0.2399 - val_acc: 0.9036\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2523 - acc: 0.8923 - val_loss: 0.2471 - val_acc: 0.8952\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2547 - acc: 0.8937 - val_loss: 0.2189 - val_acc: 0.9076\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2437 - acc: 0.8981 - val_loss: 0.2919 - val_acc: 0.8784\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2282 - acc: 0.9006 - val_loss: 0.2095 - val_acc: 0.9120\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2246 - acc: 0.9051 - val_loss: 0.2308 - val_acc: 0.9072\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2257 - acc: 0.9039 - val_loss: 0.2081 - val_acc: 0.9120\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2134 - acc: 0.9116 - val_loss: 0.2626 - val_acc: 0.8868\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2084 - acc: 0.9131 - val_loss: 0.2408 - val_acc: 0.8952\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2039 - acc: 0.9129 - val_loss: 0.2108 - val_acc: 0.9088\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2013 - acc: 0.9175 - val_loss: 0.2400 - val_acc: 0.8952\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2009 - acc: 0.9181 - val_loss: 0.1950 - val_acc: 0.9164\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2064 - acc: 0.9165 - val_loss: 0.3248 - val_acc: 0.8696\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1912 - acc: 0.9198 - val_loss: 0.2602 - val_acc: 0.8888\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1885 - acc: 0.9216 - val_loss: 0.2218 - val_acc: 0.9060\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1823 - acc: 0.9238 - val_loss: 0.3212 - val_acc: 0.8744\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1874 - acc: 0.9216 - val_loss: 0.1935 - val_acc: 0.9156\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1881 - acc: 0.9232 - val_loss: 0.2688 - val_acc: 0.8880\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1764 - acc: 0.9240 - val_loss: 0.1864 - val_acc: 0.9168\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1687 - acc: 0.9292 - val_loss: 0.2181 - val_acc: 0.9104\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.1663 - acc: 0.9310 - val_loss: 0.2721 - val_acc: 0.8880\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1615 - acc: 0.9342 - val_loss: 0.2048 - val_acc: 0.9140\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1625 - acc: 0.9321 - val_loss: 0.1965 - val_acc: 0.9180\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6678 - acc: 0.5758 - val_loss: 0.6206 - val_acc: 0.6632\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.5199 - acc: 0.7394 - val_loss: 0.4231 - val_acc: 0.8100\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3784 - acc: 0.8315 - val_loss: 0.3061 - val_acc: 0.8796\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.3158 - acc: 0.8654 - val_loss: 0.2531 - val_acc: 0.8896\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3078 - acc: 0.8678 - val_loss: 0.2800 - val_acc: 0.8800\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2779 - acc: 0.8819 - val_loss: 0.2608 - val_acc: 0.8888\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.2811 - acc: 0.8791 - val_loss: 0.2636 - val_acc: 0.8864\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2684 - acc: 0.8831 - val_loss: 0.2672 - val_acc: 0.8820\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.2521 - acc: 0.8942 - val_loss: 0.2652 - val_acc: 0.8944\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2465 - acc: 0.8947 - val_loss: 0.2286 - val_acc: 0.9084\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.2471 - acc: 0.8945 - val_loss: 0.2470 - val_acc: 0.9040\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2417 - acc: 0.8990 - val_loss: 0.3112 - val_acc: 0.8732\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2296 - acc: 0.9042 - val_loss: 0.2143 - val_acc: 0.9064\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2300 - acc: 0.9054 - val_loss: 0.2160 - val_acc: 0.9152\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2309 - acc: 0.9028 - val_loss: 0.2298 - val_acc: 0.9104\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.2173 - acc: 0.9068 - val_loss: 0.2700 - val_acc: 0.8932\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2236 - acc: 0.9069 - val_loss: 0.2200 - val_acc: 0.9160\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2196 - acc: 0.9078 - val_loss: 0.2295 - val_acc: 0.9104\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2071 - acc: 0.9125 - val_loss: 0.1938 - val_acc: 0.9232\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.2117 - acc: 0.9121 - val_loss: 0.1973 - val_acc: 0.9136\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.2004 - acc: 0.9188 - val_loss: 0.2116 - val_acc: 0.9076\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1977 - acc: 0.9189 - val_loss: 0.1928 - val_acc: 0.9200\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1979 - acc: 0.9180 - val_loss: 0.2009 - val_acc: 0.9176\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2012 - acc: 0.9165 - val_loss: 0.3019 - val_acc: 0.8652\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1962 - acc: 0.9161 - val_loss: 0.2018 - val_acc: 0.9140\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1922 - acc: 0.9198 - val_loss: 0.2361 - val_acc: 0.9036\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.1926 - acc: 0.9196 - val_loss: 0.2031 - val_acc: 0.9144\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1820 - acc: 0.9232 - val_loss: 0.1917 - val_acc: 0.9176\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1953 - acc: 0.9178 - val_loss: 0.2056 - val_acc: 0.9180\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1855 - acc: 0.9234 - val_loss: 0.2344 - val_acc: 0.9028\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.6059 - acc: 0.6521 - val_loss: 0.5059 - val_acc: 0.7504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4148 - acc: 0.8110 - val_loss: 0.3737 - val_acc: 0.8224\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3123 - acc: 0.8639 - val_loss: 0.2531 - val_acc: 0.8996\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2816 - acc: 0.8776 - val_loss: 0.2907 - val_acc: 0.8724\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2544 - acc: 0.8888 - val_loss: 0.2429 - val_acc: 0.8960\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2483 - acc: 0.8942 - val_loss: 0.2393 - val_acc: 0.8984\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2236 - acc: 0.9053 - val_loss: 0.2291 - val_acc: 0.9076\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2105 - acc: 0.9102 - val_loss: 0.2372 - val_acc: 0.9052\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2025 - acc: 0.9152 - val_loss: 0.2456 - val_acc: 0.8956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1903 - acc: 0.9214 - val_loss: 0.2329 - val_acc: 0.9052\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1760 - acc: 0.9261 - val_loss: 0.2696 - val_acc: 0.8956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1586 - acc: 0.9348 - val_loss: 0.2426 - val_acc: 0.9020\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1501 - acc: 0.9381 - val_loss: 0.2584 - val_acc: 0.9028\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.1502 - acc: 0.9388 - val_loss: 0.2570 - val_acc: 0.8980\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1346 - acc: 0.9454 - val_loss: 0.2577 - val_acc: 0.9096\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1343 - acc: 0.9449 - val_loss: 0.2284 - val_acc: 0.9104\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1216 - acc: 0.9501 - val_loss: 0.2585 - val_acc: 0.9120\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1153 - acc: 0.9529 - val_loss: 0.2621 - val_acc: 0.9044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1183 - acc: 0.9525 - val_loss: 0.2519 - val_acc: 0.9024\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.1077 - acc: 0.9564 - val_loss: 0.2574 - val_acc: 0.9100\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.0992 - acc: 0.9612 - val_loss: 0.2777 - val_acc: 0.9076\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.1008 - acc: 0.9597 - val_loss: 0.2484 - val_acc: 0.9104\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.0964 - acc: 0.9612 - val_loss: 0.2768 - val_acc: 0.9080\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0963 - acc: 0.9619 - val_loss: 0.2779 - val_acc: 0.9116\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.0881 - acc: 0.9656 - val_loss: 0.2843 - val_acc: 0.9084\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0826 - acc: 0.9664 - val_loss: 0.3122 - val_acc: 0.9084\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0799 - acc: 0.9688 - val_loss: 0.3206 - val_acc: 0.9040\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.0766 - acc: 0.9686 - val_loss: 0.3016 - val_acc: 0.9044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.0741 - acc: 0.9705 - val_loss: 0.3696 - val_acc: 0.8928\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.0712 - acc: 0.9725 - val_loss: 0.3061 - val_acc: 0.9056\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6458 - acc: 0.6092 - val_loss: 0.6200 - val_acc: 0.6528\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.4573 - acc: 0.7822 - val_loss: 0.3319 - val_acc: 0.8628\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.3452 - acc: 0.8459 - val_loss: 0.2714 - val_acc: 0.8912\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.2963 - acc: 0.8723 - val_loss: 0.2466 - val_acc: 0.8920\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2790 - acc: 0.8804 - val_loss: 0.2328 - val_acc: 0.9044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.2734 - acc: 0.8833 - val_loss: 0.2718 - val_acc: 0.8812\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2524 - acc: 0.8924 - val_loss: 0.2527 - val_acc: 0.8980\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.2421 - acc: 0.8994 - val_loss: 0.3031 - val_acc: 0.8680\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2454 - acc: 0.8984 - val_loss: 0.2264 - val_acc: 0.9048\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2352 - acc: 0.9010 - val_loss: 0.2154 - val_acc: 0.9080\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2187 - acc: 0.9081 - val_loss: 0.2330 - val_acc: 0.9024\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2176 - acc: 0.9085 - val_loss: 0.2090 - val_acc: 0.9168\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2180 - acc: 0.9076 - val_loss: 0.1976 - val_acc: 0.9148\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2039 - acc: 0.9169 - val_loss: 0.2209 - val_acc: 0.9064\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1991 - acc: 0.9162 - val_loss: 0.2067 - val_acc: 0.9088\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1930 - acc: 0.9205 - val_loss: 0.1988 - val_acc: 0.9180\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.1990 - acc: 0.9148 - val_loss: 0.1959 - val_acc: 0.9164\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.1917 - acc: 0.9205 - val_loss: 0.1903 - val_acc: 0.9176\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1843 - acc: 0.9249 - val_loss: 0.2280 - val_acc: 0.9020\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1788 - acc: 0.9269 - val_loss: 0.2201 - val_acc: 0.9028\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1749 - acc: 0.9284 - val_loss: 0.2245 - val_acc: 0.9060\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1823 - acc: 0.9228 - val_loss: 0.1915 - val_acc: 0.9124\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1756 - acc: 0.9291 - val_loss: 0.1837 - val_acc: 0.9244\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1660 - acc: 0.9302 - val_loss: 0.1896 - val_acc: 0.9192\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.1652 - acc: 0.9321 - val_loss: 0.1866 - val_acc: 0.9188\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1611 - acc: 0.9344 - val_loss: 0.2141 - val_acc: 0.9092\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1618 - acc: 0.9309 - val_loss: 0.2239 - val_acc: 0.9036\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1550 - acc: 0.9365 - val_loss: 0.1826 - val_acc: 0.9244\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.1501 - acc: 0.9384 - val_loss: 0.2296 - val_acc: 0.9024\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.1500 - acc: 0.9391 - val_loss: 0.2193 - val_acc: 0.9084\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6605 - acc: 0.5904 - val_loss: 0.6036 - val_acc: 0.6628\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.5010 - acc: 0.7511 - val_loss: 0.3920 - val_acc: 0.8472\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.3583 - acc: 0.8432 - val_loss: 0.3874 - val_acc: 0.8216\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3199 - acc: 0.8611 - val_loss: 0.2566 - val_acc: 0.8908\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2941 - acc: 0.8751 - val_loss: 0.2738 - val_acc: 0.8828\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.2834 - acc: 0.8778 - val_loss: 0.2489 - val_acc: 0.9048\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.2639 - acc: 0.8863 - val_loss: 0.2402 - val_acc: 0.9096\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2474 - acc: 0.8966 - val_loss: 0.2328 - val_acc: 0.9076\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2420 - acc: 0.8976 - val_loss: 0.2369 - val_acc: 0.9028\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.2350 - acc: 0.9018 - val_loss: 0.2204 - val_acc: 0.9168\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.2363 - acc: 0.9028 - val_loss: 0.2302 - val_acc: 0.9016\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2327 - acc: 0.9019 - val_loss: 0.1966 - val_acc: 0.9180\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2182 - acc: 0.9090 - val_loss: 0.2047 - val_acc: 0.9156\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2150 - acc: 0.9098 - val_loss: 0.1931 - val_acc: 0.9188\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2145 - acc: 0.9114 - val_loss: 0.2205 - val_acc: 0.9032\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2082 - acc: 0.9124 - val_loss: 0.1966 - val_acc: 0.9164\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2058 - acc: 0.9144 - val_loss: 0.2272 - val_acc: 0.9036\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2108 - acc: 0.9119 - val_loss: 0.2678 - val_acc: 0.8856\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2046 - acc: 0.9157 - val_loss: 0.2795 - val_acc: 0.8796\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2098 - acc: 0.9116 - val_loss: 0.1970 - val_acc: 0.9224\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.1959 - acc: 0.9184 - val_loss: 0.2883 - val_acc: 0.8776\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2060 - acc: 0.9165 - val_loss: 0.2130 - val_acc: 0.9124\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1919 - acc: 0.9209 - val_loss: 0.2056 - val_acc: 0.9152\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.1838 - acc: 0.9238 - val_loss: 0.1966 - val_acc: 0.9132\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.1814 - acc: 0.9272 - val_loss: 0.1939 - val_acc: 0.9184\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1837 - acc: 0.9248 - val_loss: 0.2195 - val_acc: 0.9092\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.1746 - acc: 0.9293 - val_loss: 0.2145 - val_acc: 0.9080\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1847 - acc: 0.9245 - val_loss: 0.1840 - val_acc: 0.9264\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.1827 - acc: 0.9243 - val_loss: 0.1990 - val_acc: 0.9176\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1732 - acc: 0.9284 - val_loss: 0.1857 - val_acc: 0.9240\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6273 - acc: 0.6384 - val_loss: 0.4952 - val_acc: 0.7576\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4411 - acc: 0.7899 - val_loss: 0.3202 - val_acc: 0.8648\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3396 - acc: 0.8498 - val_loss: 0.2993 - val_acc: 0.8832\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3061 - acc: 0.8668 - val_loss: 0.2498 - val_acc: 0.8956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2673 - acc: 0.8881 - val_loss: 0.2604 - val_acc: 0.8944\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2462 - acc: 0.8962 - val_loss: 0.2766 - val_acc: 0.8828\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2349 - acc: 0.8986 - val_loss: 0.2337 - val_acc: 0.8980\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2227 - acc: 0.9042 - val_loss: 0.2355 - val_acc: 0.9028\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2088 - acc: 0.9109 - val_loss: 0.2474 - val_acc: 0.9044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2038 - acc: 0.9128 - val_loss: 0.2380 - val_acc: 0.8988\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1842 - acc: 0.9198 - val_loss: 0.2459 - val_acc: 0.8952\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1707 - acc: 0.9312 - val_loss: 0.2241 - val_acc: 0.9048\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.1713 - acc: 0.9291 - val_loss: 0.2485 - val_acc: 0.8884\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1624 - acc: 0.9341 - val_loss: 0.2418 - val_acc: 0.9024\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1396 - acc: 0.9438 - val_loss: 0.2549 - val_acc: 0.9080\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1430 - acc: 0.9430 - val_loss: 0.2402 - val_acc: 0.9076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1358 - acc: 0.9433 - val_loss: 0.3121 - val_acc: 0.8868\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.1247 - acc: 0.9492 - val_loss: 0.2676 - val_acc: 0.8996\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1215 - acc: 0.9501 - val_loss: 0.2570 - val_acc: 0.9024\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1141 - acc: 0.9531 - val_loss: 0.2550 - val_acc: 0.9052\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.1068 - acc: 0.9565 - val_loss: 0.2682 - val_acc: 0.9104\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.1033 - acc: 0.9598 - val_loss: 0.2893 - val_acc: 0.8916\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1077 - acc: 0.9569 - val_loss: 0.2857 - val_acc: 0.9008\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0951 - acc: 0.9618 - val_loss: 0.2747 - val_acc: 0.9088\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0942 - acc: 0.9615 - val_loss: 0.2730 - val_acc: 0.9056\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0898 - acc: 0.9636 - val_loss: 0.2928 - val_acc: 0.9100\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.0890 - acc: 0.9646 - val_loss: 0.3180 - val_acc: 0.9052\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.0832 - acc: 0.9667 - val_loss: 0.2988 - val_acc: 0.9084\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.0788 - acc: 0.9685 - val_loss: 0.3097 - val_acc: 0.9036\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0772 - acc: 0.9691 - val_loss: 0.3021 - val_acc: 0.9020\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6511 - acc: 0.5996 - val_loss: 0.5724 - val_acc: 0.6848\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.4528 - acc: 0.7834 - val_loss: 0.3059 - val_acc: 0.8680\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3420 - acc: 0.8515 - val_loss: 0.3366 - val_acc: 0.8552\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3103 - acc: 0.8665 - val_loss: 0.2497 - val_acc: 0.9008\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2802 - acc: 0.8822 - val_loss: 0.2515 - val_acc: 0.8880\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2671 - acc: 0.8842 - val_loss: 0.2530 - val_acc: 0.8892\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.2467 - acc: 0.8966 - val_loss: 0.2850 - val_acc: 0.8688\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2505 - acc: 0.8941 - val_loss: 0.2243 - val_acc: 0.9080\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2277 - acc: 0.9048 - val_loss: 0.2545 - val_acc: 0.8896\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.2261 - acc: 0.9054 - val_loss: 0.2146 - val_acc: 0.9044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.2294 - acc: 0.9025 - val_loss: 0.2232 - val_acc: 0.9024\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.2079 - acc: 0.9143 - val_loss: 0.2044 - val_acc: 0.9172\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2055 - acc: 0.9159 - val_loss: 0.2157 - val_acc: 0.9120\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1994 - acc: 0.9165 - val_loss: 0.2188 - val_acc: 0.9076\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.1929 - acc: 0.9201 - val_loss: 0.2121 - val_acc: 0.9124\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1948 - acc: 0.9194 - val_loss: 0.2050 - val_acc: 0.9132\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.1821 - acc: 0.9245 - val_loss: 0.2116 - val_acc: 0.9108\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1819 - acc: 0.9262 - val_loss: 0.2312 - val_acc: 0.8928\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.1863 - acc: 0.9235 - val_loss: 0.2424 - val_acc: 0.9024\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.1735 - acc: 0.9282 - val_loss: 0.2107 - val_acc: 0.9152\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.1675 - acc: 0.9299 - val_loss: 0.1998 - val_acc: 0.9160\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.1673 - acc: 0.9307 - val_loss: 0.2367 - val_acc: 0.9008\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.1652 - acc: 0.9306 - val_loss: 0.2266 - val_acc: 0.9100\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1577 - acc: 0.9371 - val_loss: 0.1939 - val_acc: 0.9220\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.1613 - acc: 0.9334 - val_loss: 0.2451 - val_acc: 0.9024\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.1547 - acc: 0.9369 - val_loss: 0.2134 - val_acc: 0.9128\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.1563 - acc: 0.9342 - val_loss: 0.2020 - val_acc: 0.9160\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1462 - acc: 0.9398 - val_loss: 0.2457 - val_acc: 0.9000\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1479 - acc: 0.9391 - val_loss: 0.1958 - val_acc: 0.9144\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.1404 - acc: 0.9407 - val_loss: 0.2121 - val_acc: 0.9152\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6638 - acc: 0.5831 - val_loss: 0.6010 - val_acc: 0.6604\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.4938 - acc: 0.7508 - val_loss: 0.3987 - val_acc: 0.8520\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.3597 - acc: 0.8413 - val_loss: 0.3065 - val_acc: 0.8796\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.3288 - acc: 0.8588 - val_loss: 0.3146 - val_acc: 0.8808\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.2958 - acc: 0.8739 - val_loss: 0.2335 - val_acc: 0.9040\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.2772 - acc: 0.8825 - val_loss: 0.2262 - val_acc: 0.9060\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2550 - acc: 0.8923 - val_loss: 0.2261 - val_acc: 0.9092\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.2511 - acc: 0.8925 - val_loss: 0.2190 - val_acc: 0.9088\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.2540 - acc: 0.8962 - val_loss: 0.2144 - val_acc: 0.9148\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.2390 - acc: 0.8989 - val_loss: 0.2456 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2227 - acc: 0.9097 - val_loss: 0.2050 - val_acc: 0.9132\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.2401 - acc: 0.9002 - val_loss: 0.2308 - val_acc: 0.8944\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2207 - acc: 0.9072 - val_loss: 0.2544 - val_acc: 0.8944\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.2179 - acc: 0.9082 - val_loss: 0.1960 - val_acc: 0.9176\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2063 - acc: 0.9138 - val_loss: 0.2052 - val_acc: 0.9180\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.2185 - acc: 0.9090 - val_loss: 0.1985 - val_acc: 0.9176\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.2088 - acc: 0.9132 - val_loss: 0.2664 - val_acc: 0.8912\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1965 - acc: 0.9186 - val_loss: 0.2170 - val_acc: 0.9100\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.1966 - acc: 0.9193 - val_loss: 0.1968 - val_acc: 0.9156\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.1890 - acc: 0.9214 - val_loss: 0.2327 - val_acc: 0.8988\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1855 - acc: 0.9231 - val_loss: 0.2180 - val_acc: 0.9100\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.1820 - acc: 0.9231 - val_loss: 0.2101 - val_acc: 0.9148\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.1934 - acc: 0.9203 - val_loss: 0.2014 - val_acc: 0.9196\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1855 - acc: 0.9229 - val_loss: 0.2107 - val_acc: 0.9084\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1800 - acc: 0.9238 - val_loss: 0.2004 - val_acc: 0.9128\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1765 - acc: 0.9272 - val_loss: 0.2215 - val_acc: 0.9048\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1703 - acc: 0.9284 - val_loss: 0.2112 - val_acc: 0.9100\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1786 - acc: 0.9256 - val_loss: 0.2025 - val_acc: 0.9176\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1692 - acc: 0.9302 - val_loss: 0.2722 - val_acc: 0.8804\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1723 - acc: 0.9265 - val_loss: 0.1897 - val_acc: 0.9200\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6180 - acc: 0.6468 - val_loss: 0.5155 - val_acc: 0.7392\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4001 - acc: 0.8172 - val_loss: 0.3177 - val_acc: 0.8636\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3236 - acc: 0.8594 - val_loss: 0.2837 - val_acc: 0.8772\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2825 - acc: 0.8800 - val_loss: 0.2448 - val_acc: 0.8964\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.2496 - acc: 0.8932 - val_loss: 0.2446 - val_acc: 0.8976\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.2389 - acc: 0.8989 - val_loss: 0.2301 - val_acc: 0.9032\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2216 - acc: 0.9065 - val_loss: 0.2300 - val_acc: 0.9044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.1954 - acc: 0.9178 - val_loss: 0.2562 - val_acc: 0.8964\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.1971 - acc: 0.9194 - val_loss: 0.2309 - val_acc: 0.9064\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1745 - acc: 0.9249 - val_loss: 0.2350 - val_acc: 0.9032\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.1609 - acc: 0.9355 - val_loss: 0.2400 - val_acc: 0.9052\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.1553 - acc: 0.9346 - val_loss: 0.2237 - val_acc: 0.9088\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.1451 - acc: 0.9409 - val_loss: 0.2156 - val_acc: 0.9148\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.1461 - acc: 0.9379 - val_loss: 0.2352 - val_acc: 0.9168\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.1409 - acc: 0.9432 - val_loss: 0.2533 - val_acc: 0.9076\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1189 - acc: 0.9518 - val_loss: 0.2635 - val_acc: 0.9076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.1207 - acc: 0.9519 - val_loss: 0.2576 - val_acc: 0.9088\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.1169 - acc: 0.9524 - val_loss: 0.2398 - val_acc: 0.9128\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.1157 - acc: 0.9544 - val_loss: 0.2571 - val_acc: 0.9028\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.1093 - acc: 0.9558 - val_loss: 0.2923 - val_acc: 0.8996\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0985 - acc: 0.9611 - val_loss: 0.3108 - val_acc: 0.8976\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.0970 - acc: 0.9608 - val_loss: 0.2610 - val_acc: 0.9092\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.0962 - acc: 0.9610 - val_loss: 0.2913 - val_acc: 0.9000\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.0900 - acc: 0.9642 - val_loss: 0.2834 - val_acc: 0.9144\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0833 - acc: 0.9662 - val_loss: 0.3154 - val_acc: 0.8992\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.0818 - acc: 0.9681 - val_loss: 0.2692 - val_acc: 0.9128\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.0734 - acc: 0.9717 - val_loss: 0.2779 - val_acc: 0.9196\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0716 - acc: 0.9723 - val_loss: 0.3091 - val_acc: 0.9116\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0748 - acc: 0.9708 - val_loss: 0.2741 - val_acc: 0.9156\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0702 - acc: 0.9729 - val_loss: 0.3052 - val_acc: 0.9012\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6703 - acc: 0.5708 - val_loss: 0.5922 - val_acc: 0.6696\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4826 - acc: 0.7611 - val_loss: 0.3140 - val_acc: 0.8660\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3407 - acc: 0.8489 - val_loss: 0.2683 - val_acc: 0.8924\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3092 - acc: 0.8670 - val_loss: 0.2781 - val_acc: 0.8820\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2729 - acc: 0.8845 - val_loss: 0.3055 - val_acc: 0.8712\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.2770 - acc: 0.8821 - val_loss: 0.2320 - val_acc: 0.9036\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2460 - acc: 0.8949 - val_loss: 0.2197 - val_acc: 0.9096\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2382 - acc: 0.8983 - val_loss: 0.2744 - val_acc: 0.8784\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2309 - acc: 0.9052 - val_loss: 0.2409 - val_acc: 0.8972\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2232 - acc: 0.9074 - val_loss: 0.2404 - val_acc: 0.8996\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.2170 - acc: 0.9096 - val_loss: 0.2010 - val_acc: 0.9120\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2102 - acc: 0.9091 - val_loss: 0.2013 - val_acc: 0.9160\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1991 - acc: 0.9180 - val_loss: 0.1921 - val_acc: 0.9148\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1929 - acc: 0.9213 - val_loss: 0.2029 - val_acc: 0.9144\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.1935 - acc: 0.9209 - val_loss: 0.2225 - val_acc: 0.9052\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1912 - acc: 0.9192 - val_loss: 0.2376 - val_acc: 0.8960\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1819 - acc: 0.9241 - val_loss: 0.2093 - val_acc: 0.9152\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1868 - acc: 0.9239 - val_loss: 0.2322 - val_acc: 0.9032\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.1794 - acc: 0.9256 - val_loss: 0.2028 - val_acc: 0.9080\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1829 - acc: 0.9231 - val_loss: 0.2218 - val_acc: 0.9096\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1637 - acc: 0.9341 - val_loss: 0.1995 - val_acc: 0.9128\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1738 - acc: 0.9288 - val_loss: 0.2141 - val_acc: 0.9088\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1678 - acc: 0.9295 - val_loss: 0.2098 - val_acc: 0.9152\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1516 - acc: 0.9385 - val_loss: 0.1906 - val_acc: 0.9164\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.1500 - acc: 0.9389 - val_loss: 0.2236 - val_acc: 0.9060\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.1526 - acc: 0.9359 - val_loss: 0.2122 - val_acc: 0.9036\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.1479 - acc: 0.9396 - val_loss: 0.2174 - val_acc: 0.9064\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.1489 - acc: 0.9391 - val_loss: 0.2445 - val_acc: 0.9036\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1449 - acc: 0.9408 - val_loss: 0.2243 - val_acc: 0.9060\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.1457 - acc: 0.9416 - val_loss: 0.2238 - val_acc: 0.9048\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6683 - acc: 0.5773 - val_loss: 0.5848 - val_acc: 0.6752\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.4647 - acc: 0.7762 - val_loss: 0.3368 - val_acc: 0.8516\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.3383 - acc: 0.8527 - val_loss: 0.2941 - val_acc: 0.8780\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.3110 - acc: 0.8642 - val_loss: 0.2803 - val_acc: 0.8768\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.2807 - acc: 0.8812 - val_loss: 0.2963 - val_acc: 0.8740\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.2653 - acc: 0.8882 - val_loss: 0.2341 - val_acc: 0.8968\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.2456 - acc: 0.8950 - val_loss: 0.2213 - val_acc: 0.9032\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.2389 - acc: 0.8988 - val_loss: 0.2138 - val_acc: 0.9088\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.2388 - acc: 0.9017 - val_loss: 0.2118 - val_acc: 0.9136\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.2157 - acc: 0.9116 - val_loss: 0.3486 - val_acc: 0.8568\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.2193 - acc: 0.9091 - val_loss: 0.2001 - val_acc: 0.9140\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.2051 - acc: 0.9149 - val_loss: 0.2064 - val_acc: 0.9180\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.2055 - acc: 0.9152 - val_loss: 0.2099 - val_acc: 0.9092\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.1902 - acc: 0.9201 - val_loss: 0.2334 - val_acc: 0.8968\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.1964 - acc: 0.9189 - val_loss: 0.2354 - val_acc: 0.9020\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.1907 - acc: 0.9207 - val_loss: 0.2418 - val_acc: 0.8968\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.1896 - acc: 0.9201 - val_loss: 0.2637 - val_acc: 0.8904\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.1888 - acc: 0.9226 - val_loss: 0.1966 - val_acc: 0.9184\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.1715 - acc: 0.9277 - val_loss: 0.1958 - val_acc: 0.9120\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.1846 - acc: 0.9245 - val_loss: 0.2112 - val_acc: 0.9156\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.1771 - acc: 0.9260 - val_loss: 0.2166 - val_acc: 0.9144\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.1703 - acc: 0.9297 - val_loss: 0.1865 - val_acc: 0.9232\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1639 - acc: 0.9335 - val_loss: 0.2571 - val_acc: 0.8948\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.1596 - acc: 0.9335 - val_loss: 0.1953 - val_acc: 0.9224\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.1609 - acc: 0.9360 - val_loss: 0.3570 - val_acc: 0.8636\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1639 - acc: 0.9322 - val_loss: 0.1926 - val_acc: 0.9180\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.1512 - acc: 0.9388 - val_loss: 0.2189 - val_acc: 0.9080\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.1611 - acc: 0.9329 - val_loss: 0.2193 - val_acc: 0.9152\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.1477 - acc: 0.9399 - val_loss: 0.2670 - val_acc: 0.8848\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.1475 - acc: 0.9399 - val_loss: 0.1838 - val_acc: 0.9260\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [1,2,3]\n",
    "layer_sizes = [100,130,150,200]\n",
    "conv_layers = [1,2,3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME =\"JustCharge-1x1PMT-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\MuonElectron\\{}'.format(NAME))\n",
    "        \n",
    "        \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(layer_size,(5,5),strides=1, input_shape= XTrainingC.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "            model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "            model.add(Dropout(0.4))\n",
    "            for l in range(conv_layer-1):                   \n",
    "                model.add(Conv2D(layer_size,(3,3),padding='same',activation=\"relu\"))              \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(Dropout(0.4))            \n",
    "            #model.add(GlobalAveragePooling2D())\n",
    "            model.add(Flatten())\n",
    "            for l in range(dense_layer-1):\n",
    "                model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                #model.add(Dropout(0.5))\n",
    "            model.add(Dense(32,activation=\"relu\"))\n",
    "            model.add(Dense(2))\n",
    "            model.add(Activation('softmax'))\n",
    "            #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "            model.compile(loss=\"binary_crossentropy\",\n",
    "                         optimizer=\"adam\",\n",
    "                          metrics=['accuracy']\n",
    "                         )   \n",
    "            filepath=\"PMTOnly_PI_22k_RANDOM-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "            #model.summary()\n",
    "            history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_108 (Conv2D)          (None, 10, 16, 130)       3380      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_107 (MaxPoolin (None, 5, 8, 130)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 5, 8, 130)         520       \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 5, 8, 130)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_109 (Conv2D)          (None, 5, 8, 130)         152230    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_108 (MaxPoolin (None, 3, 4, 130)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 4, 130)         520       \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 3, 4, 130)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_110 (Conv2D)          (None, 3, 4, 130)         152230    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_109 (MaxPoolin (None, 2, 2, 130)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 2, 2, 130)         520       \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 2, 2, 130)         0         \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 520)               0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 512)               266752    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 594,810\n",
      "Trainable params: 592,942\n",
      "Non-trainable params: 1,868\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [2]\n",
    "layer_sizes = [130]\n",
    "conv_layers = [3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME =\"JustCharge-1x1PMT-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\MuonElectron\\{}'.format(NAME))\n",
    "        \n",
    "        \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(layer_size,(5,5),strides=1, input_shape= XTrainingC.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "            model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "            for l in range(conv_layer-1):                   \n",
    "                model.add(Conv2D(layer_size,(3,3),padding='same',activation=\"relu\"))              \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))            \n",
    "            #model.add(GlobalAveragePooling2D())\n",
    "            model.add(Flatten())\n",
    "            for l in range(dense_layer-1):\n",
    "                model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "            model.add(Dense(32,activation=\"relu\"))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(2))\n",
    "            model.add(Activation('softmax'))\n",
    "            #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "            model.compile(loss=\"binary_crossentropy\",\n",
    "                         optimizer=\"adam\",\n",
    "                          metrics=['accuracy']\n",
    "                         )   \n",
    "            filepath=\"PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "            model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.7366 - acc: 0.5891\n",
      "Epoch 00001: val_acc improved from -inf to 0.50200, saving model to PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.50.model\n",
      "17000/17000 [==============================] - 11s 668us/sample - loss: 0.7363 - acc: 0.5896 - val_loss: 0.9074 - val_acc: 0.5020\n",
      "Epoch 2/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.7756\n",
      "Epoch 00002: val_acc did not improve from 0.50200\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.4615 - acc: 0.7764 - val_loss: 1.1639 - val_acc: 0.4980\n",
      "Epoch 3/40\n",
      "16500/17000 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.8624\n",
      "Epoch 00003: val_acc improved from 0.50200 to 0.66480, saving model to PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.66.model\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.3146 - acc: 0.8627 - val_loss: 0.5888 - val_acc: 0.6648\n",
      "Epoch 4/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.8816\n",
      "Epoch 00004: val_acc improved from 0.66480 to 0.82440, saving model to PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.82.model\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.2785 - acc: 0.8822 - val_loss: 0.3650 - val_acc: 0.8244\n",
      "Epoch 5/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.8913\n",
      "Epoch 00005: val_acc did not improve from 0.82440\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.2519 - acc: 0.8915 - val_loss: 0.7918 - val_acc: 0.7292\n",
      "Epoch 6/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.2420 - acc: 0.8998\n",
      "Epoch 00006: val_acc did not improve from 0.82440\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.2421 - acc: 0.8999 - val_loss: 0.8423 - val_acc: 0.7148\n",
      "Epoch 7/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9061\n",
      "Epoch 00007: val_acc improved from 0.82440 to 0.88800, saving model to PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.89.model\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.2278 - acc: 0.9059 - val_loss: 0.2525 - val_acc: 0.8880\n",
      "Epoch 8/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9102\n",
      "Epoch 00008: val_acc did not improve from 0.88800\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.2153 - acc: 0.9102 - val_loss: 0.3061 - val_acc: 0.8760\n",
      "Epoch 9/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9132\n",
      "Epoch 00009: val_acc did not improve from 0.88800\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.2067 - acc: 0.9133 - val_loss: 0.4947 - val_acc: 0.8068\n",
      "Epoch 10/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9192\n",
      "Epoch 00010: val_acc improved from 0.88800 to 0.90720, saving model to PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.91.model\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.1982 - acc: 0.9191 - val_loss: 0.2244 - val_acc: 0.9072\n",
      "Epoch 11/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9196\n",
      "Epoch 00011: val_acc did not improve from 0.90720\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.1922 - acc: 0.9198 - val_loss: 0.2458 - val_acc: 0.8964\n",
      "Epoch 12/40\n",
      "16500/17000 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9259\n",
      "Epoch 00012: val_acc did not improve from 0.90720\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.1818 - acc: 0.9262 - val_loss: 0.3644 - val_acc: 0.8648\n",
      "Epoch 13/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9289\n",
      "Epoch 00013: val_acc did not improve from 0.90720\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.1731 - acc: 0.9288 - val_loss: 0.4786 - val_acc: 0.7960\n",
      "Epoch 14/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9316\n",
      "Epoch 00014: val_acc improved from 0.90720 to 0.91040, saving model to PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.91.model\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.1701 - acc: 0.9318 - val_loss: 0.2224 - val_acc: 0.9104\n",
      "Epoch 15/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9350\n",
      "Epoch 00015: val_acc did not improve from 0.91040\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.1604 - acc: 0.9350 - val_loss: 0.4501 - val_acc: 0.8220\n",
      "Epoch 16/40\n",
      "16400/17000 [===========================>..] - ETA: 0s - loss: 0.1573 - acc: 0.9351\n",
      "Epoch 00016: val_acc improved from 0.91040 to 0.92720, saving model to PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.93.model\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.1581 - acc: 0.9344 - val_loss: 0.2062 - val_acc: 0.9272\n",
      "Epoch 17/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9390\n",
      "Epoch 00017: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.1509 - acc: 0.9392 - val_loss: 0.3146 - val_acc: 0.8704\n",
      "Epoch 18/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9426\n",
      "Epoch 00018: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.1473 - acc: 0.9427 - val_loss: 0.2361 - val_acc: 0.9068\n",
      "Epoch 19/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9440\n",
      "Epoch 00019: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.1433 - acc: 0.9439 - val_loss: 0.3224 - val_acc: 0.8700\n",
      "Epoch 20/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9424\n",
      "Epoch 00020: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.1432 - acc: 0.9427 - val_loss: 0.2057 - val_acc: 0.9180\n",
      "Epoch 21/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9433\n",
      "Epoch 00021: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.1373 - acc: 0.9434 - val_loss: 0.3323 - val_acc: 0.8736\n",
      "Epoch 22/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9481\n",
      "Epoch 00022: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.1270 - acc: 0.9479 - val_loss: 0.2748 - val_acc: 0.8896\n",
      "Epoch 23/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9484\n",
      "Epoch 00023: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.1268 - acc: 0.9479 - val_loss: 0.2080 - val_acc: 0.9168\n",
      "Epoch 24/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9504\n",
      "Epoch 00024: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.1220 - acc: 0.9504 - val_loss: 0.2910 - val_acc: 0.8880\n",
      "Epoch 25/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9527\n",
      "Epoch 00025: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.1150 - acc: 0.9526 - val_loss: 0.2642 - val_acc: 0.8976\n",
      "Epoch 26/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9534\n",
      "Epoch 00026: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.1163 - acc: 0.9534 - val_loss: 0.3003 - val_acc: 0.8880\n",
      "Epoch 27/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9570\n",
      "Epoch 00027: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.1083 - acc: 0.9569 - val_loss: 0.2721 - val_acc: 0.9016\n",
      "Epoch 28/40\n",
      "16500/17000 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9567\n",
      "Epoch 00028: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.1097 - acc: 0.9564 - val_loss: 0.3717 - val_acc: 0.8788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9601\n",
      "Epoch 00029: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.1035 - acc: 0.9601 - val_loss: 0.2576 - val_acc: 0.9020\n",
      "Epoch 30/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9576\n",
      "Epoch 00030: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.1063 - acc: 0.9575 - val_loss: 0.2146 - val_acc: 0.9260\n",
      "Epoch 31/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9618\n",
      "Epoch 00031: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.0968 - acc: 0.9616 - val_loss: 0.2296 - val_acc: 0.9224\n",
      "Epoch 32/40\n",
      "16400/17000 [===========================>..] - ETA: 0s - loss: 0.0946 - acc: 0.9622\n",
      "Epoch 00032: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.0969 - acc: 0.9612 - val_loss: 0.2160 - val_acc: 0.9196\n",
      "Epoch 33/40\n",
      "16500/17000 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9583\n",
      "Epoch 00033: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.1006 - acc: 0.9584 - val_loss: 0.3584 - val_acc: 0.8688\n",
      "Epoch 34/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9649\n",
      "Epoch 00034: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.0942 - acc: 0.9648 - val_loss: 1.6809 - val_acc: 0.6676\n",
      "Epoch 35/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9665\n",
      "Epoch 00035: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.0845 - acc: 0.9664 - val_loss: 0.2783 - val_acc: 0.9152\n",
      "Epoch 36/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9663\n",
      "Epoch 00036: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.0874 - acc: 0.9664 - val_loss: 0.3933 - val_acc: 0.8836\n",
      "Epoch 37/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9647\n",
      "Epoch 00037: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.0886 - acc: 0.9649 - val_loss: 0.2681 - val_acc: 0.9076\n",
      "Epoch 38/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9664\n",
      "Epoch 00038: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.0873 - acc: 0.9663 - val_loss: 0.4033 - val_acc: 0.8696\n",
      "Epoch 39/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9674\n",
      "Epoch 00039: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.0828 - acc: 0.9676 - val_loss: 0.2334 - val_acc: 0.9236\n",
      "Epoch 40/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9652\n",
      "Epoch 00040: val_acc did not improve from 0.92720\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.0870 - acc: 0.9649 - val_loss: 0.4626 - val_acc: 0.8560\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 40)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17000, 10, 16), (4052, 10, 16, 2), (2500, 10, 16, 2))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTraining[:,:,:,0].shape, XTest.shape,XVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,10,16,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,10,16,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,10,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5897 - acc: 0.6910 - val_loss: 0.5435 - val_acc: 0.7240\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.5390 - acc: 0.7276 - val_loss: 0.5087 - val_acc: 0.7456\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5060 - acc: 0.7512 - val_loss: 0.4890 - val_acc: 0.7592\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4920 - acc: 0.7611 - val_loss: 0.4717 - val_acc: 0.7768\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.4721 - acc: 0.7751 - val_loss: 0.4834 - val_acc: 0.7648\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4648 - acc: 0.7763 - val_loss: 0.4533 - val_acc: 0.7800\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.4563 - acc: 0.7849 - val_loss: 0.4564 - val_acc: 0.7796\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4473 - acc: 0.7902 - val_loss: 0.4545 - val_acc: 0.7796\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4418 - acc: 0.7945 - val_loss: 0.4572 - val_acc: 0.7832\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4365 - acc: 0.7954 - val_loss: 0.4632 - val_acc: 0.7788\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4276 - acc: 0.7979 - val_loss: 0.4654 - val_acc: 0.7740\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4205 - acc: 0.8035 - val_loss: 0.4504 - val_acc: 0.7840\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4134 - acc: 0.8076 - val_loss: 0.4456 - val_acc: 0.7852\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4083 - acc: 0.8134 - val_loss: 0.4369 - val_acc: 0.7896\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4011 - acc: 0.8150 - val_loss: 0.4388 - val_acc: 0.7936\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3974 - acc: 0.8182 - val_loss: 0.4417 - val_acc: 0.7832\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3867 - acc: 0.8218 - val_loss: 0.4420 - val_acc: 0.7840\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3811 - acc: 0.8274 - val_loss: 0.4384 - val_acc: 0.7920\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3719 - acc: 0.8300 - val_loss: 0.4346 - val_acc: 0.7952\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.3697 - acc: 0.8329 - val_loss: 0.4630 - val_acc: 0.7828\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3645 - acc: 0.8338 - val_loss: 0.4427 - val_acc: 0.7916\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.3602 - acc: 0.8365 - val_loss: 0.4357 - val_acc: 0.7916\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.3538 - acc: 0.8391 - val_loss: 0.4457 - val_acc: 0.7892\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3476 - acc: 0.8448 - val_loss: 0.4455 - val_acc: 0.7852\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3376 - acc: 0.8492 - val_loss: 0.4399 - val_acc: 0.7912\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.3346 - acc: 0.8498 - val_loss: 0.4414 - val_acc: 0.7936\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3253 - acc: 0.8585 - val_loss: 0.4432 - val_acc: 0.7964\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3175 - acc: 0.8587 - val_loss: 0.4464 - val_acc: 0.7912\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.3155 - acc: 0.8578 - val_loss: 0.4625 - val_acc: 0.7828\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3073 - acc: 0.8635 - val_loss: 0.4628 - val_acc: 0.7984\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.5910 - acc: 0.6842 - val_loss: 0.5732 - val_acc: 0.6884\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.5326 - acc: 0.7345 - val_loss: 0.5184 - val_acc: 0.7500\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.5036 - acc: 0.7565 - val_loss: 0.4773 - val_acc: 0.7724\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4793 - acc: 0.7699 - val_loss: 0.4604 - val_acc: 0.7832\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4635 - acc: 0.7803 - val_loss: 0.4510 - val_acc: 0.7848\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4546 - acc: 0.7839 - val_loss: 0.4407 - val_acc: 0.7944\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4474 - acc: 0.7901 - val_loss: 0.4511 - val_acc: 0.7824\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4351 - acc: 0.7990 - val_loss: 0.4404 - val_acc: 0.7944\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4293 - acc: 0.7995 - val_loss: 0.4313 - val_acc: 0.7976\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4208 - acc: 0.8041 - val_loss: 0.4217 - val_acc: 0.7984\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4095 - acc: 0.8131 - val_loss: 0.4196 - val_acc: 0.8004\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4105 - acc: 0.8100 - val_loss: 0.4185 - val_acc: 0.8060\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4052 - acc: 0.8141 - val_loss: 0.4229 - val_acc: 0.8028\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3971 - acc: 0.8187 - val_loss: 0.4396 - val_acc: 0.8012\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3918 - acc: 0.8226 - val_loss: 0.4216 - val_acc: 0.8020\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3876 - acc: 0.8239 - val_loss: 0.4191 - val_acc: 0.8020\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3824 - acc: 0.8284 - val_loss: 0.4243 - val_acc: 0.8048\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3722 - acc: 0.8313 - val_loss: 0.4163 - val_acc: 0.8028\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3673 - acc: 0.8332 - val_loss: 0.4131 - val_acc: 0.8124\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3636 - acc: 0.8346 - val_loss: 0.4175 - val_acc: 0.8052\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3544 - acc: 0.8396 - val_loss: 0.4065 - val_acc: 0.8180\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3538 - acc: 0.8417 - val_loss: 0.4187 - val_acc: 0.8104\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3459 - acc: 0.8462 - val_loss: 0.4234 - val_acc: 0.8004\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3456 - acc: 0.8456 - val_loss: 0.4169 - val_acc: 0.8056\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3410 - acc: 0.8479 - val_loss: 0.4109 - val_acc: 0.8144\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3332 - acc: 0.8527 - val_loss: 0.4206 - val_acc: 0.8044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3345 - acc: 0.8508 - val_loss: 0.4172 - val_acc: 0.8024\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3280 - acc: 0.8545 - val_loss: 0.4236 - val_acc: 0.8036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.3270 - acc: 0.8542 - val_loss: 0.4120 - val_acc: 0.8104\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3185 - acc: 0.8611 - val_loss: 0.4224 - val_acc: 0.8044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6199 - acc: 0.6443 - val_loss: 0.5353 - val_acc: 0.7284\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.5326 - acc: 0.7352 - val_loss: 0.5247 - val_acc: 0.7376\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4956 - acc: 0.7617 - val_loss: 0.4718 - val_acc: 0.7784\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4759 - acc: 0.7755 - val_loss: 0.4622 - val_acc: 0.7836\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.4619 - acc: 0.7843 - val_loss: 0.4410 - val_acc: 0.7952\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4528 - acc: 0.7878 - val_loss: 0.4391 - val_acc: 0.7964\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.4428 - acc: 0.7929 - val_loss: 0.4335 - val_acc: 0.7940\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4334 - acc: 0.8022 - val_loss: 0.4246 - val_acc: 0.7992\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4278 - acc: 0.8036 - val_loss: 0.4264 - val_acc: 0.8012\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4173 - acc: 0.8102 - val_loss: 0.4218 - val_acc: 0.7988\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4153 - acc: 0.8133 - val_loss: 0.4179 - val_acc: 0.8076\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4125 - acc: 0.8087 - val_loss: 0.4140 - val_acc: 0.8064\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4009 - acc: 0.8166 - val_loss: 0.4603 - val_acc: 0.7836\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3987 - acc: 0.8195 - val_loss: 0.4112 - val_acc: 0.8124\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3904 - acc: 0.8230 - val_loss: 0.4089 - val_acc: 0.8108\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3923 - acc: 0.8229 - val_loss: 0.4076 - val_acc: 0.8132\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3837 - acc: 0.8302 - val_loss: 0.4081 - val_acc: 0.8172\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3779 - acc: 0.8272 - val_loss: 0.4207 - val_acc: 0.8052\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3722 - acc: 0.8319 - val_loss: 0.4234 - val_acc: 0.8012\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3691 - acc: 0.8319 - val_loss: 0.4242 - val_acc: 0.8048\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3690 - acc: 0.8315 - val_loss: 0.4073 - val_acc: 0.8160\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3593 - acc: 0.8387 - val_loss: 0.4326 - val_acc: 0.8040\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3546 - acc: 0.8407 - val_loss: 0.4282 - val_acc: 0.7940\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3567 - acc: 0.8414 - val_loss: 0.4067 - val_acc: 0.8160\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3486 - acc: 0.8432 - val_loss: 0.4103 - val_acc: 0.8132\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3443 - acc: 0.8457 - val_loss: 0.4060 - val_acc: 0.8200\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3429 - acc: 0.8463 - val_loss: 0.4143 - val_acc: 0.8144\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3382 - acc: 0.8472 - val_loss: 0.4100 - val_acc: 0.8120\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3382 - acc: 0.8522 - val_loss: 0.4087 - val_acc: 0.8200\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3363 - acc: 0.8526 - val_loss: 0.4082 - val_acc: 0.8184\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.5743 - acc: 0.6997 - val_loss: 0.5266 - val_acc: 0.7304\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.5183 - acc: 0.7379 - val_loss: 0.5487 - val_acc: 0.7168\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.4853 - acc: 0.7648 - val_loss: 0.4676 - val_acc: 0.7720\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4624 - acc: 0.7780 - val_loss: 0.4516 - val_acc: 0.7868\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4501 - acc: 0.7872 - val_loss: 0.4469 - val_acc: 0.7904\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4410 - acc: 0.7946 - val_loss: 0.4435 - val_acc: 0.7892\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4275 - acc: 0.8019 - val_loss: 0.4359 - val_acc: 0.7912\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4191 - acc: 0.8050 - val_loss: 0.4394 - val_acc: 0.7936\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4122 - acc: 0.8067 - val_loss: 0.4413 - val_acc: 0.7840\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 41us/sample - loss: 0.4019 - acc: 0.8189 - val_loss: 0.4327 - val_acc: 0.7980\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3966 - acc: 0.8191 - val_loss: 0.4394 - val_acc: 0.7976\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3884 - acc: 0.8214 - val_loss: 0.4287 - val_acc: 0.7972\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3831 - acc: 0.8268 - val_loss: 0.4292 - val_acc: 0.7964\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3720 - acc: 0.8299 - val_loss: 0.4314 - val_acc: 0.7956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3692 - acc: 0.8329 - val_loss: 0.4455 - val_acc: 0.7944\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3616 - acc: 0.8400 - val_loss: 0.4384 - val_acc: 0.7928\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3561 - acc: 0.8395 - val_loss: 0.4312 - val_acc: 0.8008\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3414 - acc: 0.8486 - val_loss: 0.4349 - val_acc: 0.7976\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3420 - acc: 0.8471 - val_loss: 0.4419 - val_acc: 0.7972\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3332 - acc: 0.8504 - val_loss: 0.4369 - val_acc: 0.7988\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3277 - acc: 0.8542 - val_loss: 0.4444 - val_acc: 0.7940\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3181 - acc: 0.8592 - val_loss: 0.4695 - val_acc: 0.7912\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3133 - acc: 0.8638 - val_loss: 0.4659 - val_acc: 0.7980\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3102 - acc: 0.8619 - val_loss: 0.4476 - val_acc: 0.7948\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3050 - acc: 0.8659 - val_loss: 0.4602 - val_acc: 0.7940\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2918 - acc: 0.8718 - val_loss: 0.4651 - val_acc: 0.7936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.2945 - acc: 0.8695 - val_loss: 0.4732 - val_acc: 0.7740\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.2800 - acc: 0.8793 - val_loss: 0.4774 - val_acc: 0.7940\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.2738 - acc: 0.8805 - val_loss: 0.4594 - val_acc: 0.7952\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 38us/sample - loss: 0.2697 - acc: 0.8835 - val_loss: 0.4818 - val_acc: 0.7824\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5877 - acc: 0.6851 - val_loss: 0.5328 - val_acc: 0.7308\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.5239 - acc: 0.7391 - val_loss: 0.4935 - val_acc: 0.7576\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4956 - acc: 0.7589 - val_loss: 0.4576 - val_acc: 0.7860\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4663 - acc: 0.7781 - val_loss: 0.4434 - val_acc: 0.7888\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4525 - acc: 0.7853 - val_loss: 0.4670 - val_acc: 0.7752\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4365 - acc: 0.7985 - val_loss: 0.4297 - val_acc: 0.8032\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4282 - acc: 0.7989 - val_loss: 0.4279 - val_acc: 0.7992\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4216 - acc: 0.8036 - val_loss: 0.4272 - val_acc: 0.8056\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4142 - acc: 0.8088 - val_loss: 0.4218 - val_acc: 0.7980\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4069 - acc: 0.8124 - val_loss: 0.4186 - val_acc: 0.8060\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3971 - acc: 0.8192 - val_loss: 0.4450 - val_acc: 0.7896\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3893 - acc: 0.8241 - val_loss: 0.4173 - val_acc: 0.8124\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3822 - acc: 0.8254 - val_loss: 0.4208 - val_acc: 0.8060\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3750 - acc: 0.8324 - val_loss: 0.4425 - val_acc: 0.7928\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3683 - acc: 0.8295 - val_loss: 0.4163 - val_acc: 0.8104\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3606 - acc: 0.8401 - val_loss: 0.4153 - val_acc: 0.8104\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3562 - acc: 0.8408 - val_loss: 0.4255 - val_acc: 0.8040\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.3547 - acc: 0.8451 - val_loss: 0.4179 - val_acc: 0.8068\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3338 - acc: 0.8519 - val_loss: 0.4264 - val_acc: 0.8068\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3334 - acc: 0.8489 - val_loss: 0.4237 - val_acc: 0.8044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3250 - acc: 0.8553 - val_loss: 0.4100 - val_acc: 0.8152\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3227 - acc: 0.8571 - val_loss: 0.4155 - val_acc: 0.8052\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3107 - acc: 0.8628 - val_loss: 0.4376 - val_acc: 0.8036\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3110 - acc: 0.8664 - val_loss: 0.4286 - val_acc: 0.8000\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3082 - acc: 0.8659 - val_loss: 0.4189 - val_acc: 0.8080\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2977 - acc: 0.8705 - val_loss: 0.4196 - val_acc: 0.8080\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2943 - acc: 0.8708 - val_loss: 0.4313 - val_acc: 0.8072\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.2823 - acc: 0.8777 - val_loss: 0.4266 - val_acc: 0.8128\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2812 - acc: 0.8775 - val_loss: 0.4269 - val_acc: 0.8076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2780 - acc: 0.8775 - val_loss: 0.4259 - val_acc: 0.8008\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.5957 - acc: 0.6774 - val_loss: 0.5272 - val_acc: 0.7312\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.5280 - acc: 0.7366 - val_loss: 0.5029 - val_acc: 0.7484\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4926 - acc: 0.7611 - val_loss: 0.4671 - val_acc: 0.7756\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4662 - acc: 0.7812 - val_loss: 0.4490 - val_acc: 0.7876\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4533 - acc: 0.7886 - val_loss: 0.4614 - val_acc: 0.7780\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4395 - acc: 0.7952 - val_loss: 0.4398 - val_acc: 0.7960\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4347 - acc: 0.7990 - val_loss: 0.4372 - val_acc: 0.7980\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4226 - acc: 0.8052 - val_loss: 0.4307 - val_acc: 0.7960\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4184 - acc: 0.8077 - val_loss: 0.4216 - val_acc: 0.8052\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4090 - acc: 0.8159 - val_loss: 0.4819 - val_acc: 0.7768\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4036 - acc: 0.8166 - val_loss: 0.4191 - val_acc: 0.8104\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3958 - acc: 0.8215 - val_loss: 0.4204 - val_acc: 0.8064\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3896 - acc: 0.8211 - val_loss: 0.4182 - val_acc: 0.8092\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3803 - acc: 0.8291 - val_loss: 0.4181 - val_acc: 0.8112\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3785 - acc: 0.8311 - val_loss: 0.4140 - val_acc: 0.8096\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3728 - acc: 0.8339 - val_loss: 0.4269 - val_acc: 0.7984\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3661 - acc: 0.8371 - val_loss: 0.4506 - val_acc: 0.7996\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3576 - acc: 0.8399 - val_loss: 0.4095 - val_acc: 0.8124\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3576 - acc: 0.8407 - val_loss: 0.4158 - val_acc: 0.8096\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3488 - acc: 0.8431 - val_loss: 0.4265 - val_acc: 0.8040\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3440 - acc: 0.8469 - val_loss: 0.4178 - val_acc: 0.8096\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3453 - acc: 0.8466 - val_loss: 0.4205 - val_acc: 0.8092\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3357 - acc: 0.8536 - val_loss: 0.4071 - val_acc: 0.8156\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3314 - acc: 0.8529 - val_loss: 0.4225 - val_acc: 0.8104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3230 - acc: 0.8575 - val_loss: 0.4181 - val_acc: 0.8112\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3171 - acc: 0.8625 - val_loss: 0.4135 - val_acc: 0.8108\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3189 - acc: 0.8594 - val_loss: 0.4132 - val_acc: 0.8080\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3087 - acc: 0.8648 - val_loss: 0.4370 - val_acc: 0.8012\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3089 - acc: 0.8658 - val_loss: 0.4276 - val_acc: 0.8012\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3054 - acc: 0.8661 - val_loss: 0.4192 - val_acc: 0.8100\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.5873 - acc: 0.6900 - val_loss: 0.5361 - val_acc: 0.7312\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.5270 - acc: 0.7345 - val_loss: 0.4983 - val_acc: 0.7540\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4828 - acc: 0.7665 - val_loss: 0.5404 - val_acc: 0.7152\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4661 - acc: 0.7766 - val_loss: 0.4613 - val_acc: 0.7704\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.4576 - acc: 0.7833 - val_loss: 0.4630 - val_acc: 0.7748\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.4400 - acc: 0.7927 - val_loss: 0.4494 - val_acc: 0.7856\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4311 - acc: 0.7994 - val_loss: 0.4493 - val_acc: 0.7884\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4284 - acc: 0.8035 - val_loss: 0.4618 - val_acc: 0.7768\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.4134 - acc: 0.8108 - val_loss: 0.4439 - val_acc: 0.7880\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4111 - acc: 0.8112 - val_loss: 0.4458 - val_acc: 0.7872\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.4009 - acc: 0.8151 - val_loss: 0.4546 - val_acc: 0.7912\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3952 - acc: 0.8200 - val_loss: 0.4663 - val_acc: 0.7728\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 39us/sample - loss: 0.3829 - acc: 0.8269 - val_loss: 0.4466 - val_acc: 0.7900\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.3817 - acc: 0.8294 - val_loss: 0.4441 - val_acc: 0.7832\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3660 - acc: 0.8351 - val_loss: 0.4433 - val_acc: 0.7896\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3611 - acc: 0.8399 - val_loss: 0.4383 - val_acc: 0.7868\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3525 - acc: 0.8418 - val_loss: 0.4473 - val_acc: 0.7924\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3461 - acc: 0.8418 - val_loss: 0.4423 - val_acc: 0.7912\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3368 - acc: 0.8508 - val_loss: 0.4451 - val_acc: 0.7980\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3285 - acc: 0.8521 - val_loss: 0.4425 - val_acc: 0.7972\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.3233 - acc: 0.8563 - val_loss: 0.4457 - val_acc: 0.7940\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 42us/sample - loss: 0.3173 - acc: 0.8612 - val_loss: 0.4424 - val_acc: 0.7960\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3122 - acc: 0.8612 - val_loss: 0.4409 - val_acc: 0.7980\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3085 - acc: 0.8639 - val_loss: 0.4444 - val_acc: 0.7960\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.3006 - acc: 0.8654 - val_loss: 0.4600 - val_acc: 0.7956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2922 - acc: 0.8728 - val_loss: 0.4579 - val_acc: 0.7920\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2837 - acc: 0.8745 - val_loss: 0.4687 - val_acc: 0.7916\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2797 - acc: 0.8764 - val_loss: 0.4732 - val_acc: 0.7936\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2733 - acc: 0.8822 - val_loss: 0.4727 - val_acc: 0.7972\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 40us/sample - loss: 0.2710 - acc: 0.8825 - val_loss: 0.4640 - val_acc: 0.7940\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.5816 - acc: 0.6972 - val_loss: 0.5314 - val_acc: 0.7348\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.5254 - acc: 0.7369 - val_loss: 0.4874 - val_acc: 0.7636\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4921 - acc: 0.7596 - val_loss: 0.4653 - val_acc: 0.7780\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.4666 - acc: 0.7748 - val_loss: 0.4583 - val_acc: 0.7852\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4505 - acc: 0.7906 - val_loss: 0.4560 - val_acc: 0.7840\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4352 - acc: 0.7988 - val_loss: 0.4512 - val_acc: 0.7864\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4283 - acc: 0.8014 - val_loss: 0.4416 - val_acc: 0.7936\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.4197 - acc: 0.8055 - val_loss: 0.4312 - val_acc: 0.7984\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4105 - acc: 0.8126 - val_loss: 0.4249 - val_acc: 0.8012\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4034 - acc: 0.8147 - val_loss: 0.4275 - val_acc: 0.7984\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3957 - acc: 0.8174 - val_loss: 0.4163 - val_acc: 0.8020\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3860 - acc: 0.8197 - val_loss: 0.4190 - val_acc: 0.8032\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3794 - acc: 0.8296 - val_loss: 0.4218 - val_acc: 0.8052\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3670 - acc: 0.8350 - val_loss: 0.4145 - val_acc: 0.8040\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3617 - acc: 0.8402 - val_loss: 0.4155 - val_acc: 0.8016\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3579 - acc: 0.8372 - val_loss: 0.4299 - val_acc: 0.8076\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3426 - acc: 0.8469 - val_loss: 0.4212 - val_acc: 0.8016\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3431 - acc: 0.8451 - val_loss: 0.4062 - val_acc: 0.8140\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.3240 - acc: 0.8545 - val_loss: 0.4137 - val_acc: 0.8044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3205 - acc: 0.8566 - val_loss: 0.4148 - val_acc: 0.8080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3228 - acc: 0.8561 - val_loss: 0.4525 - val_acc: 0.7912\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3139 - acc: 0.8621 - val_loss: 0.4175 - val_acc: 0.8072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.3077 - acc: 0.8666 - val_loss: 0.4216 - val_acc: 0.8124\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.2976 - acc: 0.8698 - val_loss: 0.4319 - val_acc: 0.8072\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2885 - acc: 0.8759 - val_loss: 0.4243 - val_acc: 0.8044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.2837 - acc: 0.8766 - val_loss: 0.4225 - val_acc: 0.8088\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2789 - acc: 0.8813 - val_loss: 0.4299 - val_acc: 0.8060\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2741 - acc: 0.8803 - val_loss: 0.4356 - val_acc: 0.8064\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2660 - acc: 0.8833 - val_loss: 0.4305 - val_acc: 0.8116\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.2597 - acc: 0.8894 - val_loss: 0.4441 - val_acc: 0.8048\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6016 - acc: 0.6775 - val_loss: 0.5339 - val_acc: 0.7296\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.5227 - acc: 0.7445 - val_loss: 0.4856 - val_acc: 0.7588\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4833 - acc: 0.7688 - val_loss: 0.4610 - val_acc: 0.7816\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4664 - acc: 0.7806 - val_loss: 0.4439 - val_acc: 0.7912\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4552 - acc: 0.7885 - val_loss: 0.4422 - val_acc: 0.7980\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4352 - acc: 0.7994 - val_loss: 0.4859 - val_acc: 0.7736\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4276 - acc: 0.8040 - val_loss: 0.4241 - val_acc: 0.8008\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4175 - acc: 0.8094 - val_loss: 0.4189 - val_acc: 0.8032\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4092 - acc: 0.8149 - val_loss: 0.4156 - val_acc: 0.8028\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4018 - acc: 0.8172 - val_loss: 0.4146 - val_acc: 0.8132\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3982 - acc: 0.8210 - val_loss: 0.4275 - val_acc: 0.8000\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3881 - acc: 0.8252 - val_loss: 0.4188 - val_acc: 0.8044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3791 - acc: 0.8312 - val_loss: 0.4185 - val_acc: 0.8076\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3689 - acc: 0.8353 - val_loss: 0.4194 - val_acc: 0.8060\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3564 - acc: 0.8401 - val_loss: 0.4151 - val_acc: 0.8108\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3578 - acc: 0.8380 - val_loss: 0.4156 - val_acc: 0.8100\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3533 - acc: 0.8422 - val_loss: 0.4114 - val_acc: 0.8116\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3448 - acc: 0.8466 - val_loss: 0.4358 - val_acc: 0.8072\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3398 - acc: 0.8478 - val_loss: 0.4175 - val_acc: 0.8112\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3288 - acc: 0.8534 - val_loss: 0.4095 - val_acc: 0.8120\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3202 - acc: 0.8583 - val_loss: 0.4148 - val_acc: 0.8176\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3154 - acc: 0.8615 - val_loss: 0.4202 - val_acc: 0.8096\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3195 - acc: 0.8608 - val_loss: 0.4365 - val_acc: 0.8124\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3074 - acc: 0.8641 - val_loss: 0.4256 - val_acc: 0.8092\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2987 - acc: 0.8699 - val_loss: 0.4260 - val_acc: 0.8064\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2995 - acc: 0.8694 - val_loss: 0.4423 - val_acc: 0.8096\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2954 - acc: 0.8704 - val_loss: 0.4412 - val_acc: 0.8048\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2907 - acc: 0.8729 - val_loss: 0.4290 - val_acc: 0.8156\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2832 - acc: 0.8774 - val_loss: 0.4298 - val_acc: 0.8076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2834 - acc: 0.8763 - val_loss: 0.4225 - val_acc: 0.8144\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.5841 - acc: 0.6903 - val_loss: 0.5456 - val_acc: 0.7196\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.5145 - acc: 0.7416 - val_loss: 0.4763 - val_acc: 0.7660\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4724 - acc: 0.7774 - val_loss: 0.4811 - val_acc: 0.7604\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.4549 - acc: 0.7841 - val_loss: 0.4479 - val_acc: 0.7844\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4396 - acc: 0.7929 - val_loss: 0.4781 - val_acc: 0.7784\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4304 - acc: 0.8015 - val_loss: 0.4418 - val_acc: 0.7916\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4143 - acc: 0.8105 - val_loss: 0.4547 - val_acc: 0.7856\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4070 - acc: 0.8099 - val_loss: 0.4421 - val_acc: 0.7996\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3892 - acc: 0.8203 - val_loss: 0.4501 - val_acc: 0.7820\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3857 - acc: 0.8254 - val_loss: 0.4499 - val_acc: 0.7780\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3712 - acc: 0.8304 - val_loss: 0.4414 - val_acc: 0.7932\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3577 - acc: 0.8389 - val_loss: 0.4302 - val_acc: 0.8040\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3474 - acc: 0.8451 - val_loss: 0.4344 - val_acc: 0.8000\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.3410 - acc: 0.8498 - val_loss: 0.4405 - val_acc: 0.8000\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 45us/sample - loss: 0.3336 - acc: 0.8504 - val_loss: 0.4474 - val_acc: 0.7908\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3144 - acc: 0.8600 - val_loss: 0.4481 - val_acc: 0.7960\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3083 - acc: 0.8638 - val_loss: 0.4730 - val_acc: 0.7864\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2944 - acc: 0.8715 - val_loss: 0.4548 - val_acc: 0.7892\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2932 - acc: 0.8698 - val_loss: 0.4587 - val_acc: 0.8016\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2816 - acc: 0.8787 - val_loss: 0.4655 - val_acc: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2750 - acc: 0.8817 - val_loss: 0.4856 - val_acc: 0.7968\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2615 - acc: 0.8908 - val_loss: 0.4785 - val_acc: 0.7908\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2531 - acc: 0.8919 - val_loss: 0.4830 - val_acc: 0.7952\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2441 - acc: 0.8955 - val_loss: 0.4999 - val_acc: 0.7924\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2357 - acc: 0.9002 - val_loss: 0.5205 - val_acc: 0.7884\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2312 - acc: 0.9024 - val_loss: 0.5033 - val_acc: 0.7928\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2217 - acc: 0.9071 - val_loss: 0.5299 - val_acc: 0.7956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2144 - acc: 0.9121 - val_loss: 0.5237 - val_acc: 0.7872\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2110 - acc: 0.9133 - val_loss: 0.5262 - val_acc: 0.7900\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2042 - acc: 0.9170 - val_loss: 0.5643 - val_acc: 0.7908\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.5875 - acc: 0.6908 - val_loss: 0.5146 - val_acc: 0.7396\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.5084 - acc: 0.7483 - val_loss: 0.5156 - val_acc: 0.7408\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4770 - acc: 0.7702 - val_loss: 0.4471 - val_acc: 0.7924\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4560 - acc: 0.7851 - val_loss: 0.4604 - val_acc: 0.7816\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4415 - acc: 0.7954 - val_loss: 0.4330 - val_acc: 0.7956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4294 - acc: 0.7987 - val_loss: 0.4271 - val_acc: 0.8088\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4120 - acc: 0.8113 - val_loss: 0.4311 - val_acc: 0.8016\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4018 - acc: 0.8169 - val_loss: 0.4248 - val_acc: 0.8000\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3939 - acc: 0.8209 - val_loss: 0.4348 - val_acc: 0.7948\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3830 - acc: 0.8274 - val_loss: 0.4410 - val_acc: 0.7996\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3724 - acc: 0.8307 - val_loss: 0.4246 - val_acc: 0.8028\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3662 - acc: 0.8346 - val_loss: 0.4212 - val_acc: 0.8052\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3538 - acc: 0.8418 - val_loss: 0.4246 - val_acc: 0.8072\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3438 - acc: 0.8454 - val_loss: 0.4356 - val_acc: 0.7992\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3348 - acc: 0.8502 - val_loss: 0.4244 - val_acc: 0.7996\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3226 - acc: 0.8579 - val_loss: 0.4326 - val_acc: 0.8024\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3106 - acc: 0.8675 - val_loss: 0.4523 - val_acc: 0.7924\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2992 - acc: 0.8694 - val_loss: 0.4349 - val_acc: 0.8012\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2917 - acc: 0.8743 - val_loss: 0.4378 - val_acc: 0.8016\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2846 - acc: 0.8747 - val_loss: 0.4329 - val_acc: 0.8092\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2748 - acc: 0.8831 - val_loss: 0.4396 - val_acc: 0.8020\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2679 - acc: 0.8827 - val_loss: 0.4435 - val_acc: 0.8024\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2634 - acc: 0.8884 - val_loss: 0.4367 - val_acc: 0.8020\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2537 - acc: 0.8884 - val_loss: 0.4552 - val_acc: 0.8048\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2398 - acc: 0.8973 - val_loss: 0.4622 - val_acc: 0.7944\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2307 - acc: 0.9039 - val_loss: 0.4704 - val_acc: 0.8028\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2273 - acc: 0.9055 - val_loss: 0.4841 - val_acc: 0.7992\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2175 - acc: 0.9092 - val_loss: 0.4690 - val_acc: 0.7980\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2168 - acc: 0.9078 - val_loss: 0.4777 - val_acc: 0.8056\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2114 - acc: 0.9110 - val_loss: 0.4809 - val_acc: 0.7992\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.5924 - acc: 0.6836 - val_loss: 0.5350 - val_acc: 0.7396\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.5166 - acc: 0.7454 - val_loss: 0.5182 - val_acc: 0.7512\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4872 - acc: 0.7651 - val_loss: 0.4698 - val_acc: 0.7744\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4580 - acc: 0.7860 - val_loss: 0.4451 - val_acc: 0.7860\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.4450 - acc: 0.7931 - val_loss: 0.4302 - val_acc: 0.8024\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4309 - acc: 0.8015 - val_loss: 0.4273 - val_acc: 0.7952\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4201 - acc: 0.8059 - val_loss: 0.4242 - val_acc: 0.8016\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4070 - acc: 0.8142 - val_loss: 0.4112 - val_acc: 0.8092\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3970 - acc: 0.8221 - val_loss: 0.4276 - val_acc: 0.8032\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3911 - acc: 0.8222 - val_loss: 0.4139 - val_acc: 0.8096\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3791 - acc: 0.8301 - val_loss: 0.4192 - val_acc: 0.8100\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3707 - acc: 0.8346 - val_loss: 0.4198 - val_acc: 0.8044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3616 - acc: 0.8352 - val_loss: 0.4178 - val_acc: 0.8024\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3501 - acc: 0.8455 - val_loss: 0.4235 - val_acc: 0.8044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3465 - acc: 0.8452 - val_loss: 0.4131 - val_acc: 0.8136\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3346 - acc: 0.8529 - val_loss: 0.4482 - val_acc: 0.7924\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3240 - acc: 0.8562 - val_loss: 0.4220 - val_acc: 0.8080\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3173 - acc: 0.8591 - val_loss: 0.4272 - val_acc: 0.8068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3114 - acc: 0.8633 - val_loss: 0.4207 - val_acc: 0.8108\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2988 - acc: 0.8687 - val_loss: 0.4409 - val_acc: 0.8080\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2922 - acc: 0.8743 - val_loss: 0.4472 - val_acc: 0.8116\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2767 - acc: 0.8811 - val_loss: 0.4227 - val_acc: 0.8116\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2792 - acc: 0.8812 - val_loss: 0.4381 - val_acc: 0.8068\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2681 - acc: 0.8811 - val_loss: 0.4390 - val_acc: 0.8044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2656 - acc: 0.8861 - val_loss: 0.4463 - val_acc: 0.8060\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2615 - acc: 0.8864 - val_loss: 0.4320 - val_acc: 0.8068\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2524 - acc: 0.8944 - val_loss: 0.4383 - val_acc: 0.8108\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2468 - acc: 0.8949 - val_loss: 0.4377 - val_acc: 0.8036\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2421 - acc: 0.8960 - val_loss: 0.4599 - val_acc: 0.8076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2311 - acc: 0.9025 - val_loss: 0.4652 - val_acc: 0.8056\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.5733 - acc: 0.6954 - val_loss: 0.5124 - val_acc: 0.7400\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.5040 - acc: 0.7494 - val_loss: 0.4716 - val_acc: 0.7732\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4724 - acc: 0.7747 - val_loss: 0.4523 - val_acc: 0.7880\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4457 - acc: 0.7927 - val_loss: 0.4548 - val_acc: 0.7792\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4306 - acc: 0.7985 - val_loss: 0.4318 - val_acc: 0.7948\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.4076 - acc: 0.8098 - val_loss: 0.4316 - val_acc: 0.8004\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.3866 - acc: 0.8235 - val_loss: 0.4206 - val_acc: 0.8008\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3710 - acc: 0.8354 - val_loss: 0.4155 - val_acc: 0.8032\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3546 - acc: 0.8419 - val_loss: 0.4248 - val_acc: 0.8052\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3353 - acc: 0.8501 - val_loss: 0.4336 - val_acc: 0.8000\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.3089 - acc: 0.8651 - val_loss: 0.4290 - val_acc: 0.8040\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2880 - acc: 0.8746 - val_loss: 0.4543 - val_acc: 0.7960\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 43us/sample - loss: 0.2677 - acc: 0.8835 - val_loss: 0.4563 - val_acc: 0.7984\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.2418 - acc: 0.8969 - val_loss: 0.5091 - val_acc: 0.7956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.2169 - acc: 0.9081 - val_loss: 0.4757 - val_acc: 0.8096\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.1896 - acc: 0.9244 - val_loss: 0.5278 - val_acc: 0.7976\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.1770 - acc: 0.9287 - val_loss: 0.5749 - val_acc: 0.7892\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.1436 - acc: 0.9432 - val_loss: 0.5677 - val_acc: 0.8064\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.1274 - acc: 0.9493 - val_loss: 0.5968 - val_acc: 0.7960\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.1176 - acc: 0.9549 - val_loss: 0.7286 - val_acc: 0.7920\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.1006 - acc: 0.9613 - val_loss: 0.6705 - val_acc: 0.7912\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.0908 - acc: 0.9664 - val_loss: 0.7499 - val_acc: 0.7936\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.0804 - acc: 0.9689 - val_loss: 0.7313 - val_acc: 0.7948\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.0742 - acc: 0.9717 - val_loss: 0.8186 - val_acc: 0.7952\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.0715 - acc: 0.9736 - val_loss: 0.8081 - val_acc: 0.7960\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.0585 - acc: 0.9783 - val_loss: 0.8921 - val_acc: 0.7964\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.0607 - acc: 0.9778 - val_loss: 0.8439 - val_acc: 0.7980\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.0532 - acc: 0.9809 - val_loss: 0.8834 - val_acc: 0.7964\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.0498 - acc: 0.9821 - val_loss: 0.8493 - val_acc: 0.7976\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 44us/sample - loss: 0.0505 - acc: 0.9820 - val_loss: 0.9452 - val_acc: 0.7872\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.5823 - acc: 0.6957 - val_loss: 0.5158 - val_acc: 0.7476\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.5170 - acc: 0.7434 - val_loss: 0.4813 - val_acc: 0.7728\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.4914 - acc: 0.7612 - val_loss: 0.4728 - val_acc: 0.7740\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.4645 - acc: 0.7784 - val_loss: 0.4545 - val_acc: 0.7860\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4582 - acc: 0.7808 - val_loss: 0.4782 - val_acc: 0.7628\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4398 - acc: 0.7922 - val_loss: 0.4421 - val_acc: 0.7896\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.4335 - acc: 0.7975 - val_loss: 0.4285 - val_acc: 0.7956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4233 - acc: 0.8041 - val_loss: 0.4186 - val_acc: 0.8076\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.4113 - acc: 0.8108 - val_loss: 0.4346 - val_acc: 0.7952\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4058 - acc: 0.8143 - val_loss: 0.4128 - val_acc: 0.8092\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3974 - acc: 0.8181 - val_loss: 0.4153 - val_acc: 0.8076\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3919 - acc: 0.8209 - val_loss: 0.4185 - val_acc: 0.8072\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3838 - acc: 0.8219 - val_loss: 0.4158 - val_acc: 0.8108\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3757 - acc: 0.8285 - val_loss: 0.4108 - val_acc: 0.8120\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3729 - acc: 0.8328 - val_loss: 0.4075 - val_acc: 0.8112\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3604 - acc: 0.8359 - val_loss: 0.4072 - val_acc: 0.8084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3576 - acc: 0.8369 - val_loss: 0.4103 - val_acc: 0.8048\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3539 - acc: 0.8406 - val_loss: 0.4009 - val_acc: 0.8104\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3420 - acc: 0.8475 - val_loss: 0.4129 - val_acc: 0.8108\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3409 - acc: 0.8462 - val_loss: 0.4104 - val_acc: 0.8156\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3340 - acc: 0.8501 - val_loss: 0.4063 - val_acc: 0.8152\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3220 - acc: 0.8568 - val_loss: 0.4077 - val_acc: 0.8180\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3214 - acc: 0.8564 - val_loss: 0.4107 - val_acc: 0.8100\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3170 - acc: 0.8581 - val_loss: 0.4095 - val_acc: 0.8152\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3093 - acc: 0.8649 - val_loss: 0.4129 - val_acc: 0.8080\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3034 - acc: 0.8668 - val_loss: 0.4204 - val_acc: 0.8072\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2980 - acc: 0.8683 - val_loss: 0.4174 - val_acc: 0.8168\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2904 - acc: 0.8732 - val_loss: 0.4331 - val_acc: 0.8104\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2887 - acc: 0.8712 - val_loss: 0.4210 - val_acc: 0.8104\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2866 - acc: 0.8729 - val_loss: 0.4382 - val_acc: 0.8096\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6052 - acc: 0.6686 - val_loss: 0.5769 - val_acc: 0.6964\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.5336 - acc: 0.7361 - val_loss: 0.4904 - val_acc: 0.7652\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4978 - acc: 0.7585 - val_loss: 0.4738 - val_acc: 0.7804\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.4751 - acc: 0.776 - 1s 55us/sample - loss: 0.4737 - acc: 0.7772 - val_loss: 0.4492 - val_acc: 0.7928\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4615 - acc: 0.7822 - val_loss: 0.4430 - val_acc: 0.7928\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4517 - acc: 0.7882 - val_loss: 0.4627 - val_acc: 0.7784\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4439 - acc: 0.7940 - val_loss: 0.4400 - val_acc: 0.7932\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4395 - acc: 0.7978 - val_loss: 0.4231 - val_acc: 0.7988\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4306 - acc: 0.8034 - val_loss: 0.4294 - val_acc: 0.7948\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4269 - acc: 0.8046 - val_loss: 0.4333 - val_acc: 0.7976\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4175 - acc: 0.8101 - val_loss: 0.4233 - val_acc: 0.8064\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4144 - acc: 0.8104 - val_loss: 0.4219 - val_acc: 0.8020\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4106 - acc: 0.8122 - val_loss: 0.4180 - val_acc: 0.8032\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4045 - acc: 0.8164 - val_loss: 0.4177 - val_acc: 0.8100\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4023 - acc: 0.8191 - val_loss: 0.4290 - val_acc: 0.8040\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3883 - acc: 0.8289 - val_loss: 0.4231 - val_acc: 0.8028\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3862 - acc: 0.8273 - val_loss: 0.4198 - val_acc: 0.8072\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3890 - acc: 0.8240 - val_loss: 0.4380 - val_acc: 0.8060\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3808 - acc: 0.8302 - val_loss: 0.4161 - val_acc: 0.8092\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3749 - acc: 0.8335 - val_loss: 0.4166 - val_acc: 0.8096\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3698 - acc: 0.8346 - val_loss: 0.4239 - val_acc: 0.8008\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3732 - acc: 0.8327 - val_loss: 0.4203 - val_acc: 0.8084\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3686 - acc: 0.8346 - val_loss: 0.4136 - val_acc: 0.8128\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3648 - acc: 0.8349 - val_loss: 0.4303 - val_acc: 0.8004\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3584 - acc: 0.8416 - val_loss: 0.4088 - val_acc: 0.8144\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3544 - acc: 0.8421 - val_loss: 0.4094 - val_acc: 0.8136\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3526 - acc: 0.8443 - val_loss: 0.4319 - val_acc: 0.8056\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3522 - acc: 0.8431 - val_loss: 0.4312 - val_acc: 0.8048\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3491 - acc: 0.8434 - val_loss: 0.4064 - val_acc: 0.8132\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3445 - acc: 0.8461 - val_loss: 0.4320 - val_acc: 0.8044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.5788 - acc: 0.6934 - val_loss: 0.5387 - val_acc: 0.7140\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4898 - acc: 0.7624 - val_loss: 0.4630 - val_acc: 0.7736\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4596 - acc: 0.7838 - val_loss: 0.4462 - val_acc: 0.7848\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4311 - acc: 0.7992 - val_loss: 0.4425 - val_acc: 0.7888\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4161 - acc: 0.8074 - val_loss: 0.4330 - val_acc: 0.7972\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3947 - acc: 0.8197 - val_loss: 0.4383 - val_acc: 0.7872\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3727 - acc: 0.8314 - val_loss: 0.4363 - val_acc: 0.7992\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3493 - acc: 0.8415 - val_loss: 0.4350 - val_acc: 0.8012\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3305 - acc: 0.8550 - val_loss: 0.4475 - val_acc: 0.7916\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3058 - acc: 0.8666 - val_loss: 0.4411 - val_acc: 0.8020\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2756 - acc: 0.8816 - val_loss: 0.4536 - val_acc: 0.7980\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2542 - acc: 0.8917 - val_loss: 0.4626 - val_acc: 0.7980\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2264 - acc: 0.9046 - val_loss: 0.4858 - val_acc: 0.7960\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1952 - acc: 0.9183 - val_loss: 0.5582 - val_acc: 0.7948\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1730 - acc: 0.9302 - val_loss: 0.5867 - val_acc: 0.8008\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1590 - acc: 0.9364 - val_loss: 0.6029 - val_acc: 0.7908\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1355 - acc: 0.9471 - val_loss: 0.6241 - val_acc: 0.7960\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1119 - acc: 0.9575 - val_loss: 0.6253 - val_acc: 0.7972\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.1042 - acc: 0.9628 - val_loss: 0.6921 - val_acc: 0.7936\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.0885 - acc: 0.9674 - val_loss: 0.7478 - val_acc: 0.7932\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.0895 - acc: 0.9674 - val_loss: 0.7642 - val_acc: 0.7900\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.0701 - acc: 0.9754 - val_loss: 0.7721 - val_acc: 0.7916\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.0677 - acc: 0.9764 - val_loss: 0.8001 - val_acc: 0.7908\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 46us/sample - loss: 0.0647 - acc: 0.9765 - val_loss: 0.8097 - val_acc: 0.8008\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0616 - acc: 0.9768 - val_loss: 0.8828 - val_acc: 0.7836\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.0575 - acc: 0.9794 - val_loss: 0.8813 - val_acc: 0.7948\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.0495 - acc: 0.9820 - val_loss: 0.8754 - val_acc: 0.7956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.0474 - acc: 0.9831 - val_loss: 0.8805 - val_acc: 0.7872\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 47us/sample - loss: 0.0453 - acc: 0.9838 - val_loss: 0.9813 - val_acc: 0.7924\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.0412 - acc: 0.9846 - val_loss: 0.9630 - val_acc: 0.8000\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.5889 - acc: 0.6884 - val_loss: 0.5297 - val_acc: 0.7368\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.5157 - acc: 0.7449 - val_loss: 0.4745 - val_acc: 0.7708\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4793 - acc: 0.7685 - val_loss: 0.4499 - val_acc: 0.7880\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4588 - acc: 0.7856 - val_loss: 0.4438 - val_acc: 0.7900\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4432 - acc: 0.7944 - val_loss: 0.4382 - val_acc: 0.7964\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4298 - acc: 0.8000 - val_loss: 0.4262 - val_acc: 0.7960\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.4221 - acc: 0.8032 - val_loss: 0.4232 - val_acc: 0.8044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4137 - acc: 0.8106 - val_loss: 0.4181 - val_acc: 0.8076\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3978 - acc: 0.8179 - val_loss: 0.4120 - val_acc: 0.8116\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3893 - acc: 0.8216 - val_loss: 0.4116 - val_acc: 0.8136\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3854 - acc: 0.8228 - val_loss: 0.4152 - val_acc: 0.8100\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3717 - acc: 0.8308 - val_loss: 0.4070 - val_acc: 0.8132\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3656 - acc: 0.8336 - val_loss: 0.4111 - val_acc: 0.8108\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3585 - acc: 0.8381 - val_loss: 0.4048 - val_acc: 0.8092\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3448 - acc: 0.8443 - val_loss: 0.4047 - val_acc: 0.8160\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3414 - acc: 0.8461 - val_loss: 0.4006 - val_acc: 0.8168\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3330 - acc: 0.8492 - val_loss: 0.4121 - val_acc: 0.8196\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3266 - acc: 0.8544 - val_loss: 0.4133 - val_acc: 0.8128\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3122 - acc: 0.8655 - val_loss: 0.4214 - val_acc: 0.8056\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3135 - acc: 0.8615 - val_loss: 0.4149 - val_acc: 0.8136\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2995 - acc: 0.8718 - val_loss: 0.4122 - val_acc: 0.8088\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2944 - acc: 0.8743 - val_loss: 0.4235 - val_acc: 0.8084\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2835 - acc: 0.8752 - val_loss: 0.4313 - val_acc: 0.8148\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2807 - acc: 0.8791 - val_loss: 0.4381 - val_acc: 0.8164\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2776 - acc: 0.8782 - val_loss: 0.4487 - val_acc: 0.8060\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.2659 - acc: 0.8833 - val_loss: 0.4363 - val_acc: 0.8136\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2574 - acc: 0.8909 - val_loss: 0.4566 - val_acc: 0.8060\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2541 - acc: 0.8932 - val_loss: 0.4419 - val_acc: 0.8100\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2503 - acc: 0.8910 - val_loss: 0.4594 - val_acc: 0.8092\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2382 - acc: 0.8967 - val_loss: 0.4489 - val_acc: 0.8164\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.5902 - acc: 0.6758 - val_loss: 0.5116 - val_acc: 0.7428\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.5185 - acc: 0.7441 - val_loss: 0.4824 - val_acc: 0.7716\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4854 - acc: 0.7670 - val_loss: 0.4501 - val_acc: 0.7836\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4671 - acc: 0.7784 - val_loss: 0.4539 - val_acc: 0.7920\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4537 - acc: 0.7888 - val_loss: 0.4320 - val_acc: 0.7964\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.4431 - acc: 0.7922 - val_loss: 0.4720 - val_acc: 0.7780\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.4358 - acc: 0.7960 - val_loss: 0.4324 - val_acc: 0.7952\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4321 - acc: 0.8027 - val_loss: 0.4228 - val_acc: 0.8024\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4208 - acc: 0.8062 - val_loss: 0.4426 - val_acc: 0.7956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4156 - acc: 0.8125 - val_loss: 0.4479 - val_acc: 0.7928\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4087 - acc: 0.8121 - val_loss: 0.4196 - val_acc: 0.8048\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4009 - acc: 0.8171 - val_loss: 0.4289 - val_acc: 0.7976\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3943 - acc: 0.8211 - val_loss: 0.4133 - val_acc: 0.8080\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3943 - acc: 0.8234 - val_loss: 0.4085 - val_acc: 0.8024\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3849 - acc: 0.8257 - val_loss: 0.4327 - val_acc: 0.7960\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3790 - acc: 0.8299 - val_loss: 0.4050 - val_acc: 0.8040\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3787 - acc: 0.8293 - val_loss: 0.4058 - val_acc: 0.8112\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3656 - acc: 0.8379 - val_loss: 0.4242 - val_acc: 0.7968\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3659 - acc: 0.8354 - val_loss: 0.4117 - val_acc: 0.8084\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3603 - acc: 0.8371 - val_loss: 0.4304 - val_acc: 0.7980\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3628 - acc: 0.8384 - val_loss: 0.4198 - val_acc: 0.8040\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3494 - acc: 0.8462 - val_loss: 0.4253 - val_acc: 0.8072\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3475 - acc: 0.8460 - val_loss: 0.4048 - val_acc: 0.8140\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3443 - acc: 0.8458 - val_loss: 0.4066 - val_acc: 0.8124\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3340 - acc: 0.8563 - val_loss: 0.4038 - val_acc: 0.8152\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3394 - acc: 0.8486 - val_loss: 0.4479 - val_acc: 0.7892\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3330 - acc: 0.8517 - val_loss: 0.4137 - val_acc: 0.8100\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3247 - acc: 0.8575 - val_loss: 0.4266 - val_acc: 0.8004\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3203 - acc: 0.8543 - val_loss: 0.4172 - val_acc: 0.8088\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3169 - acc: 0.8591 - val_loss: 0.4226 - val_acc: 0.8104\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.5655 - acc: 0.7011 - val_loss: 0.4886 - val_acc: 0.7512\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4914 - acc: 0.7599 - val_loss: 0.4643 - val_acc: 0.7804\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 48us/sample - loss: 0.4504 - acc: 0.7842 - val_loss: 0.4521 - val_acc: 0.7872\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4261 - acc: 0.8019 - val_loss: 0.4459 - val_acc: 0.7892\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.4012 - acc: 0.8140 - val_loss: 0.4728 - val_acc: 0.7748\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3792 - acc: 0.8271 - val_loss: 0.4260 - val_acc: 0.8052\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3613 - acc: 0.8381 - val_loss: 0.4235 - val_acc: 0.8072\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3396 - acc: 0.8482 - val_loss: 0.4616 - val_acc: 0.7968\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.3165 - acc: 0.8592 - val_loss: 0.4290 - val_acc: 0.8044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.2882 - acc: 0.8739 - val_loss: 0.4829 - val_acc: 0.8020\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.2603 - acc: 0.8877 - val_loss: 0.4654 - val_acc: 0.8048\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.2427 - acc: 0.8961 - val_loss: 0.4627 - val_acc: 0.8016\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1997 - acc: 0.9208 - val_loss: 0.5655 - val_acc: 0.7924\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1753 - acc: 0.9292 - val_loss: 0.5523 - val_acc: 0.8040\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1612 - acc: 0.9354 - val_loss: 0.5692 - val_acc: 0.8016\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.1353 - acc: 0.9488 - val_loss: 0.6166 - val_acc: 0.7960\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1166 - acc: 0.9550 - val_loss: 0.6728 - val_acc: 0.7992\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.1006 - acc: 0.9604 - val_loss: 0.7020 - val_acc: 0.7972\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.0862 - acc: 0.9686 - val_loss: 0.7017 - val_acc: 0.7984\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.0798 - acc: 0.9704 - val_loss: 0.7621 - val_acc: 0.7936\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.0699 - acc: 0.9752 - val_loss: 0.8386 - val_acc: 0.7984\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0658 - acc: 0.9765 - val_loss: 0.8675 - val_acc: 0.7864\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0583 - acc: 0.9793 - val_loss: 0.8207 - val_acc: 0.7960\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.0559 - acc: 0.9792 - val_loss: 0.8655 - val_acc: 0.8016\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.0526 - acc: 0.9829 - val_loss: 0.8761 - val_acc: 0.8004\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.0501 - acc: 0.9826 - val_loss: 0.8348 - val_acc: 0.7992\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0427 - acc: 0.9854 - val_loss: 0.9105 - val_acc: 0.7964\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 50us/sample - loss: 0.0514 - acc: 0.9815 - val_loss: 0.8778 - val_acc: 0.7904\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.0383 - acc: 0.9873 - val_loss: 1.0099 - val_acc: 0.7868\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 49us/sample - loss: 0.0403 - acc: 0.9866 - val_loss: 0.9746 - val_acc: 0.7944\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.5842 - acc: 0.6893 - val_loss: 0.5263 - val_acc: 0.7468\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.5121 - acc: 0.7468 - val_loss: 0.4759 - val_acc: 0.7700\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4873 - acc: 0.7645 - val_loss: 0.4577 - val_acc: 0.7828\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4639 - acc: 0.7801 - val_loss: 0.4665 - val_acc: 0.7764\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4467 - acc: 0.7897 - val_loss: 0.4425 - val_acc: 0.7952\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4365 - acc: 0.7998 - val_loss: 0.4433 - val_acc: 0.7892\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4222 - acc: 0.8046 - val_loss: 0.4302 - val_acc: 0.7980\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4141 - acc: 0.8065 - val_loss: 0.4294 - val_acc: 0.7972\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3965 - acc: 0.8178 - val_loss: 0.4155 - val_acc: 0.8088\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3904 - acc: 0.8229 - val_loss: 0.4182 - val_acc: 0.8060\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3839 - acc: 0.8262 - val_loss: 0.4138 - val_acc: 0.8096\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3724 - acc: 0.8325 - val_loss: 0.4196 - val_acc: 0.8112\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3625 - acc: 0.8391 - val_loss: 0.4139 - val_acc: 0.8104\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3530 - acc: 0.8452 - val_loss: 0.4091 - val_acc: 0.8140\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3452 - acc: 0.8460 - val_loss: 0.4169 - val_acc: 0.8100\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3366 - acc: 0.8504 - val_loss: 0.4133 - val_acc: 0.8096\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3317 - acc: 0.8514 - val_loss: 0.4277 - val_acc: 0.8084\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3152 - acc: 0.8596 - val_loss: 0.4146 - val_acc: 0.8140\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3131 - acc: 0.8619 - val_loss: 0.4187 - val_acc: 0.8180\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2996 - acc: 0.8688 - val_loss: 0.4186 - val_acc: 0.8192\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2941 - acc: 0.8696 - val_loss: 0.4094 - val_acc: 0.8120\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2873 - acc: 0.8729 - val_loss: 0.4446 - val_acc: 0.8020\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2799 - acc: 0.8768 - val_loss: 0.4483 - val_acc: 0.8064\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.2756 - acc: 0.8804 - val_loss: 0.4506 - val_acc: 0.8080\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2623 - acc: 0.8862 - val_loss: 0.4414 - val_acc: 0.8056\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2613 - acc: 0.8856 - val_loss: 0.4384 - val_acc: 0.8144\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2504 - acc: 0.8945 - val_loss: 0.4447 - val_acc: 0.8220\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2409 - acc: 0.8959 - val_loss: 0.4637 - val_acc: 0.8088\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2405 - acc: 0.8971 - val_loss: 0.4473 - val_acc: 0.8076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2330 - acc: 0.9014 - val_loss: 0.4604 - val_acc: 0.8128\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6049 - acc: 0.6803 - val_loss: 0.5513 - val_acc: 0.7308\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5206 - acc: 0.7444 - val_loss: 0.4752 - val_acc: 0.7704\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4818 - acc: 0.7727 - val_loss: 0.4601 - val_acc: 0.7900\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4632 - acc: 0.7826 - val_loss: 0.4516 - val_acc: 0.7936\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.4436 - acc: 0.7916 - val_loss: 0.4424 - val_acc: 0.7988\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.4325 - acc: 0.8005 - val_loss: 0.4454 - val_acc: 0.7920\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4298 - acc: 0.8005 - val_loss: 0.4203 - val_acc: 0.8028\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4160 - acc: 0.8101 - val_loss: 0.4242 - val_acc: 0.8084\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4118 - acc: 0.8098 - val_loss: 0.4213 - val_acc: 0.8068\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.3953 - acc: 0.8169 - val_loss: 0.4207 - val_acc: 0.8060\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3926 - acc: 0.8213 - val_loss: 0.4368 - val_acc: 0.8012\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3869 - acc: 0.8269 - val_loss: 0.4330 - val_acc: 0.8032\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3807 - acc: 0.8283 - val_loss: 0.4691 - val_acc: 0.7860\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3734 - acc: 0.8318 - val_loss: 0.4328 - val_acc: 0.8020\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3678 - acc: 0.8334 - val_loss: 0.4171 - val_acc: 0.8084\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3619 - acc: 0.8354 - val_loss: 0.4178 - val_acc: 0.8116\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3544 - acc: 0.8418 - val_loss: 0.4175 - val_acc: 0.8100\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3447 - acc: 0.8454 - val_loss: 0.4057 - val_acc: 0.8088\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3424 - acc: 0.8465 - val_loss: 0.4105 - val_acc: 0.8156\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.3388 - acc: 0.8488 - val_loss: 0.4123 - val_acc: 0.8116\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3331 - acc: 0.8505 - val_loss: 0.4109 - val_acc: 0.8208\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3304 - acc: 0.8551 - val_loss: 0.4071 - val_acc: 0.8136\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3184 - acc: 0.8590 - val_loss: 0.4170 - val_acc: 0.8196\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3158 - acc: 0.8578 - val_loss: 0.4355 - val_acc: 0.8148\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3209 - acc: 0.8593 - val_loss: 0.4064 - val_acc: 0.8224\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3045 - acc: 0.8658 - val_loss: 0.4132 - val_acc: 0.8120\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3059 - acc: 0.8682 - val_loss: 0.4069 - val_acc: 0.8220\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3004 - acc: 0.8692 - val_loss: 0.4368 - val_acc: 0.8232\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2951 - acc: 0.8717 - val_loss: 0.4292 - val_acc: 0.8156\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2962 - acc: 0.8704 - val_loss: 0.4190 - val_acc: 0.8136\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.5493 - acc: 0.7151 - val_loss: 0.4841 - val_acc: 0.7704\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4858 - acc: 0.7630 - val_loss: 0.4660 - val_acc: 0.7816\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4488 - acc: 0.7893 - val_loss: 0.4429 - val_acc: 0.7860\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4287 - acc: 0.8011 - val_loss: 0.4369 - val_acc: 0.7908\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4078 - acc: 0.8133 - val_loss: 0.4893 - val_acc: 0.7672\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3901 - acc: 0.8226 - val_loss: 0.4314 - val_acc: 0.7960\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3702 - acc: 0.8340 - val_loss: 0.4264 - val_acc: 0.7972\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3427 - acc: 0.8502 - val_loss: 0.4378 - val_acc: 0.8008\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.3210 - acc: 0.8609 - val_loss: 0.4525 - val_acc: 0.8024\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2927 - acc: 0.8714 - val_loss: 0.4555 - val_acc: 0.7960\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2599 - acc: 0.8885 - val_loss: 0.4834 - val_acc: 0.8016\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.2403 - acc: 0.8971 - val_loss: 0.4963 - val_acc: 0.7944\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2041 - acc: 0.9159 - val_loss: 0.5376 - val_acc: 0.7852\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1861 - acc: 0.9234 - val_loss: 0.5340 - val_acc: 0.7936\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1530 - acc: 0.9385 - val_loss: 0.5990 - val_acc: 0.7916\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1281 - acc: 0.9500 - val_loss: 0.6206 - val_acc: 0.7888\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1056 - acc: 0.9591 - val_loss: 0.6611 - val_acc: 0.7972\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1021 - acc: 0.9614 - val_loss: 0.7297 - val_acc: 0.7964\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0821 - acc: 0.9708 - val_loss: 0.7568 - val_acc: 0.7924\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0712 - acc: 0.9749 - val_loss: 0.7509 - val_acc: 0.7956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0701 - acc: 0.9741 - val_loss: 0.7934 - val_acc: 0.7916\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.0580 - acc: 0.9795 - val_loss: 0.8733 - val_acc: 0.7964\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0596 - acc: 0.9789 - val_loss: 0.8369 - val_acc: 0.7940\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0516 - acc: 0.9812 - val_loss: 0.9005 - val_acc: 0.7984\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0472 - acc: 0.9837 - val_loss: 0.9379 - val_acc: 0.8004\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0494 - acc: 0.9820 - val_loss: 0.9887 - val_acc: 0.7956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0464 - acc: 0.9839 - val_loss: 0.9510 - val_acc: 0.7980\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0391 - acc: 0.9856 - val_loss: 0.9581 - val_acc: 0.7864\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0364 - acc: 0.9872 - val_loss: 0.9453 - val_acc: 0.7868\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0394 - acc: 0.9864 - val_loss: 0.9536 - val_acc: 0.7856\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.5846 - acc: 0.6900 - val_loss: 0.5184 - val_acc: 0.7420\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.5146 - acc: 0.7472 - val_loss: 0.4741 - val_acc: 0.7808\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.4793 - acc: 0.7688 - val_loss: 0.4783 - val_acc: 0.7688\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.4577 - acc: 0.7855 - val_loss: 0.4487 - val_acc: 0.7832\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4387 - acc: 0.7952 - val_loss: 0.4389 - val_acc: 0.7924\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.4271 - acc: 0.7985 - val_loss: 0.4429 - val_acc: 0.7972\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4102 - acc: 0.8087 - val_loss: 0.4161 - val_acc: 0.8052\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.4023 - acc: 0.8154 - val_loss: 0.4168 - val_acc: 0.8060\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3876 - acc: 0.8242 - val_loss: 0.4140 - val_acc: 0.8024\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3761 - acc: 0.8298 - val_loss: 0.4138 - val_acc: 0.8084\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3686 - acc: 0.8316 - val_loss: 0.4191 - val_acc: 0.8052\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3585 - acc: 0.8404 - val_loss: 0.4281 - val_acc: 0.8044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3492 - acc: 0.8466 - val_loss: 0.4051 - val_acc: 0.8140\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3442 - acc: 0.8469 - val_loss: 0.4149 - val_acc: 0.8084\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3316 - acc: 0.8502 - val_loss: 0.4176 - val_acc: 0.8024\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3166 - acc: 0.8579 - val_loss: 0.4158 - val_acc: 0.8048\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3065 - acc: 0.8642 - val_loss: 0.4250 - val_acc: 0.8036\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3007 - acc: 0.8694 - val_loss: 0.4099 - val_acc: 0.8120\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2869 - acc: 0.8752 - val_loss: 0.4273 - val_acc: 0.8092\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2843 - acc: 0.8741 - val_loss: 0.4111 - val_acc: 0.8112\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2691 - acc: 0.8829 - val_loss: 0.4460 - val_acc: 0.8012\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2689 - acc: 0.8875 - val_loss: 0.4431 - val_acc: 0.8088\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2561 - acc: 0.8889 - val_loss: 0.4467 - val_acc: 0.8048\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2508 - acc: 0.8951 - val_loss: 0.4424 - val_acc: 0.8108\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2367 - acc: 0.9017 - val_loss: 0.4512 - val_acc: 0.8040\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2326 - acc: 0.9026 - val_loss: 0.4504 - val_acc: 0.8076\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2274 - acc: 0.9046 - val_loss: 0.4462 - val_acc: 0.8076\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2174 - acc: 0.9102 - val_loss: 0.4731 - val_acc: 0.8064\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2089 - acc: 0.9134 - val_loss: 0.4669 - val_acc: 0.8032\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2012 - acc: 0.9175 - val_loss: 0.4776 - val_acc: 0.8036\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.5880 - acc: 0.6862 - val_loss: 0.5303 - val_acc: 0.7432\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4999 - acc: 0.7605 - val_loss: 0.4642 - val_acc: 0.7780\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4785 - acc: 0.7722 - val_loss: 0.4544 - val_acc: 0.7908\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.4569 - acc: 0.7875 - val_loss: 0.4356 - val_acc: 0.8004\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4393 - acc: 0.7969 - val_loss: 0.4245 - val_acc: 0.8012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4267 - acc: 0.8030 - val_loss: 0.4261 - val_acc: 0.8024\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4227 - acc: 0.8086 - val_loss: 0.4328 - val_acc: 0.8000\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4116 - acc: 0.8115 - val_loss: 0.4403 - val_acc: 0.7952\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3988 - acc: 0.8191 - val_loss: 0.4085 - val_acc: 0.8104\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3887 - acc: 0.8252 - val_loss: 0.4352 - val_acc: 0.7956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3805 - acc: 0.8275 - val_loss: 0.4118 - val_acc: 0.8028\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3738 - acc: 0.8318 - val_loss: 0.4247 - val_acc: 0.8060\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3650 - acc: 0.8362 - val_loss: 0.4183 - val_acc: 0.7996\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3579 - acc: 0.8434 - val_loss: 0.4188 - val_acc: 0.8052\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3511 - acc: 0.8445 - val_loss: 0.4264 - val_acc: 0.8028\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3461 - acc: 0.8446 - val_loss: 0.4121 - val_acc: 0.8112\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3384 - acc: 0.8488 - val_loss: 0.4257 - val_acc: 0.8136\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3313 - acc: 0.8550 - val_loss: 0.4119 - val_acc: 0.8172\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3242 - acc: 0.8552 - val_loss: 0.4231 - val_acc: 0.8088\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3209 - acc: 0.8621 - val_loss: 0.4063 - val_acc: 0.8128\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3138 - acc: 0.8655 - val_loss: 0.4182 - val_acc: 0.8096\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.3087 - acc: 0.8633 - val_loss: 0.4164 - val_acc: 0.8160\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.3028 - acc: 0.8665 - val_loss: 0.4101 - val_acc: 0.8148\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.2985 - acc: 0.8701 - val_loss: 0.4168 - val_acc: 0.8152\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2864 - acc: 0.8723 - val_loss: 0.4341 - val_acc: 0.8108\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2843 - acc: 0.8757 - val_loss: 0.4070 - val_acc: 0.8132\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.2824 - acc: 0.8781 - val_loss: 0.4106 - val_acc: 0.8100\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2761 - acc: 0.8801 - val_loss: 0.4376 - val_acc: 0.8128\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2748 - acc: 0.8835 - val_loss: 0.4432 - val_acc: 0.8140\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2665 - acc: 0.8869 - val_loss: 0.4384 - val_acc: 0.8160\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.5667 - acc: 0.7025 - val_loss: 0.5119 - val_acc: 0.7376\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4916 - acc: 0.7595 - val_loss: 0.4821 - val_acc: 0.7672\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4620 - acc: 0.7799 - val_loss: 0.4845 - val_acc: 0.7708\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.4387 - acc: 0.7963 - val_loss: 0.4477 - val_acc: 0.7904\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.4189 - acc: 0.8065 - val_loss: 0.4453 - val_acc: 0.7856\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3973 - acc: 0.8182 - val_loss: 0.4293 - val_acc: 0.7976\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3727 - acc: 0.8302 - val_loss: 0.4373 - val_acc: 0.7924\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.3569 - acc: 0.8422 - val_loss: 0.4594 - val_acc: 0.7880\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3309 - acc: 0.8539 - val_loss: 0.4575 - val_acc: 0.7956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.3028 - acc: 0.8675 - val_loss: 0.4446 - val_acc: 0.8040\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2788 - acc: 0.8801 - val_loss: 0.4488 - val_acc: 0.7900\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.2494 - acc: 0.8956 - val_loss: 0.4786 - val_acc: 0.7924\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.2245 - acc: 0.9075 - val_loss: 0.5126 - val_acc: 0.7908\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1961 - acc: 0.9211 - val_loss: 0.5807 - val_acc: 0.7876\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1809 - acc: 0.9278 - val_loss: 0.7027 - val_acc: 0.7732\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1513 - acc: 0.9400 - val_loss: 0.6450 - val_acc: 0.7880\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.1325 - acc: 0.9481 - val_loss: 0.7078 - val_acc: 0.7848\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.1164 - acc: 0.9539 - val_loss: 0.7397 - val_acc: 0.7864\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.1088 - acc: 0.9576 - val_loss: 0.8310 - val_acc: 0.7888\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0929 - acc: 0.9649 - val_loss: 0.7585 - val_acc: 0.7800\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0930 - acc: 0.9660 - val_loss: 0.7722 - val_acc: 0.7784\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0761 - acc: 0.9716 - val_loss: 0.8475 - val_acc: 0.7752\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 51us/sample - loss: 0.0719 - acc: 0.9728 - val_loss: 0.8940 - val_acc: 0.7820\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0684 - acc: 0.9749 - val_loss: 0.9961 - val_acc: 0.7752\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0651 - acc: 0.9769 - val_loss: 0.8333 - val_acc: 0.7864\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0577 - acc: 0.9794 - val_loss: 0.8714 - val_acc: 0.7836\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0559 - acc: 0.9795 - val_loss: 0.9613 - val_acc: 0.7796\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0502 - acc: 0.9814 - val_loss: 1.0619 - val_acc: 0.7776\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0499 - acc: 0.9810 - val_loss: 1.0516 - val_acc: 0.7868\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 52us/sample - loss: 0.0518 - acc: 0.9804 - val_loss: 1.1240 - val_acc: 0.7728\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.5819 - acc: 0.6906 - val_loss: 0.5176 - val_acc: 0.7360\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.5227 - acc: 0.7368 - val_loss: 0.4742 - val_acc: 0.7732\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4893 - acc: 0.7634 - val_loss: 0.4680 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4650 - acc: 0.7781 - val_loss: 0.4394 - val_acc: 0.7940\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4514 - acc: 0.7898 - val_loss: 0.4321 - val_acc: 0.7956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4392 - acc: 0.7953 - val_loss: 0.4589 - val_acc: 0.7880\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4265 - acc: 0.7988 - val_loss: 0.4165 - val_acc: 0.8088\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4191 - acc: 0.8064 - val_loss: 0.4161 - val_acc: 0.8092\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4086 - acc: 0.8124 - val_loss: 0.4485 - val_acc: 0.7864\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3981 - acc: 0.8148 - val_loss: 0.4080 - val_acc: 0.8132\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3957 - acc: 0.8192 - val_loss: 0.4007 - val_acc: 0.8188\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3846 - acc: 0.8263 - val_loss: 0.4105 - val_acc: 0.8056\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3797 - acc: 0.8244 - val_loss: 0.3968 - val_acc: 0.8224\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3740 - acc: 0.8313 - val_loss: 0.4286 - val_acc: 0.7996\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3658 - acc: 0.8332 - val_loss: 0.4258 - val_acc: 0.8024\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3549 - acc: 0.8389 - val_loss: 0.4097 - val_acc: 0.8180\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3513 - acc: 0.8426 - val_loss: 0.4028 - val_acc: 0.8136\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3450 - acc: 0.8450 - val_loss: 0.4043 - val_acc: 0.8116\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3397 - acc: 0.8462 - val_loss: 0.4058 - val_acc: 0.8108\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3328 - acc: 0.8495 - val_loss: 0.4035 - val_acc: 0.8140\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3310 - acc: 0.8524 - val_loss: 0.4405 - val_acc: 0.7984\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3228 - acc: 0.8538 - val_loss: 0.3955 - val_acc: 0.8200\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.3150 - acc: 0.8611 - val_loss: 0.4134 - val_acc: 0.8172\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3093 - acc: 0.8629 - val_loss: 0.4329 - val_acc: 0.8112\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3007 - acc: 0.8664 - val_loss: 0.4203 - val_acc: 0.8168\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3003 - acc: 0.8641 - val_loss: 0.4219 - val_acc: 0.8124\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2941 - acc: 0.8717 - val_loss: 0.4440 - val_acc: 0.8072\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2920 - acc: 0.8726 - val_loss: 0.4148 - val_acc: 0.8132\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2857 - acc: 0.8744 - val_loss: 0.4302 - val_acc: 0.8156\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2793 - acc: 0.8786 - val_loss: 0.4112 - val_acc: 0.8164\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.5997 - acc: 0.6782 - val_loss: 0.6052 - val_acc: 0.6528\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.5303 - acc: 0.7341 - val_loss: 0.5085 - val_acc: 0.7480\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4915 - acc: 0.7636 - val_loss: 0.4822 - val_acc: 0.7640\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4787 - acc: 0.7735 - val_loss: 0.4663 - val_acc: 0.7832\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4632 - acc: 0.7839 - val_loss: 0.4484 - val_acc: 0.7832\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4556 - acc: 0.7881 - val_loss: 0.4440 - val_acc: 0.7884\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4477 - acc: 0.7937 - val_loss: 0.4478 - val_acc: 0.7864\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4428 - acc: 0.7965 - val_loss: 0.4314 - val_acc: 0.7892\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4324 - acc: 0.7995 - val_loss: 0.4300 - val_acc: 0.7912\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4331 - acc: 0.8019 - val_loss: 0.4229 - val_acc: 0.8008\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.4189 - acc: 0.8089 - val_loss: 0.4643 - val_acc: 0.7828\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4131 - acc: 0.8113 - val_loss: 0.4340 - val_acc: 0.7940\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.4094 - acc: 0.8155 - val_loss: 0.4198 - val_acc: 0.8096\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.4060 - acc: 0.8145 - val_loss: 0.4197 - val_acc: 0.8008\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4019 - acc: 0.8189 - val_loss: 0.4228 - val_acc: 0.8020\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.4003 - acc: 0.8197 - val_loss: 0.4233 - val_acc: 0.8036\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3932 - acc: 0.8202 - val_loss: 0.4215 - val_acc: 0.8096\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.3873 - acc: 0.8264 - val_loss: 0.4438 - val_acc: 0.7952\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3841 - acc: 0.8234 - val_loss: 0.4387 - val_acc: 0.7996\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3776 - acc: 0.8283 - val_loss: 0.4370 - val_acc: 0.8012\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.3769 - acc: 0.8290 - val_loss: 0.4092 - val_acc: 0.8060\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3707 - acc: 0.8341 - val_loss: 0.4247 - val_acc: 0.7992\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3670 - acc: 0.8367 - val_loss: 0.4142 - val_acc: 0.8144\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3715 - acc: 0.8353 - val_loss: 0.4086 - val_acc: 0.8032\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3644 - acc: 0.8365 - val_loss: 0.4198 - val_acc: 0.8048\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.3647 - acc: 0.8336 - val_loss: 0.4050 - val_acc: 0.8104\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3584 - acc: 0.8384 - val_loss: 0.4117 - val_acc: 0.8128\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3529 - acc: 0.8404 - val_loss: 0.4138 - val_acc: 0.8100\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3516 - acc: 0.8401 - val_loss: 0.4083 - val_acc: 0.8072\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3454 - acc: 0.8470 - val_loss: 0.4242 - val_acc: 0.8052\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.5952 - acc: 0.6710 - val_loss: 0.5587 - val_acc: 0.7140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.5044 - acc: 0.7518 - val_loss: 0.4768 - val_acc: 0.7680\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.4704 - acc: 0.7734 - val_loss: 0.4637 - val_acc: 0.7752\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4462 - acc: 0.7934 - val_loss: 0.4700 - val_acc: 0.7768\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.4207 - acc: 0.8055 - val_loss: 0.4358 - val_acc: 0.7952\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.4099 - acc: 0.8090 - val_loss: 0.4338 - val_acc: 0.7924\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.3842 - acc: 0.8264 - val_loss: 0.4321 - val_acc: 0.7936\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.3626 - acc: 0.8364 - val_loss: 0.4548 - val_acc: 0.7920\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3402 - acc: 0.8479 - val_loss: 0.4542 - val_acc: 0.8016\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.3158 - acc: 0.8596 - val_loss: 0.4647 - val_acc: 0.7860\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.2965 - acc: 0.8680 - val_loss: 0.4660 - val_acc: 0.7836\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2726 - acc: 0.8838 - val_loss: 0.5005 - val_acc: 0.7908\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.2419 - acc: 0.8983 - val_loss: 0.5658 - val_acc: 0.7864\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.2182 - acc: 0.9104 - val_loss: 0.5301 - val_acc: 0.7792\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1941 - acc: 0.9203 - val_loss: 0.5869 - val_acc: 0.7848\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.1696 - acc: 0.9312 - val_loss: 0.6374 - val_acc: 0.7844\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1621 - acc: 0.9352 - val_loss: 0.6258 - val_acc: 0.7880\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1403 - acc: 0.9448 - val_loss: 0.6595 - val_acc: 0.7932\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.1245 - acc: 0.9514 - val_loss: 0.7033 - val_acc: 0.7852\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 55us/sample - loss: 0.1132 - acc: 0.9552 - val_loss: 0.7156 - val_acc: 0.7848\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.0997 - acc: 0.9616 - val_loss: 0.7761 - val_acc: 0.7780\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0982 - acc: 0.9605 - val_loss: 0.8010 - val_acc: 0.7800\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.0892 - acc: 0.9672 - val_loss: 0.8569 - val_acc: 0.7872\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.0780 - acc: 0.9705 - val_loss: 0.8757 - val_acc: 0.7784\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0801 - acc: 0.9714 - val_loss: 0.8413 - val_acc: 0.7788\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0694 - acc: 0.9744 - val_loss: 0.8987 - val_acc: 0.7772\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.0647 - acc: 0.9774 - val_loss: 0.9194 - val_acc: 0.7852\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0577 - acc: 0.9783 - val_loss: 0.9536 - val_acc: 0.7796\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 54us/sample - loss: 0.0617 - acc: 0.9771 - val_loss: 0.9511 - val_acc: 0.7760\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 53us/sample - loss: 0.0541 - acc: 0.9799 - val_loss: 0.9378 - val_acc: 0.7712\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5834 - acc: 0.6914 - val_loss: 0.5136 - val_acc: 0.7420\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.5196 - acc: 0.7434 - val_loss: 0.4746 - val_acc: 0.7648\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4790 - acc: 0.7724 - val_loss: 0.4470 - val_acc: 0.7876\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4645 - acc: 0.7770 - val_loss: 0.4495 - val_acc: 0.7892\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4459 - acc: 0.7926 - val_loss: 0.4302 - val_acc: 0.7988\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4360 - acc: 0.7948 - val_loss: 0.4220 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4245 - acc: 0.8028 - val_loss: 0.4158 - val_acc: 0.8040\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4136 - acc: 0.8081 - val_loss: 0.4225 - val_acc: 0.7992\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3973 - acc: 0.8177 - val_loss: 0.4124 - val_acc: 0.8140\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3891 - acc: 0.8212 - val_loss: 0.4210 - val_acc: 0.8116\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3828 - acc: 0.8232 - val_loss: 0.4031 - val_acc: 0.8052\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3748 - acc: 0.8306 - val_loss: 0.4094 - val_acc: 0.8072\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3562 - acc: 0.8404 - val_loss: 0.4040 - val_acc: 0.8104\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3563 - acc: 0.8382 - val_loss: 0.4083 - val_acc: 0.8184\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3426 - acc: 0.8456 - val_loss: 0.4002 - val_acc: 0.8148\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3413 - acc: 0.8472 - val_loss: 0.4081 - val_acc: 0.8152\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3263 - acc: 0.8546 - val_loss: 0.4128 - val_acc: 0.8076\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3214 - acc: 0.8576 - val_loss: 0.4052 - val_acc: 0.8216\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3191 - acc: 0.8564 - val_loss: 0.4085 - val_acc: 0.8196\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.3070 - acc: 0.8648 - val_loss: 0.4219 - val_acc: 0.8168\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2935 - acc: 0.8725 - val_loss: 0.4009 - val_acc: 0.8200\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2917 - acc: 0.8718 - val_loss: 0.4159 - val_acc: 0.8128\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2829 - acc: 0.8763 - val_loss: 0.4255 - val_acc: 0.8212\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2766 - acc: 0.8809 - val_loss: 0.4395 - val_acc: 0.8152\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2668 - acc: 0.8831 - val_loss: 0.4249 - val_acc: 0.8180\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2636 - acc: 0.8869 - val_loss: 0.4251 - val_acc: 0.8236\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2612 - acc: 0.8878 - val_loss: 0.4380 - val_acc: 0.8168\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2528 - acc: 0.8901 - val_loss: 0.4359 - val_acc: 0.8128\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.2495 - acc: 0.8921 - val_loss: 0.4711 - val_acc: 0.8096\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2381 - acc: 0.8973 - val_loss: 0.4857 - val_acc: 0.8100\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6001 - acc: 0.6746 - val_loss: 0.5312 - val_acc: 0.7436\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.5276 - acc: 0.7374 - val_loss: 0.4799 - val_acc: 0.7808\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4845 - acc: 0.7703 - val_loss: 0.4600 - val_acc: 0.7880\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4646 - acc: 0.7814 - val_loss: 0.4443 - val_acc: 0.7900\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.4520 - acc: 0.7882 - val_loss: 0.4359 - val_acc: 0.7980\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.4415 - acc: 0.7953 - val_loss: 0.4417 - val_acc: 0.7988\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4323 - acc: 0.8023 - val_loss: 0.4746 - val_acc: 0.7752\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.4275 - acc: 0.8007 - val_loss: 0.4331 - val_acc: 0.7992\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.4198 - acc: 0.8078 - val_loss: 0.4150 - val_acc: 0.8068\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4072 - acc: 0.8131 - val_loss: 0.4482 - val_acc: 0.7888\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3990 - acc: 0.8183 - val_loss: 0.4079 - val_acc: 0.8160\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3979 - acc: 0.8185 - val_loss: 0.4097 - val_acc: 0.8104\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3842 - acc: 0.8264 - val_loss: 0.4360 - val_acc: 0.8036\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.3787 - acc: 0.8304 - val_loss: 0.4214 - val_acc: 0.8124\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3758 - acc: 0.8308 - val_loss: 0.4094 - val_acc: 0.8116\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3712 - acc: 0.8325 - val_loss: 0.4073 - val_acc: 0.8136\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3687 - acc: 0.8337 - val_loss: 0.4134 - val_acc: 0.8064\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3633 - acc: 0.8382 - val_loss: 0.4200 - val_acc: 0.8128\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3549 - acc: 0.8434 - val_loss: 0.4163 - val_acc: 0.8184\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3482 - acc: 0.8443 - val_loss: 0.4148 - val_acc: 0.8152\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.3490 - acc: 0.8444 - val_loss: 0.4279 - val_acc: 0.8076\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3407 - acc: 0.8492 - val_loss: 0.4134 - val_acc: 0.8120\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3413 - acc: 0.8496 - val_loss: 0.4131 - val_acc: 0.8160\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3385 - acc: 0.8489 - val_loss: 0.4217 - val_acc: 0.8100\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3262 - acc: 0.8549 - val_loss: 0.4162 - val_acc: 0.8132\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3321 - acc: 0.8528 - val_loss: 0.4191 - val_acc: 0.8092\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.3229 - acc: 0.8569 - val_loss: 0.4291 - val_acc: 0.8132\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3141 - acc: 0.8607 - val_loss: 0.4365 - val_acc: 0.8132\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.3163 - acc: 0.8621 - val_loss: 0.4259 - val_acc: 0.8156\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.3109 - acc: 0.8621 - val_loss: 0.4251 - val_acc: 0.8168\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.5685 - acc: 0.6981 - val_loss: 0.4897 - val_acc: 0.7564\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4881 - acc: 0.7655 - val_loss: 0.4605 - val_acc: 0.7780\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4553 - acc: 0.7861 - val_loss: 0.4622 - val_acc: 0.7736\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.4263 - acc: 0.8039 - val_loss: 0.4382 - val_acc: 0.7836\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.4020 - acc: 0.8157 - val_loss: 0.4317 - val_acc: 0.7944\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.3812 - acc: 0.8254 - val_loss: 0.4264 - val_acc: 0.7972\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.3585 - acc: 0.8368 - val_loss: 0.4277 - val_acc: 0.7960\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3300 - acc: 0.8569 - val_loss: 0.4341 - val_acc: 0.8048\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.3054 - acc: 0.8678 - val_loss: 0.4491 - val_acc: 0.8024\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.2798 - acc: 0.8768 - val_loss: 0.4952 - val_acc: 0.8084\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.2452 - acc: 0.8957 - val_loss: 0.5026 - val_acc: 0.7872\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.2201 - acc: 0.9076 - val_loss: 0.5356 - val_acc: 0.8012\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1892 - acc: 0.9228 - val_loss: 0.5865 - val_acc: 0.7896\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 60us/sample - loss: 0.1681 - acc: 0.9328 - val_loss: 0.5969 - val_acc: 0.7988\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.1418 - acc: 0.9454 - val_loss: 0.6542 - val_acc: 0.7928\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1317 - acc: 0.9502 - val_loss: 0.6354 - val_acc: 0.7908\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1128 - acc: 0.9572 - val_loss: 0.7646 - val_acc: 0.7892\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.1090 - acc: 0.9594 - val_loss: 0.7304 - val_acc: 0.7920\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0934 - acc: 0.9647 - val_loss: 0.7412 - val_acc: 0.7944\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 58us/sample - loss: 0.0767 - acc: 0.9701 - val_loss: 0.8319 - val_acc: 0.7916\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0715 - acc: 0.9737 - val_loss: 0.8942 - val_acc: 0.7848\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0656 - acc: 0.9757 - val_loss: 0.8579 - val_acc: 0.7896\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0641 - acc: 0.9769 - val_loss: 0.9542 - val_acc: 0.7888\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 56us/sample - loss: 0.0626 - acc: 0.9772 - val_loss: 1.0538 - val_acc: 0.7856\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0536 - acc: 0.9811 - val_loss: 0.9310 - val_acc: 0.7940\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0528 - acc: 0.9809 - val_loss: 0.9040 - val_acc: 0.8016\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0486 - acc: 0.9825 - val_loss: 1.1396 - val_acc: 0.7892\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 59us/sample - loss: 0.0434 - acc: 0.9851 - val_loss: 1.0593 - val_acc: 0.7796\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0412 - acc: 0.9851 - val_loss: 0.9822 - val_acc: 0.7972\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 57us/sample - loss: 0.0443 - acc: 0.9842 - val_loss: 1.0008 - val_acc: 0.7976\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.5847 - acc: 0.6868 - val_loss: 0.5413 - val_acc: 0.7312\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.5104 - acc: 0.7489 - val_loss: 0.4700 - val_acc: 0.7736\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.4744 - acc: 0.7726 - val_loss: 0.4578 - val_acc: 0.7792\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4509 - acc: 0.7891 - val_loss: 0.4375 - val_acc: 0.7928\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.4389 - acc: 0.7942 - val_loss: 0.4411 - val_acc: 0.7948\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.4252 - acc: 0.8036 - val_loss: 0.4253 - val_acc: 0.8016\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.4091 - acc: 0.8121 - val_loss: 0.4199 - val_acc: 0.7984\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.4080 - acc: 0.8108 - val_loss: 0.4110 - val_acc: 0.8080\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3893 - acc: 0.8238 - val_loss: 0.4151 - val_acc: 0.8032\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3823 - acc: 0.8271 - val_loss: 0.4067 - val_acc: 0.8088\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.3751 - acc: 0.8296 - val_loss: 0.4225 - val_acc: 0.8024\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3679 - acc: 0.8335 - val_loss: 0.4152 - val_acc: 0.8088\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3564 - acc: 0.8405 - val_loss: 0.4065 - val_acc: 0.8120\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3413 - acc: 0.8441 - val_loss: 0.4018 - val_acc: 0.8088\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.3326 - acc: 0.8503 - val_loss: 0.4452 - val_acc: 0.8052\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.3251 - acc: 0.8546 - val_loss: 0.4023 - val_acc: 0.8092\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.3148 - acc: 0.8601 - val_loss: 0.4237 - val_acc: 0.8096\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.3059 - acc: 0.8639 - val_loss: 0.4156 - val_acc: 0.8140\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2955 - acc: 0.8699 - val_loss: 0.4209 - val_acc: 0.8156\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2942 - acc: 0.8696 - val_loss: 0.4260 - val_acc: 0.8136\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2826 - acc: 0.8785 - val_loss: 0.4268 - val_acc: 0.8144\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 64us/sample - loss: 0.2730 - acc: 0.8811 - val_loss: 0.4376 - val_acc: 0.8112\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2594 - acc: 0.8874 - val_loss: 0.4329 - val_acc: 0.8100\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2588 - acc: 0.8875 - val_loss: 0.4510 - val_acc: 0.8108\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2539 - acc: 0.8912 - val_loss: 0.4527 - val_acc: 0.8092\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2450 - acc: 0.8957 - val_loss: 0.4515 - val_acc: 0.8148\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2384 - acc: 0.8973 - val_loss: 0.4360 - val_acc: 0.8148\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2289 - acc: 0.9025 - val_loss: 0.4397 - val_acc: 0.8184\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.2267 - acc: 0.9031 - val_loss: 0.4467 - val_acc: 0.8076\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2213 - acc: 0.9074 - val_loss: 0.4658 - val_acc: 0.8072\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6034 - acc: 0.6732 - val_loss: 0.5254 - val_acc: 0.7328\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.5215 - acc: 0.7424 - val_loss: 0.4755 - val_acc: 0.7756\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4863 - acc: 0.7670 - val_loss: 0.4547 - val_acc: 0.7832\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4666 - acc: 0.7804 - val_loss: 0.4467 - val_acc: 0.7880\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4497 - acc: 0.7900 - val_loss: 0.4281 - val_acc: 0.8008\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4372 - acc: 0.7961 - val_loss: 0.4270 - val_acc: 0.7972\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4297 - acc: 0.8032 - val_loss: 0.4471 - val_acc: 0.7928\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.4302 - acc: 0.8024 - val_loss: 0.4175 - val_acc: 0.8096\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.4119 - acc: 0.8128 - val_loss: 0.4178 - val_acc: 0.8024\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.4035 - acc: 0.8164 - val_loss: 0.4159 - val_acc: 0.8084\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3987 - acc: 0.8178 - val_loss: 0.4147 - val_acc: 0.8080\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.3932 - acc: 0.8203 - val_loss: 0.4341 - val_acc: 0.7984\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.3852 - acc: 0.8259 - val_loss: 0.4047 - val_acc: 0.8148\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3785 - acc: 0.8286 - val_loss: 0.4057 - val_acc: 0.8180\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3676 - acc: 0.8371 - val_loss: 0.4110 - val_acc: 0.8124\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.3635 - acc: 0.8352 - val_loss: 0.4186 - val_acc: 0.8096\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3587 - acc: 0.8334 - val_loss: 0.4226 - val_acc: 0.8132\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3506 - acc: 0.8421 - val_loss: 0.4098 - val_acc: 0.8120\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3505 - acc: 0.8414 - val_loss: 0.4047 - val_acc: 0.8184\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3447 - acc: 0.8472 - val_loss: 0.4340 - val_acc: 0.8156\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.3345 - acc: 0.8511 - val_loss: 0.4264 - val_acc: 0.8144\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.3338 - acc: 0.8536 - val_loss: 0.3956 - val_acc: 0.8228\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.3240 - acc: 0.8594 - val_loss: 0.4192 - val_acc: 0.8180\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3190 - acc: 0.8567 - val_loss: 0.4334 - val_acc: 0.8116\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3054 - acc: 0.8660 - val_loss: 0.4108 - val_acc: 0.8220\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3097 - acc: 0.8623 - val_loss: 0.4188 - val_acc: 0.8164\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.3067 - acc: 0.8648 - val_loss: 0.4318 - val_acc: 0.8148\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.3025 - acc: 0.8635 - val_loss: 0.4553 - val_acc: 0.8144\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.3012 - acc: 0.8665 - val_loss: 0.4245 - val_acc: 0.8212\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2956 - acc: 0.8721 - val_loss: 0.4133 - val_acc: 0.8224\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.5658 - acc: 0.6972 - val_loss: 0.4885 - val_acc: 0.7588\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4884 - acc: 0.7629 - val_loss: 0.4812 - val_acc: 0.7692\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.4552 - acc: 0.7825 - val_loss: 0.4849 - val_acc: 0.7580\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.4289 - acc: 0.8032 - val_loss: 0.4524 - val_acc: 0.7780\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 66us/sample - loss: 0.4067 - acc: 0.8137 - val_loss: 0.4345 - val_acc: 0.7896\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3789 - acc: 0.8291 - val_loss: 0.4337 - val_acc: 0.7988\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3543 - acc: 0.8421 - val_loss: 0.4410 - val_acc: 0.7968\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.3252 - acc: 0.8550 - val_loss: 0.4599 - val_acc: 0.7896\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2983 - acc: 0.8708 - val_loss: 0.4457 - val_acc: 0.8000\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.2676 - acc: 0.8856 - val_loss: 0.5106 - val_acc: 0.7868\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.2428 - acc: 0.8979 - val_loss: 0.5096 - val_acc: 0.7940\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2091 - acc: 0.9136 - val_loss: 0.5377 - val_acc: 0.7952\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.1818 - acc: 0.9269 - val_loss: 0.5921 - val_acc: 0.7888\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.1578 - acc: 0.9361 - val_loss: 0.6132 - val_acc: 0.7872\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1420 - acc: 0.9438 - val_loss: 0.6449 - val_acc: 0.7880\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 61us/sample - loss: 0.1147 - acc: 0.9555 - val_loss: 0.7064 - val_acc: 0.7912\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.1057 - acc: 0.9589 - val_loss: 0.7230 - val_acc: 0.7872\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0926 - acc: 0.9644 - val_loss: 0.8738 - val_acc: 0.7844\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.0865 - acc: 0.9676 - val_loss: 0.7990 - val_acc: 0.7824\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0818 - acc: 0.9691 - val_loss: 0.7853 - val_acc: 0.7896\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0734 - acc: 0.9739 - val_loss: 0.8833 - val_acc: 0.7872\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0597 - acc: 0.9775 - val_loss: 1.1186 - val_acc: 0.7740\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0526 - acc: 0.9801 - val_loss: 1.0724 - val_acc: 0.7868\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 65us/sample - loss: 0.0574 - acc: 0.9792 - val_loss: 0.9674 - val_acc: 0.7800\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0462 - acc: 0.9836 - val_loss: 1.0481 - val_acc: 0.7788\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0549 - acc: 0.9806 - val_loss: 1.0411 - val_acc: 0.7776\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0455 - acc: 0.9835 - val_loss: 1.0232 - val_acc: 0.7848\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 63us/sample - loss: 0.0496 - acc: 0.9824 - val_loss: 1.0118 - val_acc: 0.7868\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.0393 - acc: 0.9865 - val_loss: 1.0365 - val_acc: 0.7812\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 62us/sample - loss: 0.0408 - acc: 0.9852 - val_loss: 0.9980 - val_acc: 0.7800\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.5859 - acc: 0.6843 - val_loss: 0.5131 - val_acc: 0.7428\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.5061 - acc: 0.7510 - val_loss: 0.4703 - val_acc: 0.7780\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4698 - acc: 0.7791 - val_loss: 0.4681 - val_acc: 0.7720\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4531 - acc: 0.7844 - val_loss: 0.4364 - val_acc: 0.8036\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.4350 - acc: 0.7965 - val_loss: 0.4240 - val_acc: 0.8052\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.4156 - acc: 0.8062 - val_loss: 0.4207 - val_acc: 0.8104\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.4038 - acc: 0.8146 - val_loss: 0.4243 - val_acc: 0.7972\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.3916 - acc: 0.8198 - val_loss: 0.4218 - val_acc: 0.8084\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.3805 - acc: 0.8242 - val_loss: 0.4475 - val_acc: 0.7932\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3663 - acc: 0.8355 - val_loss: 0.4178 - val_acc: 0.8116\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.3564 - acc: 0.8394 - val_loss: 0.4380 - val_acc: 0.7992\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.3464 - acc: 0.8456 - val_loss: 0.4039 - val_acc: 0.8168\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.3372 - acc: 0.8492 - val_loss: 0.4167 - val_acc: 0.8056\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.3250 - acc: 0.8542 - val_loss: 0.4037 - val_acc: 0.8152\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.3121 - acc: 0.8638 - val_loss: 0.4252 - val_acc: 0.8108\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2930 - acc: 0.8734 - val_loss: 0.4559 - val_acc: 0.8064\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2862 - acc: 0.8746 - val_loss: 0.4543 - val_acc: 0.8024\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.2798 - acc: 0.8771 - val_loss: 0.4472 - val_acc: 0.8072\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.2703 - acc: 0.8858 - val_loss: 0.4249 - val_acc: 0.8092\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2579 - acc: 0.8898 - val_loss: 0.4490 - val_acc: 0.8120\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2504 - acc: 0.8935 - val_loss: 0.4373 - val_acc: 0.8116\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.2444 - acc: 0.8939 - val_loss: 0.4710 - val_acc: 0.8096\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.2298 - acc: 0.9033 - val_loss: 0.4642 - val_acc: 0.8048\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.2204 - acc: 0.9070 - val_loss: 0.4633 - val_acc: 0.8096\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.2152 - acc: 0.9089 - val_loss: 0.4740 - val_acc: 0.8144\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.2089 - acc: 0.9139 - val_loss: 0.5101 - val_acc: 0.7960\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.2060 - acc: 0.9141 - val_loss: 0.5247 - val_acc: 0.8024\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.1903 - acc: 0.9208 - val_loss: 0.5167 - val_acc: 0.8044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.1933 - acc: 0.9196 - val_loss: 0.5281 - val_acc: 0.8064\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.1837 - acc: 0.9249 - val_loss: 0.5293 - val_acc: 0.8028\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.5953 - acc: 0.6792 - val_loss: 0.5265 - val_acc: 0.7432\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.5157 - acc: 0.7469 - val_loss: 0.5015 - val_acc: 0.7572\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4793 - acc: 0.7735 - val_loss: 0.4843 - val_acc: 0.7696\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4579 - acc: 0.7843 - val_loss: 0.4580 - val_acc: 0.7808\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4425 - acc: 0.7941 - val_loss: 0.4264 - val_acc: 0.7960\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4332 - acc: 0.7984 - val_loss: 0.4429 - val_acc: 0.7944\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4202 - acc: 0.8077 - val_loss: 0.4529 - val_acc: 0.7832\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.4140 - acc: 0.8096 - val_loss: 0.4235 - val_acc: 0.7996\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.4034 - acc: 0.8160 - val_loss: 0.4343 - val_acc: 0.8068\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3954 - acc: 0.8215 - val_loss: 0.4144 - val_acc: 0.8088\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3875 - acc: 0.8239 - val_loss: 0.4224 - val_acc: 0.8008\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3755 - acc: 0.8308 - val_loss: 0.4080 - val_acc: 0.8140\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.3729 - acc: 0.8343 - val_loss: 0.4125 - val_acc: 0.8116\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3651 - acc: 0.8373 - val_loss: 0.4263 - val_acc: 0.8044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3537 - acc: 0.8432 - val_loss: 0.4473 - val_acc: 0.7936\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3480 - acc: 0.8434 - val_loss: 0.4300 - val_acc: 0.8104\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.3436 - acc: 0.8452 - val_loss: 0.4339 - val_acc: 0.8068\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3401 - acc: 0.8515 - val_loss: 0.4327 - val_acc: 0.8132\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3335 - acc: 0.8538 - val_loss: 0.4114 - val_acc: 0.8132\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3227 - acc: 0.8542 - val_loss: 0.4242 - val_acc: 0.8160\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.3229 - acc: 0.8555 - val_loss: 0.4671 - val_acc: 0.8136\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.3097 - acc: 0.8666 - val_loss: 0.4207 - val_acc: 0.8196\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.3054 - acc: 0.8682 - val_loss: 0.4378 - val_acc: 0.8068\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2968 - acc: 0.8710 - val_loss: 0.4207 - val_acc: 0.8064\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.2914 - acc: 0.8705 - val_loss: 0.4512 - val_acc: 0.8152\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2870 - acc: 0.8754 - val_loss: 0.4710 - val_acc: 0.8032\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.2847 - acc: 0.8768 - val_loss: 0.4652 - val_acc: 0.8028\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2744 - acc: 0.8805 - val_loss: 0.4355 - val_acc: 0.8100\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.2731 - acc: 0.8825 - val_loss: 0.4540 - val_acc: 0.8124\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.2639 - acc: 0.8866 - val_loss: 0.4429 - val_acc: 0.8008\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [1,2,3]\n",
    "layer_sizes = [100,130,150,200]\n",
    "conv_layers = [1,2,3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME =\"JustTime-1x1PMT-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\Time\\{}'.format(NAME))\n",
    "        \n",
    "        \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(layer_size,(5,5),strides=1, input_shape= XTrainingT.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "            model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "            model.add(Dropout(0.4))\n",
    "            for l in range(conv_layer-1):                   \n",
    "                model.add(Conv2D(layer_size,(3,3),padding='same',activation=\"relu\"))              \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(Dropout(0.4))            \n",
    "            #model.add(GlobalAveragePooling2D())\n",
    "            model.add(Flatten())\n",
    "            for l in range(dense_layer-1):\n",
    "                model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                #model.add(Dropout(0.5))\n",
    "            model.add(Dense(32,activation=\"relu\"))\n",
    "            model.add(Dense(2))\n",
    "            model.add(Activation('softmax'))\n",
    "            #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "            model.compile(loss=\"binary_crossentropy\",\n",
    "                         optimizer=\"adam\",\n",
    "                          metrics=['accuracy']\n",
    "                         )   \n",
    "            filepath=\"PMTOnly_PI_22k_RANDOM-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "            #model.summary()\n",
    "            history=model.fit(XTrainingT,YTraining,\n",
    "          validation_data=(XValT,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_224 (Conv2D)          (None, 10, 16, 150)       3900      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_224 (MaxPoolin (None, 5, 8, 150)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_225 (Conv2D)          (None, 5, 8, 150)         202650    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_225 (MaxPoolin (None, 3, 4, 150)         0         \n",
      "_________________________________________________________________\n",
      "dropout_373 (Dropout)        (None, 3, 4, 150)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_226 (Conv2D)          (None, 3, 4, 150)         202650    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_226 (MaxPoolin (None, 2, 2, 150)         0         \n",
      "_________________________________________________________________\n",
      "dropout_374 (Dropout)        (None, 2, 2, 150)         0         \n",
      "_________________________________________________________________\n",
      "flatten_108 (Flatten)        (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 512)               307712    \n",
      "_________________________________________________________________\n",
      "dropout_375 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 492)               252396    \n",
      "_________________________________________________________________\n",
      "dropout_376 (Dropout)        (None, 492)               0         \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 32)                15776     \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 985,150\n",
      "Trainable params: 985,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [3]\n",
    "layer_sizes = [150]\n",
    "conv_layers = [3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME =\"Time-1x1PMT-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'BeamlikePI\\logs\\Time\\{}'.format(NAME))\n",
    "        \n",
    "        \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(layer_size,(5,5),strides=1, input_shape= XTrainingT.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "            model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "            for l in range(conv_layer-1):                   \n",
    "                model.add(Conv2D(layer_size,(3,3),padding='same',activation=\"relu\"))              \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))            \n",
    "            #model.add(GlobalAveragePooling2D())\n",
    "            model.add(Flatten())\n",
    "            for l in range(dense_layer-1):\n",
    "                model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "            model.add(Dense(32,activation=\"relu\"))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(2))\n",
    "            model.add(Activation('softmax'))\n",
    "            #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "            model.compile(loss=\"binary_crossentropy\",\n",
    "                         optimizer=\"adam\",\n",
    "                          metrics=['accuracy']\n",
    "                         )   \n",
    "            filepath=\"PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "            model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.6002 - acc: 0.7120\n",
      "Epoch 00001: val_acc improved from -inf to 0.50640, saving model to PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.51.model\n",
      "17000/17000 [==============================] - 13s 775us/sample - loss: 0.5990 - acc: 0.7127 - val_loss: 1.0977 - val_acc: 0.5064\n",
      "Epoch 2/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.7655\n",
      "Epoch 00002: val_acc did not improve from 0.50640\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.4966 - acc: 0.7652 - val_loss: 1.0957 - val_acc: 0.5064\n",
      "Epoch 3/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.4610 - acc: 0.7823\n",
      "Epoch 00003: val_acc improved from 0.50640 to 0.50680, saving model to PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.51.model\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4611 - acc: 0.7822 - val_loss: 1.1716 - val_acc: 0.5068\n",
      "Epoch 4/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.7936\n",
      "Epoch 00004: val_acc improved from 0.50680 to 0.72400, saving model to PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.72.model\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.4417 - acc: 0.7931 - val_loss: 0.5770 - val_acc: 0.7240\n",
      "Epoch 5/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8099\n",
      "Epoch 00005: val_acc improved from 0.72400 to 0.78720, saving model to PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.79.model\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4112 - acc: 0.8105 - val_loss: 0.4578 - val_acc: 0.7872\n",
      "Epoch 6/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8173\n",
      "Epoch 00006: val_acc improved from 0.78720 to 0.80200, saving model to PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.80.model\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.4004 - acc: 0.8172 - val_loss: 0.4512 - val_acc: 0.8020\n",
      "Epoch 7/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8332\n",
      "Epoch 00007: val_acc did not improve from 0.80200\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.3742 - acc: 0.8334 - val_loss: 0.5184 - val_acc: 0.7872\n",
      "Epoch 8/40\n",
      "16500/17000 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8393\n",
      "Epoch 00008: val_acc did not improve from 0.80200\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.3598 - acc: 0.8402 - val_loss: 0.4443 - val_acc: 0.7964\n",
      "Epoch 9/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8489\n",
      "Epoch 00009: val_acc did not improve from 0.80200\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.3379 - acc: 0.8491 - val_loss: 0.5496 - val_acc: 0.7804\n",
      "Epoch 10/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.8603\n",
      "Epoch 00010: val_acc did not improve from 0.80200\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.3223 - acc: 0.8608 - val_loss: 0.4765 - val_acc: 0.7988\n",
      "Epoch 11/40\n",
      "16500/17000 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.8654\n",
      "Epoch 00011: val_acc did not improve from 0.80200\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.3059 - acc: 0.8653 - val_loss: 0.4886 - val_acc: 0.7880\n",
      "Epoch 12/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.8799\n",
      "Epoch 00012: val_acc improved from 0.80200 to 0.80880, saving model to PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.81.model\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.2825 - acc: 0.8796 - val_loss: 0.4708 - val_acc: 0.8088\n",
      "Epoch 13/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.8869\n",
      "Epoch 00013: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.2665 - acc: 0.8871 - val_loss: 0.5179 - val_acc: 0.8004\n",
      "Epoch 14/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.8989\n",
      "Epoch 00014: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.2440 - acc: 0.8985 - val_loss: 0.5153 - val_acc: 0.7816\n",
      "Epoch 15/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9073\n",
      "Epoch 00015: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.2278 - acc: 0.9071 - val_loss: 0.6793 - val_acc: 0.7872\n",
      "Epoch 16/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9133\n",
      "Epoch 00016: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.2092 - acc: 0.9133 - val_loss: 0.6081 - val_acc: 0.7984\n",
      "Epoch 17/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9212\n",
      "Epoch 00017: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.1963 - acc: 0.9211 - val_loss: 0.5566 - val_acc: 0.7984\n",
      "Epoch 18/40\n",
      "16400/17000 [===========================>..] - ETA: 0s - loss: 0.1789 - acc: 0.9274\n",
      "Epoch 00018: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.1797 - acc: 0.9271 - val_loss: 0.5712 - val_acc: 0.8004\n",
      "Epoch 19/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9348\n",
      "Epoch 00019: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.1665 - acc: 0.9347 - val_loss: 0.6145 - val_acc: 0.8032\n",
      "Epoch 20/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9399\n",
      "Epoch 00020: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.1552 - acc: 0.9398 - val_loss: 0.6062 - val_acc: 0.7956\n",
      "Epoch 21/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9418\n",
      "Epoch 00021: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.1523 - acc: 0.9420 - val_loss: 0.6768 - val_acc: 0.7876\n",
      "Epoch 22/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9452\n",
      "Epoch 00022: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.1437 - acc: 0.9452 - val_loss: 0.6265 - val_acc: 0.7884\n",
      "Epoch 23/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9507\n",
      "Epoch 00023: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.1248 - acc: 0.9505 - val_loss: 0.6413 - val_acc: 0.8012\n",
      "Epoch 24/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9536\n",
      "Epoch 00024: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.1184 - acc: 0.9533 - val_loss: 0.7046 - val_acc: 0.7964\n",
      "Epoch 25/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9556\n",
      "Epoch 00025: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.1171 - acc: 0.9559 - val_loss: 0.7119 - val_acc: 0.7980\n",
      "Epoch 26/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9564\n",
      "Epoch 00026: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.1160 - acc: 0.9562 - val_loss: 0.6643 - val_acc: 0.7796\n",
      "Epoch 27/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9608\n",
      "Epoch 00027: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.1041 - acc: 0.9608 - val_loss: 0.7202 - val_acc: 0.7956\n",
      "Epoch 28/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9614\n",
      "Epoch 00028: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.1024 - acc: 0.9612 - val_loss: 0.7352 - val_acc: 0.8008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9635\n",
      "Epoch 00029: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.0966 - acc: 0.9638 - val_loss: 0.7440 - val_acc: 0.7892\n",
      "Epoch 30/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9613\n",
      "Epoch 00030: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.0985 - acc: 0.9614 - val_loss: 0.7546 - val_acc: 0.8040\n",
      "Epoch 31/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9649\n",
      "Epoch 00031: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.0911 - acc: 0.9646 - val_loss: 0.7825 - val_acc: 0.8028\n",
      "Epoch 32/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9693\n",
      "Epoch 00032: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.0882 - acc: 0.9692 - val_loss: 0.8033 - val_acc: 0.7884\n",
      "Epoch 33/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9687\n",
      "Epoch 00033: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.0888 - acc: 0.9686 - val_loss: 0.7912 - val_acc: 0.7936\n",
      "Epoch 34/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9720\n",
      "Epoch 00034: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.0760 - acc: 0.9720 - val_loss: 0.7893 - val_acc: 0.7884\n",
      "Epoch 35/40\n",
      "16700/17000 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9698\n",
      "Epoch 00035: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.0856 - acc: 0.9696 - val_loss: 0.7481 - val_acc: 0.7932\n",
      "Epoch 36/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9705\n",
      "Epoch 00036: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.0775 - acc: 0.9705 - val_loss: 0.7488 - val_acc: 0.8032\n",
      "Epoch 37/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9706\n",
      "Epoch 00037: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.0794 - acc: 0.9705 - val_loss: 0.7882 - val_acc: 0.7932\n",
      "Epoch 38/40\n",
      "16600/17000 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9739\n",
      "Epoch 00038: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.0712 - acc: 0.9734 - val_loss: 0.7934 - val_acc: 0.7900\n",
      "Epoch 39/40\n",
      "16900/17000 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9716\n",
      "Epoch 00039: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.0763 - acc: 0.9717 - val_loss: 0.7604 - val_acc: 0.7956\n",
      "Epoch 40/40\n",
      "16800/17000 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9732\n",
      "Epoch 00040: val_acc did not improve from 0.80880\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.0733 - acc: 0.9732 - val_loss: 0.8235 - val_acc: 0.8044\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxU5dn/8c+VjawkEHbCvgmooCCL+0bFDaXuW9VWsbW22tY+6tNq1f76aBf7WKuPti51wX2FKgqiotaFHZVF9iVhS0gIJJN95vr9cZ/AEANMwkxmMrner1demeXMmWtOMud7zn2fcx9RVYwxxrRdCdEuwBhjTHRZEBhjTBtnQWCMMW2cBYExxrRxFgTGGNPGWRAYY0wbZ0Fg2hQReVpE/l+I024QkdMjXZMx0WZBYIwxbZwFgTGtkIgkRbsGEz8sCEzM8Zpkfi0iX4uIT0SeFJGuIvKuiJSJyGwR6RA0/SQRWSYipSIyR0SGBj13lIgs8l73MpDa4L3OEZEl3ms/F5EjQ6zxbBFZLCK7RSRfRO5u8Pzx3vxKveev8R5PE5EHRGSjiOwSkf94j50sIgWNLIfTvdt3i8hrIjJVRHYD14jIGBH5wnuPrSLysIikBL1+uIi8LyIlIrJdRP5bRLqJSIWI5AZNN0pEikQkOZTPbuKPBYGJVRcAE4DBwLnAu8B/A51w/7c/BxCRwcCLwC1AZ2AG8G8RSfFWim8BzwEdgVe9+eK99mjgKeAGIBf4BzBdRNqFUJ8P+AGQA5wN/EREzvfm29ur9+9eTSOBJd7r/gKMAo71avovIBDiMjkPeM17z+cBP/ALb5mMB04DbvRqyAJmA+8BPYCBwAequg2YA1wcNN8rgZdUtTbEOkycsSAwservqrpdVTcDnwJzVXWxqlYDbwJHedNdAryjqu97K7K/AGm4Fe04IBl4UFVrVfU1YH7Qe1wP/ENV56qqX1WfAaq91x2Qqs5R1W9UNaCqX+PC6CTv6SuA2ar6ove+xaq6REQSgB8CN6vqZu89P/c+Uyi+UNW3vPesVNWFqvqlqtap6gZckNXXcA6wTVUfUNUqVS1T1bnec8/gVv6ISCJwGS4sTRtlQWBi1fag25WN3M/0bvcANtY/oaoBIB/o6T23WfcdWXFj0O0+wK+8ppVSESkFenmvOyARGSsiH3lNKruAH+O2zPHmsbaRl3XCNU019lwo8hvUMFhE3haRbV5z0f+EUAPANGCYiPTH7XXtUtV5zazJxAELAtPabcGt0AEQEcGtBDcDW4Ge3mP1egfdzgf+oKo5QT/pqvpiCO/7AjAd6KWq2cBjQP375AMDGnnNDqBqP8/5gPSgz5GIa1YK1nCo4EeBb4FBqtoe13R2sBpQ1SrgFdyey1XY3kCbZ0FgWrtXgLNF5DSvs/NXuOadz4EvgDrg5yKSJCLfB8YEvfZx4Mfe1r2ISIbXCZwVwvtmASWqWiUiY4DLg557HjhdRC723jdXREZ6eytPAX8VkR4ikigi470+iVVAqvf+ycBvgYP1VWQBu4FyETkM+EnQc28D3UTkFhFpJyJZIjI26PlngWuAScDUED6viWMWBKZVU9WVuPbuv+O2uM8FzlXVGlWtAb6PW+HtxPUnvBH02gW4foKHvefXeNOG4kbgXhEpA+7CBVL9fDcBZ+FCqQTXUTzCe/pW4BtcX0UJ8EcgQVV3efN8Arc34wP2OYqoEbfiAqgMF2ovB9VQhmv2ORfYBqwGTgl6/jNcJ/Uir3/BtGFiF6Yxpm0SkQ+BF1T1iWjXYqLLgsCYNkhEjgHex/VxlEW7HhNd1jRkTBsjIs/gzjG4xULAgO0RGGNMm2d7BMYY08a1uoGrOnXqpH379o12GcYY06osXLhwh6o2PDcFaIVB0LdvXxYsWBDtMowxplURkY37e86ahowxpo2zIDDGmDbOgsAYY9q4VtdH0Jja2loKCgqoqqqKdikRlZqaSl5eHsnJdv0QY0z4xEUQFBQUkJWVRd++fdl3oMn4oaoUFxdTUFBAv379ol2OMSaOxEXTUFVVFbm5uXEbAgAiQm5ubtzv9RhjWl5cBAEQ1yFQry18RmNMy4uLpiFjjIlFqsrWXVWsKSxnbVE5FTV+Ome1cz+Z7nduRgpJiftuk9f5A5RU1FDiq6G4vIZiXw0l5dWMG5DLYd3ah71OC4IwKC0t5YUXXuDGG29s0uvOOussXnjhBXJyciJUmTFtl6pSsLOSFVt3s2JrGSu27ubbbbtJTkygV8d08jqk0atDOr06ppHXIZ1eHdLJTncHYgQCSkWtH191Hb7qOipq3O3KWj/K3svAiQgCiIAglFfXsbaonLWF5azxfvtq/AesUwQ6pKfQKTOFOr9S7KthV2Vto9PeM2m4BUGsKi0t5f/+7/++EwR+v5/ExMT9vm7GjBmRLs2YuFBUVs3c9cXMXVfChmIfKYkJpCYn0i7Z/U5NSiTVu13iq2H51t2s2Lqbsqo6wK1s++ZmMLR7e/wBJX9nJfPXl1BWXbfP+2SkJBJQqKw98Mr7YLq1T2Vgl0wuGt2LAV0yGdg5k4FdMslsl8SO8moKy6opKqtmR/m+v5MTE+iYkULHjBRyM1PIzWgXdDuFnPSUQ6prfywIwuD2229n7dq1jBw5kuTkZDIzM+nevTtLlixh+fLlnH/++eTn51NVVcXNN9/MlClTgL3DZZSXl3PmmWdy/PHH8/nnn9OzZ0+mTZtGWlpalD+ZMdFRWFbF3HUlfLmumLnrS1hTWA64FfXArlnU1gWoqvNTXRugqtbvfuoC+ANKekoih3XLYtKIHgzt3p5hPdozpGsWGe2+u7rbVVFL/s4KCnZWkF9SyZZdlSQnJpCekkhGShLp7dzvjHZJZKQk0i45kfquOjdws6K692LSqUmJ9OucQWYj71WvV8d0enVM3+/z0RB3QXDPv5exfMvusM5zWI/2/O7c4ft9/v7772fp0qUsWbKEOXPmcPbZZ7N06dI9h3k+9dRTdOzYkcrKSo455hguuOACcnNz95nH6tWrefHFF3n88ce5+OKLef3117nyyivD+jmMiZRAQNleVsXG4go2FVeQmpK4p+mlU2bKfg90qG++WbmtjJXby/h2WxnLNu9i3Q4fAJntkhjdtwMXjspjXP9cDu/R/jvt6cFq/QESRUhICO3Aiuz0ZLLTszm8Z3bTP3QcibsgiAVjxozZ51j/hx56iDfffBOA/Px8Vq9e/Z0g6NevHyNHjgRg1KhRbNiwocXqNSaYr7qOjcUV1PoD1AUC1PqVOr9SGwhQ51fq/AGKfTVsLPaxobiCjcU+NhZXUF0XaHR+ackuFPI6pNGrYzpd26d6K//drNpeTnlQ80zPnDSGds/ikmN6Ma5/LsMPsuJvKLkJ05q94i4IDrTl3lIyMjL23J4zZw6zZ8/miy++ID09nZNPPrnRcwHatWu353ZiYiKVlZUtUqsx5dV1LNhQwpdeU8w3m3fhDxz8glXtkhLok5tOn9wMThrcmT65GfTNzaB3x3Sq6vzkl1SQX1JBwc5K8r2mlwUbd1JWVUd2WjJDumXx/aN7MqRbFod1y2Jw1yyyUu2s+WiIuyCIhqysLMrKGr/i365du+jQoQPp6el8++23fPnlly1cnWlrVJVdlbVU1Qa8rXq3FV/r1z1b+KUVNczzVv5LvRV/cqIwIi+HH5/Un+E9smmXlEBSYgLJCUJSYgJJiUJyQgKJCUKHjGS6ZqUesAlmcNesRh+vqKkjLTnRzouJIRYEYZCbm8txxx3H4YcfTlpaGl27dt3z3MSJE3nsscc48sgjGTJkCOPGjYtipSZeqCqFZdVs2OGaZTYU+9hY4jXT7Kj4ztEwjUlOFEb2yuHGkwcwtl8uR/fJIT0l8quElngP0zSt7prFo0eP1oYXplmxYgVDhw6NUkUtqy19VuPa69fv8LG2qJx1RXt/r9/h2+cQx6QEIa9Dmtc8445KSU9JclvxiUJSQsKe30mJQka7JA7vkU1ayv4PbzbxRUQWquroxp6zaDYmhhTsrGDOyiI+XlXENwW72LZ7b3+SCOR1SKN/p0zG9u9I/04Ze9rle+SkNqlT1ZhgFgTGRFFVrZ9560v4eFURc1YWsrbIHTaZ1yGN8QNyGdA5g/6dMxnQOZM+uemkJtsWvAk/CwJjWkjDY+YXbtzJF2uLqaz1k5KUwNh+Hbl8bB9OHtKZ/p0yrDPVtBgLAmPCrM4foLCsmk0lFazc5k6SauyY+X6dMrh4dB4nD+nCuP651l5vosaCwJhm8FXXsXDjTjaVVLCltJLNpZVsKa1kS2kV23ZX7XMcvh0zb2KdBYExIQgElOVbd/PJ6iI+WVXEwo07qfW7lX1SgtA9J5Ue2WmM7deRHjlp9OyQRs+cNIZ0y6JLVjtr5jExzYIgDJo7DDXAgw8+yJQpU0hPj61BqAwUl1d7K/4dfLq6iB3lNQAM7d6eHx7fjxMGdmZgl0w6Z7UjMcSxbYyJRRYEYbC/YahD8eCDD3LllVdaEMQIVWXu+hKe+3IjM5duoy6gdMxI4YRBnThxUGdOGNSJLu1To12mMWFlQRAGwcNQT5gwgS5duvDKK69QXV3N5MmTueeee/D5fFx88cUUFBTg9/u588472b59O1u2bOGUU06hU6dOfPTRR9H+KG1WWVUtby7ezHNfbGR1YTnZaclce1xfJo3oyfAe7UMezdKY1ij+guDd22HbN+GdZ7cj4Mz79/t08DDUs2bN4rXXXmPevHmoKpMmTeKTTz6hqKiIHj168M477wBuDKLs7Gz++te/8tFHH9GpU6fw1mxC8u223Uz9ciNvLtqMr8bPiLxs/nzhkZw7oocds2/ajPgLgiibNWsWs2bN4qijjgKgvLyc1atXc8IJJ3Drrbdy2223cc4553DCCSdEudK2a1NxBe8t28qMb7axJL+UdkkJnDuiB1eN68OIXnbZUNP2xF8QHGDLvSWoKnfccQc33HDDd55buHAhM2bM4I477uB73/sed911VxQqbHtUldWF5by3dBvvLd3G8q3uwkVH9MzmN2cN5cJReXTIiMwlAI1pDeIvCKIgeBjqM844gzvvvJMrrriCzMxMNm/eTHJyMnV1dXTs2JErr7ySzMxMnn766X1ea01D4bemsIw3Fm3mvWXbWFfkQwRG9e7Ab88eyhnDu8Xc5QKNiRYLgjAIHob6zDPP5PLLL2f8+PEAZGZmMnXqVNasWcOvf/1rEhISSE5O5tFHHwVgypQpnHnmmXTv3t06i8PAH1Bmr9jOs19s4LM1xSQmCOP6d+Ta4/pxxrCudsSPMY2wYahbmbb0WZtip6+Glxfk89wXG9lcWkmP7FSuGNeHS47pRafMdgefgTFxzoahNnFr2ZZdPPP5BqYt2UJ1XYDx/XO585yhnD60qw3LbEyILAhMq7OuqJwZ32zl7a+38u22MtKSE7lwVB4/GN+XId0avzyiMWb/4iYIVDXux3Npbc144bR+h2/Pyn+Fd9TP6D4duPvcYUw+Oo/sNBvAzZjmiosgSE1Npbi4mNzc3LgNA1WluLiY1NS209lZ4qvhpfmbePurrXsO+RzVpwN3nTOMM4/oRvfstChXaEx8iGgQiMhE4G9AIvCEqt7f4Pk+wFNAZ6AEuFJVC5r6Pnl5eRQUFFBUVBSGqmNXamoqeXl50S4j4jaXVvLEp+t4aV4+lbV+ju6dw2/PHspZR3SnR46t/I0Jt4gFgYgkAo8AE4ACYL6ITFfV5UGT/QV4VlWfEZFTgfuAq5r6XsnJyfTr1y8cZZsoWr29jMc+Xse0JZsBOP+ontxwYn8GdbV2f2MiKZJ7BGOANaq6DkBEXgLOA4KDYBjwC+/2R8BbEazHxKhFm3by6Jy1vL98O2nJifxgfF9+dEI/etrWvzEtIpJB0BPID7pfAIxtMM1XwAW45qPJQJaI5KpqcfBEIjIFmALQu3fviBVsWtaq7WXcPX0Zn68tJic9mZtPG8TVx/alow33YEyLimQQNNZr2/Cwl1uBh0XkGuATYDNQ950Xqf4T+Ce4E8rCW6ZpaZU1fh76cDWPf7KOrNQkfnv2UC4b05uMdnFx7IIxrU4kv3kFQK+g+3nAluAJVHUL8H0AEckELlDVXRGsyUTZh99u565pyyjYWclFo/K446yhtgdgTJRFMgjmA4NEpB9uS/9S4PLgCUSkE1CiqgHgDtwRRCYObd1VyT3Tl/Pesm0M7JLJy1PGMbZ/brTLMsYQwSBQ1ToRuQmYiTt89ClVXSYi9wILVHU6cDJwn4gormnop5Gqx0RHnT/As19s5IFZK6kLKL8+YwjXn9CflCQb/sGYWBEXg86Z2LOropY3Fhcw9cuNrC3ycdLgzvz+vMPpnWtDPxsTDTbonGkRqsqS/FKen7uJf3/lBoEb2SuHR684momHd4vbs76Nae0sCMwhK6+u463Fm3l+7iZWbN1NRoobBO7ysb0Z3iM72uUZYw7CgsA0mz+gPPLRGv7x8Vp8NX6GdW/PHyYfznkje5Jph4Ia02rYt9U0y05fDTe/vIRPVhVx1hHdmHLiAEbkZVvzjzGtkAWBabKv8ku58flFFJVV8z+Tj+CyMb0sAIxpxSwITMhUlRfmbeKe6cvpnNWO134yniPzcqJdljHmEFkQmJBU1vj5zVvf8MaizZw4uDN/u2QkHeyMYGPiggWBOagNO3z8eOpCVm4v45bTB/GzUweRmGBNQcbECwsC06gSXw3z1hczd30Jry0sIDFB+Nc1x3DykC7RLs0YE2YWBAaAwrIq5q0vYe66EuauL2bV9nIAUpMTOHZAJ+6ZNJxeHe2sYGPikQVBG/fe0m08MGslqwvdij8jJZFRfTty3siejOvfkSN65ti4QMbEOQuCNmrbrip+N30pM5dt57BuWdxx5mGM7Z/L4T3ak5RoK35j2hILgjYmEFCen7eJP737LTX+ALdNPIzrTuhHsq38jWmzLAjakNXby7j9jW9YuHEnxw3M5X8mH0Gf3Ixol2WMiTILgjagus7PIx+t5dE5a8hol8QDF43g+0f3tLOBjTGABUFcq/UHmL5kCw9/tIb1O3ycP7IHd54zjNzMdtEuzRgTQywI4lBVrZ9XF+Tz2Mfr2FxayWHdsnjmh2M4aXDnaJdmjIlBFgRxpLy6jue/3MgT/1lPUVk1R/fO4ffnD+eUIV2sGcgYs18WBHFgp6+Gpz/fwNOfb2BXZS3HD+zEQ5cexbj+HS0AjDEHZUHQStX5A3y6egevLSrg/eXbqakLMGFYV356ykBG9rIRQY0xobMgaGVWbS/j9YUFvLF4M0Vl1XRIT+byMb25bExvhnTLinZ5xphWyIKgFdhdVctbizfz2sICvi7YRVKCcMphXbjg6DxOPayLDQFhjDkkFgQxbsMOH9f8ax4biisY1r09d50zjEkje9DJDgE1xoSJBUEMW7RpJ9c9swBV5cXrxzF+QG60SzLGxCELghg1c9k2fv7iYrplp/L0tWPo18mGgjDGRIYFQQx6+rP13PP2ckbk5fDE1aOtGcgYE1EWBDEkEFDue3cFj3+6ngnDuvLQpUeRlpIY7bKMMXHOgiBGVNX6+dUrX/HON1u5enwf7jp3uF0X2BjTIiwIYsDGYh+3vvoV8zfs5DdnDeW6E/rZGcHGmBZjQRAlJb4a3vl6C28u3syiTaWkJCXw8OVHcc6RPaJdmjGmjbEgaEFVtX4+WFHIm4s3M2dlIXUBZUjXLG4/8zDOG9mD7tlp0S7RGNMGWRC0gNKKGv743re8/dVWyqrr6Nq+HT88vh/nj+zJ0O5Z1gxkjIkqC4IIK9xdxVVPzmP9Dh/njujB5KN6Mn5Abvx0BH/+MKz/GMZMgYGng4WaMa1ORINARCYCfwMSgSdU9f4Gz/cGngFyvGluV9UZkaypJRXsrODKJ+ZSWFbN09cew7EDO0W7pPBa9CzM+g0kp8PqWdB5KBz7MzjiIkhKiXZ1xsSX6jJISoPE8K+2IxYEIpIIPAJMAAqA+SIyXVWXB032W+AVVX1URIYBM4C+kaqpJa0tKufKJ+biq65j6nVjObp3h2iXFF6r34d/3wIDToNLpsKK6fDZQzDtRvjw9zD2Bhh1LaS1siGxA37YlQ871sDO9dB9JOSNtj2dcClcATP/G/y1kJoNqTnufyQ1x91Py4H2PaHPsbG3zHdvhfWfwODvQVoLfp8rSmDuP2DuY3DWX+DIi8L+FpHcIxgDrFHVdQAi8hJwHhAcBAq0925nA1siWE+LWbZlFz94ch4i8NKU8Qzr0X7/Ewf8MOc+WPYWTHrIfQFi3eZF8MrV0O1wuPgZSEmHEZfCkZfA2g/h84dg9t3wyV/g6KvdXkL77tGueq+6Gijb6n5K1sGO1VC8xvtZC/7qfafvdgQcc53b00lpgaE+AgFY/Kxb8fQ7AfKOgaQYObu8cqdbGWZ0gT7jm/ba5dPhzR9Dchp0GuSWfWUpVO2CWt++0x5+ofs+tMTyPpiq3fDZ3+CLR6Cu0oXWibfCMddDcmrk3nf3VvjiYVjwL7d8hpwNnQdH5K1EVSMzY5ELgYmqep13/ypgrKreFDRNd2AW0AHIAE5X1YUHmu/o0aN1wYIFEak5HBZu3Mk1/5pHZrskpl43lgGdM/c/cWUpvHG9a1ZJzYGacph4v1vptNTWUEWJ+wc/4kLoMvTg05eshycnuC/zj2ZDVtfGp9v6NXz+d1j6OmR1g2vegY79wlt72TZY8W8XpvtT64PdW/b98RXuO01CEnToC7mDoNNA9zt3IOT0gjWzYf6TsH0ptGsPIy6DY34EnYeE97PUK82Ht34CGz7d+1hSGvQeB/1Pgn4nur2UhEbOOA/43Yq6vNCtsPY3XVMEArB1Maz5wC2LgvmgAffcERfBGfdB5kGuhR0IwEd/gE//Aj1Huz3IhhsGdTUuEKp2wfI34aP/gc6HwcXPub9JNNTVwIKn4JM/QUUxHH4BjLgcvvw/WPsBZPeCU38LR1wMCWEcCr5kvQueJc9DoM6F4vG/gK7DDmm2IrJQVUc3+lwEg+Ai4IwGQTBGVX8WNM0vvRoeEJHxwJPA4ar1/2l7ppsCTAHo3bv3qI0bN0ak5kP12ZodXP/sArpktWPqdWPJ65C+/4kLv4WXLofSjXDmn9w/2RtTYPVMOOoqOPuB0LYCq8tdW33H/jBkYtMKriiBZybB9m9AEmHM9XDy7fvf7fUVuxCoLIEfve+26g5m69fw7CRIyXRh0KFP02psjL8O5j8OH/4BasoOPn2q19zQvsd3f3L6uBBITN7/61Uhfx4seBKWvQn+Guh7Aoz+IQw998CvDZUqfP0KzLjVrWgn3u/mvfFz1xm/7mMoWuF9nmzoc7wLY1/R3p+K4r0raXD/U5P/2fQ25RofrHjbrfjXfuDmi0CPo9wBAQNOhXVz4NMH3Bb79/4fHHVl4xsvlaXN+79e+yG89iO3Ipz8GBx2dtM+w6EIBGDZG/DBve772e9EOP0e6Hn03mnWzYH374KtX0HXI2DC3a6ZtLkbcKqw7Ru3B/DNay7AR14Bx/3cfbfDIFpBMB64W1XP8O7fAaCq9wVNswy315Dv3V8HjFPVwkZmCcTuHsH7y7fz0+cX0a9TBs9dN4YuWQfYZVzxNrx5g/siX/zs3uag72w5PedWVo2pLncrw88ecitmgAn3wrE/D+2fsT4EdqyCyY/Chs9g4b9cCJx2l/vSBm9N1lS4Ffq2b+AH06H32NAWDMCWJe61qTlw7QzIzgv9tQ1tmgvv/MqF14DT4Iw/QOZ+9koAklJd01W4+HbA4qluS7F0I2T1cHsIo66FjGYOE15RAm/fAsunQe/xcP6jje89lRe6Zpn1H7u/FwoZnb2fTvveLlwBn/y56WFQmg8vXAyFyyG9Eww8be/KP6PBwQ5FK10/0abPXTCd++C+GwdFK+HFy7yNnT/C6B81bUVZmg+vXAVbFsPxv3Rb3wfaw6mtdMGpAfedamqzkr8O1s+BD34PW5ccfAXfWGCcdLsLjOQQzgmqq3HLbuW7sHIGlG5yB16M/iGMvynszanRCoIkYBVwGrAZmA9crqrLgqZ5F3hZVZ8WkaHAB0BPPUBRsRYExeXV/HnmSl5ekM+RPbN55odjyEnfzxEzgQB8fD98/EfocbTbRc7u+d3plk+DN38C7TJdUPQet/e56jKY97hrdqkscV/S43/pQmHZm+6f6Mw/H/iL7yuGZ89zIXDZi+7LDm7r/d3b3D9n9xFuPr3HuiaHl69y/6yXPOe2VJtq80J49nxIz3VhsL+AO1DNs+9yK+H2PWHifTB0UvQ6FAMBWPO+68Bb+6ELnCMugnE/ga7DQ5/P6tkw7aduq/vU37ggP9TmnHr/eRBm/w6Gfx++//jBw2DLEnjhEqitcNMP+t7BmzwCAVj8HLx/p1sRn/Ar14yxZja8cYNrQw/e2Gmq2ip47zZY+DT0PxkueHJvIKm6Pp0177v32/AfqKtyzyWmuPccOMF9RzoP+e7/SiDgAi84XGvKILu31+RzUWhNPg2bkCQBOvRzTTldhrkm1y7D3ZZ9TZn7m6+c4Wqu3u3+d/qfDEPOdP/T6R2bt6wOIipB4L3xWcCDuENDn1LVP4jIvcACVZ3uHSn0OJCJ6zj+L1WddaB5xkoQ1PkDvDBvE3+ZuRJfjZ9rj+3LLyYMJqPdfr5sVbvcF2PVu66d8Zz/PXBH0/blruloVwGc9WfXhr9PAExwzTh53t81EIAP7oHPHnRf4Av/5YKkofoQKF4Nl76wNwTqqbp2/Vl3QtkW1wGcmOxWwGf9xTUfNVf+fHhusutXuOYd13dwMIEALHrGfbbqMhh3I5x0W+OfLVoKv3WB8NVLrm2+7wkuEAZP3LtSV3Ur2JoK129RU+GamuY/4VYWk/8B3Y8Mf22f/c01YRwsDFbNhFevdSuhy19pent0eSG8dwcsfQ3a58HuAteUdMnUQ9sDrLd4Krz9S7fHc/Ltbi9hzWy3JQ6uX2fg6e4nIcHr0/hgb3Na+zz3vz7gVK/D+2NY/ylU7HDPdxzg9cGc5P5uzekErtrtNgoKV0DhMve7ZN3e5rrEdq6pS/3ucwyeCEPOciEQzr3W/YhaEERCLATB/A0l3DVtGSu27ubYAbncPWk4gwM53XQAABX3SURBVLse4MLxFSXw1Blu62Xife7kq1C2ZCt3unbStR9AcoZbgTQMgIYWPAXv3Oq2Si9/Zd/dS1+xa6IpXtN4CASr8cGnf3VHAPlr4LhbYMI9B6/5YDZ9Cc99360crnkbMrs0Pt3uLW6XefFU2LII+hzn2pdD6dCOlooSF1rznnArwvRct3VYU+FCgIbfNYHxP4VT74zs0Sd7wmAyfP+J74bB/Cdgxq/d0VGXvxJaQO/P6tkw8w7oNdZtOITzc21Z4pqKSje570P/k7yV/2mun6cxpfnu+7Nmtutnqd7tHs/q7lb69R3w4QirxtRWuiaywuWwfZnb+h88EXqOCm8HcwgOOQhE5HXgKeDdhh25LS2aQVC4u4r73v2WNxdvpkd2Kr89ZxhnHt7t4ENELHgK3v4FXPEaDJrQtDcN+F1T0vZlbpd7fwEQbPX78Oo1rlPxilddKDQlBIKVrHNb8qHuJodiw39g6oWuHfzqt13buips+9prL33XtdGC28U++Xa3ZxJrx5Xvj78Ovv23WykmJru26uR0t9WXnOH9TodOg90huC3hs4dc801wGAQCrrnt87/DoDPgwqdia0+rMVW7XZNmtyObftKiv9btSaTmuL6M1vL/FCbhCILTgWuBccCrwNOq+m1YqwxRNIJAVZn65Ub++N5KauoCTDmxPzeeMoD0lBA74F641O0q3vx1y/3zbf3adfpVl8Okv7mt++I1rk9gwKktU8OBrPvY1Zc70PWBrHwXdm8GBHqN2bvb3Fjbrmmez/8Os34Lw86H8x52fRPLp7nj4SfeH5EzVk3sCFvTkIhkA5cBvwHyce37U1W1NhyFhqKlgyD4qmEnDu7MvZOG07cp1w+urYI/9XOHgp39l8gV2phdm93KdvtSt0saKyFQb80H7qiShERX15Az3ZbpwY5LN81XHwap2W7r+ow/uH4XC9u4d6AgCHkTQERygSuBq4DFwPPA8cDVwMmHXmbsqfMHuO31b3h9UQFXj+/D784dTkJTB4vb+B/XPjzoe5Ep8kCye8K177pDUg87x52lGksGnga/8E7UimQbudnr2J+5fotP/uzOCh92XrQrMjEgpCAQkTeAw4DngHNVdav31MsiEv1DeCKgqtbPTS8sYvaKQn5x+mB+ftrA5g0XvWqWOzM0Wivh1PbuGO5Ytb/OYhM5439qewFmH6HuETysqh829sT+djVas12VtVz/zALmbyzh9+cN56rxfZs3I1V3RmW/E0M7wcSYlmIhYIKEehjIUBHZM4ykiHQQkRsjVFNUFZZVcek/v2Rx/k4euvSo5ocAuM7ZnRuafqSQMca0oFCD4HpVLa2/o6o7gUM4syg2bSqu4KLHvmDDDh9PXn0M5444xOsHr5rpfg8+49CLM8aYCAm1aShBRKR+6AfvWgNxdeWRldvKuPLJudT6A7xw/ViOCsf1A1bPdBdryel96PMyxpgICXWPYCbwioicJiKnAi8C70WurJZ317SlqMKrN4wPTwhU7XYDYA2OwtFCxhjTBKHuEdwG3AD8BBDcNQSeiFRRLW3rrkrmbSjhF6cPZtCBhopoinVz3Lgi0Ths1BhjmiCkIPCGlXjU+4k7b3+1FVWYdKh9AsFWz4R22W7MFWOMiWGhnkcwCLgPGAbsOfNHVcNzxYQom/bVZkbkZTftjOEDCQTceD8DTw3PRUuMMSaCQu0j+Bdub6AOOAV4FndyWau3prCcpZt3M2lkI9cFaK5tX0P5dmsWMsa0CqEGQZqqfoAbm2ijqt4NxNCgNc03/astiMC5R4bxakCrZwHihow2xpgYF2pncZWIJACrReQm3BXHWv3YAKrK9CWbOXZALl3ah3Gsm1Uz3eXqbPA0Y0wrEOoewS1AOvBzYBRu8LmrI1VUS/m6YBcbiis4b0QYm4V8O9xlGa1ZyBjTShx0j8A7eexiVf01UI67LkFcmLZkCymJCZxx+CFckamhNbMBtSAwxrQaB90jUFU/MEqaNfRm7PIHlH9/vYWTh3QmOy2MR/asmgkZXaD7yPDN0xhjIijUPoLFwDQReRXw1T+oqm9EpKoWMHddMUVl1ZwXzqOF/HXu+qiHndvi1yM1xpjmCjUIOgLF7HukkAKtNgimLdlCRkoipw0NY593wTyo2mWjjRpjWpVQzyyOm34BgOo6PzOWbuWMw7uRmpwYvhmvmgkJSTDglPDN0xhjIizUM4v/hdsD2Ieq/jDsFbWAOSuLKKuqC2+zELiziXuPd9eDNcaYViLUpqG3g26nApOBLeEvp2VMX7KF3IwUjhuQG76ZluZD4TKY8PvwzdMYY1pAqE1DrwffF5EXgdkRqSjCyqpqmb1iO5cc04ukxDB26K6e5X7bRWiMMa1Mc9eEg4BWebWVWcu2U10X4LyRYRxpFFwQ5PSBToPDO19jjImwUPsIyti3j2Ab7hoFrc60r7aQ1yGNo8Nx8Zl6/lpY9zEcdaVdFNwY0+qE2jQUpqu1RNeO8mo+W7ODG07sT1jPjysvhLpK6Do8fPM0xpgWElLTkIhMFpHsoPs5InJ+5MqKjBnfbMUf0PAfLeQrcr8zbJA5Y0zrE2ofwe9UdVf9HVUtBX4XmZIiZ9qSLQzpmsWQbmHewanY4X5bEBhjWqFQg6Cx6UI99DQm5JdUsHDjTiaFu5MY3IijABmdwj9vY4yJsFCDYIGI/FVEBohIfxH5X2BhJAsLt+lfudMewnpd4np7moYsCIwxrU+oW/U/A+4EXvbuzwJ+G5GKImTyUT3p1j6VXh3Twz9zXxEkpkC79uGftzHGRFioRw35gNsjXEtE9chJ44JReZGZua/Y9Q/YoaPGmFYo1KOG3heRnKD7HURkZgivmygiK0VkjYh8J0hE5H9FZIn3s0pESptWfozwFUF6GIerMMaYFhRq01An70ghAFR1p4gccPxm78pmjwATgAJgvohMV9XlQfP5RdD0PwOOakrxMcNXZEcMGWNarVA7iwMismdICRHpSyOjkTYwBlijqutUtQZ4CTjvANNfBrwYYj2xpWKHBYExptUKdY/gN8B/RORj7/6JwJSDvKYnkB90vwAY29iEItIH6Ad8uJ/np9S/X+/eMTjEkW+HHTFkjGm1QtojUNX3gNHAStyRQ78CKg/yssZ6Tve3F3Ep8Jp3feTG3v+fqjpaVUd37hxjW941PqitsCAwxrRaoQ46dx1wM5AHLAHGAV+w76UrGyoAegXdz2P/1zC4FPhpKLXEHBtewhjTyoXaR3AzcAywUVVPwXXqFh3kNfOBQSLST0RScCv76Q0nEpEhQAdcsLQ+vmL324LAGNNKhRoEVapaBSAi7VT1W2DIgV6gqnXATcBMYAXwiqouE5F7RWRS0KSXAS+p6sE6n2OTnVVsjGnlQu0sLvDOI3gLeF9EdhLCpSpVdQYwo8FjdzW4f3eINcSm+iBItyAwxrROoZ5ZPNm7ebeIfARkA+9FrKrWxPYIjDGtXJNHEFXVjw8+VRtSUQzJGZCSEe1KjDGmWcJ49fY2ylcEGTa8hDGm9bIgOFQ2vIQxppWzIDhUFgTGmFbOguBQ+Yqto9gY06pZEBwKVW8IagsCY0zrZUFwKKp2QaDWmoaMMa2aBcGhqLDhJYwxrZ8FwaGwk8mMMXHAguBQWBAYY+KABcGhsCGojTFxwILgUNQPQW1HDRljWjELgkPhK4J22ZCUEu1KjDGm2SwIDoWvyPoHjDGtngXBobDhJYwxccCC4FBU2PASxpjWz4LgUFjTkDEmDlgQNFcg4O0RWNOQMaZ1syBorsqdoAELAmNMq2dB0Fx2VrExJk5YEDRXfRDYyWTGmFbOgqC5bHgJY0ycsCBoLhuC2hgTJywImstXBAikd4x2JcYYc0gsCJrLV+RCICEx2pUYY8whsSBoLhtewhgTJywImstnJ5MZY+KDBUFz+YogPTfaVRhjzCGzIGguaxoyxsQJC4Lm8NdCVakFgTEmLlgQNMeecwjsrGJjTOtnQdAcNs6QMSaORDQIRGSiiKwUkTUicvt+prlYRJaLyDIReSGS9YSNDS9hjIkjSZGasYgkAo8AE4ACYL6ITFfV5UHTDALuAI5T1Z0i0iVS9YSVz4aXMMbEj0juEYwB1qjqOlWtAV4CzmswzfXAI6q6E0BVCyNYT/hY05AxJo5EMgh6AvlB9wu8x4INBgaLyGci8qWITGxsRiIyRUQWiMiCoqKiCJXbBL4iSEiC1JxoV2KMMYcskkEgjTymDe4nAYOAk4HLgCdE5DtrV1X9p6qOVtXRnTvHQHOMr8hdh0Aa+4jGGNO6RDIICoBeQffzgC2NTDNNVWtVdT2wEhcMsc2uVWyMiSORDIL5wCAR6SciKcClwPQG07wFnAIgIp1wTUXrIlhTePiKIMOGlzDGxIeIBYGq1gE3ATOBFcArqrpMRO4VkUneZDOBYhFZDnwE/FpViyNVU9jY8BLGmDgSscNHAVR1BjCjwWN3Bd1W4JfeT+thI48aY+KInVncVLWVUFNmh44aY+KGBUFT+Xa43+kWBMaY+GBB0FQ2vIQxJs5YEDRVhQ0vYYyJLxYETWXDSxhj4owFQVNZEBhj4owFQVP5iiApFVIyo12JMcaEhQVBU9WfQ2DjDBlj4oQFQVP5iiDdhpcwxsQPC4KmsuEljDFxxoKgqWzkUWNMnLEgaApVb4/AjhgyxsQPC4KmqCmHuioLAmNMXLEgaAobXsIYE4csCJrCZ8NLGGPijwVBU9hZxcaYOGRB0BT1QWBDUBtj4ogFQVPYHoExJg5ZEDRFRTGkZEFyWrQrMcaYsLEgaApfEWTY8BLGmPhiQdAUNryEMSYOWRA0hW+HBYExJu5YEDSFb4d1FBtj4o4FQagCAajYYYeOGmPijgVBqKpKIVBnTUPGmLhjQRCqChtewhgTnywIQmUnkxlj4pQFQagsCIwxccqCIFQ2BLUxJk5ZEISqfghqu3C9MSbOWBCEylcEqTmQmBztSowxJqwsCEJlw0sYY+KUBUGobHgJY0ycSorkzEVkIvA3IBF4QlXvb/D8NcCfgc3eQw+r6hMRKWbRc/DFwweeJuAHf407ccxfA/5a76cGArUw7LyIlGaMMdEUsSAQkUTgEWACUADMF5Hpqrq8waQvq+pNkapjj/SO0HnIgaeRBEhMcf0ACcl7byd6t4dOiniZxhjT0iK5RzAGWKOq6wBE5CXgPKBhELSMw852P8YYY/YRyT6CnkB+0P0C77GGLhCRr0XkNRHpFcF6jDHGNCKSQSCNPKYN7v8b6KuqRwKzgWcanZHIFBFZICILioqKwlymMca0bZEMggIgeAs/D9gSPIGqFqtqtXf3cWBUYzNS1X+q6mhVHd25sx25Y4wx4RTJIJgPDBKRfiKSAlwKTA+eQES6B92dBKyIYD3GGGMaEbHOYlWtE5GbgJm4w0efUtVlInIvsEBVpwM/F5FJQB1QAlwTqXqMMcY0TlQbNtvHttGjR+uCBQuiXYYxxrQqIrJQVUc39pydWWyMMW2cBYExxrRxra5pSESKgI3NfHknYEcYywknq615rLbmsdqapzXX1kdVGz3sstUFwaEQkQX7ayOLNquteay25rHamidea7OmIWOMaeMsCIwxpo1ra0Hwz2gXcABWW/NYbc1jtTVPXNbWpvoIjDHGfFdb2yMwxhjTgAWBMca0cW0mCERkooisFJE1InJ7tOsJJiIbROQbEVkiIlEdP0NEnhKRQhFZGvRYRxF5X0RWe787xFBtd4vIZm/ZLRGRs6JUWy8R+UhEVojIMhG52Xs86svuALVFfdmJSKqIzBORr7za7vEe7ycic73l9rI3cGWs1Pa0iKwPWm4jW7q2oBoTRWSxiLzt3W/eclPVuP/BDXq3FugPpABfAcOiXVdQfRuATtGuw6vlROBoYGnQY38Cbvdu3w78MYZquxu4NQaWW3fgaO92FrAKGBYLy+4AtUV92eGuW5Lp3U4G5gLjgFeAS73HHwN+EkO1PQ1cGO3/Oa+uXwIvAG9795u13NrKHsGey2aqag1Qf9lM04CqfoIbCTbYeey9aNAzwPktWpRnP7XFBFXdqqqLvNtluCHVexIDy+4AtUWdOuXe3WTvR4FTgde8x6O13PZXW0wQkTzgbOAJ777QzOXWVoIg1MtmRosCs0RkoYhMiXYxjeiqqlvBrVSALlGup6GbvMudPhWtZqtgItIXOAq3BRlTy65BbRADy85r3lgCFALv4/beS1W1zpskat/XhrWpav1y+4O33P5XRNpFozbgQeC/gIB3P5dmLre2EgShXDYzmo5T1aOBM4GfisiJ0S6oFXkUGACMBLYCD0SzGBHJBF4HblHV3dGspaFGaouJZaeqflUdibuK4RhgaGOTtWxV3ps2qE1EDgfuAA4DjgE6Are1dF0icg5QqKoLgx9uZNKQlltbCYKDXjYzmlR1i/e7EHgT92WIJdvrrybn/S6Mcj17qOp278sawF3uNGrLTkSScSva51X1De/hmFh2jdUWS8vOq6cUmINrh88RkfoLZ0X9+xpU20SvqU3VXWb3X0RnuR0HTBKRDbim7lNxewjNWm5tJQgOetnMaBGRDBHJqr8NfA9YeuBXtbjpwNXe7auBaVGsZR+y7+VOJxOlZee1zz4JrFDVvwY9FfVlt7/aYmHZiUhnEcnxbqcBp+P6MD4CLvQmi9Zya6y2b4OCXXBt8C2+3FT1DlXNU9W+uPXZh6p6Bc1dbtHu9W7B3vWzcEdLrAV+E+16gurqjzuK6StgWbRrA17ENRPU4vakfoRre/wAWO397hhDtT0HfAN8jVvpdo9SbcfjdsO/BpZ4P2fFwrI7QG1RX3bAkcBir4alwF3e4/2BecAa4FWgXQzV9qG33JYCU/GOLIrWD3Aye48aatZysyEmjDGmjWsrTUPGGGP2w4LAGGPaOAsCY4xp4ywIjDGmjbMgMMaYNs6CwJgWJCIn148UaUyssCAwxpg2zoLAmEaIyJXeWPRLROQf3uBj5SLygIgsEpEPRKSzN+1IEfnSG4TszfrB20RkoIjM9sazXyQiA7zZZ4rIayLyrYg8752hakzUWBAY04CIDAUuwQ0GOBLwA1cAGcAidQMEfgz8znvJs8Btqnok7ozT+sefBx5R1RHAsbizosGN/nkL7poA/XHjxhgTNUkHn8SYNuc0YBQw39tYT8MNFhcAXvammQq8ISLZQI6qfuw9/gzwqjd+VE9VfRNAVasAvPnNU9UC7/4SoC/wn8h/LGMaZ0FgzHcJ8Iyq3rHPgyJ3NpjuQOOzHKi5pzroth/7Hpoos6YhY77rA+BCEekCe6473Af3fakf2fFy4D+qugvYKSIneI9fBXysbrz/AhE535tHOxFJb9FPYUyIbEvEmAZUdbmI/BZ31bgE3GinPwV8wHARWQjswvUjgBvu9zFvRb8OuNZ7/CrgHyJyrzePi1rwYxgTMht91JgQiUi5qmZGuw5jws2ahowxpo2zPQJjjGnjbI/AGGPaOAsCY4xp4ywIjDGmjbMgMMaYNs6CwBhj2rj/DziMirgwe8dCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "history=model.fit(XTrainingT,YTraining,\n",
    "          validation_data=(XValT,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 40)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just LAPPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "del XL,YL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "XL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(1x1)_120k_Files.pickle\",\"rb\"))\n",
    "YL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(1x1)_120k_Files.pickle\",\"rb\"))\n",
    "\n",
    "#XL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(9x24)_23k_Files.pickle\",\"rb\"))\n",
    "#YL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(9x24)_23k_Files.pickle\",\"rb\"))\n",
    "\n",
    "#XL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(15x40)_120k_Files.pickle\",\"rb\"))\n",
    "#YL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(15x40)_120k_Files.pickle\",\"rb\"))\n",
    "\n",
    "#XL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamRingCounting/pickle/X_Beam_RC_Pure_LAPPD(15x40)_110k_Files.pickle\",\"rb\"))\n",
    "#YL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamRingCounting/pickle/Y_Beam_RC_globalnorm_PMTandLAPPD5x5_110k_Files_mitTopBottom.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a9ced7e748>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACcCAYAAABxymC7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAPNElEQVR4nO3dfawc1XnH8e+vxi+1MQGHd+OQkLq0KAoOdW1SKkTqFgxCcSIZxahprTaJA0qkUBUpJFUhJUJKWjXuC1GoUxxISyA0iYmlooLlUAFK43BxbWxi3usEc127YIKhhBc7T//YuWWznt1Z78zdmT38PtLVzs45O/Pcc+c+d+7smX0UEZiZWbp+qe4AzMxscjnRm5klzonezCxxTvRmZolzojczS9wRdQeQZ5qmxwxm1R3GUGjG9J7t8cqrQ4rErHo+vofnFf6X1+JV5bWVSvSSlgJ/C0wB/jEivtDRPh34OvAbwHPAhyJiZ9F2ZzCLxVpSJrSRMeWdv9qz/eCPHhtSJGbV8/E9PJtiY9e2gS/dSJoCfBm4EDgDuFTSGR3dPgI8HxG/AqwGvjjo/szMbDBlrtEvAp6IiKci4jXgNmBZR59lwM3Z8reAJZJy/7UwM7PJUSbRzwWebnu+K1uX2yciDgAvAG/N25ikVZLGJI29jq/bmZlVpUyizzsz7/w8hX76tFZGrImIhRGxcCq938AxM7P+lUn0u4B5bc9PAca79ZF0BPAWYF+JfZqZ2WEqk+gfAOZLeoekacAKYH1Hn/XAymx5OfC98KeomZkN1cDTKyPigKRPAnfRml65NiIelnQtMBYR64EbgX+S9AStM/kVVQSdkmFML5tyhqe4WT18bDVDqXn0EXEncGfHuqvbll8BLimzDzMzK8cfgWBmljgnejOzxDnRm5klzonezCxxTvRmZolzojczS5wTvZlZ4hpZeCQVRTcqwXBuKPFNKzaIUbnRblTirEKv71VP3t+1zWf0ZmaJc6I3M0ucE72ZWeKc6M3MElemZuw8SfdI2iHpYUmfyulznqQXJG3Jvq7O25aZmU2eMrNuDgB/GhGbJc0GHpS0ISJ+1NHvvoi4uMR+zMyshIHP6CNid0RszpZfBHZwaM1YMzOrWSXz6CW9HXgPsCmn+b2SttIqM3hlRDzcZRurgFUAM5hZRVi1S2n+rr35jMrxOypxVqHX9xrxate20ole0pHAt4ErImJ/R/Nm4NSIeEnSRcAdwPz8IGMNsAbgKM1xuUEzs4qUmnUjaSqtJH9LRHynsz0i9kfES9nyncBUSceW2aeZmR2eMrNuRKsm7I6I+FKXPidm/ZC0KNvfc4Pu08zMDl+ZSzfnAH8AbJO0JVv3WeBtABFxA7AcuFzSAeBnwIqI8GUZM7MhGjjRR8T9gAr6XA9cP+g+zMysPN8Za2aWOCd6M7PEOdGbmSXOhUfMrDZNKc6TOp/Rm5klzonezCxxTvRmZolzojczS5wTvZlZ4pzozcwS50RvZpY4z6O3xuhnTnUvw5hv/fIHFxf2mbkur/5Otdso0s8+xs/t+VFVnHxv788fnP3o84X7ePRjc3q2P/mhGwq3ccHJC3q2F32vZceyKkXH92Qev6XP6CXtlLQtK/49ltMuSX8n6QlJD0k6q+w+zcysf1Wd0b8vIp7t0nYhrapS84HFwFeyRzMzG4JhXKNfBnw9Wn4AHC3ppCHs18zMqCbRB3C3pAezAt+d5gJPtz3fla37BZJWSRqTNPY63YvcmpnZ4ani0s05ETEu6Xhgg6RHIuLetva8d3wOeZfHxcHNzCZH6TP6iBjPHvcC64BFHV12AfPanp8CjJfdr5mZ9adUopc0S9LsiWXgfGB7R7f1wB9ms2/OBl6IiN1l9mtmZv1TmVrdkk6jdRYPrctA34iI6yRdBq0C4ZJEq27sUuBl4I8i4pBpmO2O0pxYrCUDx/Vm4s/zfkNTxqLO+dL9xtBPHP3MxS9r34dfKuxz6p+/3rO96PtoynEx2TbFRvbHvtybI0pdo4+Ip4Azc9bf0LYcwCfK7MfMzAbnj0AwM0ucE72ZWeKc6M3MEudEb2aWOCd6M7PEOdGbmSXOid7MLHEuPDLiRuXmm2Foyli8ePoxPdtnU7yNZy44tmf7iau/XyoGgPGPnd2z/fSv7uvZ3s94F910VXQzVBWacGzWzWf0ZmaJc6I3M0ucE72ZWeKc6M3MEjdwopd0elYQfOJrv6QrOvqcJ+mFtj5Xlw/ZzMwOx8CzbiLiUWABgKQpwDO88ZHF7e6LiIsH3Y+ZmZVT1aWbJcCTEfHjirZnZmYVqWoe/Qrg1i5t75W0lVb5wCsj4uG8Tllh8VUAM5hZUVhWhWHNQy6acz1z3aZJj6GfefJFCuPsYx9z73q2Z/vBwwmoi7Lz5PsZq6KxeLGP4ibD+LmnrvQZvaRpwPuBf8lp3gycGhFnAn8P3NFtOxGxJiIWRsTCqUwvG5aZmWWquHRzIbA5IvZ0NkTE/oh4KVu+E5gqqfctf2ZmVqkqEv2ldLlsI+nErGYskhZl+3uugn2amVmfSl2jlzQT+D3g423r/r8wOLAcuFzSAeBnwIooU43czMwOW9ni4C8Db+1Y114Y/Hrg+jL7MDOzcnxnrJlZ4pzozcwS50RvZpa4N23hkaKbPVysYPiacGNMFTcJld1HFWY/+nzpOKq4ga2K8fLvank+ozczS5wTvZlZ4pzozcwS50RvZpY4J3ozs8Q50ZuZJc6J3swsccnOo/fc29HThMIjVfjvP/mt0tsoLDxScPw+c0Hxp4Ef+PzUnu1z/rn364t+XlUZxu9q6vmirzN6SWsl7ZW0vW3dHEkbJD2ePR7T5bUrsz6PS1pZVeBmZtaffi/d3AQs7Vh3FbAxIuYDG7Pnv0DSHOAaYDGwCLim2x8EMzObHH0l+oi4F+gsMLkMuDlbvhn4QM5LLwA2RMS+iHge2MChfzDMzGwSlXkz9oSI2A2QPR6f02cu8HTb813ZukNIWiVpTNLY67xaIiwzM2s32bNulLMut8KUi4ObmU2OMol+j6STALLHvTl9dgHz2p6fAoyX2KeZmR2mMol+PTAxi2Yl8N2cPncB50s6JnsT9vxsnZmZDUlf8+gl3QqcBxwraRetmTRfAG6X9BHgJ8AlWd+FwGUR8dGI2Cfp88AD2aaujYjON3UnxajPe30zKvoM9YNDiqOso3b2jrSf+wHKfq9F8/ABDq72/PQJTYljsvSV6CPi0i5NS3L6jgEfbXu+Flg7UHRmZlaaPwLBzCxxTvRmZolzojczS5wTvZlZ4pzozcwS50RvZpY4J3ozs8QlW3jEbDK8eHrxp2wX3RDVT2GSE1d/v++Y8vQT52x638xUtI2+bvwquBGpn+Ilo1Jwpsl8Rm9mljgnejOzxDnRm5klzonezCxxhYm+S2Hwv5L0iKSHJK2TdHSX1+6UtE3SFkljVQZuZmb96eeM/iYOrfO6AXhXRLwbeAz4TI/Xvy8iFkTEwsFCNDOzMgoTfV5h8Ii4OyIOZE9/QKtylJmZNVAV8+j/GPhml7YA7pYUwD9ExJpuG5G0ClgFMIOZFYSVhlEp3FCFou9lGGNRtI9+5nQXbaPsHPl+FBVx6ccw5q/3E+eoFJxpslKJXtKfAQeAW7p0OScixiUdD2yQ9Ej2H8Ihsj8CawCO0pzcAuJmZnb4Bp51I2klcDHw+xGRm5gjYjx73AusAxYNuj8zMxvMQIle0lLg08D7I+LlLn1mSZo9sUyrMPj2vL5mZjZ5+pleeSvwH8DpknZlxcCvB2bTuhyzRdINWd+TJd2ZvfQE4H5JW4EfAv8aEf82Kd+FmZl1VXiNvkth8Bu79B0HLsqWnwLOLBWdmZmV5jtjzcwS50RvZpY4J3ozs8Spy8zIWh2lObFYS+oOYyQU3ZwDw7mp6s10Y1eRfn4mRYYxXkVFP1zwY7Rsio3sj33Ka/MZvZlZ4pzozcwS50RvZpY4J3ozs8Q50ZuZJc6J3swscU70ZmaJa+Q8ekn/A/y4bdWxwLM1hXM4RiHOUYgRHGfVHGe1mhjnqRFxXF5DIxN9J0ljo1BzdhTiHIUYwXFWzXFWa1TinOBLN2ZmiXOiNzNL3Kgk+q5FxRtmFOIchRjBcVbNcVZrVOIERuQavZmZDW5UzujNzGxATvRmZolrdKKXtFTSo5KekHRV3fF0I2mnpG1ZofSxuuOZIGmtpL2StretmyNpg6THs8dj6owxiykvzs9JeiYb0y2SLqozxiymeZLukbRD0sOSPpWtb8yY9oixUeMpaYakH0ramsX5F9n6d0jalI3lNyVNa2icN0n6r7bxXFBnnIUiopFfwBTgSeA0YBqwFTij7ri6xLoTOLbuOHLiOhc4C9jetu4vgauy5auALzY0zs8BV9YdW0ecJwFnZcuzgceAM5o0pj1ibNR4AgKOzJanApuAs4HbgRXZ+huAyxsa503A8rrHsd+vJp/RLwKeiIinIuI14DZgWc0xjZSIuBfY17F6GXBztnwz8IGhBpWjS5yNExG7I2JztvwisAOYS4PGtEeMjRItL2VPp2ZfAfwO8K1sfe3HZ484R0qTE/1c4Om257to4AGbCeBuSQ9KWlV3MAVOiIjd0EoKwPE1x9PLJyU9lF3aqf0SUztJbwfeQ+sMr5Fj2hEjNGw8JU2RtAXYC2yg9R/8TyPiQNalEb/znXFGxMR4XpeN52pJ02sMsVCTE31e7cOm/iU9JyLOAi4EPiHp3LoDSsBXgHcCC4DdwF/XG84bJB0JfBu4IiL21x1PnpwYGzeeEXEwIhYAp9D6D/7X87oNN6qcADrilPQu4DPArwG/CcwBPl1jiIWanOh3AfPanp8CjNcUS08RMZ497gXW0Tpom2qPpJMAsse9NceTKyL2ZL9gPwe+SkPGVNJUWgn0loj4Tra6UWOaF2NTxxMgIn4K/Duta99HSzoia2rU73xbnEuzS2QREa8CX6NB45mnyYn+AWB+9i78NGAFsL7mmA4haZak2RPLwPnA9t6vqtV6YGW2vBL4bo2xdDWRODMfpAFjKknAjcCOiPhSW1NjxrRbjE0bT0nHSTo6W/5l4HdpvZ9wD7A861b78dklzkfa/rCL1vsItR+fvTT6zthsCtjf0JqBszYirqs5pENIOo3WWTzAEcA3mhKnpFuB82h9pOoe4BrgDlozG94G/AS4JCJqfSO0S5zn0brMELRmNX184jp4XST9NnAfsA34ebb6s7SugTdiTHvEeCkNGk9J76b1ZusUWiect0fEtdnv0220Lof8J/Dh7Ky5aXF+DziO1iXmLcBlbW/aNk6jE72ZmZXX5Es3ZmZWASd6M7PEOdGbmSXOid7MLHFO9GZmiXOiNzNLnBO9mVni/g+ZrO6XjJ910gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XL[30,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aa878c0278>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACcCAYAAABxymC7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAPDklEQVR4nO3df6zVd33H8edLLj9WwFFKiwgIdiNuTWexRWrDZrDdKu0acQnNaDZHNg2208Qma2J1S+u6mNgt07lh2rEVi6ZSW5VKMqIlpUvrpsiFQYFRLTK0t5fAKrYUa6HXvvfH+d55PJxzvof7/d57vudzX4/k5nzP9/M53+/7fs857/u9n/P5nrciAjMzS9fruh2AmZmNLid6M7PEOdGbmSXOid7MLHFO9GZmievrdgDNTNLkmMLUbodhNio0eVJun9cmtX9r6qWX27b//IL890/fqVfbtsfpM4X38boz7Wf15f0e40knr4t2z8kr/JQzcVrN2golekkrgM8CE4B/jYhPNbRPBr4AXAH8GPjDiDiSt90pTOVKXVMkNLPK6pu3MLfPKwsvaL+N7bvatr/w+1fl7mPWfxxt2z50+EjhfUx7rv0fi7zfYzzp5HXR7jnZEY+1bBvx0I2kCcDngOuAS4CbJF3S0O39wE8i4teBzwB3j3R/ZmY2MkXG6JcChyLicEScAR4EVjb0WQlszJa/Alwjqem/FmZmNjqKJPq5wLN19weydU37RMQQ8CLQ9H9SSWsl9Uvqf5XTBcIyM7N6RRJ9szPzxk9eOulTWxmxPiKWRMSSiUwuEJaZmdUrkugHgPl19+cBg636SOoDfhU4UWCfZmZ2jook+p3AIklvljQJWA1saeizBViTLa8Ctoe/Rc3MbEyNeHplRAxJ+jDwTWrTKzdExAFJdwH9EbEFuA/4oqRD1M7kV5cRtFmV9V28sG173tRJgClHfty+Q84+8qZOlmHGF7896vsYT/KmsxZRaB59RGwFtjasu6Nu+RXgxiL7MDOzYvwVCGZmiXOiNzNLnBO9mVninOjNzBLnRG9mljgnejOzxDnRm5klrpKFR8ZC3kUto3nxgo1vuRdDUfz198L78r8rPs+swlvw+6hs7fKWBloXLvEZvZlZ4pzozcwS50RvZpY4J3ozs8QVqRk7X9Ljkg5KOiDpI036LJf0oqQ92c8dzbZlZmajp8ismyHgLyJit6TpwC5J2yLivxv6PRkRNxTYj5mZFTDiM/qIOBoRu7Pll4CDnF0z1szMuqyUefSSFgJvA3Y0ab5K0l5qZQZvi4gDLbaxFlgLMIXzygjLesx4ubbh+WVzcvvkzWHPOxbTnjuTu4++7bvatj+fMxe/k+Im4+U5HSvtjldE6+e8cKKXNA34KnBrRJxsaN4NLIiIU5KuBx4BFjUPMtYD6wFer5kuN2hmVpJCs24kTaSW5B+IiK81tkfEyYg4lS1vBSZKKuOCOzMz61CRWTeiVhP2YER8ukWfN2T9kLQ021/+9d9mZlaaIkM3y4D3Afsk7cnWfRx4E0BE3AusAm6RNAT8DFgdER6WMTMbQyNO9BHxLUA5fdYB60a6DzMzK85XxpqZJc6J3swscU70ZmaJG7eFR3yhRvWk8pzk/R4zOvg9hwrGcGpu6yIUw2blXMzUyQVReV5ZeEHb9r5EnvOq8xm9mVninOjNzBLnRG9mljgnejOzxDnRm5klzonezCxxTvRmZokbt/Po8+QVTIB05n3bL+Q973nzwgGmHGn/Ba2dvG6Grr6ibXte0ZAdd9+Tu4+/Ov5bbdsf2fQ7bdtn7zydu4+8OG1sFD6jl3RE0r6s+Hd/k3ZJ+kdJhyQ9Jenyovs0M7POlXVG/66IeL5F23XUqkotAq4E7sluzcxsDIzFGP1K4AtR8x1ghqT8oplmZlaKMhJ9AI9K2pUV+G40F3i27v5Atu6XSForqV9S/6vkj/2ZmVlnyhi6WRYRg5IuArZJejoinqhrb1ac5KwqUy4ObmY2Ogqf0UfEYHZ7HNgMLG3oMgDMr7s/Dxgsul8zM+tMoUQvaaqk6cPLwLXA/oZuW4A/yWbfvAN4MSKKf/+pmZl1pOjQzWxgs6ThbX0pIr4h6Wb4/wLhW4HrgUPAy8CfFtznmPAc+fEp73mfMjZh5M/Fz3n8pZ/989x9LHi4/T/Wsxe2/6zs2Nsn5+5jNsWuB7ByFEr0EXEYuKzJ+nvrlgP4UJH9mJnZyPkrEMzMEudEb2aWOCd6M7PEOdGbmSXOid7MLHFO9GZmiXOiNzNLnAuPmJWsjIvtim5j6mDxr4vKuyBq7t3/mbuNvEIueRd+WTl8Rm9mljgnejOzxDnRm5klzonezCxxI070kt6SFQQf/jkp6daGPsslvVjX547iIZuZ2bkY8aybiPgesBhA0gTgOWqFRxo9GRE3jHQ/ZmZWTFlDN9cAP4iIH5a0PTMzK0lZ8+hXA5tatF0laS+18oG3RcSBZp2ywuJrAaZwXklhpS9vnjK4iEqZSpkjf3X7YhzQQeGRnDh++sZmpZobLJvTtnn2zvaFR/za6x2Fz+glTQLeAzzcpHk3sCAiLgP+CXik1XYiYn1ELImIJRPJr1xjZmadKWPo5jpgd0Qca2yIiJMRcSpb3gpMlDSrhH2amVmHykj0N9Fi2EbSG5QVlJW0NNtf+/9JzcysVIXG6CWdB/we8MG6dfWFwVcBt0gaAn4GrM5qyJqZ2RgpWhz8ZeCChnX1hcHXAeuK7MPMzIrxlbFmZolzojczS5wTvZlZ4lx4pMf5gpSx1clFQrm278rvU3A/Cx4ezO2T99op5Xe1SvAZvZlZ4pzozcwS50RvZpY4J3ozs8Q50ZuZJc6J3swscU70ZmaJ8zx6s3MwVtctFN1PKQVSSthG3lx8XwcyNjo6o5e0QdJxSfvr1s2UtE3SM9nt+S0euybr84ykNWUFbmZmnel06OZ+YEXDutuBxyJiEfBYdv+XSJoJ3AlcCSwF7mz1B8HMzEZHR4k+Ip4ATjSsXglszJY3Au9t8tB3A9si4kRE/ATYxtl/MMzMbBQV+TB2dkQcBchuL2rSZy7wbN39gWzdWSStldQvqf9V2hclNjOzzo32rJtmpeibVphycXAzs9FRJNEfkzQHILs93qTPADC/7v48IP9r9czMrDRFEv0WYHgWzRrg6036fBO4VtL52Yew12brzMxsjHQ0j17SJmA5MEvSALWZNJ8CHpL0fuBHwI1Z3yXAzRHxgYg4IelvgJ3Zpu6KiMYPdc2sQdH552V8l7y/rz4dHSX6iLipRdM1Tfr2Ax+ou78B2DCi6MzMrDB/BYKZWeKc6M3MEudEb2aWOCd6M7PEOdGbmSXOid7MLHFO9GZmiXPhEUtGJxfw9Eqhi0oUHrn6ivYdtu/K3YYvqqoGn9GbmSXOid7MLHFO9GZmiXOiNzNLXG6ib1EY/O8kPS3pKUmbJc1o8dgjkvZJ2iOpv8zAzcysM52c0d/P2XVetwGXRsRbge8DH2vz+HdFxOKIWDKyEM3MrIjcRN+sMHhEPBoRQ9nd71CrHGVmZhVUxjz6PwO+3KItgEclBfDPEbG+1UYkrQXWAkzhvBLCsvGmV+bIj4Uyrinoy5knn9J1C6krlOgl/SUwBDzQosuyiBiUdBGwTdLT2X8IZ8n+CKwHeL1mNi0gbmZm527Es24krQFuAP4oIpom5ogYzG6PA5uBpSPdn5mZjcyIEr2kFcBHgfdExMst+kyVNH14mVph8P3N+pqZ2ejpZHrlJuDbwFskDWTFwNcB06kNx+yRdG/W942StmYPnQ18S9Je4LvAv0XEN0bltzAzs5Zyx+hbFAa/r0XfQeD6bPkwcFmh6MzMrDBfGWtmljgnejOzxDnRm5klzoVHLPfCF1/0Ys34ddE7fEZvZpY4J3ozs8Q50ZuZJc6J3swscU70ZmaJc6I3M0ucE72ZWeLU4huGu0rS/wI/rFs1C3i+S+Gci16IsxdiBMdZNsdZrirGuSAiLmzWUMlE30hSfy/UnO2FOHshRnCcZXOc5eqVOId56MbMLHFO9GZmieuVRN+yqHjF9EKcvRAjOM6yOc5y9UqcQI+M0ZuZ2cj1yhm9mZmNkBO9mVniKp3oJa2Q9D1JhyTd3u14WpF0RNK+rFB6f7fjGSZpg6TjkvbXrZspaZukZ7Lb87sZYxZTszg/Iem57JjukXR9N2PMYpov6XFJByUdkPSRbH1ljmmbGCt1PCVNkfRdSXuzOP86W/9mSTuyY/llSZMqGuf9kv6n7ngu7macuSKikj/ABOAHwMXAJGAvcEm342oR6xFgVrfjaBLXO4HLgf116/4WuD1bvh24u6JxfgK4rduxNcQ5B7g8W54OfB+4pErHtE2MlTqegIBp2fJEYAfwDuAhYHW2/l7glorGeT+wqtvHsdOfKp/RLwUORcThiDgDPAis7HJMPSUingBONKxeCWzMljcC7x3ToJpoEWflRMTRiNidLb8EHATmUqFj2ibGSomaU9ndidlPAFcDX8nWd/312SbOnlLlRD8XeLbu/gAVfMFmAnhU0i5Ja7sdTI7ZEXEUakkBuKjL8bTzYUlPZUM7XR9iqidpIfA2amd4lTymDTFCxY6npAmS9gDHgW3U/oN/ISKGsi6VeM83xhkRw8fzk9nx/IykyV0MMVeVE72arKvqX9JlEXE5cB3wIUnv7HZACbgH+DVgMXAU+PvuhvMLkqYBXwVujYiT3Y6nmSYxVu54RsTPI2IxMI/af/C/2azb2EbVJICGOCVdCnwM+A3g7cBM4KNdDDFXlRP9ADC/7v48YLBLsbQVEYPZ7XFgM7UXbVUdkzQHILs93uV4moqIY9kb7DXgX6jIMZU0kVoCfSAivpatrtQxbRZjVY8nQES8APw7tbHvGZL6sqZKvefr4lyRDZFFRJwGPk+FjmczVU70O4FF2afwk4DVwJYux3QWSVMlTR9eBq4F9rd/VFdtAdZky2uAr3cxlpaGE2fmD6jAMZUk4D7gYER8uq6pMse0VYxVO56SLpQ0I1v+FeB3qX2e8DiwKuvW9ddnizifrvvDLmqfI3T99dlOpa+MzaaA/QO1GTgbIuKTXQ7pLJIupnYWD9AHfKkqcUraBCyn9pWqx4A7gUeozWx4E/Aj4MaI6OoHoS3iXE5tmCGozWr64PA4eLdI+m3gSWAf8Fq2+uPUxsArcUzbxHgTFTqekt5K7cPWCdROOB+KiLuy99OD1IZD/gv44+ysuWpxbgcupDbEvAe4ue5D28qpdKI3M7Piqjx0Y2ZmJXCiNzNLnBO9mVninOjNzBLnRG9mljgnejOzxDnRm5kl7v8AVuK1Nw3wEf8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XL[7,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120005, 2), (120005, 15, 40, 2))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YL.shape,XL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 120k Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "(85000, 3, 8, 2) (20000, 3, 8, 2) (15005, 3, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(XL, YL))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:20]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:85000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[85000:105000]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[105000:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23 k Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "(17000, 9, 24, 2) (2500, 9, 24, 2) (4052, 9, 24, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(XL, YL))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:20]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:17000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[17000:19500]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[19500:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85000, 3, 8, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTraining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "y_ints = [y.argmax() for y in YTrainingnew]\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_ints),\n",
    "                                                 y_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTrainingnew=[]\n",
    "XTrainingnew=[]\n",
    "index=index2=index3=0\n",
    "for i in YTraining:\n",
    "    \n",
    "    if i[0]==1 and index2 <35000:\n",
    "        XTrainingnew.append(XTraining[index])\n",
    "        YTrainingnew.append(YTraining[index])\n",
    "        index2=index2+1\n",
    "        \n",
    "    if i[1]==1 and index3 <35000:\n",
    "        XTrainingnew.append(XTraining[index])\n",
    "        YTrainingnew.append(YTraining[index])\n",
    "        index3=index3+1\n",
    "        \n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTrainingnew=np.array(YTrainingnew)\n",
    "XTrainingnew=np.array(XTrainingnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 3, 8, 2)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrainingnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yvalnew=[]\n",
    "XValnew=[]\n",
    "index=index2=index3=0\n",
    "for i in Yval:\n",
    "    \n",
    "    if i[0]==1 and index2 <7000:\n",
    "        XValnew.append(XVal[index])\n",
    "        Yvalnew.append(Yval[index])\n",
    "        index2=index2+1\n",
    "        \n",
    "    if i[1]==1 and index3 <7000:\n",
    "        XValnew.append(XVal[index])\n",
    "        Yvalnew.append(Yval[index])\n",
    "        index3=index3+1\n",
    "        \n",
    "    index=index+1\n",
    "Yvalnew=np.array(Yvalnew)\n",
    "XValnew=np.array(XValnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bestes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_191\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_337 (Conv2D)          (None, 3, 8, 190)         1710      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_337 (MaxPoolin (None, 2, 4, 190)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_579 (Bat (None, 2, 4, 190)         760       \n",
      "_________________________________________________________________\n",
      "dropout_579 (Dropout)        (None, 2, 4, 190)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_338 (Conv2D)          (None, 2, 4, 190)         144590    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_338 (MaxPoolin (None, 1, 2, 190)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_580 (Bat (None, 1, 2, 190)         760       \n",
      "_________________________________________________________________\n",
      "dropout_580 (Dropout)        (None, 1, 2, 190)         0         \n",
      "_________________________________________________________________\n",
      "flatten_189 (Flatten)        (None, 380)               0         \n",
      "_________________________________________________________________\n",
      "dense_431 (Dense)            (None, 32)                12192     \n",
      "_________________________________________________________________\n",
      "batch_normalization_581 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_581 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_432 (Dense)            (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 160,206\n",
      "Trainable params: 159,382\n",
      "Non-trainable params: 824\n",
      "_________________________________________________________________\n",
      "Train on 70000 samples, validate on 14000 samples\n",
      "Epoch 1/30\n",
      "70000/70000 [==============================] - 43s 607us/sample - loss: 0.5312 - acc: 0.7452 - val_loss: 0.4776 - val_acc: 0.7749\n",
      "Epoch 2/30\n",
      "70000/70000 [==============================] - 14s 203us/sample - loss: 0.4700 - acc: 0.7741 - val_loss: 0.4775 - val_acc: 0.7719\n",
      "Epoch 3/30\n",
      "70000/70000 [==============================] - 15s 220us/sample - loss: 0.4596 - acc: 0.7812 - val_loss: 0.4597 - val_acc: 0.7836\n",
      "Epoch 4/30\n",
      "70000/70000 [==============================] - 15s 216us/sample - loss: 0.4508 - acc: 0.7850 - val_loss: 0.4405 - val_acc: 0.7932\n",
      "Epoch 5/30\n",
      "70000/70000 [==============================] - 15s 215us/sample - loss: 0.4441 - acc: 0.7901 - val_loss: 0.4485 - val_acc: 0.7789\n",
      "Epoch 6/30\n",
      "70000/70000 [==============================] - 14s 203us/sample - loss: 0.4374 - acc: 0.7920 - val_loss: 0.4610 - val_acc: 0.7839\n",
      "Epoch 7/30\n",
      "70000/70000 [==============================] - 15s 214us/sample - loss: 0.4320 - acc: 0.7975 - val_loss: 0.4368 - val_acc: 0.7941\n",
      "Epoch 8/30\n",
      "70000/70000 [==============================] - 14s 200us/sample - loss: 0.4247 - acc: 0.8018 - val_loss: 0.4119 - val_acc: 0.8069\n",
      "Epoch 9/30\n",
      "70000/70000 [==============================] - 15s 216us/sample - loss: 0.4194 - acc: 0.8052 - val_loss: 0.4473 - val_acc: 0.7817\n",
      "Epoch 10/30\n",
      "70000/70000 [==============================] - 14s 201us/sample - loss: 0.4118 - acc: 0.8071 - val_loss: 0.4022 - val_acc: 0.8081\n",
      "Epoch 11/30\n",
      "70000/70000 [==============================] - 15s 215us/sample - loss: 0.4069 - acc: 0.8131 - val_loss: 0.4073 - val_acc: 0.8107\n",
      "Epoch 12/30\n",
      "70000/70000 [==============================] - 14s 201us/sample - loss: 0.4040 - acc: 0.8142 - val_loss: 0.4177 - val_acc: 0.8031\n",
      "Epoch 13/30\n",
      "70000/70000 [==============================] - 15s 216us/sample - loss: 0.3999 - acc: 0.8167 - val_loss: 0.3941 - val_acc: 0.8172\n",
      "Epoch 14/30\n",
      "70000/70000 [==============================] - 14s 201us/sample - loss: 0.3952 - acc: 0.8184 - val_loss: 0.4006 - val_acc: 0.8159\n",
      "Epoch 15/30\n",
      "70000/70000 [==============================] - 15s 214us/sample - loss: 0.3905 - acc: 0.8215 - val_loss: 0.4105 - val_acc: 0.8076\n",
      "Epoch 16/30\n",
      "70000/70000 [==============================] - 14s 206us/sample - loss: 0.3902 - acc: 0.8214 - val_loss: 0.3886 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "70000/70000 [==============================] - 15s 216us/sample - loss: 0.3866 - acc: 0.8244 - val_loss: 0.4640 - val_acc: 0.7821\n",
      "Epoch 18/30\n",
      "70000/70000 [==============================] - 14s 203us/sample - loss: 0.3862 - acc: 0.8247 - val_loss: 0.4203 - val_acc: 0.8060\n",
      "Epoch 19/30\n",
      "70000/70000 [==============================] - 15s 212us/sample - loss: 0.3813 - acc: 0.8282 - val_loss: 0.3809 - val_acc: 0.8245\n",
      "Epoch 20/30\n",
      "70000/70000 [==============================] - 14s 204us/sample - loss: 0.3799 - acc: 0.8269 - val_loss: 0.3803 - val_acc: 0.8246\n",
      "Epoch 21/30\n",
      "70000/70000 [==============================] - 14s 207us/sample - loss: 0.3776 - acc: 0.8269 - val_loss: 0.3824 - val_acc: 0.8278\n",
      "Epoch 22/30\n",
      "70000/70000 [==============================] - 15s 211us/sample - loss: 0.3755 - acc: 0.8298 - val_loss: 0.3888 - val_acc: 0.8188\n",
      "Epoch 23/30\n",
      "70000/70000 [==============================] - 14s 203us/sample - loss: 0.3744 - acc: 0.8310 - val_loss: 0.3812 - val_acc: 0.8265\n",
      "Epoch 24/30\n",
      "70000/70000 [==============================] - 15s 213us/sample - loss: 0.3725 - acc: 0.8311 - val_loss: 0.3858 - val_acc: 0.8186\n",
      "Epoch 25/30\n",
      "70000/70000 [==============================] - 14s 205us/sample - loss: 0.3710 - acc: 0.8322 - val_loss: 0.3745 - val_acc: 0.8269\n",
      "Epoch 26/30\n",
      "70000/70000 [==============================] - 15s 215us/sample - loss: 0.3672 - acc: 0.8335 - val_loss: 0.4011 - val_acc: 0.8139\n",
      "Epoch 27/30\n",
      "70000/70000 [==============================] - 14s 204us/sample - loss: 0.3652 - acc: 0.8344 - val_loss: 0.4089 - val_acc: 0.8045\n",
      "Epoch 28/30\n",
      "70000/70000 [==============================] - 15s 215us/sample - loss: 0.3636 - acc: 0.8352 - val_loss: 0.4134 - val_acc: 0.8032\n",
      "Epoch 29/30\n",
      "70000/70000 [==============================] - 14s 204us/sample - loss: 0.3634 - acc: 0.8351 - val_loss: 0.3854 - val_acc: 0.8226\n",
      "Epoch 30/30\n",
      "70000/70000 [==============================] - 15s 214us/sample - loss: 0.3615 - acc: 0.8372 - val_loss: 0.3888 - val_acc: 0.8207\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "dense_layers = [1]\n",
    "layer_sizes = [190]\n",
    "conv_layers = [2]\n",
    "kernal_size = [(2,2)]\n",
    "for filter_size in kernal_size:\n",
    "    for dense_layer in dense_layers:\n",
    "        for layer_size in layer_sizes:\n",
    "            for conv_layer in conv_layers:\n",
    "\n",
    "                NAME =\"PMT-MuEl-{}-filter_size-{}-conv-{}-nodes-{}-dense\".format(filter_size,conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "                tensorboard = TensorBoard(log_dir = 'logs\\PMTsmall\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(layer_size,filter_size,strides=1, input_shape= XTraining.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                for l in range(conv_layer-1):                   \n",
    "                    model.add(Conv2D(layer_size,filter_size,padding='same',activation=\"relu\"))              \n",
    "                    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))            \n",
    "                #model.add(GlobalAveragePooling2D())\n",
    "                model.add(Flatten())\n",
    "                for l in range(dense_layer-1):\n",
    "                    model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))\n",
    "                model.add(Dense(32,activation=\"relu\"))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(2))\n",
    "                model.add(Activation('softmax'))\n",
    "                #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "                model.compile(loss=\"binary_crossentropy\",\n",
    "                             optimizer=\"adam\",\n",
    "                              metrics=['accuracy']\n",
    "                             )   \n",
    "                filepath=\"LAPPD(1x1)_PID_120k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "                checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "                #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "                model.summary()\n",
    "                history=model.fit(\n",
    "                    \n",
    "                    XTrainingnew,YTrainingnew,\n",
    "              #validation_data=(XVal,Yval)\n",
    "                 validation_data=(XValnew,Yvalnew)   \n",
    "                    \n",
    "              ,batch_size=100,\n",
    "                shuffle=True,\n",
    "                class_weight=class_weights,\n",
    "                callbacks=[\n",
    "                            #monitor,\n",
    "                            #checkpoint,\n",
    "                            #tensorboard \n",
    "                ],\n",
    "              epochs= 30)\n",
    "                print(history.history.keys())\n",
    "                # summarize history for accuracy\n",
    "                plt.plot(history.history['acc'])\n",
    "                plt.plot(history.history['val_acc'])\n",
    "                plt.title('model accuracy')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'test'], loc='upper left')\n",
    "                plt.show()\n",
    "                # summarize history for loss\n",
    "                plt.plot(history.history['loss'])\n",
    "                plt.plot(history.history['val_loss'])\n",
    "                plt.title('model loss')\n",
    "                plt.ylabel('loss')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'test'], loc='upper left')\n",
    "                plt.show()\n",
    "\n",
    "                print(\"Ende des Versuchs: \")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  2.3800367363132424\n",
      "Test accuracy:  0.7064312\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,9,24,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,9,24,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,9,24,1)\n",
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,9,24,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,9,24,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,9,24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.6991 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6944 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6942 - acc: 0.5001 - val_loss: 0.6970 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6945 - acc: 0.4915 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6939 - acc: 0.4966 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.7000 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6943 - acc: 0.4948 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6939 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6932 - acc: 0.5049 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6935 - acc: 0.5006 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6934 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6933 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6934 - acc: 0.5009 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6935 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6933 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6933 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6932 - acc: 0.5034 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6934 - acc: 0.4916 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6933 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6931 - acc: 0.5075 - val_loss: 0.6954 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6938 - acc: 0.5022 - val_loss: 0.7340 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6938 - acc: 0.4942 - val_loss: 0.7051 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 70us/sample - loss: 0.6934 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 215us/sample - loss: 0.6968 - acc: 0.4954 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6945 - acc: 0.5049 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6940 - acc: 0.5007 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6946 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.6938 - acc: 0.5079 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6941 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6938 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6934 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6941 - acc: 0.4912 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6935 - acc: 0.4961 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 67us/sample - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6949 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6934 - acc: 0.5036 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6933 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.5001 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 69us/sample - loss: 0.6933 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6934 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 68us/sample - loss: 0.6933 - acc: 0.4960 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.5041 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6934 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.5019 - val_loss: 0.6942 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.4916 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4994 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 239us/sample - loss: 0.7066 - acc: 0.5031 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6946 - acc: 0.5100 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6937 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6933 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.4952 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6939 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6936 - acc: 0.4997 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6933 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6933 - acc: 0.4886 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6932 - acc: 0.5012 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.4923 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6933 - acc: 0.5025 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.5007 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.5008 - val_loss: 0.8325 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.5040 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6931 - acc: 0.5111 - val_loss: 0.7057 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6934 - acc: 0.4940 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6934 - acc: 0.4941 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6932 - acc: 0.5057 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6933 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6931 - acc: 0.5036 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 309us/sample - loss: 0.8167 - acc: 0.4974 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.7259 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.7049 - acc: 0.5016 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6984 - acc: 0.5049 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6970 - acc: 0.4916 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6958 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6946 - acc: 0.5017 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6940 - acc: 0.5016 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.5031 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6941 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.5049 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6938 - acc: 0.4992 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6934 - acc: 0.5067 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6936 - acc: 0.5085 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.5022 - val_loss: 0.6954 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6940 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6935 - acc: 0.5061 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6939 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.5048 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6939 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.5005 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6938 - acc: 0.5005 - val_loss: 0.6952 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.5108 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6938 - acc: 0.4954 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 243us/sample - loss: 0.7052 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6958 - acc: 0.5006 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6950 - acc: 0.4998 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6945 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6937 - acc: 0.4972 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6936 - acc: 0.4917 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.4994 - val_loss: 0.7098 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6947 - acc: 0.5025 - val_loss: 0.8923 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6935 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6933 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6934 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.4996 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4999 - val_loss: 0.6994 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.5051 - val_loss: 0.7077 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6937 - acc: 0.5025 - val_loss: 0.7681 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.4986 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.4957 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 71us/sample - loss: 0.6932 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6936 - acc: 0.4928 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6933 - acc: 0.5026 - val_loss: 0.6960 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6936 - acc: 0.4939 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5031 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 229us/sample - loss: 0.6980 - acc: 0.4980 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6946 - acc: 0.5015 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6946 - acc: 0.5040 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6940 - acc: 0.4974 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6935 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6932 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6933 - acc: 0.4976 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6934 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6935 - acc: 0.4925 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6933 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6933 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 76us/sample - loss: 0.6934 - acc: 0.4922 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6933 - acc: 0.5031 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6934 - acc: 0.4931 - val_loss: 0.6996 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6934 - acc: 0.5026 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6936 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6937 - acc: 0.4992 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6933 - acc: 0.5024 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 75us/sample - loss: 0.6934 - acc: 0.4961 - val_loss: 0.6938 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 77us/sample - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 72us/sample - loss: 0.6937 - acc: 0.4974 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 74us/sample - loss: 0.6936 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6936 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 73us/sample - loss: 0.6938 - acc: 0.5014 - val_loss: 0.7525 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 245us/sample - loss: 0.7076 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6938 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.5035 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6935 - acc: 0.4869 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6935 - acc: 0.4952 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.5074 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4930 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6937 - acc: 0.4965 - val_loss: 0.7009 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.5051 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5048 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6935 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6938 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4908 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.4951 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.4975 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.4931 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4934 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 264us/sample - loss: 0.8525 - acc: 0.4995 - val_loss: 0.7174 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.7305 - acc: 0.4899 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.7069 - acc: 0.4988 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.7017 - acc: 0.5003 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6968 - acc: 0.5011 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6953 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6948 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6943 - acc: 0.5016 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6941 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6941 - acc: 0.4963 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6939 - acc: 0.4992 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6942 - acc: 0.4956 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6938 - acc: 0.5029 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6935 - acc: 0.5056 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6939 - acc: 0.5019 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6937 - acc: 0.5005 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5065 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6939 - acc: 0.4972 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6937 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6939 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6938 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6938 - acc: 0.5021 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6935 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6937 - acc: 0.5045 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6935 - acc: 0.5065 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6940 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6937 - acc: 0.4984 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.5011 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 258us/sample - loss: 0.7049 - acc: 0.5006 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6977 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6967 - acc: 0.4962 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6951 - acc: 0.4911 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6950 - acc: 0.5016 - val_loss: 0.6974 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6961 - acc: 0.4940 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4937 - val_loss: 0.6979 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4965 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.4957 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.4986 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6932 - acc: 0.5029 - val_loss: 0.8747 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6937 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6941 - acc: 0.5004 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6949 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6934 - acc: 0.5036 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6938 - acc: 0.5012 - val_loss: 0.7536 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.5107 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.5033 - val_loss: 0.7008 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 4s 245us/sample - loss: 0.7151 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6955 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6973 - acc: 0.4926 - val_loss: 0.6962 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6957 - acc: 0.4966 - val_loss: 0.6973 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6936 - acc: 0.5040 - val_loss: 0.6968 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6942 - acc: 0.4942 - val_loss: 0.7072 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6942 - acc: 0.4944 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6953 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.4941 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6934 - acc: 0.4967 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4966 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6932 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6935 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6934 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.5002 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6945 - acc: 0.4974 - val_loss: 0.7704 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6940 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5026 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6934 - acc: 0.4968 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 288us/sample - loss: 0.7108 - acc: 0.4930 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4945 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6936 - acc: 0.4972 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6932 - acc: 0.5075 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6933 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6988 - acc: 0.5001 - val_loss: 0.6993 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6936 - acc: 0.5033 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6932 - acc: 0.5035 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6935 - acc: 0.4929 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6933 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6935 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6935 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6933 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6953 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 300us/sample - loss: 0.8582 - acc: 0.4992 - val_loss: 0.7036 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.7314 - acc: 0.4963 - val_loss: 0.7209 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.7238 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.7013 - acc: 0.5056 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6990 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6961 - acc: 0.4991 - val_loss: 0.6949 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6950 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6940 - acc: 0.5013 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6937 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6940 - acc: 0.4938 - val_loss: 0.6946 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6936 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6939 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6936 - acc: 0.5049 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6938 - acc: 0.5010 - val_loss: 0.6953 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.5055 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5033 - val_loss: 0.6937 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6937 - acc: 0.5023 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6939 - acc: 0.4929 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6937 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6940 - acc: 0.5009 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6937 - acc: 0.4940 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5064 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6936 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6938 - acc: 0.4956 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6935 - acc: 0.5074 - val_loss: 0.7945 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6939 - acc: 0.4968 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 272us/sample - loss: 0.7031 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6981 - acc: 0.5012 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6945 - acc: 0.5015 - val_loss: 0.6978 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6936 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.7177 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6935 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6936 - acc: 0.4907 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6931 - acc: 0.5066 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6936 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6935 - acc: 0.4996 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4917 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6952 - acc: 0.4961 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6935 - acc: 0.5007 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6932 - acc: 0.5052 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6935 - acc: 0.5034 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6934 - acc: 0.5021 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.5057 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6941 - acc: 0.4966 - val_loss: 0.7059 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6936 - acc: 0.4998 - val_loss: 0.7030 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 275us/sample - loss: 0.7107 - acc: 0.4950 - val_loss: 0.6964 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6978 - acc: 0.4974 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6965 - acc: 0.4987 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6961 - acc: 0.4965 - val_loss: 0.8081 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6974 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6944 - acc: 0.4989 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.5025 - val_loss: 0.6958 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6935 - acc: 0.4984 - val_loss: 0.6971 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6933 - acc: 0.5006 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5024 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5064 - val_loss: 0.7020 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6934 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.5032 - val_loss: 0.6933 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6935 - acc: 0.4931 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6933 - acc: 0.5038 - val_loss: 0.7379 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6954 - acc: 0.4933 - val_loss: 0.7100 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6938 - acc: 0.4959 - val_loss: 0.6967 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.5035 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6937 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6932 - acc: 0.5042 - val_loss: 0.6968 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6937 - acc: 0.4975 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.5026 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6935 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6937 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4972 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 305us/sample - loss: 0.7133 - acc: 0.5046 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6947 - acc: 0.4994 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6939 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6936 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.4970 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6934 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6933 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6952 - acc: 0.5010 - val_loss: 0.9545 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6939 - acc: 0.5032 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6934 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6935 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6954 - acc: 0.4970 - val_loss: 0.6958 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.4992 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6935 - acc: 0.5057 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5029 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6936 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6935 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6935 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6934 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6934 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6933 - acc: 0.5042 - val_loss: 0.6978 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6934 - acc: 0.5024 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6934 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6933 - acc: 0.5044 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 399us/sample - loss: 0.8393 - acc: 0.5012 - val_loss: 1.0158 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.7334 - acc: 0.5003 - val_loss: 0.7640 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.7102 - acc: 0.4966 - val_loss: 0.7005 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.7004 - acc: 0.5053 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6972 - acc: 0.5025 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6958 - acc: 0.4962 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6940 - acc: 0.5045 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6940 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6941 - acc: 0.4945 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6937 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6937 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6937 - acc: 0.4993 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6934 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6937 - acc: 0.5017 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6936 - acc: 0.5032 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6940 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6982 - acc: 0.4969 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6934 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6939 - acc: 0.4989 - val_loss: 0.7054 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6937 - acc: 0.5008 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6937 - acc: 0.4992 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6936 - acc: 0.5038 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6938 - acc: 0.5004 - val_loss: 0.7227 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6945 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 290us/sample - loss: 0.6981 - acc: 0.5023 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6953 - acc: 0.4936 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6939 - acc: 0.4984 - val_loss: 0.6958 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6943 - acc: 0.4964 - val_loss: 0.6945 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6940 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6934 - acc: 0.4998 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6935 - acc: 0.5038 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6936 - acc: 0.5054 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6932 - acc: 0.5070 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6932 - acc: 0.5055 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6931 - acc: 0.5019 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6935 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6933 - acc: 0.4997 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6935 - acc: 0.4878 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.5081 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6931 - acc: 0.5063 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.4945 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6936 - acc: 0.4999 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4935 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6932 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6932 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6932 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 280us/sample - loss: 0.6969 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6945 - acc: 0.5017 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6949 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6945 - acc: 0.4864 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6941 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6938 - acc: 0.5022 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6937 - acc: 0.5039 - val_loss: 0.7158 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6942 - acc: 0.4959 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6935 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.5088 - val_loss: 0.6965 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6937 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6936 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6937 - acc: 0.4912 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6934 - acc: 0.4969 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6933 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6931 - acc: 0.5067 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 78us/sample - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6932 - acc: 0.5003 - val_loss: 0.7018 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 79us/sample - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 81us/sample - loss: 0.6937 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6930 - acc: 0.5072 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 80us/sample - loss: 0.6934 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6932 - acc: 0.4997 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 302us/sample - loss: 0.7135 - acc: 0.4960 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6942 - acc: 0.5014 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6942 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6935 - acc: 0.5026 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6935 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6936 - acc: 0.4914 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.4963 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4922 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6932 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5034 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6933 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6933 - acc: 0.4974 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5010 - val_loss: 0.7667 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6936 - acc: 0.4960 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6932 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.5020 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6935 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6933 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6934 - acc: 0.5017 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6934 - acc: 0.5052 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6935 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6933 - acc: 0.5013 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6933 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6932 - acc: 0.5079 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6946 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 333us/sample - loss: 0.9228 - acc: 0.4969 - val_loss: 0.7006 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.7498 - acc: 0.4979 - val_loss: 0.7059 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.7165 - acc: 0.4983 - val_loss: 0.7087 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.7044 - acc: 0.5033 - val_loss: 0.7103 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6977 - acc: 0.5059 - val_loss: 0.6972 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6968 - acc: 0.5009 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6952 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6946 - acc: 0.5029 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6943 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.5058 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6941 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6937 - acc: 0.5033 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6938 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6943 - acc: 0.4955 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6941 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6936 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6939 - acc: 0.5016 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6937 - acc: 0.5066 - val_loss: 0.6954 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6937 - acc: 0.5048 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6937 - acc: 0.5021 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6937 - acc: 0.5049 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6940 - acc: 0.4984 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6939 - acc: 0.4981 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6935 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6940 - acc: 0.4972 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.5072 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6939 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6959 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 297us/sample - loss: 0.6997 - acc: 0.4977 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6967 - acc: 0.5005 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6952 - acc: 0.4984 - val_loss: 0.7037 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6949 - acc: 0.5058 - val_loss: 0.6962 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6944 - acc: 0.4950 - val_loss: 0.7055 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6940 - acc: 0.4969 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6942 - acc: 0.5012 - val_loss: 0.6952 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6937 - acc: 0.4925 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6933 - acc: 0.4966 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6937 - acc: 0.4931 - val_loss: 0.6977 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6934 - acc: 0.4925 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6934 - acc: 0.4977 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6932 - acc: 0.5087 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6942 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.5017 - val_loss: 0.7440 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6934 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6933 - acc: 0.4981 - val_loss: 0.6968 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.5068 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6935 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 297us/sample - loss: 0.7018 - acc: 0.5048 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6959 - acc: 0.5045 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6954 - acc: 0.5060 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6953 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6944 - acc: 0.4964 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6939 - acc: 0.4982 - val_loss: 0.6954 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 82us/sample - loss: 0.6938 - acc: 0.4932 - val_loss: 0.6936 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6935 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6932 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.4941 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6934 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6932 - acc: 0.5036 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 1s 85us/sample - loss: 0.6934 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 1s 87us/sample - loss: 0.6933 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6933 - acc: 0.4981 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6936 - acc: 0.5030 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6937 - acc: 0.4940 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6934 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6935 - acc: 0.4952 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 1s 86us/sample - loss: 0.6932 - acc: 0.5034 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 1s 84us/sample - loss: 0.6933 - acc: 0.5023 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 328us/sample - loss: 0.6992 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6934 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6933 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6933 - acc: 0.5004 - val_loss: 0.6992 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6935 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.4937 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6933 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.5008 - val_loss: 2.0101 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6936 - acc: 0.5015 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6937 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4957 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6936 - acc: 0.5035 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6933 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6935 - acc: 0.4967 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6934 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6931 - acc: 0.5064 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.4886 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6933 - acc: 0.5029 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.5036 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 345us/sample - loss: 0.8444 - acc: 0.5026 - val_loss: 0.6970 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.7316 - acc: 0.5069 - val_loss: 0.7082 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.7151 - acc: 0.5006 - val_loss: 0.7015 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.7048 - acc: 0.5024 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6996 - acc: 0.5005 - val_loss: 0.6975 - val_acc: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6969 - acc: 0.4962 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6953 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5136 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6944 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6942 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6941 - acc: 0.4982 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6940 - acc: 0.5012 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6962 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6940 - acc: 0.4943 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6939 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.5030 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6938 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6938 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6937 - acc: 0.5042 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6936 - acc: 0.4978 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6936 - acc: 0.5064 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.5032 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6937 - acc: 0.5026 - val_loss: 0.6995 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6953 - acc: 0.5032 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6942 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6936 - acc: 0.5015 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6938 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6937 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 5s 323us/sample - loss: 0.7034 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6970 - acc: 0.4993 - val_loss: 0.6976 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6970 - acc: 0.4939 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6947 - acc: 0.5008 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6940 - acc: 0.5011 - val_loss: 0.7028 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6949 - acc: 0.5022 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4993 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4898 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6931 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5002 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4937 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6934 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.4941 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6937 - acc: 0.4953 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6933 - acc: 0.5051 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6933 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.4957 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.5001 - val_loss: 1.1859 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6936 - acc: 0.4974 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6933 - acc: 0.5017 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6933 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 327us/sample - loss: 0.6999 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6959 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6935 - acc: 0.4989 - val_loss: 0.6989 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 94us/sample - loss: 0.6936 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6932 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6933 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6933 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6935 - acc: 0.4954 - val_loss: 0.7013 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6935 - acc: 0.4929 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6943 - acc: 0.4967 - val_loss: 0.7290 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6936 - acc: 0.5043 - val_loss: 0.7004 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6936 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 91us/sample - loss: 0.6934 - acc: 0.4999 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6934 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6936 - acc: 0.4953 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6934 - acc: 0.5022 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6934 - acc: 0.4998 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6932 - acc: 0.5063 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6936 - acc: 0.5048 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 1s 88us/sample - loss: 0.6942 - acc: 0.4951 - val_loss: 0.7645 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 90us/sample - loss: 0.6935 - acc: 0.5055 - val_loss: 0.6948 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6937 - acc: 0.5024 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 93us/sample - loss: 0.6934 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 88us/sample - loss: 0.6936 - acc: 0.5000 - val_loss: 0.6956 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6935 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 92us/sample - loss: 0.6936 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 89us/sample - loss: 0.6934 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 340us/sample - loss: 0.7083 - acc: 0.4972 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6945 - acc: 0.4966 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6935 - acc: 0.5002 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6935 - acc: 0.5010 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6937 - acc: 0.4972 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 105us/sample - loss: 0.6932 - acc: 0.5081 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.5053 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6937 - acc: 0.4979 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6936 - acc: 0.4976 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6973 - acc: 0.5018 - val_loss: 0.6952 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.4985 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6934 - acc: 0.4984 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6936 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6935 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6935 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6936 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6936 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6934 - acc: 0.5030 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6934 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6936 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6933 - acc: 0.5058 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5000 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6950 - acc: 0.5005 - val_loss: 7.2387 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6979 - acc: 0.4968 - val_loss: 0.6956 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6937 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 106us/sample - loss: 0.6935 - acc: 0.4996 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 371us/sample - loss: 0.8238 - acc: 0.4989 - val_loss: 0.7324 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.7223 - acc: 0.4937 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.7073 - acc: 0.4971 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6978 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6965 - acc: 0.4951 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6948 - acc: 0.5044 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6935 - acc: 0.5079 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6940 - acc: 0.4966 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.5023 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6935 - acc: 0.5016 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6936 - acc: 0.5058 - val_loss: 0.6952 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6941 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.5068 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5058 - val_loss: 0.6956 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6933 - acc: 0.5029 - val_loss: 0.7026 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6938 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6976 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6939 - acc: 0.4932 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6936 - acc: 0.4987 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6935 - acc: 0.5011 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6939 - acc: 0.4935 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.5015 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4995 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4907 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6935 - acc: 0.4974 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6937 - acc: 0.4994 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 344us/sample - loss: 0.7160 - acc: 0.4992 - val_loss: 0.6959 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6980 - acc: 0.4998 - val_loss: 0.6989 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6941 - acc: 0.5081 - val_loss: 0.7035 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6938 - acc: 0.5014 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6937 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6933 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6937 - acc: 0.4952 - val_loss: 0.7427 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.7028 - acc: 0.5042 - val_loss: 0.7252 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6945 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6935 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6935 - acc: 0.5018 - val_loss: 0.6982 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.4924 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6935 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5001 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.5045 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.5026 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6934 - acc: 0.5064 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6953 - acc: 0.5004 - val_loss: 0.7324 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6936 - acc: 0.5029 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6932 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5021 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.5000 - val_loss: 0.7850 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 103us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6935 - acc: 0.5014 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 6s 345us/sample - loss: 0.7011 - acc: 0.5038 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6974 - acc: 0.4971 - val_loss: 0.7169 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6958 - acc: 0.5023 - val_loss: 0.7066 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 104us/sample - loss: 0.6934 - acc: 0.5038 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6936 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6937 - acc: 0.4941 - val_loss: 1.3461 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6939 - acc: 0.4988 - val_loss: 0.6951 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.4961 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6934 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 102us/sample - loss: 0.6933 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.5080 - val_loss: 0.7514 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6935 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6934 - acc: 0.4954 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 95us/sample - loss: 0.6935 - acc: 0.4901 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6934 - acc: 0.4946 - val_loss: 0.7061 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6936 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6933 - acc: 0.5031 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6935 - acc: 0.4929 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 100us/sample - loss: 0.6931 - acc: 0.5013 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 101us/sample - loss: 0.6944 - acc: 0.4956 - val_loss: 0.7013 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 98us/sample - loss: 0.6935 - acc: 0.5022 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6935 - acc: 0.5062 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6936 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 97us/sample - loss: 0.6933 - acc: 0.5063 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 96us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 99us/sample - loss: 0.6936 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 386us/sample - loss: 0.7197 - acc: 0.5029 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6950 - acc: 0.5000 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6934 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.5022 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6934 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6950 - acc: 0.4951 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6936 - acc: 0.5027 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6936 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6936 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6935 - acc: 0.4986 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6935 - acc: 0.4898 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6934 - acc: 0.4983 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.5042 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6936 - acc: 0.4927 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6935 - acc: 0.4926 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6935 - acc: 0.4943 - val_loss: 0.7327 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6935 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6935 - acc: 0.4992 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6936 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5083 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6936 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6934 - acc: 0.5059 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.4995 - val_loss: 0.7104 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6938 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6938 - acc: 0.4929 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6934 - acc: 0.4954 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 397us/sample - loss: 0.8456 - acc: 0.5011 - val_loss: 0.7070 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.7230 - acc: 0.5063 - val_loss: 0.6995 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.7064 - acc: 0.4990 - val_loss: 0.7119 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6988 - acc: 0.5005 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6960 - acc: 0.4974 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6943 - acc: 0.5038 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6943 - acc: 0.5013 - val_loss: 0.6965 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6939 - acc: 0.5036 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6938 - acc: 0.5030 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6935 - acc: 0.5026 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5059 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6935 - acc: 0.5046 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6939 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6935 - acc: 0.4989 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6958 - acc: 0.4993 - val_loss: 0.7042 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6938 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6957 - acc: 0.5021 - val_loss: 0.7313 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6940 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6937 - acc: 0.4938 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6939 - acc: 0.4945 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6937 - acc: 0.4936 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6935 - acc: 0.5003 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6938 - acc: 0.4924 - val_loss: 0.6943 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6936 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6938 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6944 - acc: 0.5006 - val_loss: 0.6976 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6940 - acc: 0.4918 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6934 - acc: 0.4997 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 410us/sample - loss: 0.7017 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.5043 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6932 - acc: 0.5045 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6935 - acc: 0.4942 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6932 - acc: 0.5063 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6935 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6932 - acc: 0.5044 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6932 - acc: 0.5036 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6933 - acc: 0.4928 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5029 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6933 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 107us/sample - loss: 0.6932 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6934 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.4917 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6933 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6933 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 114us/sample - loss: 0.6933 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6932 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 110us/sample - loss: 0.6933 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.4954 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6933 - acc: 0.4932 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.4941 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6932 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6933 - acc: 0.4954 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 388us/sample - loss: 0.7023 - acc: 0.4989 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6938 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6959 - acc: 0.5034 - val_loss: 1.1817 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6934 - acc: 0.4875 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6934 - acc: 0.5012 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6946 - acc: 0.5034 - val_loss: 1.1723 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6938 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6934 - acc: 0.4891 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6933 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6933 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6947 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6934 - acc: 0.4979 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6933 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 112us/sample - loss: 0.6933 - acc: 0.4956 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 109us/sample - loss: 0.6934 - acc: 0.4951 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6929 - acc: 0.5095 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 113us/sample - loss: 0.6934 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6935 - acc: 0.4969 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6934 - acc: 0.4958 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6933 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 108us/sample - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6933 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 111us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6965 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 415us/sample - loss: 0.8681 - acc: 0.4996 - val_loss: 0.7042 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.7421 - acc: 0.4956 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.7127 - acc: 0.4906 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.7010 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6962 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6949 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6947 - acc: 0.4942 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6937 - acc: 0.5079 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6939 - acc: 0.5010 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6941 - acc: 0.4932 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6941 - acc: 0.4978 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6941 - acc: 0.5027 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6936 - acc: 0.5053 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6942 - acc: 0.4949 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6940 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6940 - acc: 0.4993 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6939 - acc: 0.4978 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6940 - acc: 0.4949 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6941 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6940 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.5078 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6938 - acc: 0.4985 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6938 - acc: 0.5045 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6938 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6938 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6937 - acc: 0.5040 - val_loss: 0.6948 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6938 - acc: 0.4970 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6938 - acc: 0.5014 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6942 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 427us/sample - loss: 0.8536 - acc: 0.4939 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.7370 - acc: 0.5001 - val_loss: 0.7000 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.7102 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.7007 - acc: 0.4995 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 136us/sample - loss: 0.6964 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6955 - acc: 0.4952 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6943 - acc: 0.4996 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6944 - acc: 0.5027 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6942 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6943 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6941 - acc: 0.4966 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6940 - acc: 0.4973 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6939 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6942 - acc: 0.4900 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.4960 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6940 - acc: 0.4972 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6941 - acc: 0.4965 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6937 - acc: 0.5032 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6938 - acc: 0.5046 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6942 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6935 - acc: 0.5057 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6937 - acc: 0.5032 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.5039 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6940 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6940 - acc: 0.4952 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6937 - acc: 0.5049 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 404us/sample - loss: 0.7181 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6941 - acc: 0.4962 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6934 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6936 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.4964 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6934 - acc: 0.4954 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6935 - acc: 0.4876 - val_loss: 0.7368 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6986 - acc: 0.4936 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6933 - acc: 0.4989 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6934 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6933 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6933 - acc: 0.4969 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6933 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6933 - acc: 0.4932 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6933 - acc: 0.4943 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6932 - acc: 0.5014 - val_loss: 0.7333 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.7134 - acc: 0.5060 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6961 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6945 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6942 - acc: 0.5014 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6944 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6941 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6939 - acc: 0.5064 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6942 - acc: 0.4994 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6939 - acc: 0.4976 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6940 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6939 - acc: 0.4957 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 408us/sample - loss: 0.7219 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6936 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6934 - acc: 0.4992 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 118us/sample - loss: 0.6935 - acc: 0.5047 - val_loss: 0.7061 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6934 - acc: 0.4988 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6964 - acc: 0.4983 - val_loss: 1.0882 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6937 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6936 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.495 - 2s 120us/sample - loss: 0.6934 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6935 - acc: 0.5014 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6934 - acc: 0.5087 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6935 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6933 - acc: 0.5049 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6935 - acc: 0.5015 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6934 - acc: 0.5026 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6937 - acc: 0.4974 - val_loss: 0.7047 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6948 - acc: 0.4932 - val_loss: 7.7572 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.7104 - acc: 0.5038 - val_loss: 0.6984 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6947 - acc: 0.5016 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6940 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6946 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6942 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 117us/sample - loss: 0.6938 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 115us/sample - loss: 0.6942 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 120us/sample - loss: 0.6939 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 116us/sample - loss: 0.6937 - acc: 0.5033 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 119us/sample - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 7s 440us/sample - loss: 0.8183 - acc: 0.5040 - val_loss: 0.7160 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.7236 - acc: 0.5024 - val_loss: 0.6952 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.7033 - acc: 0.4999 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6970 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6949 - acc: 0.5007 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6943 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6941 - acc: 0.5001 - val_loss: 0.6952 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6940 - acc: 0.4988 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6934 - acc: 0.5045 - val_loss: 0.6958 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6938 - acc: 0.5071 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6940 - acc: 0.5006 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6938 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6940 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6942 - acc: 0.5028 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6939 - acc: 0.5024 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6938 - acc: 0.5021 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6937 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6938 - acc: 0.5034 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6941 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6941 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6938 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6940 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.6940 - acc: 0.5009 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6940 - acc: 0.4984 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.6935 - acc: 0.4995 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6937 - acc: 0.5045 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6940 - acc: 0.5041 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6939 - acc: 0.4971 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 479us/sample - loss: 0.8265 - acc: 0.5041 - val_loss: 0.6974 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.7285 - acc: 0.5018 - val_loss: 0.7079 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.7054 - acc: 0.4941 - val_loss: 0.6968 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.6968 - acc: 0.5045 - val_loss: 0.7012 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6957 - acc: 0.4959 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6945 - acc: 0.4962 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6941 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6937 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6940 - acc: 0.4995 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6937 - acc: 0.5018 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6941 - acc: 0.5001 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6941 - acc: 0.5025 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6939 - acc: 0.4990 - val_loss: 0.6950 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6940 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6937 - acc: 0.5028 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.6939 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6940 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6937 - acc: 0.5021 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6939 - acc: 0.5024 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6940 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6937 - acc: 0.4986 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6938 - acc: 0.5036 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6941 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.4962 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6936 - acc: 0.5045 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 446us/sample - loss: 0.7321 - acc: 0.4946 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6943 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6937 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6963 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6934 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6934 - acc: 0.4979 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6933 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6933 - acc: 0.4971 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6934 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.7109 - acc: 0.4934 - val_loss: 1.2930 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.7062 - acc: 0.4980 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6948 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6944 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6940 - acc: 0.5066 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6939 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6939 - acc: 0.5031 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6934 - acc: 0.4993 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6941 - acc: 0.5059 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6939 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6939 - acc: 0.4891 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6941 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6938 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6937 - acc: 0.5013 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6941 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6932 - acc: 0.5053 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 461us/sample - loss: 0.7386 - acc: 0.4952 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6939 - acc: 0.4962 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6951 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6952 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6936 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6935 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6938 - acc: 0.4982 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6936 - acc: 0.4999 - val_loss: 1.1605 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6953 - acc: 0.5002 - val_loss: 2.5172 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6941 - acc: 0.4959 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 135us/sample - loss: 0.6935 - acc: 0.4999 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6935 - acc: 0.4902 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6935 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6934 - acc: 0.5049 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6934 - acc: 0.5020 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6933 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 130us/sample - loss: 0.6934 - acc: 0.5059 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6936 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6938 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6936 - acc: 0.4978 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.7073 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6951 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.6941 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6943 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6938 - acc: 0.5037 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6941 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6941 - acc: 0.4993 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 483us/sample - loss: 0.8926 - acc: 0.4953 - val_loss: 0.7250 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.7538 - acc: 0.4996 - val_loss: 0.7268 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.7153 - acc: 0.5009 - val_loss: 0.7212 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.7044 - acc: 0.4971 - val_loss: 0.6996 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6980 - acc: 0.5019 - val_loss: 0.6954 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6958 - acc: 0.4969 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6938 - acc: 0.5071 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6944 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6943 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6940 - acc: 0.4979 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6940 - acc: 0.4959 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6939 - acc: 0.5015 - val_loss: 0.6963 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6940 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6935 - acc: 0.5045 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6939 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6936 - acc: 0.5060 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6942 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6937 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6940 - acc: 0.4957 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6938 - acc: 0.4995 - val_loss: 0.6989 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6938 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6939 - acc: 0.4969 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6936 - acc: 0.4991 - val_loss: 0.6959 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6938 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 138us/sample - loss: 0.6938 - acc: 0.4940 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 502us/sample - loss: 0.9130 - acc: 0.5008 - val_loss: 0.8937 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.7589 - acc: 0.4972 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.7173 - acc: 0.5009 - val_loss: 0.7033 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.7028 - acc: 0.5052 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6968 - acc: 0.5052 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.6955 - acc: 0.5051 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6949 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.5027 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6941 - acc: 0.4982 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6941 - acc: 0.5011 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6938 - acc: 0.5007 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6940 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6939 - acc: 0.4997 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6935 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6942 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6940 - acc: 0.5021 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6938 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6937 - acc: 0.5068 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6940 - acc: 0.5023 - val_loss: 0.6953 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6938 - acc: 0.4958 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6940 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6938 - acc: 0.4996 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6941 - acc: 0.4976 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6936 - acc: 0.5006 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6940 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6941 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.6934 - acc: 0.5074 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 495us/sample - loss: 0.7321 - acc: 0.4935 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6942 - acc: 0.4967 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6935 - acc: 0.5032 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6936 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6988 - acc: 0.5028 - val_loss: 3.7493 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.7207 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6970 - acc: 0.5064 - val_loss: 0.6977 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6945 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6939 - acc: 0.5035 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6943 - acc: 0.4966 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.6938 - acc: 0.5005 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6943 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.6940 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6943 - acc: 0.4941 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6942 - acc: 0.4922 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6941 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6939 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6939 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6936 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6942 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6937 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 147us/sample - loss: 0.6940 - acc: 0.4913 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6936 - acc: 0.4993 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6935 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6940 - acc: 0.4970 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6940 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 8s 493us/sample - loss: 0.7526 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6942 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6971 - acc: 0.5026 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 2s 145us/sample - loss: 0.6950 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6935 - acc: 0.4959 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6933 - acc: 0.5080 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6934 - acc: 0.5046 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6934 - acc: 0.5038 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6936 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6933 - acc: 0.5031 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6935 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.7183 - acc: 0.4938 - val_loss: 0.7678 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6980 - acc: 0.5017 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6946 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6944 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6942 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6937 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6938 - acc: 0.5050 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 2s 146us/sample - loss: 0.6940 - acc: 0.4981 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 2s 141us/sample - loss: 0.6937 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 2s 142us/sample - loss: 0.6938 - acc: 0.5031 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6938 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 2s 144us/sample - loss: 0.6943 - acc: 0.4985 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6939 - acc: 0.4997 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6940 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 2s 140us/sample - loss: 0.6938 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 2s 143us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 535us/sample - loss: 0.8434 - acc: 0.5007 - val_loss: 0.6966 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.7342 - acc: 0.4998 - val_loss: 0.7194 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.7085 - acc: 0.5065 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.7001 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.6972 - acc: 0.4990 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6945 - acc: 0.5058 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6945 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6943 - acc: 0.4974 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6937 - acc: 0.5057 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6941 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6940 - acc: 0.4992 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6940 - acc: 0.5031 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6939 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6939 - acc: 0.4990 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6934 - acc: 0.5074 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6943 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6937 - acc: 0.5101 - val_loss: 0.6951 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6933 - acc: 0.5040 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.6939 - acc: 0.5045 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6937 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6935 - acc: 0.5003 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6943 - acc: 0.4941 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6938 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.6940 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6937 - acc: 0.5031 - val_loss: 0.6980 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6940 - acc: 0.4997 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6972 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6938 - acc: 0.4979 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 556us/sample - loss: 0.8280 - acc: 0.4989 - val_loss: 0.7281 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.7232 - acc: 0.5070 - val_loss: 0.6959 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.7051 - acc: 0.5030 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6970 - acc: 0.5109 - val_loss: 0.7052 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6963 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6949 - acc: 0.4993 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6944 - acc: 0.5004 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6945 - acc: 0.4939 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6938 - acc: 0.5058 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.4991 - val_loss: 0.6962 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.4991 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4979 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6937 - acc: 0.4982 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6938 - acc: 0.5064 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6941 - acc: 0.4977 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6937 - acc: 0.5047 - val_loss: 0.6957 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6944 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6939 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6943 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6939 - acc: 0.4993 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6938 - acc: 0.4989 - val_loss: 0.6943 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6939 - acc: 0.4967 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.4976 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6935 - acc: 0.4989 - val_loss: 0.6951 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4945 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 538us/sample - loss: 0.8706 - acc: 0.4974 - val_loss: 0.7203 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.7454 - acc: 0.4878 - val_loss: 0.6992 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.7122 - acc: 0.5046 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.7017 - acc: 0.5055 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6992 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6959 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6954 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6951 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6943 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6942 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6943 - acc: 0.4934 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6940 - acc: 0.5008 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6939 - acc: 0.4961 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6938 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6940 - acc: 0.5011 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.6939 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 156us/sample - loss: 0.6937 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6940 - acc: 0.4960 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6938 - acc: 0.4972 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6937 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6942 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6940 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6941 - acc: 0.4884 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6936 - acc: 0.4984 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 9s 555us/sample - loss: 0.7888 - acc: 0.5018 - val_loss: 0.7090 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.7171 - acc: 0.4956 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.7016 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6966 - acc: 0.4963 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6949 - acc: 0.4973 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6947 - acc: 0.4939 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6944 - acc: 0.4878 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4987 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6941 - acc: 0.4949 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6940 - acc: 0.4910 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6941 - acc: 0.5025 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 155us/sample - loss: 0.6936 - acc: 0.5011 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6934 - acc: 0.5064 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6944 - acc: 0.4943 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4989 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6936 - acc: 0.5064 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 150us/sample - loss: 0.6939 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 148us/sample - loss: 0.6942 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6938 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 151us/sample - loss: 0.6937 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 154us/sample - loss: 0.6940 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6936 - acc: 0.5052 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 152us/sample - loss: 0.6941 - acc: 0.4975 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6939 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6938 - acc: 0.5042 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 149us/sample - loss: 0.6935 - acc: 0.5039 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 577us/sample - loss: 0.8370 - acc: 0.5039 - val_loss: 0.6956 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.7281 - acc: 0.5058 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.7074 - acc: 0.5047 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6991 - acc: 0.5074 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6966 - acc: 0.5045 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6939 - acc: 0.5080 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6947 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6940 - acc: 0.4990 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.5006 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6940 - acc: 0.5013 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6936 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6940 - acc: 0.4975 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6937 - acc: 0.5020 - val_loss: 0.6961 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6938 - acc: 0.5011 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6942 - acc: 0.4973 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.5006 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6939 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6938 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6940 - acc: 0.5002 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6941 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6935 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6935 - acc: 0.5041 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6938 - acc: 0.5026 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.4988 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 611us/sample - loss: 0.8295 - acc: 0.4977 - val_loss: 0.6969 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.7287 - acc: 0.4942 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.7076 - acc: 0.4966 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6998 - acc: 0.4964 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6973 - acc: 0.5017 - val_loss: 0.6953 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6945 - acc: 0.5017 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6942 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6944 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6942 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6938 - acc: 0.5031 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6935 - acc: 0.5079 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6939 - acc: 0.5069 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6939 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6942 - acc: 0.4936 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6940 - acc: 0.5028 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6938 - acc: 0.5008 - val_loss: 0.6958 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6939 - acc: 0.4978 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6942 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6940 - acc: 0.5002 - val_loss: 0.6949 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6939 - acc: 0.5001 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6936 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6938 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6937 - acc: 0.5052 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6939 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6938 - acc: 0.5058 - val_loss: 0.6963 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6935 - acc: 0.5081 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6941 - acc: 0.4983 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6940 - acc: 0.4928 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6940 - acc: 0.4944 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 593us/sample - loss: 0.8503 - acc: 0.4956 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.7347 - acc: 0.4961 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.7073 - acc: 0.5005 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.7002 - acc: 0.4962 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6954 - acc: 0.5032 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6948 - acc: 0.5032 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6943 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6943 - acc: 0.4932 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6938 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6943 - acc: 0.4942 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6941 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6937 - acc: 0.5044 - val_loss: 0.6962 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6937 - acc: 0.4953 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6938 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6940 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6941 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6937 - acc: 0.5037 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6977 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6942 - acc: 0.4942 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6940 - acc: 0.4909 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6941 - acc: 0.4922 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6936 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6939 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6939 - acc: 0.5022 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 161us/sample - loss: 0.6937 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6942 - acc: 0.4946 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6937 - acc: 0.5002 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6937 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6938 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 10s 600us/sample - loss: 0.9122 - acc: 0.4995 - val_loss: 0.7239 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.7563 - acc: 0.4969 - val_loss: 0.7097 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.7180 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.7054 - acc: 0.4960 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6988 - acc: 0.5007 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6957 - acc: 0.5001 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6951 - acc: 0.5000 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6946 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6941 - acc: 0.5026 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6937 - acc: 0.5027 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6942 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6938 - acc: 0.5051 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 158us/sample - loss: 0.6938 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6943 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6939 - acc: 0.4981 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6937 - acc: 0.4988 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6938 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6939 - acc: 0.5018 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6938 - acc: 0.5021 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6941 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 160us/sample - loss: 0.6935 - acc: 0.5070 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 157us/sample - loss: 0.6941 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 163us/sample - loss: 0.6936 - acc: 0.5042 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 164us/sample - loss: 0.6938 - acc: 0.5069 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6937 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6937 - acc: 0.5027 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 162us/sample - loss: 0.6935 - acc: 0.5014 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 159us/sample - loss: 0.6939 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 629us/sample - loss: 0.8637 - acc: 0.5039 - val_loss: 0.7010 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.7387 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.7112 - acc: 0.5044 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.7011 - acc: 0.5056 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6979 - acc: 0.4982 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6962 - acc: 0.4931 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6950 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6944 - acc: 0.5038 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6938 - acc: 0.4998 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6943 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6940 - acc: 0.5025 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6941 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6940 - acc: 0.5026 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6941 - acc: 0.4954 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6940 - acc: 0.4990 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6936 - acc: 0.5028 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6938 - acc: 0.5016 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 176us/sample - loss: 0.6942 - acc: 0.4891 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6939 - acc: 0.4975 - val_loss: 0.7002 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6958 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6940 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6942 - acc: 0.4901 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6937 - acc: 0.5082 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6938 - acc: 0.5055 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 175us/sample - loss: 0.6939 - acc: 0.4991 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6939 - acc: 0.5009 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6939 - acc: 0.5031 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 670us/sample - loss: 0.8479 - acc: 0.4978 - val_loss: 0.6949 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 192us/sample - loss: 0.7365 - acc: 0.4915 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.7063 - acc: 0.5059 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.7015 - acc: 0.4964 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6970 - acc: 0.5011 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6953 - acc: 0.4995 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6952 - acc: 0.4954 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6944 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6942 - acc: 0.4947 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 191us/sample - loss: 0.6942 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 191us/sample - loss: 0.6940 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6939 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6940 - acc: 0.4966 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6942 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6939 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6936 - acc: 0.5078 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6937 - acc: 0.4986 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6940 - acc: 0.4989 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6939 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6942 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 188us/sample - loss: 0.6943 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6939 - acc: 0.4924 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6939 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6941 - acc: 0.5022 - val_loss: 0.6944 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6936 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6939 - acc: 0.5019 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6940 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 660us/sample - loss: 0.8298 - acc: 0.4914 - val_loss: 0.7028 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.7234 - acc: 0.5002 - val_loss: 0.6945 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.7071 - acc: 0.5005 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6988 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6964 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6951 - acc: 0.5016 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6944 - acc: 0.5023 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6946 - acc: 0.4956 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6942 - acc: 0.4968 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6943 - acc: 0.4986 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 168us/sample - loss: 0.6940 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6941 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.4945 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6940 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6938 - acc: 0.4970 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6939 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6937 - acc: 0.5029 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6942 - acc: 0.4974 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6938 - acc: 0.5011 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6941 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6939 - acc: 0.4906 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6941 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6941 - acc: 0.4937 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6938 - acc: 0.5021 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 169us/sample - loss: 0.6937 - acc: 0.5064 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6940 - acc: 0.4992 - val_loss: 0.6943 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6937 - acc: 0.5045 - val_loss: 0.6942 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6936 - acc: 0.5058 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6937 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 11s 653us/sample - loss: 0.8279 - acc: 0.5063 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.7307 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.7095 - acc: 0.4989 - val_loss: 0.6941 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6993 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6984 - acc: 0.4921 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 174us/sample - loss: 0.6953 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6948 - acc: 0.4991 - val_loss: 0.6943 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6942 - acc: 0.5006 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6944 - acc: 0.4906 - val_loss: 0.6944 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 172us/sample - loss: 0.6942 - acc: 0.4975 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6941 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6939 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6940 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6939 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6937 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6943 - acc: 0.4995 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6937 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6939 - acc: 0.4996 - val_loss: 0.6945 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6938 - acc: 0.4984 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6943 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 165us/sample - loss: 0.6938 - acc: 0.4959 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.5038 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6938 - acc: 0.5040 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 166us/sample - loss: 0.6942 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6937 - acc: 0.5001 - val_loss: 0.6954 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 171us/sample - loss: 0.6940 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 173us/sample - loss: 0.6938 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 167us/sample - loss: 0.6939 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 170us/sample - loss: 0.6935 - acc: 0.5055 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 688us/sample - loss: 0.8392 - acc: 0.5034 - val_loss: 0.6966 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.7350 - acc: 0.4995 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.7066 - acc: 0.5088 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.7033 - acc: 0.4912 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6974 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6964 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6941 - acc: 0.5021 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6941 - acc: 0.5054 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6942 - acc: 0.4973 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6939 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6937 - acc: 0.5030 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6940 - acc: 0.4982 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6939 - acc: 0.4998 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6937 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6940 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6936 - acc: 0.5048 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6939 - acc: 0.5017 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6942 - acc: 0.4955 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6940 - acc: 0.4939 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6938 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6939 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6938 - acc: 0.4993 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6937 - acc: 0.5046 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.6939 - acc: 0.4986 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6938 - acc: 0.5029 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6938 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6933 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6938 - acc: 0.5002 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6941 - acc: 0.4904 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6939 - acc: 0.4968 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 742us/sample - loss: 0.8648 - acc: 0.5093 - val_loss: 0.8575 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.7387 - acc: 0.4992 - val_loss: 0.8785 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.7089 - acc: 0.5049 - val_loss: 0.8639 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.7028 - acc: 0.4962 - val_loss: 0.6978 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6968 - acc: 0.4986 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6951 - acc: 0.5012 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6942 - acc: 0.5025 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6949 - acc: 0.4914 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.6936 - acc: 0.5045 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 191us/sample - loss: 0.6943 - acc: 0.5002 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6940 - acc: 0.4932 - val_loss: 0.6966 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6940 - acc: 0.5015 - val_loss: 0.6955 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6940 - acc: 0.4991 - val_loss: 0.7190 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6941 - acc: 0.4976 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.6939 - acc: 0.5041 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6940 - acc: 0.4950 - val_loss: 0.6942 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6939 - acc: 0.5013 - val_loss: 0.6957 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6941 - acc: 0.4924 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6941 - acc: 0.4946 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 192us/sample - loss: 0.6941 - acc: 0.4944 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 191us/sample - loss: 0.6936 - acc: 0.5038 - val_loss: 0.7157 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6941 - acc: 0.4970 - val_loss: 0.6971 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6940 - acc: 0.5016 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6941 - acc: 0.4933 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6940 - acc: 0.4976 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.6939 - acc: 0.4954 - val_loss: 0.6946 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 200us/sample - loss: 0.6938 - acc: 0.5037 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 195us/sample - loss: 0.6938 - acc: 0.5047 - val_loss: 0.6953 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 190us/sample - loss: 0.6935 - acc: 0.5066 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6938 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 709us/sample - loss: 0.8655 - acc: 0.4969 - val_loss: 0.6963 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.7417 - acc: 0.4951 - val_loss: 0.6957 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.7131 - acc: 0.4970 - val_loss: 0.6950 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.7021 - acc: 0.4989 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6984 - acc: 0.5017 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6953 - acc: 0.5028 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6959 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6943 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6946 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6945 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6942 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6936 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 188us/sample - loss: 0.6945 - acc: 0.4922 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6940 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6937 - acc: 0.5021 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 180us/sample - loss: 0.6940 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6939 - acc: 0.4955 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6937 - acc: 0.5079 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6940 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6937 - acc: 0.4986 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6940 - acc: 0.4945 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6938 - acc: 0.4982 - val_loss: 0.6936 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 188us/sample - loss: 0.6939 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6939 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6937 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6940 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6936 - acc: 0.5072 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6937 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6938 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 12s 732us/sample - loss: 0.8876 - acc: 0.4931 - val_loss: 0.6995 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.7400 - acc: 0.5004 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.7118 - acc: 0.5012 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.7035 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6969 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6954 - acc: 0.4995 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6945 - acc: 0.5026 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.6941 - acc: 0.4992 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 185us/sample - loss: 0.6937 - acc: 0.5050 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6942 - acc: 0.5001 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6938 - acc: 0.5027 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 177us/sample - loss: 0.6940 - acc: 0.4991 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6940 - acc: 0.4948 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6943 - acc: 0.4922 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6937 - acc: 0.4966 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6939 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 184us/sample - loss: 0.6942 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 187us/sample - loss: 0.6940 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6937 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6935 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6942 - acc: 0.4956 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 179us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6942 - acc: 0.4948 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 181us/sample - loss: 0.6943 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6939 - acc: 0.4960 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 182us/sample - loss: 0.6937 - acc: 0.5075 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 186us/sample - loss: 0.6938 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 189us/sample - loss: 0.6940 - acc: 0.4915 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 183us/sample - loss: 0.6936 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 178us/sample - loss: 0.6941 - acc: 0.4932 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 752us/sample - loss: 0.8551 - acc: 0.4992 - val_loss: 0.6958 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 3s 203us/sample - loss: 0.7365 - acc: 0.5020 - val_loss: 0.7198 - val_acc: 0.4956\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 3s 200us/sample - loss: 0.7103 - acc: 0.5017 - val_loss: 0.7632 - val_acc: 0.4956\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.7011 - acc: 0.4971 - val_loss: 0.7050 - val_acc: 0.4956\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6972 - acc: 0.4978 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6952 - acc: 0.4994 - val_loss: 0.6945 - val_acc: 0.4956\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6945 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6943 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6943 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6941 - acc: 0.4952 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.6939 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6940 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6937 - acc: 0.4995 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.6939 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6937 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6938 - acc: 0.5018 - val_loss: 0.6938 - val_acc: 0.4956\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6941 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6937 - acc: 0.5036 - val_loss: 0.7021 - val_acc: 0.5044\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6939 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 3s 202us/sample - loss: 0.6941 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 3s 197us/sample - loss: 0.6938 - acc: 0.4951 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6939 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6938 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 193us/sample - loss: 0.6940 - acc: 0.4965 - val_loss: 0.6939 - val_acc: 0.4956\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6941 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 3s 194us/sample - loss: 0.6938 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 196us/sample - loss: 0.6935 - acc: 0.5057 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 3s 198us/sample - loss: 0.6943 - acc: 0.4899 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 3s 199us/sample - loss: 0.6938 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 3s 200us/sample - loss: 0.6937 - acc: 0.5034 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/30\n",
      "17000/17000 [==============================] - 13s 791us/sample - loss: 0.8547 - acc: 0.4934 - val_loss: 0.6999 - val_acc: 0.5044\n",
      "Epoch 2/30\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.7297 - acc: 0.4981 - val_loss: 0.6964 - val_acc: 0.5044\n",
      "Epoch 3/30\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.7078 - acc: 0.4995 - val_loss: 0.7227 - val_acc: 0.5044\n",
      "Epoch 4/30\n",
      "17000/17000 [==============================] - 4s 212us/sample - loss: 0.6989 - acc: 0.5041 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 5/30\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.6980 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 6/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6949 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 7/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6950 - acc: 0.4969 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "17000/17000 [==============================] - 3s 206us/sample - loss: 0.6943 - acc: 0.4966 - val_loss: 0.6940 - val_acc: 0.5044\n",
      "Epoch 9/30\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6944 - acc: 0.4996 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 10/30\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6940 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 11/30\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.6937 - acc: 0.4991 - val_loss: 0.6949 - val_acc: 0.4956\n",
      "Epoch 12/30\n",
      "17000/17000 [==============================] - 4s 215us/sample - loss: 0.6942 - acc: 0.4938 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 13/30\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.6936 - acc: 0.5022 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 14/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6941 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 15/30\n",
      "17000/17000 [==============================] - 3s 206us/sample - loss: 0.6941 - acc: 0.4936 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 16/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6940 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 17/30\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.6940 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 18/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.6943 - acc: 0.4885 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 19/30\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6933 - acc: 0.5071 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 20/30\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.6939 - acc: 0.5006 - val_loss: 0.6943 - val_acc: 0.5044\n",
      "Epoch 21/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6940 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 22/30\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6939 - acc: 0.5011 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 23/30\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6937 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.6939 - acc: 0.5045 - val_loss: 0.6973 - val_acc: 0.5044\n",
      "Epoch 25/30\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6939 - acc: 0.4972 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 26/30\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.6938 - acc: 0.5019 - val_loss: 0.6947 - val_acc: 0.5044\n",
      "Epoch 27/30\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.6940 - acc: 0.5005 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 28/30\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6940 - acc: 0.4942 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 29/30\n",
      "17000/17000 [==============================] - 4s 216us/sample - loss: 0.6937 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/30\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.6939 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4956\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [0,1,2,3]\n",
    "layer_sizes = [15,50,100,150]\n",
    "conv_layers = [0,1,2,3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME =\"LAPPD-Charge-3x3-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\LAPPD\\{}'.format(NAME))\n",
    "        \n",
    "        \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(layer_size,(9,9),strides=1, input_shape= XTrainingC.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "            model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "            for l in range(conv_layer-1):                   \n",
    "                model.add(Conv2D(layer_size,(3,3),padding='same',activation=\"relu\"))              \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))            \n",
    "            #model.add(GlobalAveragePooling2D())\n",
    "            model.add(Flatten())\n",
    "            for l in range(dense_layer-1):\n",
    "                model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "            model.add(Dense(32,activation=\"relu\"))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(2))\n",
    "            model.add(Activation('softmax'))\n",
    "            #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "            model.compile(loss=\"binary_crossentropy\",\n",
    "                         optimizer=\"adam\",\n",
    "                          metrics=['accuracy']\n",
    "                         )   \n",
    "            #filepath=\"LAPPD_Charge_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "            #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "            #model.summary()\n",
    "            history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        tensorboard \n",
    "            ],\n",
    "          epochs= 30)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 10, 16, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_147 (Flatten)        (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_396 (Bat (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dense_364 (Dense)            (None, 150)               24150     \n",
      "_________________________________________________________________\n",
      "batch_normalization_397 (Bat (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dropout_429 (Dropout)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_398 (Bat (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense_365 (Dense)            (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_399 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_430 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_366 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_400 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_431 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_367 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_401 (Bat (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_432 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_368 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 49,592\n",
      "Trainable params: 48,272\n",
      "Non-trainable params: 1,320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=XTrainingC.shape[1:], name='img')\n",
    "x= layers.Flatten()(inputs)\n",
    "\n",
    "x= layers.BatchNormalization()(x)\n",
    "x = layers.Dense(150, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "x = layers.Dense(100, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(50, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(50, activation='sigmoid')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "#outputs = layers.Dense(1,activation='sigmoid')(x)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='Model')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/60\n",
      "17000/17000 [==============================] - 18s 1ms/sample - loss: 0.7725 - acc: 0.5490 - val_loss: 0.6910 - val_acc: 0.5080\n",
      "Epoch 2/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.6965 - acc: 0.5804 - val_loss: 0.6971 - val_acc: 0.4972\n",
      "Epoch 3/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.6659 - acc: 0.5988 - val_loss: 0.8294 - val_acc: 0.4916\n",
      "Epoch 4/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.6433 - acc: 0.6241 - val_loss: 1.2116 - val_acc: 0.4924\n",
      "Epoch 5/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.6168 - acc: 0.6520 - val_loss: 1.0946 - val_acc: 0.4980\n",
      "Epoch 6/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.5943 - acc: 0.6675 - val_loss: 1.0440 - val_acc: 0.5052\n",
      "Epoch 7/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5803 - acc: 0.6821 - val_loss: 0.8442 - val_acc: 0.5368\n",
      "Epoch 8/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.5629 - acc: 0.6976 - val_loss: 0.7114 - val_acc: 0.5752\n",
      "Epoch 9/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5510 - acc: 0.7059 - val_loss: 0.7817 - val_acc: 0.5684\n",
      "Epoch 10/60\n",
      "17000/17000 [==============================] - 4s 217us/sample - loss: 0.5388 - acc: 0.7194 - val_loss: 0.5805 - val_acc: 0.6780\n",
      "Epoch 11/60\n",
      "17000/17000 [==============================] - 4s 214us/sample - loss: 0.5303 - acc: 0.7227 - val_loss: 0.5940 - val_acc: 0.6692\n",
      "Epoch 12/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.5170 - acc: 0.7356 - val_loss: 0.5833 - val_acc: 0.7000\n",
      "Epoch 13/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5137 - acc: 0.7383 - val_loss: 0.6219 - val_acc: 0.6516\n",
      "Epoch 14/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.5070 - acc: 0.7411 - val_loss: 0.6265 - val_acc: 0.6596\n",
      "Epoch 15/60\n",
      "17000/17000 [==============================] - 4s 210us/sample - loss: 0.4968 - acc: 0.7489 - val_loss: 0.6253 - val_acc: 0.6664\n",
      "Epoch 16/60\n",
      "17000/17000 [==============================] - 4s 225us/sample - loss: 0.4932 - acc: 0.7531 - val_loss: 0.5172 - val_acc: 0.7412\n",
      "Epoch 17/60\n",
      "17000/17000 [==============================] - 4s 216us/sample - loss: 0.4886 - acc: 0.7550 - val_loss: 0.5273 - val_acc: 0.7344\n",
      "Epoch 18/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4800 - acc: 0.7628 - val_loss: 0.5229 - val_acc: 0.7344\n",
      "Epoch 19/60\n",
      "17000/17000 [==============================] - 4s 224us/sample - loss: 0.4739 - acc: 0.7686 - val_loss: 0.6194 - val_acc: 0.6628\n",
      "Epoch 20/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4696 - acc: 0.7696 - val_loss: 0.5572 - val_acc: 0.6968\n",
      "Epoch 21/60\n",
      "17000/17000 [==============================] - 4s 219us/sample - loss: 0.4666 - acc: 0.7702 - val_loss: 0.5248 - val_acc: 0.7304\n",
      "Epoch 22/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4589 - acc: 0.7748 - val_loss: 0.6128 - val_acc: 0.6864\n",
      "Epoch 23/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4531 - acc: 0.7789 - val_loss: 0.5372 - val_acc: 0.7136\n",
      "Epoch 24/60\n",
      "17000/17000 [==============================] - 4s 220us/sample - loss: 0.4478 - acc: 0.7837 - val_loss: 0.5587 - val_acc: 0.7048\n",
      "Epoch 25/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4407 - acc: 0.7875 - val_loss: 0.5635 - val_acc: 0.7152\n",
      "Epoch 26/60\n",
      "17000/17000 [==============================] - 4s 228us/sample - loss: 0.4455 - acc: 0.7846 - val_loss: 0.5409 - val_acc: 0.7212\n",
      "Epoch 27/60\n",
      "17000/17000 [==============================] - 4s 228us/sample - loss: 0.4327 - acc: 0.7913 - val_loss: 0.5898 - val_acc: 0.6964\n",
      "Epoch 28/60\n",
      "17000/17000 [==============================] - 4s 218us/sample - loss: 0.4282 - acc: 0.7929 - val_loss: 0.4820 - val_acc: 0.7620\n",
      "Epoch 29/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4225 - acc: 0.7973 - val_loss: 0.7286 - val_acc: 0.6396\n",
      "Epoch 30/60\n",
      "17000/17000 [==============================] - 4s 219us/sample - loss: 0.4234 - acc: 0.7972 - val_loss: 0.6614 - val_acc: 0.6588\n",
      "Epoch 31/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4210 - acc: 0.7986 - val_loss: 0.5719 - val_acc: 0.7124\n",
      "Epoch 32/60\n",
      "17000/17000 [==============================] - 4s 221us/sample - loss: 0.4142 - acc: 0.8012 - val_loss: 0.6036 - val_acc: 0.7104\n",
      "Epoch 33/60\n",
      "17000/17000 [==============================] - 4s 222us/sample - loss: 0.4086 - acc: 0.8086 - val_loss: 0.5979 - val_acc: 0.7024\n",
      "Epoch 34/60\n",
      "17000/17000 [==============================] - 4s 226us/sample - loss: 0.4024 - acc: 0.8143 - val_loss: 0.5356 - val_acc: 0.7376\n",
      "Epoch 35/60\n",
      "17000/17000 [==============================] - 4s 229us/sample - loss: 0.4004 - acc: 0.8108 - val_loss: 0.5683 - val_acc: 0.7284\n",
      "Epoch 36/60\n",
      "17000/17000 [==============================] - 3s 205us/sample - loss: 0.3974 - acc: 0.8162 - val_loss: 0.5234 - val_acc: 0.7380\n",
      "Epoch 37/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.4016 - acc: 0.8093 - val_loss: 0.5233 - val_acc: 0.7396\n",
      "Epoch 38/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3890 - acc: 0.8193 - val_loss: 0.5317 - val_acc: 0.7400\n",
      "Epoch 39/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3914 - acc: 0.8185 - val_loss: 0.6418 - val_acc: 0.7000\n",
      "Epoch 40/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3840 - acc: 0.8219 - val_loss: 0.6487 - val_acc: 0.6844\n",
      "Epoch 41/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3811 - acc: 0.8218 - val_loss: 0.4687 - val_acc: 0.7836\n",
      "Epoch 42/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.3787 - acc: 0.8275 - val_loss: 0.5854 - val_acc: 0.7232\n",
      "Epoch 43/60\n",
      "17000/17000 [==============================] - 4s 214us/sample - loss: 0.3798 - acc: 0.8241 - val_loss: 0.6663 - val_acc: 0.6800\n",
      "Epoch 44/60\n",
      "17000/17000 [==============================] - 4s 209us/sample - loss: 0.3764 - acc: 0.8297 - val_loss: 0.4995 - val_acc: 0.7684\n",
      "Epoch 45/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3710 - acc: 0.8273 - val_loss: 0.5634 - val_acc: 0.7296\n",
      "Epoch 46/60\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.3666 - acc: 0.8316 - val_loss: 0.5257 - val_acc: 0.7656\n",
      "Epoch 47/60\n",
      "17000/17000 [==============================] - 3s 206us/sample - loss: 0.3671 - acc: 0.8316 - val_loss: 0.7167 - val_acc: 0.6768\n",
      "Epoch 48/60\n",
      "17000/17000 [==============================] - 4s 206us/sample - loss: 0.3614 - acc: 0.8368 - val_loss: 0.5259 - val_acc: 0.7456\n",
      "Epoch 49/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3647 - acc: 0.8304 - val_loss: 0.5340 - val_acc: 0.7488\n",
      "Epoch 50/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3590 - acc: 0.8355 - val_loss: 0.5137 - val_acc: 0.7568\n",
      "Epoch 51/60\n",
      "17000/17000 [==============================] - 4s 213us/sample - loss: 0.3590 - acc: 0.8387 - val_loss: 0.5292 - val_acc: 0.7492\n",
      "Epoch 52/60\n",
      "17000/17000 [==============================] - 4s 212us/sample - loss: 0.3514 - acc: 0.8398 - val_loss: 0.8775 - val_acc: 0.6280\n",
      "Epoch 53/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3514 - acc: 0.8401 - val_loss: 0.4859 - val_acc: 0.7756\n",
      "Epoch 54/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3473 - acc: 0.8418 - val_loss: 0.6815 - val_acc: 0.6984\n",
      "Epoch 55/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3466 - acc: 0.8432 - val_loss: 0.4904 - val_acc: 0.7676\n",
      "Epoch 56/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3449 - acc: 0.8418 - val_loss: 0.6592 - val_acc: 0.7016\n",
      "Epoch 57/60\n",
      "17000/17000 [==============================] - 4s 208us/sample - loss: 0.3412 - acc: 0.8446 - val_loss: 0.6384 - val_acc: 0.6980\n",
      "Epoch 58/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3340 - acc: 0.8489 - val_loss: 0.7159 - val_acc: 0.6776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60\n",
      "17000/17000 [==============================] - 4s 207us/sample - loss: 0.3411 - acc: 0.8453 - val_loss: 0.6283 - val_acc: 0.7140\n",
      "Epoch 60/60\n",
      "17000/17000 [==============================] - 4s 211us/sample - loss: 0.3342 - acc: 0.8488 - val_loss: 0.6639 - val_acc: 0.7080\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "            optimizer='adam',\n",
    "            #optimizer = keras.optimizers.RMSprop(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        #checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 60)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_layers = [0,1,2,3]\n",
    "\n",
    "\n",
    "#for dense_layer in dense_layers:\n",
    "\n",
    "\n",
    "NAME =\"LAPPD-Charge-3x3-MuEl-{}-conv-{}-nodes-{}-dense\".format(conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "tensorboard = TensorBoard(log_dir = 'logs\\LAPPDPerceptron\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "\n",
    "for l in range(3):\n",
    "    model.add(Dense(512-l*50 ,activation=\"relu\" ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "#model.add(Dense(32,activation=\"relu\"))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "              metrics=['accuracy']\n",
    "             )   \n",
    "#filepath=\"LAPPD_Charge_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "#model.summary()\n",
    "#         history=model.fit(XTrainingC,YTraining,\n",
    "#       validation_data=(XValC,Yval)\n",
    "#       ,batch_size=100,\n",
    "#         shuffle=True,\n",
    "#         class_weight='balanced',\n",
    "#         callbacks=[\n",
    "#                     #monitor,\n",
    "#                     #checkpoint,\n",
    "#                     tensorboard \n",
    "#         ],\n",
    "#       epochs= 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0514 14:02:27.200614  2760 deprecation.py:323] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 2500 samples\n",
      "Epoch 1/40\n",
      "17000/17000 [==============================] - 4s 258us/sample - loss: 0.8613 - acc: 0.5003 - val_loss: 0.7572 - val_acc: 0.4956\n",
      "Epoch 2/40\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.7364 - acc: 0.4989 - val_loss: 0.7997 - val_acc: 0.4956\n",
      "Epoch 3/40\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.7093 - acc: 0.5022 - val_loss: 0.7038 - val_acc: 0.4956\n",
      "Epoch 4/40\n",
      "17000/17000 [==============================] - 2s 127us/sample - loss: 0.7009 - acc: 0.5011 - val_loss: 0.6967 - val_acc: 0.4956\n",
      "Epoch 5/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6969 - acc: 0.4987 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 6/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6950 - acc: 0.5035 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 7/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6942 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 8/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6945 - acc: 0.5024 - val_loss: 0.6951 - val_acc: 0.4956\n",
      "Epoch 9/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6940 - acc: 0.5039 - val_loss: 0.6938 - val_acc: 0.5044\n",
      "Epoch 10/40\n",
      "17000/17000 [==============================] - 2s 131us/sample - loss: 0.6940 - acc: 0.5033 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 11/40\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6939 - acc: 0.5005 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 12/40\n",
      "17000/17000 [==============================] - 2s 134us/sample - loss: 0.6940 - acc: 0.4968 - val_loss: 0.6937 - val_acc: 0.4956\n",
      "Epoch 13/40\n",
      "17000/17000 [==============================] - 2s 129us/sample - loss: 0.6941 - acc: 0.4986 - val_loss: 0.6934 - val_acc: 0.5044\n",
      "Epoch 14/40\n",
      "17000/17000 [==============================] - 2s 128us/sample - loss: 0.6940 - acc: 0.4962 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 15/40\n",
      "17000/17000 [==============================] - 2s 139us/sample - loss: 0.6942 - acc: 0.4928 - val_loss: 0.6936 - val_acc: 0.4956\n",
      "Epoch 16/40\n",
      "17000/17000 [==============================] - 3s 153us/sample - loss: 0.6939 - acc: 0.4991 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 17/40\n",
      "17000/17000 [==============================] - 2s 132us/sample - loss: 0.6943 - acc: 0.4919 - val_loss: 0.6955 - val_acc: 0.4956\n",
      "Epoch 18/40\n",
      "17000/17000 [==============================] - 2s 121us/sample - loss: 0.6934 - acc: 0.5065 - val_loss: 0.6939 - val_acc: 0.5044\n",
      "Epoch 19/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6942 - acc: 0.5002 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 20/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6940 - acc: 0.4986 - val_loss: 0.6947 - val_acc: 0.4956\n",
      "Epoch 21/40\n",
      "17000/17000 [==============================] - 2s 122us/sample - loss: 0.6940 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 22/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6940 - acc: 0.4917 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "Epoch 23/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6939 - acc: 0.4969 - val_loss: 0.6935 - val_acc: 0.4956\n",
      "Epoch 24/40\n",
      "17000/17000 [==============================] - 2s 123us/sample - loss: 0.6940 - acc: 0.5019 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 25/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.4956\n",
      "Epoch 26/40\n",
      "17000/17000 [==============================] - 2s 137us/sample - loss: 0.6940 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 27/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6939 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 28/40\n",
      "17000/17000 [==============================] - 2s 126us/sample - loss: 0.6939 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 29/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6937 - acc: 0.4998 - val_loss: 0.6933 - val_acc: 0.5044\n",
      "Epoch 30/40\n",
      "17000/17000 [==============================] - 2s 133us/sample - loss: 0.6940 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 31/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6936 - acc: 0.5023 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 32/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6940 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5044\n",
      "Epoch 33/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6938 - acc: 0.4988 - val_loss: 0.6934 - val_acc: 0.4956\n",
      "Epoch 34/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6959 - val_acc: 0.4956\n",
      "Epoch 35/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6939 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 36/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6940 - acc: 0.4931 - val_loss: 0.6948 - val_acc: 0.4956\n",
      "Epoch 37/40\n",
      "17000/17000 [==============================] - 2s 125us/sample - loss: 0.6937 - acc: 0.4977 - val_loss: 0.6935 - val_acc: 0.5044\n",
      "Epoch 38/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5033 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 39/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6938 - acc: 0.5004 - val_loss: 0.6941 - val_acc: 0.4956\n",
      "Epoch 40/40\n",
      "17000/17000 [==============================] - 2s 124us/sample - loss: 0.6936 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5044\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOy9d5wkV3nv/X26p8OEnjyzaTYnWG1QYiWwEBJBrBCSAPnqJRrwBdn4EmwDF2RfE21f7vtiX8DmCoOvfLlgkySCAKFoCckorqTValfaKO1qZ2YnT0/snp7uPu8fVdVT01PVVdVhwk59P5/+7Gx3narT1VX1nN95whGlFD4+Pj4+Pm4JLHQHfHx8fHyWFr7h8PHx8fHxhG84fHx8fHw84RsOHx8fHx9P+IbDx8fHx8cTvuHw8fHx8fGEbzh8fAogIv9HRP7a5banROSNle6Tj89C4xsOHx8fHx9P+IbDx2cZICJVC90Hn3MH33D4LHn0KaJPi8hBEZkQkf8tIitE5DciMiYi94lIk2n760TksIjEReRBEXml6bMLRORpvd2PgGjesd4qIgf0to+IyG6XfbxGRJ4RkVEROSMiX8j7/DJ9f3H98w/o71eLyN+JyGkRGRGR/9Dfu0JEOi3Owxv1v78gIreJyPdFZBT4gIjsFZFH9WOcFZF/FJGwqf15InKviAyJSK+I/IWIrBSRSRFpMW13kYj0i0jIzXf3OffwDYfPucINwJuAbcC1wG+AvwBa0a7zjwOIyDbgB8CfAm3AncAvRSSsP0R/DnwPaAZ+ou8Xve2FwK3AHwEtwD8Bd4hIxEX/JoA/ABqBa4CPiMjb9P2u0/v7D3qfzgcO6O2+ClwEvEbv038Fsi7PyfXAbfox/xXIAH+mn5NXA28A/kTvQwy4D7gLWA1sAe5XSvUADwI3mvb7XuCHSqlpl/3wOcfwDYfPucI/KKV6lVJdwMPA40qpZ5RSU8DPgAv07f4f4NdKqXv1B99XgWq0B/OlQAj4mlJqWil1G/Ck6RgfBv5JKfW4UiqjlPouMKW3K4hS6kGl1HNKqaxS6iCa8Xqd/vF7gPuUUj/QjzuolDogIgHgD4FPKKW69GM+on8nNzyqlPq5fsyEUuoppdRjSqm0UuoUmuEz+vBWoEcp9XdKqaRSakwp9bj+2XfRjAUiEgTehWZcfZYpvuHwOVfoNf2dsPh/nf73auC08YFSKgucAdbon3Wp2ZU/T5v+Xg98Up/qiYtIHFirtyuIiFwiIg/oUzwjwB+jjfzR93HSolkr2lSZ1WduOJPXh20i8isR6dGnr/7WRR8AfgHsEJFNaKpuRCn1RJF98jkH8A2Hz3KjG80AACAigvbQ7ALOAmv09wzWmf4+A/yNUqrR9KpRSv3AxXH/DbgDWKuUagC+BRjHOQNstmgzACRtPpsAakzfI4g2zWUmv/T1LcARYKtSqh5tKs+pDyilksCP0ZTR+/DVxrLHNxw+y40fA9eIyBt05+4n0aabHgEeBdLAx0WkSkTeAew1tf0O8Me6ehARqdWd3jEXx40BQ0qppIjsBd5t+uxfgTeKyI36cVtE5HxdDd0K/L2IrBaRoIi8WvepHAOi+vFDwH8DnHwtMWAUGBeRVwAfMX32K2CliPypiEREJCYil5g+/7/AB4DrgO+7+L4+5zC+4fBZViiljqLN1/8D2oj+WuBapVRKKZUC3oH2gBxG84f81NR2P5qf4x/1z0/o27rhT4AvicgY8Dk0A2bs92XgLWhGbAjNMb5H//hTwHNovpYh4H8AAaXUiL7Pf0ZTSxPArCgrCz6FZrDG0Izgj0x9GEObhroW6AGOA1eaPv8dmlP+ad0/4rOMEX8hJx8fHzeIyL8D/6aU+ueF7ovPwuIbDh8fH0dE5FXAvWg+mrGF7o/PwuJPVfn4+BRERL6LluPxp77R8AFfcfj4+Pj4eMRXHD4+Pj4+nlgWhc9aW1vVhg0bFrobPj4+PkuKp556akAplZ8ftDwMx4YNG9i/f/9Cd8PHx8dnSSEip63e96eqfHx8fHw84RsOHx8fHx9P+IbDx8fHx8cTy8LHYcX09DSdnZ0kk8mF7kpFiUajdHR0EAr5a+74+PiUh2VrODo7O4nFYmzYsIHZxVDPHZRSDA4O0tnZycaNGxe6Oz4+PucIy3aqKplM0tLScs4aDQARoaWl5ZxXVT4+PvPLsjUcwDltNAyWw3f08fGZX5a14fDx8YJSip/sP0NyOrPQXfHxWVB8w7FAxONx/tf/+l+e273lLW8hHo9XoEc+TpzoG+fTtx3kN4fOLnRXfHwWFN9wLBB2hiOTKTyavfPOO2lsbKxUt3wKMD6VBqBvdGqBe+Ljs7As26iqheazn/0sJ0+e5PzzzycUClFXV8eqVas4cOAAzz//PG9729s4c+YMyWSST3ziE9x0003ATPmU8fFxrr76ai677DIeeeQR1qxZwy9+8Quqq6sX+JuduyT0Kar+Md9w+CxvfMMBfPGXh3m+e7Ss+9yxup7PX3ue7edf+cpXOHToEAcOHODBBx/kmmuu4dChQ7mw2VtvvZXm5mYSiQSvetWruOGGG2hpaZm1j+PHj/ODH/yA73znO9x4443cfvvtvPe97y3r9/CZwfBtDIz7hsNneVPRqSoR2SciR0XkhIh81uLzD4hIv4gc0F8fMn32fhE5rr/eb3o/LCLfFpFjInJERG6o5HeYL/bu3Tsr1+Ib3/gGe/bs4dJLL+XMmTMcP358TpuNGzdy/vnnA3DRRRdx6tSp+erusiSRygLQ7xsOn2VOxRSHiASBbwJvAjqBJ0XkDqXU83mb/kgp9dG8ts3A54GLAQU8pbcdBv4S6FNKbRORANBcal8LKYP5ora2Nvf3gw8+yH333cejjz5KTU0NV1xxhWUuRiQSyf0dDAZJJBLz0tflymRK83EMjKUWuCc+PgtLJRXHXuCEUupFpVQK+CFwvcu2bwbuVUoN6cbiXmCf/tkfAv8dQCmVVUoNlLnf80IsFmNszHoVzpGREZqamqipqeHIkSM89thj89w7HyuMqSpfcfgsdyrp41gDnDH9vxO4xGK7G0TkcuAY8GdKqTM2bdeIiBFO9GURuQI4CXxUKdWbv1MRuQm4CWDdunUlfpXy09LSwu/93u+xc+dOqqurWbFiRe6zffv28a1vfYvdu3ezfft2Lr300gXsqY+B4RwfnkwxnckSCvpBiT7Lk0oaDquU5fwFzn8J/EApNSUifwx8F3h9gbZVQAfwO6XUn4vInwNfBd43Z2Olvg18G+Diiy9elAur/9u//Zvl+5FIhN/85jeWnxl+jNbWVg4dOpR7/1Of+lTZ++czG8PHoRQMTaRYUR9d4B75+CwMlRwydQJrTf/vALrNGyilBpVShu7/DnCRQ9tBYBL4mf7+T4ALy9ttHx9rJqfTub/9kFyf5UwlDceTwFYR2SgiYeCdwB3mDURklem/1wEv6H/fDVwlIk0i0gRcBdytlFJoKuUKfbs3APnOdh+fipBMzSRn+n4On+VMxaaqlFJpEfkomhEIArcqpQ6LyJeA/UqpO4CPi8h1QBoYAj6gtx0SkS+jGR+ALymlhvS/PwN8T0S+BvQDH6zUd/DxMZOYziCiTVX5isNnOVPRBECl1J3AnXnvfc70983AzTZtbwVutXj/NHB5eXvq4+NMYjrLyvooZ0eSfhKgz7LGDwvx8XFJIpWmqSZMXaTKVxw+yxrfcPj4uCQxnaE6HKQtFvENh8+yxjccC0SxZdUBvva1rzE5OVnmHvk4kUhlqA4FaauL+FNVPssa33AsEL7hWHokprNEQ0FaY2Ffcfgsa/zquAuEuaz6m970Jtrb2/nxj3/M1NQUb3/72/niF7/IxMQEN954I52dnWQyGf7qr/6K3t5euru7ufLKK2ltbeWBBx5Y6K+ybEjqU1XNNSF+Nz640N3x8VkwfMMB8JvPQs9z5d3nyl1w9VdsPzaXVb/nnnu47bbbeOKJJ1BKcd111/HQQw/R39/P6tWr+fWvfw1oNawaGhr4+7//ex544AFaW1vL22efgkym0tSEgrTWRRhJTDOVzhCpCi50t3x85h1/qmoRcM8993DPPfdwwQUXcOGFF3LkyBGOHz/Orl27uO+++/jMZz7Dww8/TENDw0J3dVmTSM04xwEGxv0quT7LE19xQEFlMB8opbj55pv5oz/6ozmfPfXUU9x5553cfPPNXHXVVXzuc5+z2IPPfJDUfRw5wzE2xZpGf8VFn+WHrzgWCHNZ9Te/+c3ceuutjI+PA9DV1UVfXx/d3d3U1NTw3ve+l0996lM8/fTTc9r6zA/pTJZUJku1PlUFfva4z/LFVxwLhLms+tVXX8273/1uXv3qVwNQV1fH97//fU6cOMGnP/1pAoEAoVCIW265BYCbbrqJq6++mlWrVvnO8XnCKKleM2uqyjccPssT33AsIPll1T/xiU/M+v/mzZt585vfPKfdxz72MT72sY9VtG8+szEMRzQcpKUuDPiKw2f54k9V+fi4IKmvxVEdChKpCtJQHfIVh8+yxTccPj4uMBRHdUgLv22tC/ul1X2WLcvacGjLe5zbLIfvOB9MprRFnGrCmuHw61X5LGeWrY8jGo0yODhIS0sLIlYr1S5tpqbTVAUCDA8PEY16XOJUKchmILhAl0cmXbljZ7UpJwLexkw5H0dQM8RtsSiHukbK2jUyaRjvtf9cBGKrtH/nG6e+LWYW8rw5kc0A4vl6dEUiDtMJqFtR9v0vW8PR0dFBZ2cn/f39C92VspNKZwmOd5OqitHUuoKOjg5vOzh2N/z0w/BnhyFaX5lO2jHaDV8/Hz54J3RcXP793/ZBCNXA22/x1Cw5naGNOJf8cBf8wS9orasvv+K4/T/D8z8vvM0bPgev/WR5j+uGn/8xPPeT+T9uubj80/D6/7bQvZjLrW+GTVdUpm9P/Qvc9wX4i7MQrinrrpet4QiFQmzcuHGhu1F2Uuks7/7GXdw2+i5uj7yNC2/+rved9B+BqVEYOzv/hmPoJchMQf/RyhiO/qMQ8p60l0hlWSt9BDJTMHCUtthljE+lc9nkZWHopFaq5lUftv78vs/D0IvlOZZXBk9C+3lwydwk1UXPA3+j9X8x0n8UYisrs+9EHILhoq53J5at4ThX+eYDJ+jr74EIZCeG6B1NsqLe41RVYnj2v/NJpY+dGIZ00nuz6QwNMpHbh5EEODA+xdrmMo3mEnHYcBlc9H7rzx//lrbNQpAYhjUX2vdtMfP0dxfmWnYik9YGaJX6TZNxiDZWZIquos5xEdknIkdF5ISIfNbi8w+ISL+IHNBfHzJ99n4ROa6/5lytInKHiByqZP+XGoe7R/jmAye4fps2wmiQCX57rIipuKR+IS/EQ8o4drKCN1MR+06k0jRgGI54Lgmwr5zTVQn9Rrcj2rhwhiPp0LfFTLSxctdTKSR1H1mlftNEHKor85tVzHCISBD4JnA1sAN4l4jssNj0R0qp8/XXP+ttm4HPA5cAe4HPi0iTad/vAMYr1felyHQmy6d/cpDGmjA37W0BoLUqwW+PFmE4EhV+eLs5diVupumkpjaSIzNOcrfdMiuOZJy2ujJnj2fSkBorfKNXL9ADMJvVzlmFHkIVp3oBDW4h5mOQVCFjX0nFsRc4oZR6USmVAn4IXO+y7ZuBe5VSQ0qpYeBeYB+AiNQBfw78dQX6vGS55cGTPH92lL9+205iSrOpq8JJHj7eTzrj7SF5zioOY58qqz2kPZBIZS0VR9kc5MboczEqjtSYds58xVFeKjlIMva71BQHsAY4Y/p/p/5ePjeIyEERuU1E1rpo+2Xg74CCS+CJyE0isl9E9p+LkVNmXjg7yj/8+3Gu3bOafTtX5m6SxsAko8k0z5zxdmGOxQcANF/JfFPJm8m8T4/7T0xnaArMKI7m2jAiZVQcxoNtMSqOhIu+LWaqG4tSmRUnqftdUmOa4iz7/pem4rDyyORno/0S2KCU2g3cBxghQJZtReR8YItS6mdOB1dKfVspdbFS6uK2tjYv/V5STGeyfPq2Z6mPhvjidedpb+o3ejQ9RjAgPHi0z9M+J0e01e3uf+YYLw/O8xK186E4ith/IpWmOaifi0ScUDBAU00Zl5A1Hs5OiiM1Dpnp8hzTLUkXfVvMRBuLUpkVxzx4SZY5J8jY/xJUHJ3AWtP/O4Bu8wZKqUGllHHnfQe4yKHtq4GLROQU8B/ANhF5sOw9X0L8029PcqhLm6JqrtWK7xk3ukxP8Kq1dZ4c5D0jSSLpUQDqsuP8wa2Pz29NpsWsOEQ3HPr5basrY/a4Mfp0UhxQmYdMIc4FxQGLz89RwkDGEcMvtQQVx5PAVhHZKCJh4J3AHeYNRGSV6b/XAS/of98NXCUiTbpT/CrgbqXULUqp1UqpDcBlwDGl1BUV/A6LmqM9Y3z9/uNcs2sVV+8ynUrTDXLVpiiHukbpG3MXgvrzZ85Qr88CvnZtiJ7RJH/4f55kfKoCUtqKxao4prOmcFytbWssXD6j6lZxmLedL84FxQGLz89RwkDGkalRQC09xaGUSgMfRTMCLwA/VkodFpEvich1+mYfF5HDIvIs8HHgA3rbITRfxpP660v6ez46aX2KKhYN8cXrz5v9oekGuaxDS9V56NiA4z6VUtz11DECos0oNsoE33z3hRzuHuUj33+KVHoe5ojNiqPcdbZKURypDPWGc1yfL2+ri5Sv0KFbH4d52/nCVxyVYdZApsx5JhU29hXN41BK3amU2qaU2qyU+hv9vc8ppe7Q/75ZKXWeUmqPUupKpdQRU9tblVJb9Ne/WOz7lFJqZyX7v5j5zsMvcbBzhC9df14uGS2H6QbZWp+mLRZx5ec43D3KwIBpu0ScN7xyBV95xy4ePj7Ap297lmy2wkUTjQs+Ow3TZfavlKQ40tTnIsAVTI3SFoswMJYqTyFJX3FUjuWoOCps7Jd1ddylzE/2n+E1m1u4xjxFZZCMQ7WW9iLJES7f2sbDxwfIODz0b3uqk9ZgQvtPdVPuRvtPF6/lv+7bzi8OdPPXv36hchV3ldIdenrKTiVupnAMJFiU4qhV4zN9S8ZprYuQmM4wkcqU3rdkHKqiECqQ5b+QikOCEInN73HLxWJWHKbrqez7hqWpOHwqQzar6Iwn2LmmwbqybyIOTRtzf1+xvY2RxDQHCoTlTmey3PFsN69fH9LeaNo460b7yOs288Hf28Ctv3uJf3qoQvWSpic1pWH0vRI3U3VTUWGt06kpompq1nnNLSFbDge5U9Y4mBTHPJfPSMYh2rA4q8u6YTErjqYNM3+Xe9/gKw6fGQYmpkils6xptCleljRdkMk4r93aSkDgtwWmq357tJ+hiRRXbtAjs5o2QDoBae2hKCL81TU7uHbPar7ymyPc/lRn+b6QgXGxV/Jmqm4oKpEulBqd3TddcQDl8XMkXYROLqTiWKr+DdCUUhEqs+Ik41rJ86pq29/0qdPDJKeLULS+4vDJp2tYm06yNBzZLCRHoWm99v9EnMaaMBesa+LBAmG5tz/dSWtdmFc26g5wi4d3ICB89T/tZu/GZr74y8Pl93ck8wxHJRRHtLEoxRGaHpndt3Jnj7tRHFUR7SGzED6OperfAE0pRRsWoeIYmbkeLX7TwfEp/tO3HuF7j54uYt8uwrtLwDccS5CuuG44miwMx9QIoKC2XVt3Qr9ZXretjYOdI5bho/HJFPe/0Md1e9ZQNZX3gMy72SJVQX7/wg5Gk2leGpwo11fSmBfF0Vic4pieqzhyU1XzpThgYbLHl7rigMVZryppuh4tftOueIKsouAUsy2JOARC2jOgAviGYwmSUxxWhsM8t2l6QF6xXcuef/j4XNXxq4NnSWWyvOPCNdoFHKiC+jWz92di99oGAA52VsihtwgVR3V6bHbfEnGaasIEpFyKw2Wy1kLUq1rqigMWX72qbEbLtSigOHpGtNyrg11F9NswShXyS/mGYwnSFU8Qi1ZRHw3N/dA8t2l6QO5c3UBrXZgHLarl3v50J69YGeO81fUzUyYF5tO3tseoCQd59kyZM5iNm6dxLSCLRnGkM1lqsrrhqF+tjeSScYIBoaUusgCKYwEyx33FUV6M37CA4ujVByRnhhIMT6S87d/N1GcJ+IZjCdI1nLB3jNsojkBAuHxrGw8d658Vlvti/zjPvBznHReu0SK0zPLZvD8TwYCwc3VD5RRHdVP556TTU5qzP2dQR1wnGCbTpqzxvBFiaznKjphHn07Mt+JQqqKlK+aNxaY4DB9E7nqaOxjoHZmp9vCc1/Xt3Q5EisQ3HEuQrniCDqtpKrBVHACv297G8OT0rAf+z57pIiDwtvNNU1MOigNgd0cDh7tHmfZasr0QiTggEGko/wgx36CqDEy5K3o3aV7EKW+E2BYrg+Ewjz6dmG8fx9SYdq58xVFeknnXo8Vv2jOaJBbVKj94Nhy+4vDJpxjFAfDarW2IkCt6mM0qfvp0F6/d2ka7sbxsTnE0zN5fHrvXNjKVznKst4wVR5NxbY3zQKD8I8R8g2p+z6lpSlMc6WA1BEN5iiPMwLjHaYR8zKNPJ+ZbcSz1rHGDqDeVWXESedfj1KimPE30jibZ1FbHxtZa7+reVxw+ZkYS04xNpa0d41BQcTTXhtnT0Zjzczz+0hBd8YTmFDcwRirBEITrbB+uezo0w1JWP4d5lFRpxWF+z6npdIYGJpgO68bUQnGUlE3vpk6VQXVj5dZvsGKp16kyqPamMitOvuKAOb6rnpEkK2IRdq1p4LlOX3H4lMBMDodNmF0iDsEwhKot12+4Ynsbz3bGGZpI8dOnO6mLVHHVjpUz7c0jlQKj23XNNTTWhMrr58g/dkUUR5NnxWEsG5sxDIfJqLXVRUhlsowmS3iQu6lTZWDzkKkY55LigMXj58hXHDCnIkDvaJKVDVF2dzTQPZJ0PyU6D0v9+oZjiVEwhwNmQidFLNdveN22NpSCew73cOdzZ7lm1yqqw0Htw/wa/gXm00WEXWsaeNbrSKgQi1RxTKbSNMgE2Yi14oASQ3K9Kg5zm0pzLikOWDx+DkvFMdO3RCrDaDLNivoou9Zo190ht34Oo6S6rzh8DDqHtYqxBX0c5lG78Z7O7o5GmmpC/H93H2UilZk9TWWsLe1CcQDs6WjkWO8YiXIU+QNrxVGuOelSfBzTWkl1NcugzpRWhxINRzGKY74egL7iqAyJOAQj2syAhVHrGdUiqlbWRzlvTQMicNDtIM3LQKRIfMOxxOgaThCpCtBaF7bewJysZfGADAaEy7e1MTiRoqOpmldtaJ5pm/8Ac4jg2d3RQCareP5smVRHvuLIpGA6Ub59g+b09+rj0J3js4yavhRpazmyxz0pDqOa6jwVOswpjqb5OV6lqFTF5WLJHyQZ7+kYyX8rG6LURarY1FrLc24TAb0MRIrENxxLjK54gjVN1dZVccFRccBMFvk7LuwgEDDtJ/8B5qQ41mrblcVBrpTjzVQSybhWUj1YNVP0zouPgwnE6JtphFg2xVEV1UafTsz3lEsyvrRLqhssVIFIO/IHScZ7Or264lhRr11fuzsafcWxXDg1UOZaTuiGw26aChwVB8BVO1byB69ez/suXT+7rUfFsaI+yor6SHkc5NMJTWEUuJlKwmxQjaJ3LvednEpSK1MEavRRq8moNVSHCAWldMXhdnQ431MuifiiL6n+5V89z5//6EDhjRZqESw7HAZJM4ZDC5PftaaBvrGp3PsFWeqKQ0T2ichRETkhIp+1+PwDItIvIgf014dMn71fRI7rr/fr79WIyK9F5Ii+5OxXKtn/UjjYGeeKrz7ILw50lXW/XcMFkv/ARnHMntaojVTxpet35hy7OawUx/QkpO3zFDyNhAphdWzz++XYv/lG8pBIl53Uzl+otml2HxNxAgGhpbbEJEAvJT0WQnEscsf4/lNDPHnaYWVpjyqz4pgVRyiqKc48H0dtOEhMLyu0u8OoD+fiXlvKikNEgsA3gauBHcC7RGSHxaY/Ukqdr7/+WW/bDHweuATYC3xeRIxJ1q8qpV4BXAD8nohcXanvUApGtNHX7zvuuPKeWxKpDIMTKXvFYRUVBe5vFivF4dB+T0cDLw5MMJKYtt2mpGNXQnEYx3G5bzWpbVdVO1dxgBZZNW+KwyitPq+KY3Ebjv6xKedcGo8qs+LkG+S88PPe0SQrGmZWg9yxup6AwHNu1P0SVxx7gRNKqReVUingh8D1Ltu+GbhXKTWklBoG7gX2KaUmlVIPAOj7fBroqEDfS+Zoj1aG+8WBCX75bHdZ9ukYimuE4RkXpNf1G+xG/QXa7+7QtnEdKuj12GVVHA0z//egOETfLljTPLuPpuzxkhZz8lpEcD7LZyxyxaGUon98iuR0lrEph1yahShJb0d+NeS837RnJMnK+hnDUROuYmt7zF3pEaPCdbi2nD2eRSUNxxrgjOn/nfp7+dwgIgdF5DYRWeu2rYg0AtcC95evy+XjaM8YF61vYvuKGN/49/KojpzhsEv+swqd9HKzGGtLh+tm2pr3a4EhoZ8t1c+xiBVHwFijxMaolVyvymvZ8vks2LfIFUd8cprpjHZvOf4GC1GS3opsRls3p6DimJplOAB2dTTwXNeIc5UC4zeroF+qkobDqtf53/iXwAal1G7gPuC7btqKSBXwA+AbSinLBbBF5CYR2S8i+/v77Ve+qwRKKY72jPGKlTE+8catvNg/wa8Olq46Cq7DAdbJWl5ulvwa/i4UR2NNmPUtNRwsNbIqX3FEGgBZFD6OwFSeUQvXaiM6U4XcwfFU8SsiJjxm+RahOMaS03zrtyd59OQgaS+FKZNxxgN1fO+x0zx6ctDTMeeDPpOxcDQci0VxGAm5Noojm1X0jSVn6sfp7O5oYGA8xdkRBwf5PKjEShqOTmCt6f8dwKynp1JqUCll/NrfAS5y2fbbwHGl1NfsDq6U+rZS6mKl1MVtbW1FfoXi6BlNMppM84qVMfadt1JTHfeXrjq64pMEA8KKfKe2ga3icPlQzx9duvSRaA7yMiuOQEAreFiOEWI6pTn5rQyqiwTDKmO98VlRWY2zFEc6q4gX4+cxRp8VVhx3HerhK785wru+8xgX/fV9fOKHz284ECcAACAASURBVHDHs92WvqlsVvHMy8N89a4jZCaH+e4zcf7q54f44i8PezrmfNDvxXAsFsVh5bw2/aZDkymmM4qV9bPvcyOD3NFBPg8qsaqC+34S2CoiG4Eu4J3Au80biMgqpdRZ/b/XAS/of98N/K3JIX4VcLPe5q+BBuBDLFKO9GiF1LavrCcQED72hi189N+e4VcHu7n+fKvZOnd0DSdYWR+lKmhj7+0Ux0inuwNYOezM+7VhT0cDv3y2WxslxaIFty14bJjthyjXlIydQVUZrZaXQ45CyDAcNiNE8xKyzbU2iZm2ffNQUt187F5vD3FjmvMf330BDxzp54GjffziQDdVAeFVG5p5444VrGmM8sCRfu4/0sfA+BQxSfCpSJaLtm/kuqrV3PN8D9msmp37s8D0j8+MvvuWiuKwcl6b1uQwJ/+ZeeWqeqoCwnNdcfbtXIktyTjUtJS1y/lUzHAopdIi8lE0IxAEblVKHRaRLwH7lVJ3AB8XkeuANDAEfEBvOyQiX0YzPgBf0t/rAP4SOAI8rSfB/aMRjbVYOGYYjhXaA+ktO1extf04//DvJ3jr7tUEi7zxjOQ/W+wekL2H3B0gEZ+dIexBcQAcPDPCG3cUaTgScW16KhCcffxyjBCtsp/NRtHBcETSoySJEK0yGQWTUWs1JQFuW+ExUa6Ykh5FGNSu4QTtsQhv3b2at+5eTSarOHBmmPte6OO+53v58q+eB6AuUsXrtrfxxle2c+XKFPwTXLpjMyemm7nj2W56x5KsanCRqDhP9I1qxsLVEr5mlbmQeSl2imNqBLKZOTkcuU1CQbatiLlTHM2by9njOVRScaCUuhO4M++9z5n+vhldSVi0vRW4Ne+9Tqz9H4uKoz1jrKyP0lCjxWAHAsLH37CVj/3gGX793Fmu27O6qP12DSe4dFOBkUQ5fBzNG2f+HwxBqNax/c41Wqjgwc44b9yxwt2xrI5d3TD7vUorjtxna+c0MRNJjzIZjDHrNq5uhEktd6CkQofFFBE0r99gNrQF6B5JsNoUxh0MCBetb+ai9c18Zt8reHlwkp7RJOevbSRcpSvanudyx9vYoEXovDQwsagMR//YFNWhIE01IXc+Dpcqs6LYKQ6A5MhMnaqGuYOw3R0N3HW4B6WUffWIJe7jWLYc6Rlj+8rZF+Zbdq1ia3sd/3D/8aKcqNOZLD2jSWfFEQhByBR15WX9Bqu5URfy3ggVLKlSrt2xy6o4vE/DAVRnxpgM5D1oLBRHUbkcxSoO8FRavTte+NpZ11LD3o3NM0YDZj3g1rdo19SpgUn3/ZwH+samaK+P0FYfpW/MwWm8WLLH7RSH/lnv6BQiM9eVmV0dDcQnp+kctqnhlp/LVSF8w1Fm0pksJ/rH5xiOYED42Bu2crxvnDsPnbVpbU/PSJKsKlAVF2ZCTs0jEbcPGWNt6fyRikvFsmettgZ50QsaWY2S5k1xFKYmM06yKs9wmIxafbSKcFVgfhUHzKkIYEc2q5xL1VhhesCtbqgmXBXg1GD5y+iUQv/YFG11EdrcrP2+WOpVFVIciTi9I0la6yKELHyZu9fo08J2g7T8CtcVwjccZebU4CSpdDbn3zBzza5VbG6r5RtFqA5jhNHRZJPDAdb5AG5vFmNt6SIUB2h+juFCIyEnCimOUkurl6g4atUYU/mGwzBq2Swioj245l1xuHsADk6kSKWzrLaY+iiI6QEXCAjrm2t4qQL110pBC1uNuMveX0yKw1hszcD0m/aMJufkcBhsW1lHOBjgoF2l3HnIGgffcJSdo7mIqrmGI6j7Oo71jvObQz2e9uuYNQ7aCNRq1A7ON4tdfZtoo6uR7R7dQV50IqBd3zNTpZdWz63pnZc5Dq4evnVqnFSofvab1Y250uoArcUmARp9q6Di6M5dOwUGHVbk9W1Da21FCneWQk5xxCIMTqQK56h4PG8VIzE8N0HP1Lfe0eQcx7hBpCrIK1bF7JeSnYc6VeAbjrJztGeUgMCW9jrLz9+6e3VRqsNI/ltVaNRoN2oH5/Ub7EYqLv0M21fGtJFQMX4Oo6R6sWrJiWRcy4YPhmbeC8dAAq6+W0xNkA5bOO5hJiTXzVSJFeYFfdziceRsGI7VjR4VRzKunaOwNgja2FrL6aHJ4hMdy0xyWlslr70+SnssglKaurJlsSzmZFVixvSb9owmWdlgk6uFls/xXNeI9e/gK46lydHeMTa01hINWUe7BAPCx16/laO9Y9x12L3q6IpP0haL2O4XsPYTuF3AppDicHGjhasCvHJ1Pc+eKeKmNEqq5y8WVK6pBSuDGghoCsThu6VTU9RJcq7hyDNqbbEwA+MFHlp2FBMB49GgzpSq8ejjMEqqB7THxIaWWlLpLN0jZVpcq0QMQ20oDvN7lsx3ZWE7kvG517ret+mJYeKT07ZTVaBFVo0l05wesghU8BXH0sQoNVKIa/esZlOrpjrcOpNdOTetHpBuR1mFFIdDaXWDPR0NHOoa8Z4hb3exl1NxWN1ILhz/yQlNqeXWGze3hVmKY2hiyvt3LybL16NB7YonqA0HaagOOW9sJk8FbmhdXJFVRsJfW71Lw2GozMWgOPJ/81A1BCMkRrWyLvnlRszsyjnILb6HrziWHpMpbRTglAQWDAgfvnwTR3rGclnmTnQNOyT/GWF4dg/fUhSH+fMC7O5oZCKV4cX+ccdtZ2F3sVdScYArx39qVMvVUNHCiqM1FiGrYKjQVIkVxSgOY/0Glw/A7riWw2Eb929H3pTKxlY9l2ORRFbNUhxuVmI0VOaiUBzW12NyTDMchRTH1hV1RKoC1n6OvPu4pOKbBfANRxk50TeOUjgqDoDLtrQC8PiLzoXjsllFdzxJRyHFYZRUz39Aul2/oZDiMH9egD25Srke/RyLWHGkxvUFghym0YpeQrbYukIeEju74rOT/1yTpzhWxKJEQwFOLxIHeb+et9FuUhyucjk8Xk89TkUFvZJfUt0g2khaV7hWyX8GoWCAHavrOWhVYl2vcJ2tquVvfv08+772EGcrMLXoG44yYq5R5cTa5ho6mqp57EWHlcvQEstSmay7ciM2IxlXisNqbWkPimNTWx214aD3goeLWHFMT2i/T8DBqJnrVXmi2CxfD3WXnJL/bMlTHIGAsKGldtHkcvSPTREQaKnVfH/10Sp3uRwerqcHj/Zx6X+/n9+dGCixtzpWJdVNfVN63+yiqgx2r2ngsNW0cDKOqm7kk7cd5DsPv8Q1u1cVXz+uAL7hKCNHe8aIhgKsa3YX9njJxhYef2nQMUql041zs9DcppuQ2sQwlmtLe1AcwYCwc01D+RSHMT1UUcVR+Lxk9LIigZq89uE6zdCaSqtDMYqjyCxfl4ojkcowVGjVyEJYRLqtb1k8uRx9Y1O01EVytd/aYi5yaTwqjvte6AXgf957rPjkVjNWJdVNfQtMxanWjWAhdunTwi8NzJ4WTk8M05uK8rNnuvjUVdv44nXnFV0brxC+4Sgjx3rH2Noec/1DXbqpmeHJaY73FfYJOK7DAc6Kwylz3G4VOo8hjHvWNvJC9yiptIc1H+yMXiCoFT4sRXEYJdXtFIdDgqGx3niwtnn2ByKzRv0556wXxVFo9OmES8XRVWworlKW18SG1lrODCXKthxyKRg5HAbtsWjZFcdDxwaoDQfZf3qY350ow3okDvdpKDXKivqIoz/Kag3ywfEpnjtxmp5UlK+8Yxcfff1W734tl/iGo4xY1agqhFGw8DEHP4ercEpHxeFiqsru4WrevwO7OxpIZbK5REhXWJVUzx3fOWTW1b7tjKJR9M4O/XuHapvmfmY6r7WRKqpDQQa8KI5Co08noo25MtyF6HZaNdKO1LhlJYGNLbWkMtncfhcSo06VQVss4lxa3YPiODUwwctDk/zZm7axsj7K1+8vg+pwuE+rM2OO01QAm9vqqA4Fc4bjzNAkv/+tRwmmRuhYvZp37l1XWj8dcGU4ROR2EblGRHxDY8PQRIr+sSnLUiN2rG2uYU1jtbPhGE5QH60iFi0QTumoOFw4x8uhOPQM8gNe/BxWJdXNxy9FcRS6Ud0YxUSchApTXW2xfnPeeXU1VWKmlJh7l4qj6OQ/mxpaG1pnquQuNPmKw9USvh7K2Dx0XFs59I2vXMGfXLmZJ08N80ipqyA63Ke1aoJV9c5h09q0cD3PdY3wwtlRbrjlEQbHp9hWn6G1tcgK1R5wawhuQVuE6biIfEVEXlHBPi1JCpUaKcQlm5p5/KWhgiMZbR0OhxFjpRRHVVirtuvy4d3RVE1TTYiDXhIBrUqqG5S6+I6T4jBvY4FMjTBCLdVWiZd559VVvSQzpcTcRxtnSqsXoCueICDOztY52NTQMkJyF9pBns0qBsbnKo7JVIaJqQKVoN2oTJ2Hjg2wrrmGDa213HjxWlbUR/j6fe5zrywp8JsbId/ra11UskbL53iua4Qbv/UoARFu+8hriKZHK578By4Nh1LqPqXUe4ALgVPAvSLyiIh8UEQ8ZhWdmxzt0VaJcxOKa+bSTS0MTaQK+jm6hl0k/yXj2jrYYZuRsVNpdTvFAZ7kvYhw3uoG1/kpuWPbPTwXWHEEp0YYUbVEwxa3Sp5Ra60Le3OOl6o4wNF31RXXVo20qrRaEBvF0R6LUBMOLrjiGJ5Mkc6qPB9H+bLHU+ksj54c4LVbtbD5aCjIR163mSdODfGoixB6Wwr85kbp/o6ou1yg3R0NpNJZVjREuf1PXsO29rp5KakOHnwcItKCtkLfh4BngK+jGZJ7K9KzBUYpxaMnB13lWYBWaqSxJpRzkrrl0o2F/RxKKTqHJ+lwCqc0Hr5WzjCn0upGSXW7C86jQ3FLex0n+8fd1zQqFJK6wIqjKuVNcXgyHKUqDnCMCusuJYfDfBwdEWF9y8IXOzR8GeYM65lcjgK/gcup16dfHmYileHybW259965dx3tsQhfu+94kb2m4G8+lNVmFVZF3OWN7Nu5kv92zSv5yR+9WhtYGhWuF4viEJGfAg8DNcC1SqnrlFI/Ukp9DLCu5rfEUQr+8mfP8T/uOuJq+yM9Y2xfEfMcxbC2uZrVDVEet8nnGElMM5HKOBsOp4evsY0VThecxxDGLe11TKYynB11mTi1iBVHKDXKiLIxHEa0mj510VoXYXhymulCFVrNlEVxFD433fFkcYajwDohG1trODW4sGVHclnjsdlTVebPLHGpOB461k9VQHjN5pkVN6OhIB+5YjNPvDTEo8X6OqxKquv0p7X32kPuAg+ioSAfeu0mmox17osp0V8kbhXHPyqldiil/rtSatYqREqpi+0aicg+ETkqIidE5LMWn39ARPpF5ID++pDps/eLyHH99X7T+xeJyHP6Pr8hFYo3CwSE91y6nqdfjnPIKkPThFKKYy5qVFkhIly6qYXHXhy0nDs11rcoqk6VgVMindMFV4TiAC2T3hVORq+U0uolKo5wepRxqaPKaqrHmC+f0qblPC8hWxbFYd/3bFZxdsShVI0dBa6JDS21nBmaLFzCvMLkFIfZcORyaQoMWFwqjoePD3DhuqY5ASnv0lXH1+8/VkSvKTgz0JvS1FNLsIQ1bWDxKA7glSKS642INInInxRqICJB4JvA1cAO4F0issNi0x8ppc7XX/+st20GPg9cAuwFPi8iRjzkLcBNwFb9tc/ld/DM71/YQTQU4F8fP11wu87hBBOpDNuKMBygOcgHJ1KWD1pX63CAdcVNA6fS6rkLzq59k2fFAR4MRyGj57a6b6F955dUN4jUO5ZWj6bHGA/YiGqjb/q52bVGc24+dKzfXd8KjD4dcaE4+senmM6oEhSHaOcojw0ttaT1VQUXCivF0VQTpioghSPbXCiOwfEpDnWP5PwbZqKhIH/8us089uKQYzSkJQUGSZ1JzXA0UOQ04CJUHB9WSuXOtFJqGPiwQ5u9wAml1ItKqRTwQ+B6l8d7M3CvUmpIP9a9wD4RWQXUK6UeVdrw/P8Cb3O5T8801IS4fs8afv5MNyOJadvtjIiqYhQHmPI5Xpo7XdXlRXEUmmoytrHCacrE43RRS22YxpqQO8MxndAUhVPfi/Vz2EWLgXNp9cw0kewkiaDN75r3ENq1poHNbbXc/nSnu74V8ks54UJxzOT/FFFyIhmfVVLdzGIIye0bS1IXqaImPJNhHQgIrXUR+kZL83H8x4kBlGKWf8PMuy9ZR1tMi7DyTIFB0ssJzQhWpYpY08bYNywqxREwTwnpaiLs0GYNcMb0/079vXxuEJGDInKbiKx1aLtG/9tpn4jITSKyX0T29/e7HAFa8L5XrycxneH2p+wfBkd7NcPhVBXXjnXNNaxqiFqOYLriCaKhAM21Dqe70APSaXTqNGVS3QjTE5CxN55mRIQtbXWcdGM43BzbvJ1XChlU47i2BlW7gZNBm9pjeQ8hEeGGizp48tQwp92EqxZbpwpcKY6ZQYfH5D8oeN5myqsvnOHoH5uyDERxzKWJ1ANS8Hr67bF+mmpC7FxjHSJuqI5HX3QfPJOjwG/eNa5IESptkASLSnHcDfxYRN4gIq8HfgDc5dDGahiVP4n/S2CDUmo3cB/wXYe2bvapvanUt5VSFyulLm5rsx45uGHnmgYuWNfI9x87bRu/fbRnjDWN1YUT9Apg+Dket/BzGKG4BV05diXVDcqhOAq1t2BLex0n3JRXd3vsSigO47gOBjVZZWM4LIza285fgwj87Jku574VWxkXcus3FPpNik7+g4Lnra0uQm04uKAO8r685D8Dx8g2B5WplOLh4wNctrWtYOmg91yyjta6CF+/36PqKPCb94wkmQzGShskwaJSHJ8B/h34CPBfgPuB/+rQphNYa/p/B9Bt3kApNaiUMn7l7wAXObTt1P+23WcleN+l63lxYMI2a/Sox1IjVlyysZmB8RQn+2eP4lwl/6XGtPWv7R5CTus3uB31e/RzDE2knNenWNSKQ3t/OmTz21oYtdWN1bxmcws/fbrLOVGsFMUBjqHK3fEEMaeKA3YUOG8iwobW2rJNVSmleOHsqKc2A2NTtNXPNRztbsqOFAj2ONIzRv/YlKV/w4ymOjbxyMlBnrCYYralwG/eO5pkqqq+tEGSBDWfXoVxmwCYVUrdopT6faXUDUqpf1JKFU5ZhSeBrSKyUUTCwDuBO8wb6D4Lg+uAF/S/7wau0p3wTcBVwN16RNeYiFyqT539AfALN9+hFN6yaxVNNSG+9+hcJ3kqneVk/3jJhsOubpXrlf+gtAekVUl1c1vzcVyw2a2DfAkojumwe8UBcMOFHbw8NMn+0y7WeS9lWsHB9+Tq2rHD4bxtaC1fefUHj/Zz9dcf5kiPe+NRSHEMjjusxFggvNwIbLh8q/MsxXsuWa+rDpcRVtksJEctz+tUOsPgRIpMuL70QVKFChuacZvHsVX3QTwvIi8ar0JtlFJp4KNoRuAF4MdKqcMi8iURuU7f7OMiclhEngU+jpZgiFJqCPgymvF5EviS/h5oquefgRPASeA3Hr5vUURDQW581VrufaF3zqIoLw1MkM6qoh3jButbalhZP9vPMZlKMzSRcpfDAaU9IK1Kqpvbmo/jgi1tLg2Hk+IwCh8uoOLI5K83bmCUVs87L28+byU14SA/dXKSV1hxdMWTxRsOh/O2saWWzuGE+5yVAhjXyPPd7gzHZCrN+FR6VrkRgzY3KzEWUBwPHe9n+4pYwYWUcrsJB3nX3rX87sQgiZTTOBqtEjLK8rzmptdKSXh1GiSVEbdTVf+CFgabBq5Ei2b6nlMjpdSdSqltSqnNSqm/0d/7nFLqDv3vm5VS5yml9iilrlRKHTG1vVUptUV//Yvp/f1KqZ36Pj+qylIk35n37F1PVil+8MSZWe8bo6RiHeMGIjKnblW3m6q4UB7F4dTWfBwXrGmspjoU9KA4bEKBjdLqxdxMmWnNqe/GoFpdRnpWdsauvVFaPe+81Eaq2LdzJb969izJaZsHSoHRp2scFEfRWeNKuVIcmazizFDpfg4j+stpeQED85Kx+bhaidFGcSRSGZ58aZjLtxWepjKzVp9GdlWjrMAgqVdPlg3UNFVukFRG3BqOaqXU/YAopU4rpb4AvL5y3Vp8rGup4YptbfzgiZdnjbKO9oxRFRA2t5U+r3jpphb6x6Z4UZ877nSzDgeUSXE4tDUfxwWBgLCprdbZQZ67mWxG9aAVQCzmZnJrULNpSFlMu+jfV0UK9M3mIfT7F3YwNpXm3ud7rdvlRp82BtMNBfJrxqfSjCSmi0v+S01o56RA3zYakVVlmK7KGY5eb4aj3aJwo6FCCi4hW239cH7spUFSmSyvdTFNZdAa06IdXRmOAtOyPSNa+3Bd8zmlOJJ6SfXjIvJREXk70F7Bfi1K/uDVG+gfm+KewzMPg2O9Y2xqqyVcVXrFecPPYZQfcbUOB3hQHDbx4RVQHKDXrHKjOCL11iXVzccv5mZya1DN25pJxEkQJhwpcP5tpj0u3dTC6oao/XRVOSJgqu1/05mIqhKyxgv0bUOLkctRBsWhD5BO9LkrjFlYcURnbWOJjcp86Fg/kaoAezc22zSci7Hy48C4i8KEBRRHj644qmMtmhLNFjEFuAgVx5+i1an6OFrk03uB9xdscQ5y+bY21jZX873HTuXeO9IzVvI0lcGGlhraY5Gcn6NrOEFVQJxLYldacRil1T0+vLe01dEVTxQuc+3GQeyx5MmsfRvt7ShkFJNxRlUt0bB3oxYICG+7YA0PHR+wHv2WI+Y+2qgpF4vS6iUl/7kohdJcGyYWqSpLLke37jd8eWjSfmrPxEyBw7mGw1AABXM5bFTmw8cHuGRTC1GrumQ2tOiGY7BExdE7miRcFSBa3wwoXZF6ZDEpDj3Z70al1LhSqlMp9UE9suqxeejfoiIYEN5zyXoee3GI471jjE+l6RxOlOwYN8ivW9U5nGBlQ9R5KdpE3L6kukGh9RvcOGmLKDZolB45WWi6qtBaHOZjL4DiUIk4cVVLTajA+s8FjNo7Luwgk1XcccAiYrxcigMsqx5XWnEYIbmlTlVNTKWJT05z3up6sspdNnr/2BTBgNBcMzcptiZcRV2kyl2hQ9Nv3h1PcKJvnMsdwnDzaan1MFXl4ONYWR9Fii2xY7PUb6VwNBx62O1FlSomuNS48eK1hKsCfP+x0xzrNRZvsgnXLIJLN7XQNzbFqcFJuuIJ54gqmBlpFPqJ7B4yxgXnZtTv8eG9dYWLyKpFrDgyk3GtpLrVWhzm9jbnZUt7HXvWNnL70xbJgOVSHOZ9mTDUanusMooDyhOSayij1+nlPdw4yPvGkrTWhQnYDKgcczksfvNcGK5NmRHbXYWCxKJV7qaqCvo4NMNRdPi5UeF6sSgOnWeAX4jI+0TkHcarkh1brDTXhnnrrlXc/nQXT+tx+l6Wi3Xikk3a/OpjLw7qWeMuykW4GWnYrd9grC1dAcWxvqWWqoAUNhxu1c6C+DiG7Uuqm9sn5s6XG9xw4RpeODs6N9S0nIrD4nfpjrtUq1a4LPe+saWGruEEqXTxIbmG4bhsaysBgRO9zn4Ou3IjBq1O2eMWv/lDx/tZWR9la7v3IJe2OpdLBifiEAhp07559I4mtam3YhNeSynRXwRuDUczMIgWSXWt/nprpTq12Hnvq9czPpXmlgdPUhMOulMFLtnUWktbLMJ/HB+gdyzpLirGzdym3QPSbWnvIhRHKBhgfUtNeRRHOgnTLtf3MO/baK/ztfuOccuDJ2e2cfJxUFt4ztthKdJrd68mFJS5TvIKK46i1+EAT4ojqzTfRLEYjvFNrXVsaHERhYfm4yikpNpiEQY8KI5MVvEfxwe4fFur5/V0AFrqwu59HBYJekopekZLVByllOgvAreZ4x+0eP1hpTu3WLlgbSM719QzOJFi24qYrWQuBsPPcd8LvSgFHW5ufk+KI++CdDtSKXJBJceaVW4Vh7GtF5JxCNXOKqn+64Nnuetwz8w2RtE7i31LUls21lyBdQ4OI8Sm2jCvf0U7Pz/QPXv9ikS8+JLqLo7dFU+4u3asSMaxK6luxqiSW4qDvCuuTam1xSJsbq9zFZLbb5M1btDuUXE82xlnNJn2PE1l0FoXcR9VZfFgH02kSU5ntaTDc0lxiMi/iMit+a9Kd26xIiK879L1QHmnqQwu2djMlC7/l7LiAM1wnB6ctJ7OmE5qSsJt373eTBYGtX98iqEJ00PFKHqXv+9MmuD0uDZV5eTjgILn5h0XdjAwPsXDxwdm3nTjl3LC5tjpTJae0RIVh01JdTMb9ZDcUvwcXcMJVjVqU2pb2+t4aWCiYDZ6JqsYGJ+yjKgyaItFGJtK22dz5w2iHjrWjwj83mZvjnEDzXB4UBx59OpRdyvONcUB/Ar4tf66H6gHXK7Sc25y3Z41nLe6nitfUXzlXTuMfA5wkcMB86c4UuOuS6sbbGmvI5NV1mXGvRzbvL1b8gzqVDpDfHKaofzRoZVR1IMIRpymqlwYtSu3t9NUE5q9Tkc5ImBsjt03ptVqKtpwuCyF0lQbpqE6VFKxw25TPa2tK+pI210rOkMTKbKKgj4Ox+zxPJX50LF+dnc0zizB6pHWughxN0sG2yiOnhHNcKxsiOpVj8PnhuJQSt1uev0rcCOws7JdW9xUh4P8+uOvZd/OVc4be2RzW20usWiVUxy+UtpDbj4UB1iGfhZiS5umyCz9HDkfhEP2dJkUx6BuMCZSmdn5AlbTcPp5cnSOuzBq4aoA1+1ZzT3P984sCFaOmHujtHresV2vGmmHh+KLpUZWdZnKomxt166VQtNVRk5MeyHDYSzhO27jEzOpzNHkNAfOxHmdxzBcMy11msFxrASdjFte60by38r6qKZAiwkGWaSKI5+twLpydsRnBhHhsi0trG2uJlLlkIxkhOE5jTRC1Vpp9VIUB3h+eG9u16YzLA2HhYNYKcWDR/tm+wPKpDjMI9BZ0asNEQAAIABJREFUN7mV4tC/pxaOW5riAG26KpXOcudzZ2e2L8fo0CJUubuU5D/wVHxxY0sNp4rMHp/OZOkdTeZ8MZvaClwrOlZLxuZjOM7dZI8feDlOVsElJpXvlVY39bHA9jfv1RVH7jsVE37uVOG6zLj1cYyJyKjxQluA6TOV7dry5gvXncf3//Mlzht6ic6xGskk4tq622GHC66IelWgJWStaay2dpBbRD09cnKQD/zLk/zCnDRXbFJU3o1qnoeeZTgsFYcWtjzilADo0qjt7tCXlTVWkixXlq/Fb9pVSvIfeFIc61tq6R5JuMr4zqdnJElWzSijmnAVHU3VBXM5clnjDlFV5m0t0X/zp18eRgT2rC3+t2hzU6/KWGzNptxIU01oZkq0WMVRqMJ1mXE7VRVTStWbXtuUUrdXunPLmcaaMOtbCmSCG3jJB7AaySTdOUKLVRygrc3hVnEYI/JZ65IYBRDLqDgGPSiOaCHneCSmjfQczouI8JZdq3jq5WHNaVthxdFUEyocDVYIL4qjtRZVZEiulYHb2l5X0HC4URzNtWEC4k5xPP1ynO0rYtRFijxXQEuti3pVU6PYlVTvHU3OLitUrOKYJ/8GuFccbxeRBtP/G0XkbZXrlo9ryqE43LQtUnGAVrPqZP842fzFdfJ8HJms4m69gOQTp0yrqgWCmkPTy82Umdac+dXWhmNWZJWhOMxJfG59HCIUWorUzI5V9SgFx3tH3Pml3GClOIaLLKcO7isJ6JQSkjuzJrrJcKyIcbJ/3HYhpv6xKWLRqoIBC8GA0FLnEJIbbUQl4hx4eZgL1pVQoRgt4RAc6lUVuE97R6dmr/9RtOJYZIYD+LxSKucVVUrFgc9Xpks+niiH4nDT1i7z3AVb2utITmdzI8xZx4aconjm5WEGxqfYs7aR04OTuWiT3PG93EyGE9+sOManqNJzboYmTNFh1Y2QnYZp06hZP0+jOBgOo70Lo/aKVVpexMnOs9iNPj1jqThKCMWdntTOhWsfR/EhuVb1tLa01ZFKZ+kctlYwTlnjBm5yOTKTw4wm01y4rrTfoTYcJBoKFJ6qKnCf5pL/TH07JxSHzXbFazuf8mE8yOdLcRSZBAgWTs/E8KyS6ncd6iEcDPCZfduBPNVR3eDNaBnb5imOdS01BAMyV3HA7O+WjDMtEVQwSlXQxTSei76ta64hGgpwpqt79nFLwcI/013KkrFeriegoSZEU02oqPLqXfEErXWRWephi17fzC6yqm8sWTCiyqDNRb2qQDIOqJIVh4g4JwHanNfpTFbPS8lXHCPeSqsvUsWxX0T+XkQ2i8gmEfmfwFOV7JiPS7zEb1ut3+B2pFIVgarqoqaqttoajpmLXSnFXYd7uGxrK3s3NFMXqeKJl8x+Do+jMIvwxIHxKdpjEZpqwnOjqmD2d0vESVbFiIZc3CIukyODAWH7ihi9fT2zj1sK1Y2zSquPJKYZm0qXtmSsx75taK0tbqoqnpgT+WUMMuz8HJricI4Wa3OaqqpuJKDSrIxm2NTqwpfoQItTEqDNfdo/NoVSzFUcXkurL1LF8TEgBfwI+DGQAP6LUyMR2SciR0XkhIh8tsB2vy8iSkQu1v8f1rPVnxORZ0XkCtO279LfPygid4lI8QHY5wKJuOacDbsozma1foOXkUqRVWqbasO01IbnGg5TSfXD3aN0DifYd95KqoIBLlrfxBMvmRWH16mquTeq8dBpqQ3ncjoAW8UxGagrHIprbu/yvGxfGWNooG/2cUshF9WlPWRKKqcORdXQ2thSXC5HVzwxJ9ekPhpiZX2U4zaLOvU5lBsxaItpD/I5fjUD/fu9enWwLCWD2urCDorD+rzmcjgaTN/JayCKR79UOXAbVTWhlPqsUupi/fUXSqmCV4q+jsc3gauBHcC7RGSHxXYxtAWiHje9/WH9uLuANwF/JyIBEakCvg5cqZTaDRwEPurmO5yz2BROsyQ/ic9YW9rtSKXYKrXokVX5Ibmmi/2uQz0EBN64YwUAezc2c6x3fEYZlEFxGDWOmmvdKY7xQJ2zf8No7/K8bF9Zj5Qzyzev793lSP4z79cFG1prOTuStC/xYYFSynZKzW7lyImpNJOpTMFyIwbtsQjprCKesK50kKjSws8vXlH6yp3gouyIzW/eN2oqN2LgNRDFbYXrMuI2qupeEWk0/b9JRO52aLYXOKGUelEplQJ+CFxvsd2Xgf8XMKd57kArbYJSqg+IAxcDor9q9fVB6gGLVXKWEV5GGvk5B8ba0hVWHKAXO+wbR+VHLukX+12He7hkYwvNetmHS/Xy8jnVUaLimJhKM5HK0BaL0FwXnpvHAXMUxzh1VLsJabWKyrLhlStjNMjE7OOWQl7fZ0JcS0j+M+/XBUZk1ekh96pjcCJFcjprqYy26CG5Ku98Gj4Ld4ojqrexzh4/MaoVvtzZXHxJeDOtdRGtHIqdwrEpqZ4rN5Lv4zDauGGes8bB/VRVqx5JBYBSahjnNcfXAGdM/+/U38shIhcAa5VSv8pr+yxwvYhUichGtOVq1yqlpoGPAM+hGYwdwP+2OriI3CQi+0Vkf39/v+MXXLJ4UQz5Dm6vI98SFMeWtjpGEtOz5bxu9E70jXGib5yrd63MfbRrTSORqsCM4Yg2eiutnnczGaPB1rowzTXhuXkckKc4RhiVOqrd+jgKlFY3s31ljAYmZh+3FPL63hVPEA4GaK11frhaUoTiyEVWefBzzGS3zzUcW1fUMZnK0D0y+7c2fBZuFEeu7IiNn+PQkKbQtzZ4T1y0oqUuTCarGJ60ma6ymRnoGZ0iFBSazKsZelUc81ynCtwbjqyI5EqMiMgGwGl4ZTV3kmsjIgHgfwKftNjuVjRDsx/4GvAIkBaREJrhuABYjTZVdbPVwZVS3zam1trayl+IcNFQiuLwOlIpUXFAnoNcv5mM3I2rdswYjnBVgAvXNfHEqcGZY5v77kQyro3uqmZn9bbFtKmqkcT0TFmTSAMgcxTHqHIoN2LgYYTYUhdhVSRJWqosF/TxTN6xtVDcaPHz9rmS6g7L+ZpY36p9Dy+RVbkcDosptZmaVbP9HIZ6cBOO62Q4nunTHkU1GeeFo9xglB0ZtKtXZXOf9o4maY/l/V7nkOL4S+A/ROR7IvI94LfYPLBNdAJrTf/vYPa0UgytUOKDInIKuBS4Q0QuVkqllVJ/ppQ6Xyl1PdAIHAfOB1BKnVSajv0x8BqX3+HcZKkoDsNwGH4OU0n1uw71cMG6xtlJUGh+jue7RxlNThd3M1lkjbfFIrmidMOT+vx3IADR+pnvls3A1ChxVePexwGuz83a6hRj1JWnPES+4hieLN4xDvp5q3euJGCiPhqiLRYpvGBXHl0FFIdd+Ha/i3IjBu0Fyo4opXj0rK40ihwI5WMYDtsFpGzu056R5Jzr/pxRHEqpu9B8DEfRIqs+iRZZVYgnga0islFEwsA7gTtM+xxRSrUqpTYopTYAjwHXKaX2i0iNiNQCiMibgLRS6nmgC9ghIoaEeBPwgsvvem4y34qjiNLqAKsaotSGgzNOT70PQ9lanusaYd95K+e0uWRjM1kFT50eLu5mssgaNxQHFKhXpQcPDGcdSqqb24Lrh9DKcJLhbM3sQo7FYqk4SjAcRdbQOm91PYe63IePdsUT1IaDNFSH5nzWbBOF1zemJXA2WrTJpzZSRU04aKk4XhqYoDMRQtks4FUMRr0q2yVk7RTHWF7yH2hKNBBa+opDRD6E5qz+pP76HvCFQm2UUmm0iKe70R7uP1ZKHRaRL4nIdQ6HbAeeFpEX0Iopvk/fZzfwReAhETmIpkD+1s13OCcxSqrPp+IAz6XVQUuSmlWzSu/DgX5tymDfzrmG44J1TVQFRPNzRD0WOrRQHAHR6goZhmPQnARodr7ryVqDmWpq3ExVeTRqLcEEcVXDqcHil1zNYazfkIxr1WbHksXncEDRNbR2rWngeN+Y68iqrmEtFNduqdYtFjWrjKxxt9NwbTbZ40+/HEcRIBv2WMamAEa9qkG7kFwLxWFEls0JZBDxFgyyWBUH8AngVcBppdSVaD4GR4+zUupOvSDiZqXU3+jvfU4pdYfFtlcopfbrf59SSm1XSr1SKfVGpdRp03bf0t/frZS6Vik1mL+vZYNRUt3tSCN//YZiFIe5nUe2tJkMh96HR7ozvHJVvWVBx+pwkN0dDZrhKFVxjE/RXBvR6hjpN7m94tD+Hcy4nKryqDjq0VYWPNpThvl1Y/2GRJyekSRKuVz8y44iFcfONQ1kFTx/dtTV9l0O2e1bV9RxvHdsVmRVn8tyIwZ2SYBPvzxMLFpFoKb4qdd8GqpDVAXEPiTXQnH0j0+RnM7S0WTh6/ISfu62wnUZcWs4kkqpJICIRJRSR4DtleuWjyuKGWlU5z8gndeWzlHsuhg6m9vr6BlNMpaczvVhf2/WcprKYO/GFg52xkkE9ZuiaMWRolX3bVhOVc1SHNq//elqohVQHJH0GKPUcaTH3UPW1fGT8dLLqUNJigNwPV1lXsDJii1tdYwm07OmfvrHplyVGzFor49YhuM+83Kc89c2IiUEe+QTCAgtdWFrw2GUVM87r516gMDaZovz4FVxuKlwXUbcHqlTz+P4OXCviPyC5Z4/sRgoZm7TXFcpMeztgitVcehOz5P9E7mbIq5qLaepDC7Z2Mx0RvFMn+4PKEFxGKPVphptjnxO9rj5vAADbhVHOKaN+Fyel0Ayjoo2cKQcigNyo9OSk/+gaMWxqkHLyHdjOCam0sQnpwv2c+sKfeVIU82q/rFkyYpjfCrN0Z5RrT5VCcEeVrTU2tSrMkqq553XM3op+rXlUBzz6N8A987xtyul4kqpLwB/hZY78bZKdszHBcUqDvPI2kvbEhXHrGgZ/aZoaG5n2wr7cikXbWgiIPD46VHtAe3mZjJKqpvrVJmmOaqCARqqQ3MVh5HEZyqp7srHYSxF6ua86KPPSKyl7IrDMByr8qN03GKUrihCcYgI561p4DkXhqNQDofB1ryaVelMlsGJlKs6VQZtsQijyfSsRaYOdmor/l24rrGk8HIrWmMR69LqNvdpZ4GQZM+KYx79G1DE0rFKqd8qpe74/9s79+BG7vuwf74ECD4AHO/4vKfuobuTdNbpeZFTvyInVSLZkWQ3iiu/xm1Te6aNx43jTmzXtePY6aT1RLGbqcaOksp2Ezd27NpTVdVUcWRLmVS1rPPp/byTfNLxjnfkHd8kSBDAt3/sLrgEFyAWDwLEfT8zHBKLXeCLH7H73e/brQY3GknFFofPVRXm2LzFEb61OsDu3m7aI8Lx0RkWZpzQ1JvesK9ogBScVM9D2zctxznKOZm84L0rr6quasfdF48xPl9gcXit1f1jY8uxOLzjy7kIuXefyc19nBpPMbuYKe/1y3hvp9tsrLxMsCC8luoV3r0e3rGJ46Oza04DHC5DcQwkO0h2RvM9qy7MpVEtr4bDw0vb9buPnnjd+R9du6v2Fkd/sX5VRc7T4Yl5+hOx4IFbrWBxGE3KBrM4opE29vbHeWV0llNnzjCjXfza4Z1rHnfDnj6OvT5BrrOnvJOp4ESdTmVIZ3MrWlX0xmOMzwZUj6cmYWESjXSwSIiLcNlKzdlnS5/Tk6smAfJ8jKMGGVXe61XA4R09ZHPKC2sEyMtxqYkIB3xZeMs1HCFcVQG1HMdem+DSgTg93e0rrcwaMJDoYGx2cVWrlGLn6anxVHBg3Nu33NbqG8HiMJqIii0O9448rMXR3gnRzqrMe69n1djYOWYlztU7165QvmFvL4uZHHOSCHVx9k6mMV/VuMeqRod+pZiaJOtWTpdVOe4dH0KpDQ3WUHF0boaFaUYm5qqv4fBerwKuLDNAfnoiRbRN1izkOzCYzCuOMFXjHoXV46rKE6cml+dv+K3MGtCXiJHO5JgptCKLnKenJubZWUx5dm7Gaa1ehjvTLA4jFAuTTkv1jhBpeP75DZX4s6s07/cPJHh9fJ6F6XGka0tJN5XHL+xxTvSxTFdFFsdYQHO8vkSRflWuxZGJOZlmZcU4vONDKLW+/kESHdHaxDnc+Q3Tkxeqz6jKv154dmzuYkt3+5pxjtOTKbb2dBJZox7jwFCC87NpxufSVVkc3rGvXZhnfC7NdZ7iqDLZo5B825FCd1WAxZHNOTUcu3pLWBz+Y4sRtsN1jTDFsZFJTTpB2TCtK/xFfJVk0FQZULx0MEFOIcEsXZv6yjqmL9HBgcEEw6mOmlocE/O+bqYFFke63bl7LttVFdLikK4tXLY1WZvMKlf2WGa6oRaHiHDljh6eOb22q6ocl9qlvmQK7+LfX0ZnXI++eAyRZcVx7HUnNnfdbvfzVel6LSTfdqQwQB5gcZybXmApq8EZVf591/pOhe1wXSNMcWxkKrnT8PafPuN84SqyOMJXjnt4mVW9bfNs2lL+DK4b9vby6mwULevivHJMp7/diEdvvINsTp0+WLDK4ki3O1Zc2cFxz+JYy1/uU2qXbU3y4sj0ap94WFzZe5hbNVEvFFVaHOBWkJ+bKRkg96rG12I5s2qG0ZlFerraQwX+o5E2+uKxfIzjidcnSXRE800U62VxrOpXtTAJbVGILRe5eqm4RV1V5VocDagaB1McG5tKfJve/hMnVz4ul2otjoEE0TZhIJpyCrDK5Ia9vYxlupFMCjIlBubAaotjxmld7e+L1JdvO+IbFOUdm5piMeq4qkLFOHIZ5w6wFL67z8u3JpleyOSnwFWMK3uPzLGtp3EWBziKI5PTopbUUjbH2enygvjbe5yWL57FESa+4dHvq+U49voEV+/qWXaR1dziWNmJOY93nvo8A8vFf1VaHHllX93c9LCY4tjILEyG/8J0FSiO0MdvqepE62yP8Je/9UY2MRfqAnXD3l6mcO/YyjmZ2rudOeksT/7zx1O86vGJvOLwtVZfmGQ+4iqOsi0Odx3LuUN07z4v3+q8x4sjVbqrfBZHxTUc4FNq5bdUL8QLkBeLc5ybXiBXZluUtjbJJ1OMhqwa9xhIOplO8+kML56dcdJwPWpscfS6rrFVKbkBnoFTE/OIlBi4Feb7BOaqMkJQaXAbfIqjguOrPNH+0e6EYzmEeO9tPV1E485UwLJOJn/x3+wi/QUXnd5Ci6Mt4rQTn78Ai9PMtzlukrItjnIvQr67z8vc6uiq4xzuZ+1tmw8VA1jFwqQzh6OtwjoQHNfL5u52nh0OVhyl5nAEsX8gwfFzlVscg8lOzs8s8vTwFNmcLsc3oOYWRzTSxpbugLYjAZ6BU+MphpKddESLrHWY75N//3XCFMdGptLgNlTnqkrPQLaKwrUK20Bv37YNgNz8GgWIBQrVszj8FG2tPvk6AHNtIWMc5V6EfFZiT3c723s6q8+scl9vR+di5QOcoOKqcT8iwuESFeSl5nAEsX/I6W82MpWq3OKYWXRa88NKi6MzYIBXlfQH9asKsDiGJ+aDe1R5eK3VzeIwakql7SFqYXFAVQHy5RhEODfZvp3O5OGRc2fXeP2plQ0OZ1ffrRZtdOiuy6wzDiZcASCUd4foW/PLtiarr+Vo72KJdrZ1VBkrqVFa55U7eni5SID8TMhGjF4geymrFVkcA8kO0tkcj7w0xt7+OFvivhGtnpVZ735VARbH8ESqeEYVLLdWN4vDqCnp2XAt1T28+Q2Tbqf6Si2Wak62Ci2OQ/uc6cU/Hz699uu7cmZzyoUAxdHZHiEei6xudOiuy7QkaI8I7ZEyT5EwFofvc1++bRMnRmdJZ6oY6iTCtMQZiFapOGpUSOYFyIMUYti2KF5mFZQ3+a8Q7//++GvjXHtJwGergevVT2C/qgKFvJTNMTKVKp5R5ZetnO/TOrdUB1McG5dK7zREnDv9bJpQLdU9wo5wDaJCi2PbVqeL7sjIyNqv78o5PpcmV6THUW8ixnjhMKeso0imNUSfKu9YCG1xXL41SSanvHq+/LGrhagqk7lueiNVVkDXyOI4XCJAPjxRXg2Hx67ebmJR5zJVWYzDOUaV5YpxP2GaCZbBqn5V+Zbqy+89MukkCOwsllHll62smNn6tlQHUxwbl2p8m94xlXzh8hZHZY0OgYqVnnjK4MJo6doH38XZ8zcHBY174x0rq8d9azmZi5cfGIfl1uphLQ43s6oad9WFuTSTGqeHypUPUDOLY+eWLnq62gNbj5xZYw5HIZE2YV+/4zas1FXlcd16WByJDmYXfR150zOguRXrempijRoOv2whv0/rRV0Vh4jcLCIvicgJEflUif3uEBEVkSPu45iIfF1EnhGRp0TkRt++MRG5R0ReFpEXReQ36vkZmpZqfJveMZUcW0uLI+wXPhIlHYkTTU/n8+BXkc04J2uJ4j+PvsJ+Vb71GNcyZ3F4eK3VS61LwECffQNx2iPCC1Wk5J6dWmBK48S1CsVRw9YVxQLkqrrm5L8gvNkclQbHwWkd42WxraAOFgcsf++CztPhiRJzOAplC2nBrhd1UxwiEgHuBm4BDgHvFZFDAfslgY8Bj/k2fxhAVQ8DNwF3iYgn62eAUVU96L7uI/X6DE1NTSyOKpROTWIc4esFtLOHHpkrfode0FI9qE+VR288tlzHASvWYzzbHb49+Vp3iAF3n+2RNi4dSPBSFZlVZyZTTBGnM1NFkH0p5bjpanT36gXIFzPLAfLxuTQLS7nQg6befGkf+wbiKwo4yyXZEaUj2sZVO3uIBsWr6mBxgC/NO+A8PTWeItIma9fcXKQWxw3ACVV91Z3d8W3g9oD9vgh8CfBH9g4BDwGo6igwCRxxn/sXwB+5z+VU9Xx9xG9yNrrFEUtCJGAOwRpE4730MMfLo8UUx8oTNahPlUdvPObOeXDdXt56RDqYzkTLb3DosdYdYpH/2eVV9qw6O+1YHO1LVaT11rh1xeEdPSxlVwbIz0w6p3jYflp33nAJP/rEjWU1xCxERHjfGy/hfW/cHbxDua1iymRV25GA//mpiXm29XQGK7JVsq3RWr3VLA5gB3DK93jY3ZZHRK4Fdqnq/QXHPgXcLiJREdkLXA/scsfXAnxRRI6JyHdFZKhO8jc3jbI4vNbq1VocFX7ZI91b6I+mOH6uiFum4EQdm1mkOxYh3rFaSfXGYyxmcsyn3bvizmWFmlrKhotxeMeXWpci/7PLt21iZGqBqfmlcO/ncmZygVlJIOXObwiiwky3YgQFyE9POi6aqmaGVMDv3/oGbrt6e/CTnW5CxFIR12dI+grbjgT8z9dMxfXLpjnHUi1GC1ocQbcHebXuup6+DHwiYL97cRTNUeArwKNABogCO4H/q6rXAf8P+OPANxf5iIgcFZGjY2Nj1XyO5iQ1Gb6lukc1FgdUb95X82Xv7KE/Os/L54pZHCsbHJ6fXSxaTb2qlqNrWaGm0tlwMQ7v+Aosjsu2ehXklVkMI1MptLMHKXd+QxA1tjh29XaxqTO6IkDuxaXWDAqvJ7VwvfpY1SE3yOIYX6P4r1C2Yt+pKkb9Vks9FccwsMv3eCdwxvc4CVwJPCwiJ4FfBO4TkSOqmlHVj6vqNap6O7AZOA5cAOaBH7iv8V3guqA3V9V7VPWIqh4ZGBio5edqDhYmw7dU96jG4oDqA4rVfNm7NtPDHCdGZ8nmAtwLARZHsWycoo0OuzazsJStfYyjiMVxhZdZVUwZrsHI1AJt3WX2NipGjS2O5RbrfosjRTwWqShWUTdq4Xr1v1x7hGRHdDklt+B/vrCUZXRmsfjkvyDZiv1Pqxz1Ww31VByPAwdEZK+IxIA7gfu8J1V1SlX7VXWPqu4BfgLcpqpHRaRbxCndFZGbgIyqPq+OM/p/ATe6L/MrwPN1/AzNS5UX3xW/w1ITi6PCRnqdm+nOzbKYyfH6eEDdQmGMI6DdiMeyxeHeHfosjvlqLI5i/vIiFsfQpg56utorzqwamUoRS/SufI+w1KE99+EdPbx0djlA7qXiVhKrqBs1tjjAKQJcYXH4Wqp7LVdqYnE0qGoc6qg4VDUDfBR4EHgB+BtVfU5EviAit61x+CBwTEReAD4JfND33CeBz4vI0+72IFdX61OVu2djWxzRbIp2MsHuqkKLI6Bq3KMvXjCxrSDGETo4vtYo0iIWh4i4rUfCu5lyOeXs1ALdm/pXvkdYamxxgJNZtZRVXj7rxKNOT5Y3h2NdqbHFAY4luyLG4Wup7s3hKDvG4b1GEA3qUwVOzKBuqOoDwAMF2z5XZN8bfX+fBC4rst9rwNtqJmQpvv+RfNO7puPcc7DzyNr7BVELi+PVh+Hemys7fvZc1UrvO7EvsPXBBDxWcCGafB2iXRDtIJ3JMTm/VFRx9LqBzIl5f2t15z1SS1k6K8mqAvjmrU5bl0ImT60a6ONxxdYk3/vZMLmchmpUeGEuzVJWSXpDsR74PejuDSc3wNSw87uKluqFeAHyZ89McXhnD6cnUly9c/0vciXx/mc//Cw8+qc1eck/mpp1CgDv7YHzxwsyqrw4TxmKwzvu7/4AfvLV1c8vzqzcbx2pq+LY8LS1Q6SJ/LF+tl8DV7+vsmO3XQPXfgD2vLWy4w/fATMj+HIdwrH3rXD5r1d27L63w/6byL16lplM2+r/T9+lcNV7ALgwVzwVFyAeixCLtq1srf5LnyS79+2k/348vKtq79vgwK9CpkjPqL59cOW7A+NSl23dxFw6y+lSc6gDGJlyLkRdWy+DN7zbaQtfCb174Ypbq2qpXsjuvm6SnVGeOT3F7ekME/NL1Y22rQc9l8Dh98DsGo0zQ9AWjTG/sOh8N4cOwYFfyz83PDFPLNpWXjFjchtc/V6YLtKbrbsXLnuncz6vM6Y4SvGuuxstQX2IdcPtVXy2Azc5P42gfz984Hvc/fWfMjK1wP/5UHHjc6051SJCb3eMcX9vobf/OxYWM8CD4RVH7z54/3fDHeNy+TYns+qFkemQisNRUtv6euA3v1HRe9cLEeHK7T08e3qdSyeNAAAT7ElEQVQq3xW3qTKqwKkl+o0/r+lL3vfDl/nPDx3n+AduWdUkc3g8xc7NXeVZlW0RePfXaipbrbBeVcaG5OBQklfH5shki9ctlGo34tFb2HYE8nUdoWMcVXDQbYcRtmfViHtBrmryXx05vLOHF0dmOHm+MTUcjcAbGlb4vQKn+K/p4jwVYIrD2JAcGEqSzuY4eaF4R9hyFEdfIray0SHkG9SFTsetgkRHlEt6u0NXkI9MLRCLtuUzxJqNK3f0kM7m+PFLo0D4qvGNyECx2eO4xX8hLMpmxRSHsSE5OOTMaTheovZh2VVV/KIaZHGkXMURunK8Sg4OJYsXNhZhZGqBbT2dzZXi6sMLkP/t8+eItglDm5rTMqoly0WAK79Xc4sZxufS5WVUNTmmOIwNyX53wM/LxVqP4Nzx9XS1F5/rTBHF4bqqQsc4quTAUIKfn59jqYT7rZCRqRRbm/hivLu3m2RHlLGZRbb2dBKpZrTtBqGvsF+VS9nt1DcApjiMDUl3LMqu3q7izQ5xajhKWRvg5NzPLmZWdHH1YhzrbXEcGEyQySmvlXC/FXJmcqGp3T9tbcIbdjiV8RdDfAOWLdxCV9XwuFf8ZxaHYTSMy4aSa7qq1hr+0+sWAU7MLTcY9GIc625xuPO1T5RQhn5yOeXc9ELTBsY9PHfVxaI4Em4r98LY2an8HI6Nvw6mOIwNy4GhZEnXjqM4Sl9Ue/P9qpbvDhsV47h00CkMLNr5t4Dzs4tkctr0iuNKT3G0wAWzHESE/kTHalfVeIqu9kjTJjKEwRSHsWE5OJRgKaucPD8X+HypPlUeqzrk0rgYR3csyo7NXRwfLU9x5Gs4epr7gnztri2IONMOLxb6E7H8LBiP4QmnK26zJjKEwRSHsWHxXDtBXWXn0xnm0tkyXFWrFcd8g1xV4ATIT5StOByf+dYmtzgu6evmgY+9lVuvKjITowXpT3Ssyqo6Ve4cjg2AKQ5jw7J/MEGbBGdWnZ9xTtpyguPga3QILDQoOA5OgPyVsSIt4wuodKJeI7hi26a1J961EP2JDi74LA5VZXh8viUyqsAUh7GB6WyPsLsvHhggH5t1LqprWRw9Xe1E2mSlq6oBBYAe+wcTLGZyDE+snVl1dnqBjmgbW7qbtJ/aRUx/0ikszbk3ANOpDDOLmZbIqAJTHMYG58BgIrBorpyqcXDSRbd0t6/IgEktZWmPyKo+Q+vBftf9Vk6A/MxkqqmL/y5m+uIdZHPKZMrJ1mulGg4wxWFscA4OJTl5YX5FHQaUrzjAiXNMFATHG2FtwHJh44mxtRWHUzXeGheiVsPrV+XVcgznFYdZHIbRcA4MJcjmlJ8XZFaNzSzSJsvDmkpRWD2eSlcwxKlG9HS1M7SpoyyL4+zUAts2N3dg/GKlsAjwVAsV/4EpDmODU6yr7Nhsmt54R1ktLvriHavqOBqRUeWxfzCxZhFgNqec3QDFfxcrhf2qTk3Mk+yMNte89SowxWFsaPYNxIm0yao79LGZtduNeGyJt68KjjfKVQVOmvGJ0Vm02OxynDvZbE7NVdWk9Bf0qxpuoVRcqLPiEJGbReQlETkhIp8qsd8dIqIicsR9HBORr4vIMyLylIjcGHDMfSLybB3FNzYAHdEIe/q6VwXIS80aL6Q33sFkaimfAruwlG1IKq7H/sEEc+lsvsAviDNNPofjYmezm6237Kpyiv9ahbopDhGJAHcDtwCHgPeKyKGA/ZLAx4DHfJs/DKCqh4GbgLtEpM13zD8ByquSMlqeg0PJVdXW58voU+XRF4+hujx7fL6BMQ5wMsWAkhXkG6Vq/GKlrU3oi8e4MJt2ajgmUi0TGIf6Whw3ACdU9VVVTQPfBm4P2O+LwJcA/+3VIeAhAFUdBSYBzxpJAL8L/GH9RDc2EgeGkrx2YS7fnFBVy2pw6FFYPZ5KNz7GAaVnjXiKY7sFx5uWvkQH52cXuTCXJrWUbYnmhh71VBw7gFO+x8Putjwici2wS1XvLzj2KeB2EYmKyF7gemCX+9wXgbuAkhVSIvIRETkqIkfHxsaq+BhGs3NwKEFO4RU3hXV6IUM6m1uzT5VHYfX4QoNjHH2JDnrjsZKtR0YmU3S2t7VMsLUV6U/EOD+7yKlxtytui2RUQX0VR1A6Sz7a57qevgx8ImC/e3EUzVHgK8CjQEZErgH2q+oP1npzVb1HVY+o6pGBgYFK5Dc2CF5mlRcgD1PDAdDrBtE9V1Wjs6rAy6wq7ara3tMaDfNalQG3X9WpCSce1UquqmgdX3uYZSsBYCdwxvc4CVwJPOx++bcC94nIbap6FPi4t6OIPAocB34JuF5ETrqyD4rIw6p6Yx0/h9Hk7OmLE22TfLPDvOIo0+JYbq2+HONoZHAcnDjH/U+PoKqBymFkKmU1HE1Of7JjhcXRKlXjUF+L43HggIjsFZEYcCdwn/ekqk6par+q7lHVPcBPgNtU9aiIdItIHEBEbgIyqvq8qn5VVbe7+78FeNmUhhGLtrFvYLlnldfOulyLY0u3G+OY9VkcDVYc+wcTTKWWVrXm9hiZWmDrpta5ELUiffEYi5kcL52doS8eI95Rz/v09aVun0RVMyLyUeBBIALcq6rPicgXgKOqel+JwweBB0UkB5wGPlgvOY3W4MBQkmeGp4Dwrqr2SBubOqOMzzm1EelMruGuqvw0wHOzDBYMo8pkc4zOLFpgvMnxajmeODXRUtYG1NdVhao+ADxQsO1zRfa90ff3SeCyNV77JI6ryzA4OJjkgWdGSKWznJ9dpD0ioQLHfYkOLsylGzY2tpADQ8s9q960v3/Fc2Nu8V+zz+G42PH6VZ0aT3HVzs0Nlqa2WOW40RIcHEqgCidGZ/OT/8IEjr1+VY0aG1vIYLKDZGc0sGdVfg6H1XA0Nf7OBa1UNQ6mOIwW4YCbWfXyuRmn3UiZbiqPvOJo0NjYQkSEA4MJjgf0rDrrFf+Zq6qp6fclZ7Saq8oUh9ES7OnrJhZpyyuOcjOqPPqazOKA4im53sjYbRYcb2q8bD1orRoOMMVhtAjRiJNZ9fK5mVB9qjx64zEm5tPMN4nFAU6A/PxsesWsEHBcVd2xCJu6WidLpxVpjyxPZ2ylqnEwxWG0EAeHkrx0dobxuXRFimMpq4xOO26gZlAc+4eChzqdnU6x1Sb/bQj6XMt3I8yFD4MpDqNlODiU4MzUAtmchlYcfW4g87TbdbYZXFX5ZocFAfIzkwsWGN8g9CdiDG3qaGgLm3pgtq7RMngBclgZmCwHrwhweKJ5FMf2ni662iOrAuRnpxZ464H+IkcZzcStV29ndDq4iHMjY4rDaBkO+hRHaIvDHTF72lMcTXCH2NYmqwLkTvGfTf7bKLz/jbsbLUJdMFeV0TJc0ttNR9T5SofNquotdFU1geIAx13lVxznZhbJKWxrMZ+5sbEwxWG0DBH3Dh0qsThWKo7OJnBVgRMgH5laYGZhCYCzUzb5z2g8pjiMluKyoSTdsUjohnKd7RG6Y5H8MKdmsTj2D7iZVa7V4VWN2+Q/o5GY4jBain/99v38yXuuqehYr2CrPSK0R5rj1PAC/t4Y2Xzxn1WNGw3EguNGS7F/MJF3V4WlLx5jeCLVVKmTu7Z0EYu28UpecSwQj0VItlCLbmPj0Ry3VYbRBHgWR7O4qcCtiO+PL1sckwts22yT/4zGYorDMFy2uIqju0kC4x77fc0OR6YtFddoPKY4DMPFy6xqJlcVOD2rhidSpNJZRiZTpjiMhmOKwzBcet0iwGaoGvdzwJ018uLZacZmFy2jymg4pjgMw6WvCWMcsNyz6tFXLqCKjYw1Gk5dFYeI3CwiL4nICRH5VIn97hARFZEj7uOYiHxdRJ4RkadE5EZ3e7eI/G8ReVFEnhOR/1hP+Y2Li94mjXHs7osTaRMeeXkMgK1mcRgNpm6KQ0QiwN3ALcAh4L0icihgvyTwMeAx3+YPA6jqYeAm4C4R8WT9Y1W9HLgWeLOI3FKvz2BcXHhtR5otxhGLtrGnr5tjr00AsN1iHEaDqafFcQNwQlVfVdU08G3g9oD9vgh8CVjwbTsEPASgqqPAJHBEVedV9cfu9jRwDNhZv49gXEw0q6sKnAB5JqcAbDXFYTSYeiqOHcAp3+Nhd1seEbkW2KWq9xcc+xRwu4hERWQvcD2wq+DYzcCtuAqmEBH5iIgcFZGjY2Nj1X0S46IgX8fRZK4qcALkAMmOKMnO9gZLY1zs1LP8NKhCSfNPOq6nLwP/LGC/e4ErgKPAa8CjQMZ3bBT4a+BPVfXVoDdX1XuAewCOHDmiQfsYhp9ER5RNndF8i/VmwquGt1YjRjNQT8UxzEorYSdwxvc4CVwJPOxWwW4F7hOR21T1KPBxb0cReRQ47jv2HuC4qn6lTrIbFyEiwn0ffQv9ITvrrgee4rDAuNEM1FNxPA4ccF1Np4E7gfd5T6rqFJAfYyYiDwP/VlWPikg3IKo6JyI3ARlVfd7d7w+BHuBf1lF24yJlT3+80SIEculAAhELjBvNQd0Uh6pmROSjwINABLhXVZ8TkS8AR1X1vhKHDwIPikgOR+l8EEBEdgKfAV4EjrmWyn9R1b+o1+cwjGagsz3CZ995iOt3b2m0KIaBqLa++//IkSN69OjRRothGIaxoRCRn6nqkcLtVjluGIZhhMIUh2EYhhEKUxyGYRhGKExxGIZhGKEwxWEYhmGEwhSHYRiGEQpTHIZhGEYoTHEYhmEYobgoCgBFZAynWWIl9APnayhOLTHZKsNkqwyTrTI2smy7VXWgcONFoTiqQUSOBlVONgMmW2WYbJVhslVGK8pmrirDMAwjFKY4DMMwjFCY4libexotQAlMtsow2SrDZKuMlpPNYhyGYRhGKMziMAzDMEJhisMwDMMIhSmOIojIzSLykoicEJFPNVqeQkTkpIg8IyJPikhDp1SJyL0iMioiz/q29YrID0XkuPu7IaPrisj2eRE57a7dkyLyjgbItUtEfiwiL4jIcyLyb9ztDV+3ErI1fN1cOTpF5Kci8pQr3x+42/eKyGPu2n1HRGJNItc3ROTnvnW7Zj3lKpAxIiJPiMj97uPK1kxV7afgB2fU7SvAPiAGPAUcarRcBTKeBPobLYcry9uA64Bnfdu+BHzK/ftTwH9qItk+jzPfvpFrtg24zv07CbwMHGqGdSshW8PXzZVJgIT7dzvwGPCLwN8Ad7rbvwb8qyaR6xvAHY1eN1eu3wX+O3C/+7iiNTOLI5gbgBOq+qqqpoFvA7c3WKamRVX/Hhgv2Hw78E33728C71pXoVyKyNZwVHVEVY+5f88ALwA7aIJ1KyFbU6AOs+7DdvdHgV8GvuduX/e1KyFXUyAiO4F3An/hPhYqXDNTHMHsAE75Hg/TRCeOiwJ/KyI/E5GPNFqYAIZUdQScCxEw2GB5CvmoiDzturIa4kbzEJE9wLU4d6hNtW4FskGTrJvrcnkSGAV+iOMhmFTVjLtLQ87ZQrlU1Vu3/+Cu25dFpGO95XL5CvB7QM593EeFa2aKIxgJ2NY0dw4ub1bV64BbgN8Wkbc1WqANxFeBS4FrgBHgrkYJIiIJ4H8Av6Oq042SI4gA2Zpm3VQ1q6rXADtxPARXBO22vlKtlktErgQ+DVwO/ALQC3xyveUSkV8HRlX1Z/7NAbuWtWamOIIZBnb5Hu8EzjRIlkBU9Yz7exT4Ac7J00ycE5FtAO7v0QbLk0dVz7kneA74cxq0diLSjnNh/paqft/d3BTrFiRbs6ybH1WdBB7GiSVsFpGo+1RDz1mfXDe7rj9V1UXg6zRm3d4M3CYiJ3Fc77+MY4FUtGamOIJ5HDjgZhzEgDuB+xosUx4RiYtI0vsb+FXg2dJHrTv3AR9y//4Q8D8bKMsKvAuzy7tpwNq5/uX/Crygqn/ie6rh61ZMtmZYN1eOARHZ7P7dBfxjnDjMj4E73N3Wfe2KyPWi70ZAcGII675uqvppVd2pqntwrmc/UtX3U+maNTrK36w/wDtwskleAT7TaHkKZNuHk+n1FPBco+UD/hrHdbGEY639Fo7/9CHguPu7t4lk+0vgGeBpnAv1tgbI9RYct8DTwJPuzzuaYd1KyNbwdXPluwp4wpXjWeBz7vZ9wE+BE8B3gY4mketH7ro9C/wVbuZVo36AG1nOqqpozazliGEYhhEKc1UZhmEYoTDFYRiGYYTCFIdhGIYRClMchmEYRihMcRiGYRihMMVhGE2MiNzodTI1jGbBFIdhGIYRClMchlEDROQD7iyGJ0Xkz9xmd7MicpeIHBORh0RkwN33GhH5idv07gdes0AR2S8if+fOczgmIpe6L58Qke+JyIsi8i23AtkwGoYpDsOoEhG5AvinOI0nrwGywPuBOHBMnWaUjwC/7x7y34BPqupVOBXF3vZvAXer6tXAm3Aq3sHpTvs7ODMx9uH0HTKMhhFdexfDMNbgV4DrgcddY6ALpzlhDviOu89fAd8XkR5gs6o+4m7/JvBdt/fYDlX9AYCqLgC4r/dTVR12Hz8J7AH+of4fyzCCMcVhGNUjwDdV9dMrNop8tmC/Uv19SrmfFn1/Z7Hz1mgw5qoyjOp5CLhDRAYhPzd8N8755XUefR/wD6o6BUyIyFvd7R8EHlFn3sWwiLzLfY0OEele109hGGVidy6GUSWq+ryI/HuciYxtOJ14fxuYA94gIj8DpnDiIOC0r/6aqxheBf65u/2DwJ+JyBfc1/jNdfwYhlE21h3XMOqEiMyqaqLRchhGrTFXlWEYhhEKszgMwzCMUJjFYRiGYYTCFIdhGIYRClMchmEYRihMcRiGYRihMMVhGIZhhOL/A29Obxruqa6VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "history=model.fit(XTrainingC,YTraining,\n",
    "          validation_data=(XValC,Yval)\n",
    "          ,batch_size=100,\n",
    "            shuffle=True,\n",
    "            class_weight='balanced',\n",
    "            callbacks=[\n",
    "                        #monitor,\n",
    "                        checkpoint,\n",
    "                        #tensorboard \n",
    "            ],\n",
    "          epochs= 40)\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Ende des Versuchs: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMT, dass aussieht wie LAPPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "120k Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "XL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Beamlike_PI_Pure_LAPPD(9x24)_23k_Files.pickle\",\"rb\"))\n",
    "YL=pickle.load(open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Beamlike_PI_Pure_LAPPD(9x24)_23k_Files.pickle\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [1 0]\n",
      "Eintrag \n",
      " [0 1]\n",
      "Eintrag \n",
      " [1 0]\n",
      "(85000, 3, 8, 2) (20000, 3, 8, 2) (15005, 3, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = list(zip(XL, YL))\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:20]:\n",
    "    print(\"Eintrag \\n\", sample[1])\n",
    "\n",
    "X1 =[]\n",
    "Y1 =[]\n",
    "\n",
    "for x in training_data[:85000]:\n",
    "    \n",
    "    X1.append(x[0])\n",
    "    Y1.append(x[1])\n",
    "    \n",
    "    \n",
    "XTraining = np.array(X1)\n",
    "YTraining = np.array(Y1)\n",
    "\n",
    "X2 =[]\n",
    "Y2 =[]\n",
    "\n",
    "for x in training_data[85000:105000]:\n",
    "    \n",
    "    X2.append(x[0])\n",
    "    Y2.append(x[1])\n",
    "    \n",
    "    \n",
    "XVal = np.array(X2)\n",
    "Yval = np.array(Y2)\n",
    "\n",
    "X3 =[]\n",
    "Y3 =[]\n",
    "\n",
    "for x in training_data[105000:]:\n",
    "    \n",
    "    X3.append(x[0])\n",
    "    Y3.append(x[1])\n",
    "    \n",
    "    \n",
    "XTest = np.array(X3)\n",
    "YTest = np.array(Y3)\n",
    "\n",
    "print(XTraining.shape,XVal.shape,XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much from one kind, how much from the other: \n",
      " [7542 7463]\n",
      "How do they look like? \n",
      " [[0 1]\n",
      " [1 0]]\n",
      "Percentage of one kind: \n",
      " 49.73675441519494\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(YTest, return_counts=True, axis=0)\n",
    "print(\"How much from one kind, how much from the other: \\n\",counts)\n",
    "print(\"How do they look like? \\n\",unique)\n",
    "print(\"Percentage of one kind: \\n\", 100/(counts[0]+counts[1])*counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2040000 into shape (17000,9,24,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d7f1263e6da3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mXTrainingT\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mXTraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m17000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mXTestT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4052\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mXValT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXVal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mXTrainingC\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mXTraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m17000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mXTestC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4052\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2040000 into shape (17000,9,24,1)"
     ]
    }
   ],
   "source": [
    "XTrainingT= XTraining[:,:,:,1].reshape(17000,9,24,1)\n",
    "XTestT = XTest[:,:,:,1].reshape(4052,9,24,1)\n",
    "XValT = XVal[:,:,:,1].reshape(2500,9,24,1)\n",
    "XTrainingC= XTraining[:,:,:,0].reshape(17000,9,24,1)\n",
    "XTestC = XTest[:,:,:,0].reshape(4052,9,24,1)\n",
    "XValC = XVal[:,:,:,0].reshape(2500,9,24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.5815 - acc: 0.7127 - val_loss: 0.5033 - val_acc: 0.7552\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.5126 - acc: 0.7488 - val_loss: 0.4834 - val_acc: 0.7666\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4986 - acc: 0.7575 - val_loss: 0.4672 - val_acc: 0.7793\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4870 - acc: 0.7656 - val_loss: 0.4563 - val_acc: 0.7832\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4793 - acc: 0.7708 - val_loss: 0.4519 - val_acc: 0.7868\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4732 - acc: 0.7746 - val_loss: 0.4621 - val_acc: 0.7766\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4694 - acc: 0.7769 - val_loss: 0.4461 - val_acc: 0.7944\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4671 - acc: 0.7797 - val_loss: 0.4653 - val_acc: 0.7778\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4631 - acc: 0.7811 - val_loss: 0.4326 - val_acc: 0.7991\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4614 - acc: 0.7817 - val_loss: 0.4424 - val_acc: 0.7946\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4593 - acc: 0.7834 - val_loss: 0.4402 - val_acc: 0.7942\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4579 - acc: 0.7836 - val_loss: 0.4387 - val_acc: 0.7958\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4566 - acc: 0.7854 - val_loss: 0.4406 - val_acc: 0.7930\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4571 - acc: 0.7860 - val_loss: 0.4374 - val_acc: 0.7959\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4573 - acc: 0.7850 - val_loss: 0.4326 - val_acc: 0.7985\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4534 - acc: 0.7888 - val_loss: 0.4274 - val_acc: 0.8043\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4549 - acc: 0.7856 - val_loss: 0.4272 - val_acc: 0.8015\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4523 - acc: 0.7878 - val_loss: 0.4308 - val_acc: 0.7997\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4515 - acc: 0.7878 - val_loss: 0.4285 - val_acc: 0.8000\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4520 - acc: 0.7876 - val_loss: 0.4257 - val_acc: 0.8018\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4536 - acc: 0.7879 - val_loss: 0.4608 - val_acc: 0.7811\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4518 - acc: 0.7876 - val_loss: 0.4267 - val_acc: 0.8027\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4530 - acc: 0.7872 - val_loss: 0.4322 - val_acc: 0.7988\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4522 - acc: 0.7871 - val_loss: 0.4277 - val_acc: 0.8026\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4505 - acc: 0.7891 - val_loss: 0.4252 - val_acc: 0.8009\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4504 - acc: 0.7894 - val_loss: 0.4389 - val_acc: 0.7951\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4506 - acc: 0.7897 - val_loss: 0.4275 - val_acc: 0.7988\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4500 - acc: 0.7896 - val_loss: 0.4403 - val_acc: 0.7931\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4519 - acc: 0.7869 - val_loss: 0.4224 - val_acc: 0.8048\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4489 - acc: 0.7887 - val_loss: 0.4288 - val_acc: 0.7984\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.5677 - acc: 0.7150 - val_loss: 0.5035 - val_acc: 0.7573\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.5150 - acc: 0.7476 - val_loss: 0.4855 - val_acc: 0.7662\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.5031 - acc: 0.7550 - val_loss: 0.4775 - val_acc: 0.7699\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4899 - acc: 0.7638 - val_loss: 0.4634 - val_acc: 0.7822\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4817 - acc: 0.7694 - val_loss: 0.4552 - val_acc: 0.7861\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4746 - acc: 0.7745 - val_loss: 0.4640 - val_acc: 0.7757\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4721 - acc: 0.7760 - val_loss: 0.4548 - val_acc: 0.7836\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4685 - acc: 0.7776 - val_loss: 0.4559 - val_acc: 0.7857\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4668 - acc: 0.7800 - val_loss: 0.4585 - val_acc: 0.7799\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4656 - acc: 0.7803 - val_loss: 0.4366 - val_acc: 0.7976\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4627 - acc: 0.7824 - val_loss: 0.4450 - val_acc: 0.7934\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4619 - acc: 0.7832 - val_loss: 0.4374 - val_acc: 0.7975\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4620 - acc: 0.7816 - val_loss: 0.4396 - val_acc: 0.7935\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4590 - acc: 0.7839 - val_loss: 0.4573 - val_acc: 0.7842\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4568 - acc: 0.7844 - val_loss: 0.4367 - val_acc: 0.7983\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4566 - acc: 0.7860 - val_loss: 0.4558 - val_acc: 0.7851\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4556 - acc: 0.7859 - val_loss: 0.4403 - val_acc: 0.7936\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4543 - acc: 0.7856 - val_loss: 0.4287 - val_acc: 0.8039\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4543 - acc: 0.7873 - val_loss: 0.4363 - val_acc: 0.7977\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4522 - acc: 0.7884 - val_loss: 0.4323 - val_acc: 0.7976\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4537 - acc: 0.7876 - val_loss: 0.4402 - val_acc: 0.7919\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4526 - acc: 0.7879 - val_loss: 0.4270 - val_acc: 0.7998\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4511 - acc: 0.7890 - val_loss: 0.4251 - val_acc: 0.8020\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4512 - acc: 0.7882 - val_loss: 0.4266 - val_acc: 0.8041\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4511 - acc: 0.7893 - val_loss: 0.4261 - val_acc: 0.8023\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4492 - acc: 0.7896 - val_loss: 0.4272 - val_acc: 0.8011\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4493 - acc: 0.7897 - val_loss: 0.4227 - val_acc: 0.8040\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 67us/sample - loss: 0.4478 - acc: 0.7909 - val_loss: 0.4244 - val_acc: 0.8064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4480 - acc: 0.7911 - val_loss: 0.4333 - val_acc: 0.7964\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4494 - acc: 0.7893 - val_loss: 0.4258 - val_acc: 0.8053\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.5946 - acc: 0.6962 - val_loss: 0.5144 - val_acc: 0.7466\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.5307 - acc: 0.7361 - val_loss: 0.4991 - val_acc: 0.7582\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.5166 - acc: 0.7468 - val_loss: 0.4862 - val_acc: 0.7652\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.5066 - acc: 0.7540 - val_loss: 0.4863 - val_acc: 0.7681\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4992 - acc: 0.7574 - val_loss: 0.4714 - val_acc: 0.7735\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4952 - acc: 0.7606 - val_loss: 0.4644 - val_acc: 0.7763\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4885 - acc: 0.7635 - val_loss: 0.4575 - val_acc: 0.7847\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4851 - acc: 0.7683 - val_loss: 0.4553 - val_acc: 0.7852\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4792 - acc: 0.7710 - val_loss: 0.4479 - val_acc: 0.7909\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4774 - acc: 0.7728 - val_loss: 0.4494 - val_acc: 0.7874\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4754 - acc: 0.7732 - val_loss: 0.4431 - val_acc: 0.7910\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4740 - acc: 0.7765 - val_loss: 0.4409 - val_acc: 0.7936\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4728 - acc: 0.7757 - val_loss: 0.4376 - val_acc: 0.7944\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4716 - acc: 0.7754 - val_loss: 0.4402 - val_acc: 0.7922\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4706 - acc: 0.7778 - val_loss: 0.4388 - val_acc: 0.7940\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4691 - acc: 0.7779 - val_loss: 0.4361 - val_acc: 0.7972\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4689 - acc: 0.7767 - val_loss: 0.4368 - val_acc: 0.7957\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4669 - acc: 0.7802 - val_loss: 0.4335 - val_acc: 0.8013\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 76us/sample - loss: 0.4654 - acc: 0.7786 - val_loss: 0.4399 - val_acc: 0.7976\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4637 - acc: 0.7809 - val_loss: 0.4338 - val_acc: 0.7979\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4659 - acc: 0.7788 - val_loss: 0.4531 - val_acc: 0.7886\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4656 - acc: 0.7802 - val_loss: 0.4343 - val_acc: 0.7994\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4647 - acc: 0.7807 - val_loss: 0.4316 - val_acc: 0.8040\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4649 - acc: 0.7800 - val_loss: 0.4388 - val_acc: 0.7947\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4641 - acc: 0.7803 - val_loss: 0.4308 - val_acc: 0.7982\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4621 - acc: 0.7809 - val_loss: 0.4545 - val_acc: 0.7829\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4636 - acc: 0.7800 - val_loss: 0.4327 - val_acc: 0.7984\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4619 - acc: 0.7830 - val_loss: 0.4284 - val_acc: 0.7990\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4629 - acc: 0.7826 - val_loss: 0.4305 - val_acc: 0.8006\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4633 - acc: 0.7825 - val_loss: 0.4336 - val_acc: 0.7983\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.6496 - acc: 0.6638 - val_loss: 0.5255 - val_acc: 0.7413\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5446 - acc: 0.7313 - val_loss: 0.5092 - val_acc: 0.7537\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5313 - acc: 0.7395 - val_loss: 0.5013 - val_acc: 0.7583\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5221 - acc: 0.7460 - val_loss: 0.4904 - val_acc: 0.7614\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5175 - acc: 0.7476 - val_loss: 0.4848 - val_acc: 0.7673\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.5118 - acc: 0.7516 - val_loss: 0.4856 - val_acc: 0.7687\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.5047 - acc: 0.7550 - val_loss: 0.4757 - val_acc: 0.7739\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4987 - acc: 0.7601 - val_loss: 0.4659 - val_acc: 0.7768\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4957 - acc: 0.7616 - val_loss: 0.4633 - val_acc: 0.7804\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4920 - acc: 0.7636 - val_loss: 0.4602 - val_acc: 0.7842\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4883 - acc: 0.7653 - val_loss: 0.4589 - val_acc: 0.7838\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4847 - acc: 0.7686 - val_loss: 0.4506 - val_acc: 0.7882\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4825 - acc: 0.7700 - val_loss: 0.4558 - val_acc: 0.7839\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4816 - acc: 0.7712 - val_loss: 0.4501 - val_acc: 0.7876\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4799 - acc: 0.7709 - val_loss: 0.4491 - val_acc: 0.7881\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4776 - acc: 0.7736 - val_loss: 0.4481 - val_acc: 0.7905\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4760 - acc: 0.7732 - val_loss: 0.4439 - val_acc: 0.7959\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4762 - acc: 0.7744 - val_loss: 0.4388 - val_acc: 0.7951\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4732 - acc: 0.7761 - val_loss: 0.4392 - val_acc: 0.7933\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4726 - acc: 0.7769 - val_loss: 0.4360 - val_acc: 0.7967\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4737 - acc: 0.7762 - val_loss: 0.4478 - val_acc: 0.7933\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4745 - acc: 0.7740 - val_loss: 0.4354 - val_acc: 0.7964\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4719 - acc: 0.7765 - val_loss: 0.4372 - val_acc: 0.7987\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4707 - acc: 0.7767 - val_loss: 0.4441 - val_acc: 0.7924\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4718 - acc: 0.7759 - val_loss: 0.4340 - val_acc: 0.7954\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4700 - acc: 0.7771 - val_loss: 0.4432 - val_acc: 0.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4695 - acc: 0.7779 - val_loss: 0.4509 - val_acc: 0.7921\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4675 - acc: 0.7791 - val_loss: 0.4318 - val_acc: 0.8003\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4693 - acc: 0.7776 - val_loss: 0.4398 - val_acc: 0.7959\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.4692 - acc: 0.7790 - val_loss: 0.4381 - val_acc: 0.7963\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.5401 - acc: 0.7356 - val_loss: 0.4743 - val_acc: 0.7725\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4825 - acc: 0.7676 - val_loss: 0.4637 - val_acc: 0.7814\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4686 - acc: 0.7761 - val_loss: 0.4390 - val_acc: 0.7944\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4565 - acc: 0.7863 - val_loss: 0.4469 - val_acc: 0.7925\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4492 - acc: 0.7896 - val_loss: 0.4399 - val_acc: 0.7922\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4449 - acc: 0.7930 - val_loss: 0.4413 - val_acc: 0.7908\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4402 - acc: 0.7946 - val_loss: 0.4252 - val_acc: 0.8044\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4371 - acc: 0.7967 - val_loss: 0.4148 - val_acc: 0.8081\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4330 - acc: 0.7988 - val_loss: 0.4194 - val_acc: 0.8087\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4310 - acc: 0.8006 - val_loss: 0.4104 - val_acc: 0.8110\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4297 - acc: 0.7998 - val_loss: 0.4162 - val_acc: 0.8071\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4275 - acc: 0.8027 - val_loss: 0.4240 - val_acc: 0.8002\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4269 - acc: 0.8026 - val_loss: 0.4078 - val_acc: 0.8114\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4233 - acc: 0.8046 - val_loss: 0.4084 - val_acc: 0.8141\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4249 - acc: 0.8041 - val_loss: 0.4196 - val_acc: 0.8071\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4226 - acc: 0.8054 - val_loss: 0.4022 - val_acc: 0.8163\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4227 - acc: 0.8049 - val_loss: 0.4054 - val_acc: 0.8163\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4221 - acc: 0.8052 - val_loss: 0.4028 - val_acc: 0.8179\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4201 - acc: 0.8063 - val_loss: 0.4092 - val_acc: 0.8140\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4185 - acc: 0.8068 - val_loss: 0.4057 - val_acc: 0.8144\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4185 - acc: 0.8076 - val_loss: 0.3996 - val_acc: 0.8181\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4180 - acc: 0.8080 - val_loss: 0.4039 - val_acc: 0.8163\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4182 - acc: 0.8086 - val_loss: 0.4031 - val_acc: 0.8163\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4151 - acc: 0.8087 - val_loss: 0.4073 - val_acc: 0.8149\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4167 - acc: 0.8066 - val_loss: 0.4019 - val_acc: 0.8166\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4164 - acc: 0.8084 - val_loss: 0.4051 - val_acc: 0.8174\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4157 - acc: 0.8080 - val_loss: 0.4010 - val_acc: 0.8172\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 62us/sample - loss: 0.4158 - acc: 0.8086 - val_loss: 0.3996 - val_acc: 0.8159\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4139 - acc: 0.8107 - val_loss: 0.3960 - val_acc: 0.8199\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4146 - acc: 0.8090 - val_loss: 0.4096 - val_acc: 0.8148\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.5404 - acc: 0.7350 - val_loss: 0.4738 - val_acc: 0.7760\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4811 - acc: 0.7682 - val_loss: 0.4594 - val_acc: 0.7802\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4657 - acc: 0.7788 - val_loss: 0.4391 - val_acc: 0.7962\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4525 - acc: 0.7872 - val_loss: 0.4289 - val_acc: 0.7983\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4473 - acc: 0.7902 - val_loss: 0.4339 - val_acc: 0.7977\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4390 - acc: 0.7944 - val_loss: 0.4302 - val_acc: 0.7970\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4367 - acc: 0.7964 - val_loss: 0.4143 - val_acc: 0.8087\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4338 - acc: 0.7987 - val_loss: 0.4176 - val_acc: 0.8063\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4313 - acc: 0.8012 - val_loss: 0.4283 - val_acc: 0.7994\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4294 - acc: 0.8024 - val_loss: 0.4118 - val_acc: 0.8092\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4288 - acc: 0.8020 - val_loss: 0.4077 - val_acc: 0.8138\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4272 - acc: 0.8031 - val_loss: 0.4157 - val_acc: 0.8064\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4263 - acc: 0.8043 - val_loss: 0.4109 - val_acc: 0.8106\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4258 - acc: 0.8035 - val_loss: 0.4130 - val_acc: 0.8108\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4250 - acc: 0.8035 - val_loss: 0.4016 - val_acc: 0.8138\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4230 - acc: 0.8040 - val_loss: 0.4035 - val_acc: 0.8152\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4234 - acc: 0.8037 - val_loss: 0.4109 - val_acc: 0.8130\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4223 - acc: 0.8052 - val_loss: 0.4038 - val_acc: 0.8141\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4209 - acc: 0.8059 - val_loss: 0.4175 - val_acc: 0.8102\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4188 - acc: 0.8071 - val_loss: 0.4034 - val_acc: 0.8137\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4195 - acc: 0.8066 - val_loss: 0.4044 - val_acc: 0.8133\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4183 - acc: 0.8072 - val_loss: 0.4005 - val_acc: 0.8159\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4172 - acc: 0.8075 - val_loss: 0.4049 - val_acc: 0.8122\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4187 - acc: 0.8067 - val_loss: 0.3997 - val_acc: 0.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4176 - acc: 0.8075 - val_loss: 0.4174 - val_acc: 0.8030\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4173 - acc: 0.8077 - val_loss: 0.3992 - val_acc: 0.8166\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4167 - acc: 0.8089 - val_loss: 0.4182 - val_acc: 0.8086\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4154 - acc: 0.8106 - val_loss: 0.3982 - val_acc: 0.8183\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 63us/sample - loss: 0.4179 - acc: 0.8066 - val_loss: 0.3975 - val_acc: 0.8170\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4148 - acc: 0.8097 - val_loss: 0.4005 - val_acc: 0.8166\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.5590 - acc: 0.7276 - val_loss: 0.4802 - val_acc: 0.7691\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4906 - acc: 0.7635 - val_loss: 0.4618 - val_acc: 0.7826\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4715 - acc: 0.7753 - val_loss: 0.4373 - val_acc: 0.7957\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4575 - acc: 0.7843 - val_loss: 0.4341 - val_acc: 0.7944\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4482 - acc: 0.7904 - val_loss: 0.4347 - val_acc: 0.8032\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4405 - acc: 0.7961 - val_loss: 0.4145 - val_acc: 0.8108\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4370 - acc: 0.7981 - val_loss: 0.4116 - val_acc: 0.8110\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4325 - acc: 0.8004 - val_loss: 0.4120 - val_acc: 0.8090\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4297 - acc: 0.8015 - val_loss: 0.4043 - val_acc: 0.8148\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4284 - acc: 0.8024 - val_loss: 0.4036 - val_acc: 0.8163\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4278 - acc: 0.8026 - val_loss: 0.4196 - val_acc: 0.8090\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4257 - acc: 0.8033 - val_loss: 0.4136 - val_acc: 0.8067\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4230 - acc: 0.8064 - val_loss: 0.4045 - val_acc: 0.8179\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4224 - acc: 0.8056 - val_loss: 0.4028 - val_acc: 0.8159\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4208 - acc: 0.8074 - val_loss: 0.4059 - val_acc: 0.8145\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4202 - acc: 0.8072 - val_loss: 0.3996 - val_acc: 0.8166\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4192 - acc: 0.8074 - val_loss: 0.4047 - val_acc: 0.8145\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4183 - acc: 0.8082 - val_loss: 0.3948 - val_acc: 0.8198\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4163 - acc: 0.8092 - val_loss: 0.3945 - val_acc: 0.8202\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4164 - acc: 0.8097 - val_loss: 0.3914 - val_acc: 0.8220\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4151 - acc: 0.8104 - val_loss: 0.4001 - val_acc: 0.8156\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4136 - acc: 0.8117 - val_loss: 0.3964 - val_acc: 0.8192\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4144 - acc: 0.8096 - val_loss: 0.3908 - val_acc: 0.8217\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 74us/sample - loss: 0.4122 - acc: 0.8133 - val_loss: 0.4018 - val_acc: 0.8174\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4133 - acc: 0.8115 - val_loss: 0.3995 - val_acc: 0.8175\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4127 - acc: 0.8126 - val_loss: 0.3862 - val_acc: 0.8260\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4109 - acc: 0.8126 - val_loss: 0.4409 - val_acc: 0.7972\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4126 - acc: 0.8122 - val_loss: 0.3957 - val_acc: 0.8217\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4102 - acc: 0.8136 - val_loss: 0.3859 - val_acc: 0.8253\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4096 - acc: 0.8131 - val_loss: 0.3883 - val_acc: 0.8228\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.5610 - acc: 0.7225 - val_loss: 0.4870 - val_acc: 0.7639\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4981 - acc: 0.7613 - val_loss: 0.4651 - val_acc: 0.7793\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4773 - acc: 0.7734 - val_loss: 0.4501 - val_acc: 0.7887\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4639 - acc: 0.7815 - val_loss: 0.4569 - val_acc: 0.7844\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4536 - acc: 0.7881 - val_loss: 0.4187 - val_acc: 0.8066\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4460 - acc: 0.7929 - val_loss: 0.4237 - val_acc: 0.8047\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4397 - acc: 0.7953 - val_loss: 0.4209 - val_acc: 0.8027\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4378 - acc: 0.7977 - val_loss: 0.4159 - val_acc: 0.8078\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4354 - acc: 0.7984 - val_loss: 0.4089 - val_acc: 0.8109\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4313 - acc: 0.8010 - val_loss: 0.4086 - val_acc: 0.8148\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4284 - acc: 0.8020 - val_loss: 0.4020 - val_acc: 0.8160\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4255 - acc: 0.8049 - val_loss: 0.4006 - val_acc: 0.8169\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4276 - acc: 0.8025 - val_loss: 0.4004 - val_acc: 0.8172\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4228 - acc: 0.8058 - val_loss: 0.4127 - val_acc: 0.8084\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4218 - acc: 0.8062 - val_loss: 0.4077 - val_acc: 0.8173\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4208 - acc: 0.8069 - val_loss: 0.4001 - val_acc: 0.8169\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4187 - acc: 0.8077 - val_loss: 0.4008 - val_acc: 0.8205\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4177 - acc: 0.8094 - val_loss: 0.4063 - val_acc: 0.8097\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4169 - acc: 0.8094 - val_loss: 0.3987 - val_acc: 0.8152\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4146 - acc: 0.8105 - val_loss: 0.3973 - val_acc: 0.8203\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4156 - acc: 0.8100 - val_loss: 0.3924 - val_acc: 0.8209\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4138 - acc: 0.8098 - val_loss: 0.3931 - val_acc: 0.8209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4133 - acc: 0.8089 - val_loss: 0.3938 - val_acc: 0.8170\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4145 - acc: 0.8102 - val_loss: 0.3949 - val_acc: 0.8239\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4133 - acc: 0.8108 - val_loss: 0.3899 - val_acc: 0.8212\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4108 - acc: 0.8121 - val_loss: 0.3971 - val_acc: 0.8174\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4112 - acc: 0.8123 - val_loss: 0.3946 - val_acc: 0.8182\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4088 - acc: 0.8152 - val_loss: 0.3913 - val_acc: 0.8247\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4110 - acc: 0.8118 - val_loss: 0.3864 - val_acc: 0.8245\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4074 - acc: 0.8142 - val_loss: 0.3865 - val_acc: 0.8228\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.5225 - acc: 0.7452 - val_loss: 0.4686 - val_acc: 0.7778\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4713 - acc: 0.7750 - val_loss: 0.4513 - val_acc: 0.7875\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4551 - acc: 0.7850 - val_loss: 0.4307 - val_acc: 0.7972\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4450 - acc: 0.7927 - val_loss: 0.4275 - val_acc: 0.7993\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4385 - acc: 0.7970 - val_loss: 0.4263 - val_acc: 0.8020\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4323 - acc: 0.7999 - val_loss: 0.4234 - val_acc: 0.8002\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4265 - acc: 0.8028 - val_loss: 0.4110 - val_acc: 0.8109\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4260 - acc: 0.8038 - val_loss: 0.4113 - val_acc: 0.8105\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4216 - acc: 0.8056 - val_loss: 0.4088 - val_acc: 0.8124\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4202 - acc: 0.8056 - val_loss: 0.4077 - val_acc: 0.8102\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4196 - acc: 0.8074 - val_loss: 0.4006 - val_acc: 0.8133\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4148 - acc: 0.8109 - val_loss: 0.4017 - val_acc: 0.8154\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4137 - acc: 0.8105 - val_loss: 0.3979 - val_acc: 0.8171\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4126 - acc: 0.8106 - val_loss: 0.4023 - val_acc: 0.8174\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4106 - acc: 0.8127 - val_loss: 0.3928 - val_acc: 0.8209\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4085 - acc: 0.8136 - val_loss: 0.3978 - val_acc: 0.8202\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4093 - acc: 0.8116 - val_loss: 0.3942 - val_acc: 0.8190\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4078 - acc: 0.8136 - val_loss: 0.4073 - val_acc: 0.8126\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4090 - acc: 0.8126 - val_loss: 0.4043 - val_acc: 0.8171\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4077 - acc: 0.8140 - val_loss: 0.4003 - val_acc: 0.8121\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4056 - acc: 0.8130 - val_loss: 0.3921 - val_acc: 0.8216\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 5s 64us/sample - loss: 0.4038 - acc: 0.8154 - val_loss: 0.3899 - val_acc: 0.8229\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4038 - acc: 0.8152 - val_loss: 0.3961 - val_acc: 0.8185\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4033 - acc: 0.8159 - val_loss: 0.3939 - val_acc: 0.8203\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4022 - acc: 0.8166 - val_loss: 0.3949 - val_acc: 0.8213\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4008 - acc: 0.8167 - val_loss: 0.3866 - val_acc: 0.8239\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4002 - acc: 0.8170 - val_loss: 0.3958 - val_acc: 0.8203\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.3996 - acc: 0.8182 - val_loss: 0.3889 - val_acc: 0.8234\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.3993 - acc: 0.8185 - val_loss: 0.3973 - val_acc: 0.8190\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3992 - acc: 0.8181 - val_loss: 0.3969 - val_acc: 0.8164\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.5224 - acc: 0.7454 - val_loss: 0.4630 - val_acc: 0.7776\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4704 - acc: 0.7752 - val_loss: 0.4612 - val_acc: 0.7785\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4547 - acc: 0.7866 - val_loss: 0.4411 - val_acc: 0.7883\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4443 - acc: 0.7921 - val_loss: 0.4281 - val_acc: 0.7997\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4389 - acc: 0.7974 - val_loss: 0.4281 - val_acc: 0.7999\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4317 - acc: 0.8010 - val_loss: 0.4412 - val_acc: 0.7955\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4283 - acc: 0.8011 - val_loss: 0.4323 - val_acc: 0.7970\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4226 - acc: 0.8061 - val_loss: 0.4089 - val_acc: 0.8098\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4207 - acc: 0.8061 - val_loss: 0.4110 - val_acc: 0.8113\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4208 - acc: 0.8076 - val_loss: 0.4112 - val_acc: 0.8077\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4172 - acc: 0.8082 - val_loss: 0.4049 - val_acc: 0.8133\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4147 - acc: 0.8097 - val_loss: 0.4066 - val_acc: 0.8154\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4137 - acc: 0.8092 - val_loss: 0.4130 - val_acc: 0.8102\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4101 - acc: 0.8122 - val_loss: 0.4019 - val_acc: 0.8162\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4102 - acc: 0.8113 - val_loss: 0.4305 - val_acc: 0.7967\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4101 - acc: 0.8126 - val_loss: 0.4025 - val_acc: 0.8173\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4077 - acc: 0.8134 - val_loss: 0.4181 - val_acc: 0.8080\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4071 - acc: 0.8143 - val_loss: 0.4072 - val_acc: 0.8093\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4066 - acc: 0.8140 - val_loss: 0.3957 - val_acc: 0.8177\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4057 - acc: 0.8151 - val_loss: 0.4172 - val_acc: 0.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 5s 65us/sample - loss: 0.4056 - acc: 0.8145 - val_loss: 0.3937 - val_acc: 0.8194\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4031 - acc: 0.8157 - val_loss: 0.4229 - val_acc: 0.8035\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4035 - acc: 0.8162 - val_loss: 0.3929 - val_acc: 0.8245\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 66us/sample - loss: 0.4016 - acc: 0.8171 - val_loss: 0.3904 - val_acc: 0.8202\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3999 - acc: 0.8179 - val_loss: 0.3894 - val_acc: 0.8217\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4001 - acc: 0.8175 - val_loss: 0.4023 - val_acc: 0.8146\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3998 - acc: 0.8173 - val_loss: 0.4003 - val_acc: 0.8159\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.4012 - acc: 0.8175 - val_loss: 0.3888 - val_acc: 0.8237\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3982 - acc: 0.8186 - val_loss: 0.4019 - val_acc: 0.8184\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 65us/sample - loss: 0.3983 - acc: 0.8189 - val_loss: 0.4005 - val_acc: 0.8176\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.5368 - acc: 0.7389 - val_loss: 0.4655 - val_acc: 0.7788\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4698 - acc: 0.7754 - val_loss: 0.4405 - val_acc: 0.7939\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4506 - acc: 0.7892 - val_loss: 0.4468 - val_acc: 0.7918\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4367 - acc: 0.7962 - val_loss: 0.4148 - val_acc: 0.8069\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4302 - acc: 0.8018 - val_loss: 0.4311 - val_acc: 0.8009\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4229 - acc: 0.8059 - val_loss: 0.4108 - val_acc: 0.8132\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4192 - acc: 0.8075 - val_loss: 0.4087 - val_acc: 0.8143\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4144 - acc: 0.8098 - val_loss: 0.3928 - val_acc: 0.8209\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4120 - acc: 0.8121 - val_loss: 0.3921 - val_acc: 0.8183\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4081 - acc: 0.8131 - val_loss: 0.3904 - val_acc: 0.8207\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4057 - acc: 0.8151 - val_loss: 0.3967 - val_acc: 0.8192\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4039 - acc: 0.8148 - val_loss: 0.4017 - val_acc: 0.8162\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4000 - acc: 0.8191 - val_loss: 0.3857 - val_acc: 0.8231\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4010 - acc: 0.8183 - val_loss: 0.3885 - val_acc: 0.8223\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3969 - acc: 0.8201 - val_loss: 0.3836 - val_acc: 0.8273\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3955 - acc: 0.8210 - val_loss: 0.3900 - val_acc: 0.8202\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3944 - acc: 0.8215 - val_loss: 0.3964 - val_acc: 0.8179\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3924 - acc: 0.8224 - val_loss: 0.3793 - val_acc: 0.8261\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3907 - acc: 0.8248 - val_loss: 0.3791 - val_acc: 0.8278\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3904 - acc: 0.8230 - val_loss: 0.3849 - val_acc: 0.8241\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3896 - acc: 0.8240 - val_loss: 0.3849 - val_acc: 0.8266\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3900 - acc: 0.8228 - val_loss: 0.3892 - val_acc: 0.8207\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3868 - acc: 0.8250 - val_loss: 0.3745 - val_acc: 0.8314\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3879 - acc: 0.8237 - val_loss: 0.3742 - val_acc: 0.8324\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3849 - acc: 0.8263 - val_loss: 0.3809 - val_acc: 0.8262\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3835 - acc: 0.8262 - val_loss: 0.3843 - val_acc: 0.8264\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3818 - acc: 0.8270 - val_loss: 0.3765 - val_acc: 0.8310\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3825 - acc: 0.8275 - val_loss: 0.3763 - val_acc: 0.8306\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3837 - acc: 0.8270 - val_loss: 0.3809 - val_acc: 0.8251\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3802 - acc: 0.8293 - val_loss: 0.3754 - val_acc: 0.8307\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.5405 - acc: 0.7384 - val_loss: 0.4722 - val_acc: 0.7729\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4755 - acc: 0.7734 - val_loss: 0.4510 - val_acc: 0.7901\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4542 - acc: 0.7869 - val_loss: 0.4298 - val_acc: 0.8006\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4397 - acc: 0.7957 - val_loss: 0.4468 - val_acc: 0.7893\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4310 - acc: 0.8020 - val_loss: 0.4178 - val_acc: 0.8076\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4227 - acc: 0.8071 - val_loss: 0.4008 - val_acc: 0.8173\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4193 - acc: 0.8078 - val_loss: 0.4211 - val_acc: 0.8027\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4132 - acc: 0.8106 - val_loss: 0.4009 - val_acc: 0.8192\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4112 - acc: 0.8115 - val_loss: 0.4075 - val_acc: 0.8117\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.4060 - acc: 0.8147 - val_loss: 0.4162 - val_acc: 0.8045\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4053 - acc: 0.8148 - val_loss: 0.4112 - val_acc: 0.8091\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.4008 - acc: 0.8187 - val_loss: 0.3844 - val_acc: 0.8238\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3990 - acc: 0.8183 - val_loss: 0.3880 - val_acc: 0.8254\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 88us/sample - loss: 0.3969 - acc: 0.8196 - val_loss: 0.3832 - val_acc: 0.8232\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3954 - acc: 0.8202 - val_loss: 0.4092 - val_acc: 0.8118\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3922 - acc: 0.8226 - val_loss: 0.3827 - val_acc: 0.8246\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3911 - acc: 0.8236 - val_loss: 0.3805 - val_acc: 0.8260\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3902 - acc: 0.8230 - val_loss: 0.3756 - val_acc: 0.8288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3901 - acc: 0.8233 - val_loss: 0.3882 - val_acc: 0.8228\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3893 - acc: 0.8244 - val_loss: 0.3927 - val_acc: 0.8224\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3873 - acc: 0.8251 - val_loss: 0.3731 - val_acc: 0.8304\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3853 - acc: 0.8253 - val_loss: 0.3860 - val_acc: 0.8207\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3854 - acc: 0.8260 - val_loss: 0.3781 - val_acc: 0.8275\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3816 - acc: 0.8289 - val_loss: 0.3751 - val_acc: 0.8303\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3830 - acc: 0.8281 - val_loss: 0.3868 - val_acc: 0.8210\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3798 - acc: 0.8289 - val_loss: 0.3902 - val_acc: 0.8227\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3803 - acc: 0.8296 - val_loss: 0.3830 - val_acc: 0.8235\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3785 - acc: 0.8298 - val_loss: 0.3710 - val_acc: 0.8339\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 88us/sample - loss: 0.3786 - acc: 0.8293 - val_loss: 0.3705 - val_acc: 0.8323\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 87us/sample - loss: 0.3772 - acc: 0.8297 - val_loss: 0.3729 - val_acc: 0.8311\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.5136 - acc: 0.7495 - val_loss: 0.4610 - val_acc: 0.7814\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4650 - acc: 0.7804 - val_loss: 0.4363 - val_acc: 0.7958\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4475 - acc: 0.7901 - val_loss: 0.4512 - val_acc: 0.7897\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4372 - acc: 0.7979 - val_loss: 0.4264 - val_acc: 0.8019\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4303 - acc: 0.8015 - val_loss: 0.4184 - val_acc: 0.8059\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4248 - acc: 0.8037 - val_loss: 0.4236 - val_acc: 0.8030\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4211 - acc: 0.8056 - val_loss: 0.4177 - val_acc: 0.8090\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4178 - acc: 0.8091 - val_loss: 0.4640 - val_acc: 0.7849\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4147 - acc: 0.8087 - val_loss: 0.4010 - val_acc: 0.8155\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4110 - acc: 0.8124 - val_loss: 0.4045 - val_acc: 0.8167\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4114 - acc: 0.8117 - val_loss: 0.3948 - val_acc: 0.8198\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4069 - acc: 0.8139 - val_loss: 0.4046 - val_acc: 0.8129\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4068 - acc: 0.8140 - val_loss: 0.3937 - val_acc: 0.8185\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4045 - acc: 0.8151 - val_loss: 0.3924 - val_acc: 0.8212\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4036 - acc: 0.8155 - val_loss: 0.4028 - val_acc: 0.8134\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4016 - acc: 0.8165 - val_loss: 0.4030 - val_acc: 0.8139\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4035 - acc: 0.8149 - val_loss: 0.3920 - val_acc: 0.8207\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3994 - acc: 0.8183 - val_loss: 0.4070 - val_acc: 0.8108\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3978 - acc: 0.8191 - val_loss: 0.3926 - val_acc: 0.8194\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3972 - acc: 0.8194 - val_loss: 0.3873 - val_acc: 0.8242\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3981 - acc: 0.8185 - val_loss: 0.3878 - val_acc: 0.8241\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3956 - acc: 0.8206 - val_loss: 0.3881 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3953 - acc: 0.8197 - val_loss: 0.3831 - val_acc: 0.8274\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3929 - acc: 0.8211 - val_loss: 0.4000 - val_acc: 0.8181\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3933 - acc: 0.8213 - val_loss: 0.3921 - val_acc: 0.8213\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3910 - acc: 0.8211 - val_loss: 0.3832 - val_acc: 0.8257\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3909 - acc: 0.8222 - val_loss: 0.3923 - val_acc: 0.8211\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3933 - acc: 0.8218 - val_loss: 0.3827 - val_acc: 0.8262\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3894 - acc: 0.8232 - val_loss: 0.3860 - val_acc: 0.8219\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3885 - acc: 0.8237 - val_loss: 0.3818 - val_acc: 0.8260\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.5167 - acc: 0.7503 - val_loss: 0.4665 - val_acc: 0.7768\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4660 - acc: 0.7788 - val_loss: 0.4400 - val_acc: 0.7930\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4472 - acc: 0.7900 - val_loss: 0.4297 - val_acc: 0.7990\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4372 - acc: 0.7972 - val_loss: 0.4682 - val_acc: 0.7803\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4306 - acc: 0.8006 - val_loss: 0.4133 - val_acc: 0.8102\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4241 - acc: 0.8039 - val_loss: 0.4094 - val_acc: 0.8087\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4215 - acc: 0.8063 - val_loss: 0.4309 - val_acc: 0.7998\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4181 - acc: 0.8084 - val_loss: 0.4381 - val_acc: 0.7944\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4139 - acc: 0.8090 - val_loss: 0.4109 - val_acc: 0.8098\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4123 - acc: 0.8115 - val_loss: 0.3974 - val_acc: 0.8173\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4105 - acc: 0.8119 - val_loss: 0.4106 - val_acc: 0.8069\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4095 - acc: 0.8117 - val_loss: 0.4157 - val_acc: 0.8049\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4061 - acc: 0.8152 - val_loss: 0.4136 - val_acc: 0.8101\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4051 - acc: 0.8152 - val_loss: 0.4111 - val_acc: 0.8092\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4034 - acc: 0.8160 - val_loss: 0.3962 - val_acc: 0.8174\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4016 - acc: 0.8183 - val_loss: 0.3944 - val_acc: 0.8197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4012 - acc: 0.8178 - val_loss: 0.3975 - val_acc: 0.8181\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3988 - acc: 0.8187 - val_loss: 0.3896 - val_acc: 0.8246\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3981 - acc: 0.8196 - val_loss: 0.3953 - val_acc: 0.8192\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3976 - acc: 0.8174 - val_loss: 0.3972 - val_acc: 0.8202\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3964 - acc: 0.8186 - val_loss: 0.4023 - val_acc: 0.8144\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3972 - acc: 0.8192 - val_loss: 0.3864 - val_acc: 0.8260\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3970 - acc: 0.8175 - val_loss: 0.3926 - val_acc: 0.8231\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3935 - acc: 0.8208 - val_loss: 0.4144 - val_acc: 0.8068\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3932 - acc: 0.8197 - val_loss: 0.3926 - val_acc: 0.8197\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3921 - acc: 0.8206 - val_loss: 0.3871 - val_acc: 0.8239\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3906 - acc: 0.8229 - val_loss: 0.3967 - val_acc: 0.8194\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3918 - acc: 0.8228 - val_loss: 0.3909 - val_acc: 0.8249\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.3888 - acc: 0.8234 - val_loss: 0.3862 - val_acc: 0.8261\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.3899 - acc: 0.8218 - val_loss: 0.3857 - val_acc: 0.8247\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.5173 - acc: 0.7514 - val_loss: 0.4515 - val_acc: 0.7878\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4568 - acc: 0.7841 - val_loss: 0.4303 - val_acc: 0.7991\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4390 - acc: 0.7957 - val_loss: 0.4277 - val_acc: 0.8022\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4284 - acc: 0.8019 - val_loss: 0.4164 - val_acc: 0.8072\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4215 - acc: 0.8070 - val_loss: 0.4227 - val_acc: 0.8055\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4149 - acc: 0.8110 - val_loss: 0.3995 - val_acc: 0.8169\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4105 - acc: 0.8119 - val_loss: 0.3898 - val_acc: 0.8219\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4048 - acc: 0.8167 - val_loss: 0.3895 - val_acc: 0.8225\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4027 - acc: 0.8162 - val_loss: 0.3938 - val_acc: 0.8209\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3981 - acc: 0.8198 - val_loss: 0.3986 - val_acc: 0.8148\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3969 - acc: 0.8204 - val_loss: 0.3936 - val_acc: 0.8204\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3931 - acc: 0.8212 - val_loss: 0.3949 - val_acc: 0.8183\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3918 - acc: 0.8213 - val_loss: 0.3886 - val_acc: 0.8217\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3878 - acc: 0.8238 - val_loss: 0.3876 - val_acc: 0.8225\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3861 - acc: 0.8256 - val_loss: 0.3846 - val_acc: 0.8267\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3838 - acc: 0.8274 - val_loss: 0.3848 - val_acc: 0.8237\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 81us/sample - loss: 0.3835 - acc: 0.8261 - val_loss: 0.3786 - val_acc: 0.8304\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3790 - acc: 0.8302 - val_loss: 0.3838 - val_acc: 0.8265\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3793 - acc: 0.8286 - val_loss: 0.3758 - val_acc: 0.8302\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3768 - acc: 0.8297 - val_loss: 0.4042 - val_acc: 0.8141\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3746 - acc: 0.8319 - val_loss: 0.3808 - val_acc: 0.8262\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3735 - acc: 0.8314 - val_loss: 0.3753 - val_acc: 0.8305\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3723 - acc: 0.8331 - val_loss: 0.3760 - val_acc: 0.8312\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3743 - acc: 0.8316 - val_loss: 0.3711 - val_acc: 0.8322\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3717 - acc: 0.8334 - val_loss: 0.3778 - val_acc: 0.8286\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3700 - acc: 0.8344 - val_loss: 0.3704 - val_acc: 0.8335\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3677 - acc: 0.8341 - val_loss: 0.3852 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3672 - acc: 0.8356 - val_loss: 0.3836 - val_acc: 0.8256\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3656 - acc: 0.8355 - val_loss: 0.3747 - val_acc: 0.8306\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3656 - acc: 0.8361 - val_loss: 0.3800 - val_acc: 0.8278\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.5438 - acc: 0.7390 - val_loss: 0.4647 - val_acc: 0.7807\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4688 - acc: 0.7775 - val_loss: 0.4366 - val_acc: 0.7968\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4459 - acc: 0.7924 - val_loss: 0.4296 - val_acc: 0.8030\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4316 - acc: 0.7991 - val_loss: 0.4194 - val_acc: 0.8027\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4202 - acc: 0.8069 - val_loss: 0.4184 - val_acc: 0.8037\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4145 - acc: 0.8096 - val_loss: 0.4026 - val_acc: 0.8156\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4099 - acc: 0.8137 - val_loss: 0.3974 - val_acc: 0.8161\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4062 - acc: 0.8142 - val_loss: 0.3929 - val_acc: 0.8210\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4017 - acc: 0.8172 - val_loss: 0.3879 - val_acc: 0.8224\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3979 - acc: 0.8180 - val_loss: 0.4071 - val_acc: 0.8137\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3922 - acc: 0.8216 - val_loss: 0.4101 - val_acc: 0.8121\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3910 - acc: 0.8236 - val_loss: 0.3846 - val_acc: 0.8264\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3883 - acc: 0.8246 - val_loss: 0.3793 - val_acc: 0.8282\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3855 - acc: 0.8262 - val_loss: 0.3797 - val_acc: 0.8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3830 - acc: 0.8281 - val_loss: 0.3846 - val_acc: 0.8235\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3809 - acc: 0.8274 - val_loss: 0.3758 - val_acc: 0.8304\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3775 - acc: 0.8293 - val_loss: 0.3894 - val_acc: 0.8235\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3787 - acc: 0.8300 - val_loss: 0.3930 - val_acc: 0.8202\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3767 - acc: 0.8309 - val_loss: 0.3764 - val_acc: 0.8309\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3734 - acc: 0.8320 - val_loss: 0.3709 - val_acc: 0.8307\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.3716 - acc: 0.8340 - val_loss: 0.3743 - val_acc: 0.8300\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3698 - acc: 0.8346 - val_loss: 0.4003 - val_acc: 0.8167\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3678 - acc: 0.8356 - val_loss: 0.3844 - val_acc: 0.8224\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3662 - acc: 0.8355 - val_loss: 0.3802 - val_acc: 0.8289\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3656 - acc: 0.8360 - val_loss: 0.3770 - val_acc: 0.8295\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3623 - acc: 0.8389 - val_loss: 0.3810 - val_acc: 0.8253\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3626 - acc: 0.8373 - val_loss: 0.3682 - val_acc: 0.8339\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.3606 - acc: 0.8388 - val_loss: 0.3734 - val_acc: 0.8328\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.3592 - acc: 0.8409 - val_loss: 0.3742 - val_acc: 0.8299\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.3590 - acc: 0.8394 - val_loss: 0.3703 - val_acc: 0.8313\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.5739 - acc: 0.7152 - val_loss: 0.5055 - val_acc: 0.7558\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.5119 - acc: 0.7469 - val_loss: 0.5058 - val_acc: 0.7520\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4973 - acc: 0.7576 - val_loss: 0.4670 - val_acc: 0.7780\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4851 - acc: 0.7659 - val_loss: 0.4642 - val_acc: 0.7792\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4792 - acc: 0.7712 - val_loss: 0.4715 - val_acc: 0.7765\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4766 - acc: 0.7736 - val_loss: 0.4539 - val_acc: 0.7882\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4722 - acc: 0.7769 - val_loss: 0.4437 - val_acc: 0.7920\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4702 - acc: 0.7776 - val_loss: 0.4438 - val_acc: 0.7924\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4681 - acc: 0.7782 - val_loss: 0.4421 - val_acc: 0.7936\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4648 - acc: 0.7817 - val_loss: 0.4412 - val_acc: 0.7918\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4663 - acc: 0.7788 - val_loss: 0.4358 - val_acc: 0.7972\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4636 - acc: 0.7816 - val_loss: 0.4406 - val_acc: 0.7936\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4638 - acc: 0.7808 - val_loss: 0.4341 - val_acc: 0.7988\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4603 - acc: 0.7842 - val_loss: 0.4342 - val_acc: 0.7972\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4601 - acc: 0.7837 - val_loss: 0.4315 - val_acc: 0.7998\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4612 - acc: 0.7830 - val_loss: 0.4453 - val_acc: 0.7904\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4589 - acc: 0.7838 - val_loss: 0.4352 - val_acc: 0.7958\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4591 - acc: 0.7844 - val_loss: 0.4386 - val_acc: 0.7961\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4593 - acc: 0.7848 - val_loss: 0.4309 - val_acc: 0.7998\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4573 - acc: 0.7839 - val_loss: 0.4424 - val_acc: 0.7918\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4579 - acc: 0.7847 - val_loss: 0.4287 - val_acc: 0.8011\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4567 - acc: 0.7856 - val_loss: 0.4319 - val_acc: 0.7991\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4568 - acc: 0.7852 - val_loss: 0.4330 - val_acc: 0.7980\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4569 - acc: 0.7856 - val_loss: 0.4300 - val_acc: 0.7973\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4562 - acc: 0.7850 - val_loss: 0.4312 - val_acc: 0.7999\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4547 - acc: 0.7878 - val_loss: 0.4310 - val_acc: 0.7989\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4552 - acc: 0.7854 - val_loss: 0.4268 - val_acc: 0.8006\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4544 - acc: 0.7869 - val_loss: 0.4296 - val_acc: 0.7990\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4531 - acc: 0.7869 - val_loss: 0.4325 - val_acc: 0.7950\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4561 - acc: 0.7853 - val_loss: 0.4333 - val_acc: 0.7987\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.5802 - acc: 0.7095 - val_loss: 0.5053 - val_acc: 0.7544\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.5147 - acc: 0.7462 - val_loss: 0.4877 - val_acc: 0.7649\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.5032 - acc: 0.7527 - val_loss: 0.4748 - val_acc: 0.7725\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4922 - acc: 0.7611 - val_loss: 0.4621 - val_acc: 0.7789\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4849 - acc: 0.7672 - val_loss: 0.4615 - val_acc: 0.7821\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4788 - acc: 0.7699 - val_loss: 0.4504 - val_acc: 0.7862\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4741 - acc: 0.7713 - val_loss: 0.4529 - val_acc: 0.7854\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4716 - acc: 0.7761 - val_loss: 0.4563 - val_acc: 0.7804\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4705 - acc: 0.7756 - val_loss: 0.4458 - val_acc: 0.7896\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4685 - acc: 0.7783 - val_loss: 0.4414 - val_acc: 0.7944\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4657 - acc: 0.7786 - val_loss: 0.4394 - val_acc: 0.7957\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4661 - acc: 0.7789 - val_loss: 0.4437 - val_acc: 0.7912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4667 - acc: 0.7789 - val_loss: 0.4387 - val_acc: 0.7957\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4648 - acc: 0.7810 - val_loss: 0.4367 - val_acc: 0.7936\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4642 - acc: 0.7794 - val_loss: 0.4398 - val_acc: 0.7937\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4630 - acc: 0.7813 - val_loss: 0.4386 - val_acc: 0.7984\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 68us/sample - loss: 0.4615 - acc: 0.7827 - val_loss: 0.4390 - val_acc: 0.7972\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4622 - acc: 0.7816 - val_loss: 0.4379 - val_acc: 0.7975\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4626 - acc: 0.7804 - val_loss: 0.4341 - val_acc: 0.7983\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 70us/sample - loss: 0.4610 - acc: 0.7812 - val_loss: 0.4341 - val_acc: 0.8000\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4622 - acc: 0.7808 - val_loss: 0.4353 - val_acc: 0.7992\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4604 - acc: 0.7831 - val_loss: 0.4325 - val_acc: 0.7998\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4594 - acc: 0.7828 - val_loss: 0.4333 - val_acc: 0.7986\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4596 - acc: 0.7834 - val_loss: 0.4425 - val_acc: 0.7911\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4580 - acc: 0.7831 - val_loss: 0.4327 - val_acc: 0.7962\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4597 - acc: 0.7830 - val_loss: 0.4329 - val_acc: 0.8008\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4591 - acc: 0.7826 - val_loss: 0.4403 - val_acc: 0.7945\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4585 - acc: 0.7831 - val_loss: 0.4310 - val_acc: 0.7980\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4567 - acc: 0.7852 - val_loss: 0.4290 - val_acc: 0.8021\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 69us/sample - loss: 0.4563 - acc: 0.7848 - val_loss: 0.4352 - val_acc: 0.7981\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.5843 - acc: 0.7058 - val_loss: 0.5044 - val_acc: 0.7537\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.5212 - acc: 0.7450 - val_loss: 0.4887 - val_acc: 0.7642\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.5088 - acc: 0.7526 - val_loss: 0.4809 - val_acc: 0.7695\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.5008 - acc: 0.7563 - val_loss: 0.4693 - val_acc: 0.7771\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4950 - acc: 0.7606 - val_loss: 0.4660 - val_acc: 0.7792\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4915 - acc: 0.7629 - val_loss: 0.4619 - val_acc: 0.7827\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4847 - acc: 0.7666 - val_loss: 0.4650 - val_acc: 0.7814\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4827 - acc: 0.7689 - val_loss: 0.4526 - val_acc: 0.7868\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4782 - acc: 0.7724 - val_loss: 0.4572 - val_acc: 0.7879\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4767 - acc: 0.7732 - val_loss: 0.4429 - val_acc: 0.7918\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4754 - acc: 0.7738 - val_loss: 0.4404 - val_acc: 0.7951\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4713 - acc: 0.7769 - val_loss: 0.4631 - val_acc: 0.7782\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4714 - acc: 0.7770 - val_loss: 0.4344 - val_acc: 0.7981\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4690 - acc: 0.7776 - val_loss: 0.4356 - val_acc: 0.7990\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4683 - acc: 0.7788 - val_loss: 0.4347 - val_acc: 0.7977\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4664 - acc: 0.7805 - val_loss: 0.4399 - val_acc: 0.7957\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4673 - acc: 0.7796 - val_loss: 0.4354 - val_acc: 0.7963\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4672 - acc: 0.7783 - val_loss: 0.4306 - val_acc: 0.8019\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4646 - acc: 0.7807 - val_loss: 0.4361 - val_acc: 0.7962\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4646 - acc: 0.7801 - val_loss: 0.4430 - val_acc: 0.7984\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4641 - acc: 0.7805 - val_loss: 0.4293 - val_acc: 0.8043\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4644 - acc: 0.7807 - val_loss: 0.4321 - val_acc: 0.7981\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4625 - acc: 0.7820 - val_loss: 0.4401 - val_acc: 0.7950\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4633 - acc: 0.7801 - val_loss: 0.4342 - val_acc: 0.8005\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4621 - acc: 0.7826 - val_loss: 0.4449 - val_acc: 0.7903\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4618 - acc: 0.7830 - val_loss: 0.4268 - val_acc: 0.8053\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4612 - acc: 0.7834 - val_loss: 0.4306 - val_acc: 0.7984\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4615 - acc: 0.7820 - val_loss: 0.4266 - val_acc: 0.8023\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4613 - acc: 0.7833 - val_loss: 0.4346 - val_acc: 0.7951\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4607 - acc: 0.7848 - val_loss: 0.4245 - val_acc: 0.8035\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.6232 - acc: 0.6736 - val_loss: 0.5219 - val_acc: 0.7477\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5401 - acc: 0.7352 - val_loss: 0.5043 - val_acc: 0.7560\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5271 - acc: 0.7423 - val_loss: 0.5054 - val_acc: 0.7495\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5169 - acc: 0.7473 - val_loss: 0.4917 - val_acc: 0.7684\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5102 - acc: 0.7521 - val_loss: 0.4816 - val_acc: 0.7729\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5030 - acc: 0.7568 - val_loss: 0.4760 - val_acc: 0.7782\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4975 - acc: 0.7603 - val_loss: 0.4688 - val_acc: 0.7825\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4939 - acc: 0.7626 - val_loss: 0.4582 - val_acc: 0.7826\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4879 - acc: 0.7669 - val_loss: 0.4570 - val_acc: 0.7872\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4880 - acc: 0.7676 - val_loss: 0.4555 - val_acc: 0.7839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4864 - acc: 0.7686 - val_loss: 0.4541 - val_acc: 0.7887\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4832 - acc: 0.7702 - val_loss: 0.4471 - val_acc: 0.7942\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4800 - acc: 0.7709 - val_loss: 0.4464 - val_acc: 0.7908\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4790 - acc: 0.7734 - val_loss: 0.4456 - val_acc: 0.7966\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4776 - acc: 0.7736 - val_loss: 0.4447 - val_acc: 0.7955\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4789 - acc: 0.7716 - val_loss: 0.4411 - val_acc: 0.7921\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4771 - acc: 0.7741 - val_loss: 0.4425 - val_acc: 0.7914\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4748 - acc: 0.7759 - val_loss: 0.4384 - val_acc: 0.7968\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4745 - acc: 0.7767 - val_loss: 0.4488 - val_acc: 0.7928\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4763 - acc: 0.7749 - val_loss: 0.4467 - val_acc: 0.7904\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4762 - acc: 0.7754 - val_loss: 0.4374 - val_acc: 0.7958\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4759 - acc: 0.7734 - val_loss: 0.4430 - val_acc: 0.7940\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4729 - acc: 0.7774 - val_loss: 0.4426 - val_acc: 0.7952\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4731 - acc: 0.7781 - val_loss: 0.4401 - val_acc: 0.7972\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4700 - acc: 0.7770 - val_loss: 0.4327 - val_acc: 0.7980\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4713 - acc: 0.7765 - val_loss: 0.4402 - val_acc: 0.7972\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4726 - acc: 0.7771 - val_loss: 0.4348 - val_acc: 0.7988\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4720 - acc: 0.7783 - val_loss: 0.4370 - val_acc: 0.7961\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4712 - acc: 0.7769 - val_loss: 0.4334 - val_acc: 0.7982\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4711 - acc: 0.7774 - val_loss: 0.4358 - val_acc: 0.8009\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.5539 - acc: 0.7307 - val_loss: 0.4776 - val_acc: 0.7698\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4862 - acc: 0.7653 - val_loss: 0.4588 - val_acc: 0.7800\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4703 - acc: 0.7756 - val_loss: 0.4525 - val_acc: 0.7842\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4604 - acc: 0.7825 - val_loss: 0.4351 - val_acc: 0.7972\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4523 - acc: 0.7878 - val_loss: 0.4254 - val_acc: 0.8033\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4465 - acc: 0.7912 - val_loss: 0.4220 - val_acc: 0.8059\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4426 - acc: 0.7948 - val_loss: 0.4208 - val_acc: 0.8023\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4389 - acc: 0.7958 - val_loss: 0.4340 - val_acc: 0.7986\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4359 - acc: 0.7983 - val_loss: 0.4278 - val_acc: 0.8004\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4328 - acc: 0.8009 - val_loss: 0.4177 - val_acc: 0.8081\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4316 - acc: 0.7988 - val_loss: 0.4142 - val_acc: 0.8065\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4306 - acc: 0.8010 - val_loss: 0.4236 - val_acc: 0.8002\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4281 - acc: 0.8036 - val_loss: 0.4115 - val_acc: 0.8103\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4262 - acc: 0.8032 - val_loss: 0.4121 - val_acc: 0.8090\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4249 - acc: 0.8037 - val_loss: 0.4041 - val_acc: 0.8105\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4236 - acc: 0.8038 - val_loss: 0.4069 - val_acc: 0.8125\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4239 - acc: 0.8054 - val_loss: 0.4061 - val_acc: 0.8134\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4215 - acc: 0.8045 - val_loss: 0.4077 - val_acc: 0.8122\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4225 - acc: 0.8055 - val_loss: 0.4166 - val_acc: 0.8091\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4208 - acc: 0.8061 - val_loss: 0.4073 - val_acc: 0.8135\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4207 - acc: 0.8054 - val_loss: 0.4024 - val_acc: 0.8152\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4202 - acc: 0.8066 - val_loss: 0.4016 - val_acc: 0.8142\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4190 - acc: 0.8066 - val_loss: 0.4024 - val_acc: 0.8147\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4214 - acc: 0.8060 - val_loss: 0.3990 - val_acc: 0.8173\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4196 - acc: 0.8062 - val_loss: 0.4000 - val_acc: 0.8203\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4166 - acc: 0.8082 - val_loss: 0.3961 - val_acc: 0.8185\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4188 - acc: 0.8082 - val_loss: 0.4002 - val_acc: 0.8160\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4162 - acc: 0.8092 - val_loss: 0.3963 - val_acc: 0.8177\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4153 - acc: 0.8088 - val_loss: 0.4068 - val_acc: 0.8128\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4156 - acc: 0.8090 - val_loss: 0.4062 - val_acc: 0.8145\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.5569 - acc: 0.7279 - val_loss: 0.4807 - val_acc: 0.7706\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4874 - acc: 0.7660 - val_loss: 0.4597 - val_acc: 0.7819\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4692 - acc: 0.7772 - val_loss: 0.4411 - val_acc: 0.7929\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4571 - acc: 0.7845 - val_loss: 0.4301 - val_acc: 0.7987\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4494 - acc: 0.7900 - val_loss: 0.4382 - val_acc: 0.7952\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4429 - acc: 0.7937 - val_loss: 0.4461 - val_acc: 0.7886\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4410 - acc: 0.7924 - val_loss: 0.4221 - val_acc: 0.8007\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4363 - acc: 0.7961 - val_loss: 0.4156 - val_acc: 0.8073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4349 - acc: 0.7979 - val_loss: 0.4314 - val_acc: 0.7965\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4313 - acc: 0.7996 - val_loss: 0.4157 - val_acc: 0.8059\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4300 - acc: 0.8012 - val_loss: 0.4165 - val_acc: 0.8064\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4273 - acc: 0.8022 - val_loss: 0.4111 - val_acc: 0.8077\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4285 - acc: 0.8027 - val_loss: 0.4088 - val_acc: 0.8105\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4252 - acc: 0.8027 - val_loss: 0.4158 - val_acc: 0.8092\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4237 - acc: 0.8041 - val_loss: 0.4098 - val_acc: 0.8098\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4243 - acc: 0.8034 - val_loss: 0.4021 - val_acc: 0.8143\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4220 - acc: 0.8056 - val_loss: 0.4026 - val_acc: 0.8144\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4206 - acc: 0.8062 - val_loss: 0.4082 - val_acc: 0.8120\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4192 - acc: 0.8060 - val_loss: 0.4029 - val_acc: 0.8152\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4204 - acc: 0.8065 - val_loss: 0.4031 - val_acc: 0.8132\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4177 - acc: 0.8082 - val_loss: 0.4014 - val_acc: 0.8154\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4188 - acc: 0.8064 - val_loss: 0.4049 - val_acc: 0.8132\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4158 - acc: 0.8090 - val_loss: 0.4041 - val_acc: 0.8131\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4173 - acc: 0.8062 - val_loss: 0.4033 - val_acc: 0.8136\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4160 - acc: 0.8086 - val_loss: 0.4205 - val_acc: 0.8056\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4163 - acc: 0.8090 - val_loss: 0.4085 - val_acc: 0.8102\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4138 - acc: 0.8086 - val_loss: 0.4061 - val_acc: 0.8129\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 6s 71us/sample - loss: 0.4157 - acc: 0.8091 - val_loss: 0.4064 - val_acc: 0.8118\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 72us/sample - loss: 0.4135 - acc: 0.8102 - val_loss: 0.4002 - val_acc: 0.8163\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 73us/sample - loss: 0.4134 - acc: 0.8102 - val_loss: 0.4075 - val_acc: 0.8123\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.5463 - acc: 0.7326 - val_loss: 0.4810 - val_acc: 0.7669\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4871 - acc: 0.7657 - val_loss: 0.4543 - val_acc: 0.7841\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4720 - acc: 0.7764 - val_loss: 0.4426 - val_acc: 0.7926\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4599 - acc: 0.7845 - val_loss: 0.4301 - val_acc: 0.7965\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4499 - acc: 0.7890 - val_loss: 0.4272 - val_acc: 0.8045\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4439 - acc: 0.7944 - val_loss: 0.4345 - val_acc: 0.8001\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4382 - acc: 0.7973 - val_loss: 0.4213 - val_acc: 0.8056\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4335 - acc: 0.8003 - val_loss: 0.4096 - val_acc: 0.8124\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4312 - acc: 0.8015 - val_loss: 0.4024 - val_acc: 0.8176\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4268 - acc: 0.8033 - val_loss: 0.4107 - val_acc: 0.8108\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4275 - acc: 0.8037 - val_loss: 0.4029 - val_acc: 0.8169\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4236 - acc: 0.8053 - val_loss: 0.4179 - val_acc: 0.8081\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4230 - acc: 0.8058 - val_loss: 0.4048 - val_acc: 0.8145\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4200 - acc: 0.8063 - val_loss: 0.3980 - val_acc: 0.8178\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4208 - acc: 0.8064 - val_loss: 0.3953 - val_acc: 0.8169\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4181 - acc: 0.8089 - val_loss: 0.3961 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4174 - acc: 0.8099 - val_loss: 0.3988 - val_acc: 0.8188\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4158 - acc: 0.8100 - val_loss: 0.4127 - val_acc: 0.8101\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4140 - acc: 0.8110 - val_loss: 0.3982 - val_acc: 0.8181\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4141 - acc: 0.8115 - val_loss: 0.3915 - val_acc: 0.8210\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4147 - acc: 0.8114 - val_loss: 0.3882 - val_acc: 0.8247\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4117 - acc: 0.8112 - val_loss: 0.3951 - val_acc: 0.8217\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4099 - acc: 0.8132 - val_loss: 0.3898 - val_acc: 0.8257\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4124 - acc: 0.8116 - val_loss: 0.3925 - val_acc: 0.8220\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4115 - acc: 0.8126 - val_loss: 0.3896 - val_acc: 0.8221\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4097 - acc: 0.8142 - val_loss: 0.3960 - val_acc: 0.8191\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4097 - acc: 0.8133 - val_loss: 0.3907 - val_acc: 0.8206\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4106 - acc: 0.8122 - val_loss: 0.3872 - val_acc: 0.8238\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4106 - acc: 0.8124 - val_loss: 0.3897 - val_acc: 0.8249\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4099 - acc: 0.8147 - val_loss: 0.3928 - val_acc: 0.8194\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.5868 - acc: 0.7104 - val_loss: 0.4866 - val_acc: 0.7625\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.5004 - acc: 0.7589 - val_loss: 0.4674 - val_acc: 0.7750\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4809 - acc: 0.7696 - val_loss: 0.4477 - val_acc: 0.7897\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4677 - acc: 0.7787 - val_loss: 0.4345 - val_acc: 0.7988\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4571 - acc: 0.7866 - val_loss: 0.4381 - val_acc: 0.7932\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4487 - acc: 0.7901 - val_loss: 0.4228 - val_acc: 0.8025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4431 - acc: 0.7938 - val_loss: 0.4222 - val_acc: 0.8045\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4397 - acc: 0.7953 - val_loss: 0.4215 - val_acc: 0.8030\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4353 - acc: 0.7982 - val_loss: 0.4086 - val_acc: 0.8120\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4339 - acc: 0.7993 - val_loss: 0.4208 - val_acc: 0.8026\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4299 - acc: 0.8011 - val_loss: 0.4035 - val_acc: 0.8158\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4299 - acc: 0.8002 - val_loss: 0.4044 - val_acc: 0.8108\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4251 - acc: 0.8044 - val_loss: 0.4158 - val_acc: 0.8083\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4238 - acc: 0.8038 - val_loss: 0.3953 - val_acc: 0.8191\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4242 - acc: 0.8052 - val_loss: 0.3936 - val_acc: 0.8209\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4214 - acc: 0.8052 - val_loss: 0.4030 - val_acc: 0.8159\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4210 - acc: 0.8043 - val_loss: 0.3966 - val_acc: 0.8204\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4199 - acc: 0.8078 - val_loss: 0.3917 - val_acc: 0.8213\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4189 - acc: 0.8071 - val_loss: 0.3981 - val_acc: 0.8166\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4170 - acc: 0.8075 - val_loss: 0.3920 - val_acc: 0.8206\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4164 - acc: 0.8079 - val_loss: 0.3897 - val_acc: 0.8236\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4171 - acc: 0.8082 - val_loss: 0.3961 - val_acc: 0.8173\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4152 - acc: 0.8108 - val_loss: 0.4101 - val_acc: 0.8116\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4161 - acc: 0.8081 - val_loss: 0.3931 - val_acc: 0.8209\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4135 - acc: 0.8114 - val_loss: 0.3955 - val_acc: 0.8189\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4136 - acc: 0.8098 - val_loss: 0.3949 - val_acc: 0.8164\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4141 - acc: 0.8101 - val_loss: 0.3876 - val_acc: 0.8213\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4116 - acc: 0.8123 - val_loss: 0.4018 - val_acc: 0.8144\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4099 - acc: 0.8124 - val_loss: 0.3995 - val_acc: 0.8167\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4098 - acc: 0.8134 - val_loss: 0.3857 - val_acc: 0.8252\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.5267 - acc: 0.7443 - val_loss: 0.4788 - val_acc: 0.7717\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4674 - acc: 0.7777 - val_loss: 0.4435 - val_acc: 0.7907\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4514 - acc: 0.7886 - val_loss: 0.4301 - val_acc: 0.7975\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4430 - acc: 0.7928 - val_loss: 0.4273 - val_acc: 0.8024\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4348 - acc: 0.7976 - val_loss: 0.4210 - val_acc: 0.8031\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4305 - acc: 0.8008 - val_loss: 0.4094 - val_acc: 0.8099\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4269 - acc: 0.8016 - val_loss: 0.4052 - val_acc: 0.8145\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4239 - acc: 0.8051 - val_loss: 0.4234 - val_acc: 0.8031\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4217 - acc: 0.8052 - val_loss: 0.4039 - val_acc: 0.8170\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4175 - acc: 0.8082 - val_loss: 0.4231 - val_acc: 0.8041\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4165 - acc: 0.8082 - val_loss: 0.4453 - val_acc: 0.7911\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4144 - acc: 0.8095 - val_loss: 0.3998 - val_acc: 0.8170\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4138 - acc: 0.8094 - val_loss: 0.4059 - val_acc: 0.8153\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4107 - acc: 0.8122 - val_loss: 0.3953 - val_acc: 0.8216\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4099 - acc: 0.8120 - val_loss: 0.4056 - val_acc: 0.8129\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4071 - acc: 0.8139 - val_loss: 0.3989 - val_acc: 0.8156\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4067 - acc: 0.8126 - val_loss: 0.4019 - val_acc: 0.8126\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4058 - acc: 0.8142 - val_loss: 0.4075 - val_acc: 0.8153\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4038 - acc: 0.8148 - val_loss: 0.3945 - val_acc: 0.8188\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4051 - acc: 0.8144 - val_loss: 0.3911 - val_acc: 0.8206\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4035 - acc: 0.8150 - val_loss: 0.3927 - val_acc: 0.8202\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4018 - acc: 0.8162 - val_loss: 0.4087 - val_acc: 0.8119\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4015 - acc: 0.8174 - val_loss: 0.4084 - val_acc: 0.8082\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4011 - acc: 0.8166 - val_loss: 0.4022 - val_acc: 0.8155\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.4001 - acc: 0.8183 - val_loss: 0.4160 - val_acc: 0.8059\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3987 - acc: 0.8187 - val_loss: 0.3896 - val_acc: 0.8228\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3991 - acc: 0.8192 - val_loss: 0.3942 - val_acc: 0.8217\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3987 - acc: 0.8176 - val_loss: 0.3870 - val_acc: 0.8227\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3980 - acc: 0.8189 - val_loss: 0.3863 - val_acc: 0.8250\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3965 - acc: 0.8191 - val_loss: 0.3908 - val_acc: 0.8234\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.5159 - acc: 0.7487 - val_loss: 0.4717 - val_acc: 0.7768\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4712 - acc: 0.7769 - val_loss: 0.4478 - val_acc: 0.7925\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4557 - acc: 0.7858 - val_loss: 0.4282 - val_acc: 0.8000\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4457 - acc: 0.7919 - val_loss: 0.4553 - val_acc: 0.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4386 - acc: 0.7966 - val_loss: 0.4411 - val_acc: 0.7930\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4321 - acc: 0.8001 - val_loss: 0.4130 - val_acc: 0.8097\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4288 - acc: 0.8021 - val_loss: 0.4106 - val_acc: 0.8105\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4245 - acc: 0.8037 - val_loss: 0.4201 - val_acc: 0.8061\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4231 - acc: 0.8053 - val_loss: 0.4115 - val_acc: 0.8084\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4211 - acc: 0.8062 - val_loss: 0.3976 - val_acc: 0.8194\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4172 - acc: 0.8081 - val_loss: 0.4095 - val_acc: 0.8097\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4155 - acc: 0.8090 - val_loss: 0.4019 - val_acc: 0.8176\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4164 - acc: 0.8088 - val_loss: 0.4022 - val_acc: 0.8164\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4144 - acc: 0.8101 - val_loss: 0.4255 - val_acc: 0.8034\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4120 - acc: 0.8120 - val_loss: 0.3959 - val_acc: 0.8206\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4126 - acc: 0.8106 - val_loss: 0.4062 - val_acc: 0.8126\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4104 - acc: 0.8109 - val_loss: 0.3936 - val_acc: 0.8200\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4090 - acc: 0.8123 - val_loss: 0.4009 - val_acc: 0.8161\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.4079 - acc: 0.8135 - val_loss: 0.3929 - val_acc: 0.8205\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4071 - acc: 0.8136 - val_loss: 0.3916 - val_acc: 0.8213\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.4065 - acc: 0.8145 - val_loss: 0.4119 - val_acc: 0.8095\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4039 - acc: 0.8148 - val_loss: 0.3977 - val_acc: 0.8183\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 80us/sample - loss: 0.4040 - acc: 0.8144 - val_loss: 0.4000 - val_acc: 0.8161\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4045 - acc: 0.8151 - val_loss: 0.3865 - val_acc: 0.8261\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4034 - acc: 0.8159 - val_loss: 0.3900 - val_acc: 0.8235\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4023 - acc: 0.8168 - val_loss: 0.3883 - val_acc: 0.8234\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.4014 - acc: 0.8161 - val_loss: 0.4121 - val_acc: 0.8084\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4008 - acc: 0.8178 - val_loss: 0.3851 - val_acc: 0.8262\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3998 - acc: 0.8180 - val_loss: 0.3912 - val_acc: 0.8216\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 6s 75us/sample - loss: 0.3989 - acc: 0.8175 - val_loss: 0.3840 - val_acc: 0.8266\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.5541 - acc: 0.7335 - val_loss: 0.4640 - val_acc: 0.7796\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4768 - acc: 0.7728 - val_loss: 0.4514 - val_acc: 0.7883\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4563 - acc: 0.7838 - val_loss: 0.4542 - val_acc: 0.7883\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4416 - acc: 0.7937 - val_loss: 0.4147 - val_acc: 0.8052\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4331 - acc: 0.7986 - val_loss: 0.4309 - val_acc: 0.8032\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4266 - acc: 0.8034 - val_loss: 0.4271 - val_acc: 0.8043\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4204 - acc: 0.8074 - val_loss: 0.3975 - val_acc: 0.8194\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4171 - acc: 0.8077 - val_loss: 0.4025 - val_acc: 0.8167\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4140 - acc: 0.8118 - val_loss: 0.4071 - val_acc: 0.8112\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4085 - acc: 0.8129 - val_loss: 0.3970 - val_acc: 0.8196\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4064 - acc: 0.8154 - val_loss: 0.3891 - val_acc: 0.8234\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4057 - acc: 0.8157 - val_loss: 0.4157 - val_acc: 0.8098\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4011 - acc: 0.8165 - val_loss: 0.4027 - val_acc: 0.8150\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4010 - acc: 0.8174 - val_loss: 0.3924 - val_acc: 0.8223\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3981 - acc: 0.8198 - val_loss: 0.3855 - val_acc: 0.8249\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3963 - acc: 0.8201 - val_loss: 0.3861 - val_acc: 0.8223\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3955 - acc: 0.8201 - val_loss: 0.3822 - val_acc: 0.8260\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3948 - acc: 0.8225 - val_loss: 0.4036 - val_acc: 0.8152\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3916 - acc: 0.8222 - val_loss: 0.3777 - val_acc: 0.8310\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3923 - acc: 0.8227 - val_loss: 0.3832 - val_acc: 0.8256\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3903 - acc: 0.8231 - val_loss: 0.3873 - val_acc: 0.8215\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.3881 - acc: 0.8237 - val_loss: 0.3984 - val_acc: 0.8164\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3878 - acc: 0.8241 - val_loss: 0.3793 - val_acc: 0.8296\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3870 - acc: 0.8238 - val_loss: 0.3750 - val_acc: 0.8309\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3858 - acc: 0.8245 - val_loss: 0.3737 - val_acc: 0.8307\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 88us/sample - loss: 0.3852 - acc: 0.8257 - val_loss: 0.3747 - val_acc: 0.8289\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3856 - acc: 0.8261 - val_loss: 0.3758 - val_acc: 0.8303\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3833 - acc: 0.8276 - val_loss: 0.3848 - val_acc: 0.8237\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3827 - acc: 0.8272 - val_loss: 0.3790 - val_acc: 0.8280\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.3818 - acc: 0.8284 - val_loss: 0.3801 - val_acc: 0.8281\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.5330 - acc: 0.7404 - val_loss: 0.4659 - val_acc: 0.7774\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4744 - acc: 0.7743 - val_loss: 0.4398 - val_acc: 0.7914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4514 - acc: 0.7894 - val_loss: 0.4213 - val_acc: 0.8054\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4400 - acc: 0.7952 - val_loss: 0.4089 - val_acc: 0.8117\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.4285 - acc: 0.8034 - val_loss: 0.4112 - val_acc: 0.8088\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 100us/sample - loss: 0.4230 - acc: 0.8052 - val_loss: 0.4004 - val_acc: 0.8175\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4169 - acc: 0.8086 - val_loss: 0.3969 - val_acc: 0.8170\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4139 - acc: 0.8107 - val_loss: 0.3985 - val_acc: 0.8134\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4089 - acc: 0.8132 - val_loss: 0.3922 - val_acc: 0.8205\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4061 - acc: 0.8165 - val_loss: 0.3922 - val_acc: 0.8209\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4040 - acc: 0.8158 - val_loss: 0.3905 - val_acc: 0.8209\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4022 - acc: 0.8179 - val_loss: 0.3884 - val_acc: 0.8202\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3983 - acc: 0.8203 - val_loss: 0.3895 - val_acc: 0.8227\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3970 - acc: 0.8197 - val_loss: 0.3826 - val_acc: 0.8245\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3955 - acc: 0.8197 - val_loss: 0.3966 - val_acc: 0.8169\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3936 - acc: 0.8220 - val_loss: 0.4122 - val_acc: 0.8083\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3922 - acc: 0.8219 - val_loss: 0.3780 - val_acc: 0.8281\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3903 - acc: 0.8238 - val_loss: 0.4086 - val_acc: 0.8113\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3880 - acc: 0.8253 - val_loss: 0.3815 - val_acc: 0.8253\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3867 - acc: 0.8246 - val_loss: 0.3834 - val_acc: 0.8281\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.3877 - acc: 0.8254 - val_loss: 0.3900 - val_acc: 0.8235\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3837 - acc: 0.8263 - val_loss: 0.3785 - val_acc: 0.8256\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3833 - acc: 0.8270 - val_loss: 0.3762 - val_acc: 0.8278\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3856 - acc: 0.8260 - val_loss: 0.3773 - val_acc: 0.8309\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3821 - acc: 0.8278 - val_loss: 0.3814 - val_acc: 0.8246\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3799 - acc: 0.8291 - val_loss: 0.3754 - val_acc: 0.8302\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3799 - acc: 0.8291 - val_loss: 0.4102 - val_acc: 0.8112\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3785 - acc: 0.8294 - val_loss: 0.3790 - val_acc: 0.8286\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3789 - acc: 0.8306 - val_loss: 0.3794 - val_acc: 0.8275\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3770 - acc: 0.8312 - val_loss: 0.3722 - val_acc: 0.8321\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.5230 - acc: 0.7460 - val_loss: 0.4627 - val_acc: 0.7818\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4657 - acc: 0.7763 - val_loss: 0.4446 - val_acc: 0.7899\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4507 - acc: 0.7879 - val_loss: 0.4285 - val_acc: 0.7994\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4401 - acc: 0.7950 - val_loss: 0.4388 - val_acc: 0.7930\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4332 - acc: 0.7997 - val_loss: 0.4691 - val_acc: 0.7829\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4289 - acc: 0.8016 - val_loss: 0.4202 - val_acc: 0.8019\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4252 - acc: 0.8045 - val_loss: 0.4077 - val_acc: 0.8084\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4223 - acc: 0.8051 - val_loss: 0.4369 - val_acc: 0.8004\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4200 - acc: 0.8070 - val_loss: 0.4160 - val_acc: 0.8115\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4179 - acc: 0.8084 - val_loss: 0.4488 - val_acc: 0.7853\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4165 - acc: 0.8082 - val_loss: 0.3981 - val_acc: 0.8158\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4128 - acc: 0.8103 - val_loss: 0.4117 - val_acc: 0.8106\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4125 - acc: 0.8105 - val_loss: 0.4157 - val_acc: 0.8100\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4111 - acc: 0.8118 - val_loss: 0.4002 - val_acc: 0.8171\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4110 - acc: 0.8115 - val_loss: 0.4078 - val_acc: 0.8137\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4062 - acc: 0.8151 - val_loss: 0.3943 - val_acc: 0.8188\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4058 - acc: 0.8145 - val_loss: 0.3975 - val_acc: 0.8183\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4051 - acc: 0.8155 - val_loss: 0.4120 - val_acc: 0.8109\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4048 - acc: 0.8138 - val_loss: 0.3917 - val_acc: 0.8186\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4007 - acc: 0.8168 - val_loss: 0.3965 - val_acc: 0.8173\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.4005 - acc: 0.8171 - val_loss: 0.3936 - val_acc: 0.8194\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3998 - acc: 0.8178 - val_loss: 0.3986 - val_acc: 0.8164\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3994 - acc: 0.8173 - val_loss: 0.3984 - val_acc: 0.8188\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3970 - acc: 0.8186 - val_loss: 0.3936 - val_acc: 0.8194\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3956 - acc: 0.8192 - val_loss: 0.3899 - val_acc: 0.8235\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3975 - acc: 0.8181 - val_loss: 0.3861 - val_acc: 0.8223\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3971 - acc: 0.8192 - val_loss: 0.4135 - val_acc: 0.8097\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3945 - acc: 0.8201 - val_loss: 0.3902 - val_acc: 0.8173\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 77us/sample - loss: 0.3947 - acc: 0.8208 - val_loss: 0.3850 - val_acc: 0.8231\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.3935 - acc: 0.8203 - val_loss: 0.3872 - val_acc: 0.8211\n",
      "Train on 85000 samples, validate on 20000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.5241 - acc: 0.7454 - val_loss: 0.4647 - val_acc: 0.7796\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4684 - acc: 0.7776 - val_loss: 0.4421 - val_acc: 0.7929\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4504 - acc: 0.7879 - val_loss: 0.4938 - val_acc: 0.7603\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4399 - acc: 0.7953 - val_loss: 0.4288 - val_acc: 0.7993\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4319 - acc: 0.7982 - val_loss: 0.4201 - val_acc: 0.8044\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4269 - acc: 0.8020 - val_loss: 0.4231 - val_acc: 0.8065\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 7s 79us/sample - loss: 0.4238 - acc: 0.8046 - val_loss: 0.4082 - val_acc: 0.8123\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4187 - acc: 0.8063 - val_loss: 0.4064 - val_acc: 0.8131\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4156 - acc: 0.8097 - val_loss: 0.4064 - val_acc: 0.8134\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4152 - acc: 0.8110 - val_loss: 0.3950 - val_acc: 0.8199\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4117 - acc: 0.8119 - val_loss: 0.4019 - val_acc: 0.8170\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4109 - acc: 0.8115 - val_loss: 0.3965 - val_acc: 0.8178\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4084 - acc: 0.8131 - val_loss: 0.4038 - val_acc: 0.8132\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4070 - acc: 0.8132 - val_loss: 0.4089 - val_acc: 0.8120\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4063 - acc: 0.8150 - val_loss: 0.3983 - val_acc: 0.8184\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 7s 78us/sample - loss: 0.4049 - acc: 0.8143 - val_loss: 0.3902 - val_acc: 0.8227\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4016 - acc: 0.8171 - val_loss: 0.4435 - val_acc: 0.7915\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.4019 - acc: 0.8165 - val_loss: 0.3979 - val_acc: 0.8203\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 7s 84us/sample - loss: 0.4006 - acc: 0.8183 - val_loss: 0.4077 - val_acc: 0.8133\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.3985 - acc: 0.8180 - val_loss: 0.3888 - val_acc: 0.8220\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 7s 85us/sample - loss: 0.3987 - acc: 0.8175 - val_loss: 0.3918 - val_acc: 0.8237\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 7s 86us/sample - loss: 0.3970 - acc: 0.8192 - val_loss: 0.3860 - val_acc: 0.8227\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3976 - acc: 0.8186 - val_loss: 0.3865 - val_acc: 0.8220\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3932 - acc: 0.8217 - val_loss: 0.3849 - val_acc: 0.8256\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3954 - acc: 0.8204 - val_loss: 0.3848 - val_acc: 0.8252\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3939 - acc: 0.8210 - val_loss: 0.3865 - val_acc: 0.8250\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3921 - acc: 0.8223 - val_loss: 0.3865 - val_acc: 0.8232\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3928 - acc: 0.8224 - val_loss: 0.4037 - val_acc: 0.8135\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 7s 83us/sample - loss: 0.3923 - acc: 0.8218 - val_loss: 0.4037 - val_acc: 0.8157\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 7s 82us/sample - loss: 0.3911 - acc: 0.8219 - val_loss: 0.3841 - val_acc: 0.8266\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.5151 - acc: 0.7496 - val_loss: 0.4698 - val_acc: 0.7764\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4552 - acc: 0.7858 - val_loss: 0.4321 - val_acc: 0.7972\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4349 - acc: 0.7974 - val_loss: 0.4157 - val_acc: 0.8090\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4238 - acc: 0.8050 - val_loss: 0.4018 - val_acc: 0.8165\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4184 - acc: 0.8082 - val_loss: 0.4191 - val_acc: 0.8083\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4128 - acc: 0.8114 - val_loss: 0.4042 - val_acc: 0.8153\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4078 - acc: 0.8129 - val_loss: 0.3991 - val_acc: 0.8158\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4043 - acc: 0.8159 - val_loss: 0.4489 - val_acc: 0.7929\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4021 - acc: 0.8175 - val_loss: 0.3989 - val_acc: 0.8194\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3971 - acc: 0.8201 - val_loss: 0.4135 - val_acc: 0.8074\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3968 - acc: 0.8193 - val_loss: 0.3859 - val_acc: 0.8242\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3927 - acc: 0.8222 - val_loss: 0.4236 - val_acc: 0.8029\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3915 - acc: 0.8225 - val_loss: 0.3918 - val_acc: 0.8196\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3876 - acc: 0.8243 - val_loss: 0.3861 - val_acc: 0.8262\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3862 - acc: 0.8247 - val_loss: 0.3815 - val_acc: 0.8267\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3850 - acc: 0.8263 - val_loss: 0.3871 - val_acc: 0.8223\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3824 - acc: 0.8278 - val_loss: 0.3900 - val_acc: 0.8225\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3808 - acc: 0.8290 - val_loss: 0.3860 - val_acc: 0.8245\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3805 - acc: 0.8277 - val_loss: 0.3748 - val_acc: 0.8292\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3768 - acc: 0.8311 - val_loss: 0.3962 - val_acc: 0.8189\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3753 - acc: 0.8305 - val_loss: 0.3888 - val_acc: 0.8227\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3754 - acc: 0.8306 - val_loss: 0.3866 - val_acc: 0.8236\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3747 - acc: 0.8313 - val_loss: 0.3866 - val_acc: 0.8233\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3704 - acc: 0.8334 - val_loss: 0.3704 - val_acc: 0.8349\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3726 - acc: 0.8335 - val_loss: 0.3836 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3698 - acc: 0.8356 - val_loss: 0.3758 - val_acc: 0.8293\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3692 - acc: 0.8346 - val_loss: 0.3685 - val_acc: 0.8356\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3672 - acc: 0.8354 - val_loss: 0.3771 - val_acc: 0.8293\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3647 - acc: 0.8368 - val_loss: 0.3710 - val_acc: 0.8314\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3675 - acc: 0.8366 - val_loss: 0.3675 - val_acc: 0.8341\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.5289 - acc: 0.7448 - val_loss: 0.4819 - val_acc: 0.7727\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4652 - acc: 0.7797 - val_loss: 0.4329 - val_acc: 0.7996\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4437 - acc: 0.7932 - val_loss: 0.4215 - val_acc: 0.8041\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4292 - acc: 0.8028 - val_loss: 0.4144 - val_acc: 0.8099\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4213 - acc: 0.8057 - val_loss: 0.4093 - val_acc: 0.8100\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4154 - acc: 0.8114 - val_loss: 0.3970 - val_acc: 0.8177\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4095 - acc: 0.8130 - val_loss: 0.4021 - val_acc: 0.8135\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4044 - acc: 0.8167 - val_loss: 0.3890 - val_acc: 0.8220\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4016 - acc: 0.8183 - val_loss: 0.3879 - val_acc: 0.8227\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3976 - acc: 0.8187 - val_loss: 0.3886 - val_acc: 0.8215\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3930 - acc: 0.8231 - val_loss: 0.3948 - val_acc: 0.8176\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3893 - acc: 0.8233 - val_loss: 0.3839 - val_acc: 0.8244\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3877 - acc: 0.8248 - val_loss: 0.3825 - val_acc: 0.8246\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3845 - acc: 0.8267 - val_loss: 0.3902 - val_acc: 0.8231\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3847 - acc: 0.8266 - val_loss: 0.3808 - val_acc: 0.8250\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3808 - acc: 0.8275 - val_loss: 0.3800 - val_acc: 0.8273\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3795 - acc: 0.8285 - val_loss: 0.3761 - val_acc: 0.8284\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3771 - acc: 0.8293 - val_loss: 0.3742 - val_acc: 0.8298\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3740 - acc: 0.8324 - val_loss: 0.3746 - val_acc: 0.8296\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3713 - acc: 0.8335 - val_loss: 0.3703 - val_acc: 0.8318\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3700 - acc: 0.8340 - val_loss: 0.3780 - val_acc: 0.8284\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3698 - acc: 0.8342 - val_loss: 0.4279 - val_acc: 0.8032\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3677 - acc: 0.8356 - val_loss: 0.3732 - val_acc: 0.8331\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3672 - acc: 0.8362 - val_loss: 0.3739 - val_acc: 0.8293\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3656 - acc: 0.8363 - val_loss: 0.3674 - val_acc: 0.8317\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3630 - acc: 0.8371 - val_loss: 0.3699 - val_acc: 0.8322\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3612 - acc: 0.8393 - val_loss: 0.4070 - val_acc: 0.8141\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3619 - acc: 0.8380 - val_loss: 0.3696 - val_acc: 0.8331\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3613 - acc: 0.8386 - val_loss: 0.3712 - val_acc: 0.8324\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3599 - acc: 0.8400 - val_loss: 0.3694 - val_acc: 0.8332\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.5500 - acc: 0.7297 - val_loss: 0.4882 - val_acc: 0.7631\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4990 - acc: 0.7569 - val_loss: 0.4697 - val_acc: 0.7757\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4856 - acc: 0.7655 - val_loss: 0.4538 - val_acc: 0.7842\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4738 - acc: 0.7741 - val_loss: 0.4500 - val_acc: 0.7861\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4635 - acc: 0.7798 - val_loss: 0.4360 - val_acc: 0.7983\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4606 - acc: 0.7816 - val_loss: 0.4360 - val_acc: 0.7973\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4544 - acc: 0.7862 - val_loss: 0.4459 - val_acc: 0.7881\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4501 - acc: 0.7888 - val_loss: 0.4608 - val_acc: 0.7844\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4455 - acc: 0.7914 - val_loss: 0.4332 - val_acc: 0.7994\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4440 - acc: 0.7926 - val_loss: 0.4203 - val_acc: 0.8041\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4403 - acc: 0.7953 - val_loss: 0.4247 - val_acc: 0.8041\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4394 - acc: 0.7951 - val_loss: 0.4173 - val_acc: 0.8055\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4357 - acc: 0.7974 - val_loss: 0.4212 - val_acc: 0.8038\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4320 - acc: 0.8002 - val_loss: 0.4160 - val_acc: 0.8069\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4320 - acc: 0.7991 - val_loss: 0.4195 - val_acc: 0.8061\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4313 - acc: 0.8000 - val_loss: 0.4341 - val_acc: 0.7969\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4278 - acc: 0.8025 - val_loss: 0.4117 - val_acc: 0.8102\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4262 - acc: 0.8031 - val_loss: 0.4256 - val_acc: 0.8031\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4256 - acc: 0.8042 - val_loss: 0.4125 - val_acc: 0.8117\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4246 - acc: 0.8036 - val_loss: 0.4044 - val_acc: 0.8166\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4237 - acc: 0.8043 - val_loss: 0.4183 - val_acc: 0.8070\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4215 - acc: 0.8075 - val_loss: 0.4136 - val_acc: 0.8090\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4200 - acc: 0.8055 - val_loss: 0.4090 - val_acc: 0.8134\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4207 - acc: 0.8064 - val_loss: 0.4198 - val_acc: 0.8059\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4201 - acc: 0.8061 - val_loss: 0.4215 - val_acc: 0.8020\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4188 - acc: 0.8070 - val_loss: 0.4073 - val_acc: 0.8106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4176 - acc: 0.8076 - val_loss: 0.4137 - val_acc: 0.8092\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4155 - acc: 0.8085 - val_loss: 0.4183 - val_acc: 0.8059\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4136 - acc: 0.8099 - val_loss: 0.4173 - val_acc: 0.8058\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4143 - acc: 0.8098 - val_loss: 0.4075 - val_acc: 0.8117\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 12s 143us/sample - loss: 0.5475 - acc: 0.7294 - val_loss: 0.4907 - val_acc: 0.7612\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4945 - acc: 0.7613 - val_loss: 0.4592 - val_acc: 0.7838\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4759 - acc: 0.7727 - val_loss: 0.4514 - val_acc: 0.7861\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4622 - acc: 0.7817 - val_loss: 0.4374 - val_acc: 0.7947\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4563 - acc: 0.7862 - val_loss: 0.4293 - val_acc: 0.8002\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4501 - acc: 0.7893 - val_loss: 0.4284 - val_acc: 0.8007\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4458 - acc: 0.7922 - val_loss: 0.4295 - val_acc: 0.8033\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4428 - acc: 0.7938 - val_loss: 0.4275 - val_acc: 0.8036\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4399 - acc: 0.7952 - val_loss: 0.4197 - val_acc: 0.8069\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4370 - acc: 0.7960 - val_loss: 0.4302 - val_acc: 0.7996\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4344 - acc: 0.7975 - val_loss: 0.4220 - val_acc: 0.8050\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4338 - acc: 0.7984 - val_loss: 0.4144 - val_acc: 0.8101\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4312 - acc: 0.8008 - val_loss: 0.4186 - val_acc: 0.8082\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4307 - acc: 0.8014 - val_loss: 0.4156 - val_acc: 0.8092\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4278 - acc: 0.8018 - val_loss: 0.4155 - val_acc: 0.8088\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4270 - acc: 0.8019 - val_loss: 0.4132 - val_acc: 0.8116\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4250 - acc: 0.8033 - val_loss: 0.4178 - val_acc: 0.8064\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4261 - acc: 0.8043 - val_loss: 0.4080 - val_acc: 0.8131\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4237 - acc: 0.8028 - val_loss: 0.4069 - val_acc: 0.8130\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4228 - acc: 0.8053 - val_loss: 0.4075 - val_acc: 0.8142\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4200 - acc: 0.8072 - val_loss: 0.4089 - val_acc: 0.8129\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4216 - acc: 0.8060 - val_loss: 0.4054 - val_acc: 0.8154\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4196 - acc: 0.8061 - val_loss: 0.4089 - val_acc: 0.8125\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4176 - acc: 0.8069 - val_loss: 0.4164 - val_acc: 0.8080\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4159 - acc: 0.8087 - val_loss: 0.4072 - val_acc: 0.8141\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4144 - acc: 0.8088 - val_loss: 0.4030 - val_acc: 0.8174\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4154 - acc: 0.8098 - val_loss: 0.4097 - val_acc: 0.8113\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4147 - acc: 0.8089 - val_loss: 0.4204 - val_acc: 0.8062\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4143 - acc: 0.8102 - val_loss: 0.4190 - val_acc: 0.8071\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4110 - acc: 0.8117 - val_loss: 0.4057 - val_acc: 0.8144\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.5853 - acc: 0.7050 - val_loss: 0.5125 - val_acc: 0.7498\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.5273 - acc: 0.7396 - val_loss: 0.4947 - val_acc: 0.7617\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.5132 - acc: 0.7504 - val_loss: 0.4930 - val_acc: 0.7615\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.5011 - acc: 0.7568 - val_loss: 0.4725 - val_acc: 0.7726\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4901 - acc: 0.7648 - val_loss: 0.4611 - val_acc: 0.7814\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4832 - acc: 0.7686 - val_loss: 0.4689 - val_acc: 0.7781\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4786 - acc: 0.7737 - val_loss: 0.4528 - val_acc: 0.7836\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4741 - acc: 0.7755 - val_loss: 0.4466 - val_acc: 0.7896\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4718 - acc: 0.7762 - val_loss: 0.4398 - val_acc: 0.7939\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4699 - acc: 0.7773 - val_loss: 0.4338 - val_acc: 0.7966\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4689 - acc: 0.7784 - val_loss: 0.4508 - val_acc: 0.7904\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4682 - acc: 0.7794 - val_loss: 0.4366 - val_acc: 0.7950\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4664 - acc: 0.7798 - val_loss: 0.4452 - val_acc: 0.7922\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4647 - acc: 0.7820 - val_loss: 0.4332 - val_acc: 0.7983\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4658 - acc: 0.7799 - val_loss: 0.4371 - val_acc: 0.7950\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4649 - acc: 0.7811 - val_loss: 0.4354 - val_acc: 0.7987\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4636 - acc: 0.7812 - val_loss: 0.4275 - val_acc: 0.8026\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4625 - acc: 0.7804 - val_loss: 0.4321 - val_acc: 0.7991\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4627 - acc: 0.7815 - val_loss: 0.4479 - val_acc: 0.7883\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4590 - acc: 0.7850 - val_loss: 0.4241 - val_acc: 0.8045\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4623 - acc: 0.7822 - val_loss: 0.4296 - val_acc: 0.8001\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4592 - acc: 0.7833 - val_loss: 0.4235 - val_acc: 0.8041\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4597 - acc: 0.7836 - val_loss: 0.4299 - val_acc: 0.7994\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4593 - acc: 0.7840 - val_loss: 0.4261 - val_acc: 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4589 - acc: 0.7844 - val_loss: 0.4259 - val_acc: 0.8041\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4566 - acc: 0.7844 - val_loss: 0.4310 - val_acc: 0.7995\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4569 - acc: 0.7865 - val_loss: 0.4239 - val_acc: 0.8025\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4567 - acc: 0.7844 - val_loss: 0.4218 - val_acc: 0.8064\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4568 - acc: 0.7847 - val_loss: 0.4230 - val_acc: 0.8077\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4567 - acc: 0.7844 - val_loss: 0.4223 - val_acc: 0.8069\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.6025 - acc: 0.6929 - val_loss: 0.5132 - val_acc: 0.7505\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.5331 - acc: 0.7392 - val_loss: 0.4973 - val_acc: 0.7596\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.5202 - acc: 0.7464 - val_loss: 0.4915 - val_acc: 0.7630\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.5080 - acc: 0.7542 - val_loss: 0.4766 - val_acc: 0.7732\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4998 - acc: 0.7617 - val_loss: 0.4758 - val_acc: 0.7737\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4937 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7589\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4890 - acc: 0.7670 - val_loss: 0.4875 - val_acc: 0.7732\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4868 - acc: 0.7668 - val_loss: 0.4753 - val_acc: 0.7742\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4815 - acc: 0.7699 - val_loss: 0.4581 - val_acc: 0.7860\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4812 - acc: 0.7716 - val_loss: 0.4524 - val_acc: 0.7897\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4795 - acc: 0.7706 - val_loss: 0.4529 - val_acc: 0.7895\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4793 - acc: 0.7722 - val_loss: 0.4518 - val_acc: 0.7905\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4753 - acc: 0.7741 - val_loss: 0.4887 - val_acc: 0.7657\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4737 - acc: 0.7760 - val_loss: 0.4461 - val_acc: 0.7926\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4730 - acc: 0.7756 - val_loss: 0.4613 - val_acc: 0.7833\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4738 - acc: 0.7754 - val_loss: 0.4424 - val_acc: 0.7929\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4718 - acc: 0.7770 - val_loss: 0.4520 - val_acc: 0.7910\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4723 - acc: 0.7765 - val_loss: 0.4403 - val_acc: 0.7943\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4706 - acc: 0.7771 - val_loss: 0.4418 - val_acc: 0.7944\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4710 - acc: 0.7775 - val_loss: 0.4509 - val_acc: 0.7875\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4714 - acc: 0.7767 - val_loss: 0.4489 - val_acc: 0.7837\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4709 - acc: 0.7779 - val_loss: 0.4434 - val_acc: 0.7947\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4688 - acc: 0.7780 - val_loss: 0.4464 - val_acc: 0.7936\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4706 - acc: 0.7760 - val_loss: 0.4444 - val_acc: 0.7964\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4669 - acc: 0.7794 - val_loss: 0.4377 - val_acc: 0.7955\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4684 - acc: 0.7793 - val_loss: 0.4452 - val_acc: 0.7912\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4666 - acc: 0.7807 - val_loss: 0.4413 - val_acc: 0.7952\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4666 - acc: 0.7790 - val_loss: 0.4411 - val_acc: 0.7933\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4668 - acc: 0.7793 - val_loss: 0.4379 - val_acc: 0.7975\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4666 - acc: 0.7795 - val_loss: 0.4456 - val_acc: 0.7937\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.5234 - acc: 0.7454 - val_loss: 0.4625 - val_acc: 0.7762\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4650 - acc: 0.7797 - val_loss: 0.4348 - val_acc: 0.7952\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4449 - acc: 0.7920 - val_loss: 0.4293 - val_acc: 0.8025\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4356 - acc: 0.7973 - val_loss: 0.4140 - val_acc: 0.8082\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4281 - acc: 0.8016 - val_loss: 0.4306 - val_acc: 0.7994\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4237 - acc: 0.8036 - val_loss: 0.4189 - val_acc: 0.8061\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4164 - acc: 0.8092 - val_loss: 0.4093 - val_acc: 0.8110\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4150 - acc: 0.8093 - val_loss: 0.4356 - val_acc: 0.7922\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4092 - acc: 0.8125 - val_loss: 0.3984 - val_acc: 0.8203\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4068 - acc: 0.8148 - val_loss: 0.3958 - val_acc: 0.8169\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4029 - acc: 0.8169 - val_loss: 0.4087 - val_acc: 0.8094\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4008 - acc: 0.8180 - val_loss: 0.4002 - val_acc: 0.8164\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3975 - acc: 0.8180 - val_loss: 0.4127 - val_acc: 0.8107\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3952 - acc: 0.8206 - val_loss: 0.4246 - val_acc: 0.8048\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3911 - acc: 0.8228 - val_loss: 0.3899 - val_acc: 0.8211\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3925 - acc: 0.8226 - val_loss: 0.3947 - val_acc: 0.8191\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3887 - acc: 0.8239 - val_loss: 0.3902 - val_acc: 0.8213\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3862 - acc: 0.8252 - val_loss: 0.3856 - val_acc: 0.8268\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3842 - acc: 0.8252 - val_loss: 0.3915 - val_acc: 0.8227\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3816 - acc: 0.8286 - val_loss: 0.4113 - val_acc: 0.8113\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3792 - acc: 0.8282 - val_loss: 0.4089 - val_acc: 0.8176\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3796 - acc: 0.8285 - val_loss: 0.3952 - val_acc: 0.8217\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3757 - acc: 0.8299 - val_loss: 0.4005 - val_acc: 0.8170\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3752 - acc: 0.8314 - val_loss: 0.3899 - val_acc: 0.8234\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3721 - acc: 0.8326 - val_loss: 0.3996 - val_acc: 0.8171\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3721 - acc: 0.8323 - val_loss: 0.4095 - val_acc: 0.8149\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3710 - acc: 0.8340 - val_loss: 0.3869 - val_acc: 0.8258\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3720 - acc: 0.8332 - val_loss: 0.3883 - val_acc: 0.8237\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3699 - acc: 0.8333 - val_loss: 0.3906 - val_acc: 0.8224\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3680 - acc: 0.8346 - val_loss: 0.3938 - val_acc: 0.8238\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.5309 - acc: 0.7402 - val_loss: 0.4722 - val_acc: 0.7711\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4694 - acc: 0.7765 - val_loss: 0.4387 - val_acc: 0.7957\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4514 - acc: 0.7884 - val_loss: 0.4259 - val_acc: 0.8031\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4402 - acc: 0.7948 - val_loss: 0.4467 - val_acc: 0.7904\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4322 - acc: 0.7990 - val_loss: 0.4183 - val_acc: 0.8079\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4241 - acc: 0.8046 - val_loss: 0.4099 - val_acc: 0.8129\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4209 - acc: 0.8062 - val_loss: 0.4051 - val_acc: 0.8155\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4169 - acc: 0.8079 - val_loss: 0.4000 - val_acc: 0.8158\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4092 - acc: 0.8112 - val_loss: 0.4135 - val_acc: 0.8073\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4086 - acc: 0.8128 - val_loss: 0.4066 - val_acc: 0.8146\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4054 - acc: 0.8139 - val_loss: 0.3952 - val_acc: 0.8210\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4039 - acc: 0.8153 - val_loss: 0.3936 - val_acc: 0.8205\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3986 - acc: 0.8186 - val_loss: 0.3959 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3957 - acc: 0.8193 - val_loss: 0.3911 - val_acc: 0.8231\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3937 - acc: 0.8203 - val_loss: 0.4167 - val_acc: 0.8073\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3920 - acc: 0.8219 - val_loss: 0.3918 - val_acc: 0.8230\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3903 - acc: 0.8219 - val_loss: 0.3874 - val_acc: 0.8239\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3866 - acc: 0.8235 - val_loss: 0.3974 - val_acc: 0.8206\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3860 - acc: 0.8242 - val_loss: 0.3905 - val_acc: 0.8208\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3827 - acc: 0.8270 - val_loss: 0.3886 - val_acc: 0.8212\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3791 - acc: 0.8273 - val_loss: 0.3921 - val_acc: 0.8195\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3788 - acc: 0.8282 - val_loss: 0.3930 - val_acc: 0.8202\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3775 - acc: 0.8295 - val_loss: 0.3949 - val_acc: 0.8192\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3747 - acc: 0.8303 - val_loss: 0.3853 - val_acc: 0.8252\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3753 - acc: 0.8299 - val_loss: 0.3852 - val_acc: 0.8240\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3708 - acc: 0.8328 - val_loss: 0.3864 - val_acc: 0.8253\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3694 - acc: 0.8346 - val_loss: 0.3929 - val_acc: 0.8216\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3697 - acc: 0.8329 - val_loss: 0.4097 - val_acc: 0.8160\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3663 - acc: 0.8351 - val_loss: 0.3913 - val_acc: 0.8236\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3648 - acc: 0.8349 - val_loss: 0.3840 - val_acc: 0.8263\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.5725 - acc: 0.7221 - val_loss: 0.4784 - val_acc: 0.7708\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4871 - acc: 0.7652 - val_loss: 0.4504 - val_acc: 0.7862\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4655 - acc: 0.7800 - val_loss: 0.4354 - val_acc: 0.7994\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4526 - acc: 0.7879 - val_loss: 0.4508 - val_acc: 0.7937\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4446 - acc: 0.7930 - val_loss: 0.4198 - val_acc: 0.8033\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4403 - acc: 0.7954 - val_loss: 0.4437 - val_acc: 0.7917\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4335 - acc: 0.7990 - val_loss: 0.4188 - val_acc: 0.8056\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4293 - acc: 0.8014 - val_loss: 0.4199 - val_acc: 0.8023\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4292 - acc: 0.8009 - val_loss: 0.4207 - val_acc: 0.8055\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4247 - acc: 0.8048 - val_loss: 0.4060 - val_acc: 0.8127\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4214 - acc: 0.8055 - val_loss: 0.4148 - val_acc: 0.8087\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4195 - acc: 0.8083 - val_loss: 0.4094 - val_acc: 0.8098\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4181 - acc: 0.8081 - val_loss: 0.4137 - val_acc: 0.8089\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4170 - acc: 0.8081 - val_loss: 0.3924 - val_acc: 0.8214\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4136 - acc: 0.8096 - val_loss: 0.3927 - val_acc: 0.8202\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4129 - acc: 0.8099 - val_loss: 0.3877 - val_acc: 0.8229\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4110 - acc: 0.8127 - val_loss: 0.4089 - val_acc: 0.8112\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4097 - acc: 0.8137 - val_loss: 0.3860 - val_acc: 0.8237\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4096 - acc: 0.8123 - val_loss: 0.3845 - val_acc: 0.8237\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4075 - acc: 0.8144 - val_loss: 0.3941 - val_acc: 0.8197\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4075 - acc: 0.8138 - val_loss: 0.4278 - val_acc: 0.8022\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4068 - acc: 0.8142 - val_loss: 0.3936 - val_acc: 0.8188\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4058 - acc: 0.8147 - val_loss: 0.3838 - val_acc: 0.8263\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4047 - acc: 0.8159 - val_loss: 0.3981 - val_acc: 0.8160\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4026 - acc: 0.8155 - val_loss: 0.4026 - val_acc: 0.8139\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4025 - acc: 0.8175 - val_loss: 0.3885 - val_acc: 0.8237\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4019 - acc: 0.8170 - val_loss: 0.4009 - val_acc: 0.8159\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4004 - acc: 0.8181 - val_loss: 0.3819 - val_acc: 0.8260\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3994 - acc: 0.8187 - val_loss: 0.3866 - val_acc: 0.8234\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3997 - acc: 0.8184 - val_loss: 0.3849 - val_acc: 0.8242\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.5649 - acc: 0.7206 - val_loss: 0.4851 - val_acc: 0.7721\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4977 - acc: 0.7601 - val_loss: 0.4626 - val_acc: 0.7807\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4774 - acc: 0.7713 - val_loss: 0.4633 - val_acc: 0.7801\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4647 - acc: 0.7797 - val_loss: 0.4450 - val_acc: 0.7936\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4540 - acc: 0.7871 - val_loss: 0.4282 - val_acc: 0.8008\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4467 - acc: 0.7907 - val_loss: 0.4180 - val_acc: 0.8060\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4409 - acc: 0.7954 - val_loss: 0.4124 - val_acc: 0.8138\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4377 - acc: 0.7977 - val_loss: 0.4097 - val_acc: 0.8138\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4336 - acc: 0.8004 - val_loss: 0.4207 - val_acc: 0.8117\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4308 - acc: 0.8002 - val_loss: 0.4056 - val_acc: 0.8127\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4282 - acc: 0.8010 - val_loss: 0.4176 - val_acc: 0.8069\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4256 - acc: 0.8034 - val_loss: 0.4069 - val_acc: 0.8114\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4253 - acc: 0.8036 - val_loss: 0.4003 - val_acc: 0.8187\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4228 - acc: 0.8062 - val_loss: 0.4325 - val_acc: 0.7994\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4208 - acc: 0.8065 - val_loss: 0.4086 - val_acc: 0.8121\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4198 - acc: 0.8067 - val_loss: 0.3955 - val_acc: 0.8210\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4179 - acc: 0.8083 - val_loss: 0.3965 - val_acc: 0.8191\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4161 - acc: 0.8078 - val_loss: 0.4028 - val_acc: 0.8156\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4159 - acc: 0.8098 - val_loss: 0.3946 - val_acc: 0.8196\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4151 - acc: 0.8090 - val_loss: 0.3899 - val_acc: 0.8215\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4145 - acc: 0.8096 - val_loss: 0.3910 - val_acc: 0.8196\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4138 - acc: 0.8118 - val_loss: 0.3894 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4118 - acc: 0.8128 - val_loss: 0.3864 - val_acc: 0.8232\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4115 - acc: 0.8117 - val_loss: 0.3830 - val_acc: 0.8259\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4116 - acc: 0.8117 - val_loss: 0.3838 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4106 - acc: 0.8104 - val_loss: 0.3929 - val_acc: 0.8188\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4092 - acc: 0.8132 - val_loss: 0.3857 - val_acc: 0.8235\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4099 - acc: 0.8131 - val_loss: 0.3904 - val_acc: 0.8222\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4088 - acc: 0.8141 - val_loss: 0.3869 - val_acc: 0.8228\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4091 - acc: 0.8135 - val_loss: 0.3850 - val_acc: 0.8258\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.5291 - acc: 0.7465 - val_loss: 0.4710 - val_acc: 0.7811\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4594 - acc: 0.7838 - val_loss: 0.4352 - val_acc: 0.7961\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4429 - acc: 0.7931 - val_loss: 0.4309 - val_acc: 0.7985\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4307 - acc: 0.8007 - val_loss: 0.4119 - val_acc: 0.8080\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4230 - acc: 0.8055 - val_loss: 0.4054 - val_acc: 0.8156\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4156 - acc: 0.8086 - val_loss: 0.4337 - val_acc: 0.7950\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4104 - acc: 0.8130 - val_loss: 0.4005 - val_acc: 0.8194\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4056 - acc: 0.8146 - val_loss: 0.3982 - val_acc: 0.8173\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.4015 - acc: 0.8163 - val_loss: 0.4047 - val_acc: 0.8140\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3978 - acc: 0.8183 - val_loss: 0.3913 - val_acc: 0.8202\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3934 - acc: 0.8222 - val_loss: 0.3945 - val_acc: 0.8198\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3910 - acc: 0.8225 - val_loss: 0.4009 - val_acc: 0.8135\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3868 - acc: 0.8258 - val_loss: 0.4041 - val_acc: 0.8137\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3819 - acc: 0.8278 - val_loss: 0.3835 - val_acc: 0.8248\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3813 - acc: 0.8274 - val_loss: 0.3892 - val_acc: 0.8215\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3790 - acc: 0.8292 - val_loss: 0.3885 - val_acc: 0.8236\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3776 - acc: 0.8292 - val_loss: 0.3828 - val_acc: 0.8269\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3738 - acc: 0.8328 - val_loss: 0.3884 - val_acc: 0.8237\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3722 - acc: 0.8320 - val_loss: 0.4326 - val_acc: 0.7945\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3687 - acc: 0.8340 - val_loss: 0.3841 - val_acc: 0.8255\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3668 - acc: 0.8349 - val_loss: 0.3944 - val_acc: 0.8177\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3630 - acc: 0.8378 - val_loss: 0.3822 - val_acc: 0.8252\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3615 - acc: 0.8382 - val_loss: 0.3940 - val_acc: 0.8208\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3605 - acc: 0.8389 - val_loss: 0.3813 - val_acc: 0.8265\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3549 - acc: 0.8405 - val_loss: 0.3827 - val_acc: 0.8261\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3555 - acc: 0.8409 - val_loss: 0.3910 - val_acc: 0.8220\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3518 - acc: 0.8426 - val_loss: 0.3886 - val_acc: 0.8238\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3497 - acc: 0.8441 - val_loss: 0.4018 - val_acc: 0.8160\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3452 - acc: 0.8455 - val_loss: 0.3838 - val_acc: 0.8245\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3431 - acc: 0.8468 - val_loss: 0.3860 - val_acc: 0.8240\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.5304 - acc: 0.7468 - val_loss: 0.5145 - val_acc: 0.7475\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4568 - acc: 0.7852 - val_loss: 0.4474 - val_acc: 0.7930\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4381 - acc: 0.7962 - val_loss: 0.4302 - val_acc: 0.8005\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4267 - acc: 0.8020 - val_loss: 0.4135 - val_acc: 0.8101\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4197 - acc: 0.8062 - val_loss: 0.4169 - val_acc: 0.8070\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4135 - acc: 0.8097 - val_loss: 0.4099 - val_acc: 0.8116\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4091 - acc: 0.8130 - val_loss: 0.4034 - val_acc: 0.8126\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4041 - acc: 0.8148 - val_loss: 0.3993 - val_acc: 0.8171\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3996 - acc: 0.8172 - val_loss: 0.3920 - val_acc: 0.8205\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3962 - acc: 0.8194 - val_loss: 0.3941 - val_acc: 0.8196\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3915 - acc: 0.8223 - val_loss: 0.3895 - val_acc: 0.8234\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3882 - acc: 0.8253 - val_loss: 0.3902 - val_acc: 0.8248\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3856 - acc: 0.8249 - val_loss: 0.3915 - val_acc: 0.8199\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3815 - acc: 0.8269 - val_loss: 0.4292 - val_acc: 0.7974\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3785 - acc: 0.8287 - val_loss: 0.4140 - val_acc: 0.8129\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3743 - acc: 0.8322 - val_loss: 0.3916 - val_acc: 0.8238\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3723 - acc: 0.8330 - val_loss: 0.3914 - val_acc: 0.8194\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3707 - acc: 0.8325 - val_loss: 0.3938 - val_acc: 0.8208\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3668 - acc: 0.8361 - val_loss: 0.3892 - val_acc: 0.8234\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3629 - acc: 0.8370 - val_loss: 0.4091 - val_acc: 0.8167\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3604 - acc: 0.8382 - val_loss: 0.3932 - val_acc: 0.8224\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3585 - acc: 0.8391 - val_loss: 0.3827 - val_acc: 0.8284\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3584 - acc: 0.8387 - val_loss: 0.3826 - val_acc: 0.8278\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3537 - acc: 0.8420 - val_loss: 0.3877 - val_acc: 0.8243\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3518 - acc: 0.8432 - val_loss: 0.3839 - val_acc: 0.8256\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3489 - acc: 0.8436 - val_loss: 0.3968 - val_acc: 0.8216\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3490 - acc: 0.8451 - val_loss: 0.3880 - val_acc: 0.8262\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3450 - acc: 0.8466 - val_loss: 0.3875 - val_acc: 0.8263\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3428 - acc: 0.8472 - val_loss: 0.3992 - val_acc: 0.8177\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3406 - acc: 0.8482 - val_loss: 0.3941 - val_acc: 0.8271\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.5385 - acc: 0.7401 - val_loss: 0.4600 - val_acc: 0.7821\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4697 - acc: 0.7767 - val_loss: 0.4363 - val_acc: 0.7962\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4474 - acc: 0.7911 - val_loss: 0.4238 - val_acc: 0.8043\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4347 - acc: 0.7982 - val_loss: 0.4221 - val_acc: 0.8054\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4244 - acc: 0.8046 - val_loss: 0.4666 - val_acc: 0.7820\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4194 - acc: 0.8074 - val_loss: 0.4311 - val_acc: 0.7980\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4149 - acc: 0.8092 - val_loss: 0.4014 - val_acc: 0.8154\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4095 - acc: 0.8128 - val_loss: 0.3985 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4074 - acc: 0.8144 - val_loss: 0.4030 - val_acc: 0.8152\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4024 - acc: 0.8162 - val_loss: 0.3987 - val_acc: 0.8166\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4000 - acc: 0.8180 - val_loss: 0.3922 - val_acc: 0.8219\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3963 - acc: 0.8191 - val_loss: 0.3904 - val_acc: 0.8231\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3932 - acc: 0.8206 - val_loss: 0.3818 - val_acc: 0.8277\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3908 - acc: 0.8232 - val_loss: 0.3865 - val_acc: 0.8242\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3896 - acc: 0.8231 - val_loss: 0.3902 - val_acc: 0.8200\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3866 - acc: 0.8247 - val_loss: 0.3773 - val_acc: 0.8299\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3857 - acc: 0.8245 - val_loss: 0.3794 - val_acc: 0.8284\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3846 - acc: 0.8254 - val_loss: 0.3897 - val_acc: 0.8195\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3821 - acc: 0.8273 - val_loss: 0.3721 - val_acc: 0.8323\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3801 - acc: 0.8273 - val_loss: 0.3777 - val_acc: 0.8295\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3786 - acc: 0.8284 - val_loss: 0.3933 - val_acc: 0.8201\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3773 - acc: 0.8290 - val_loss: 0.3833 - val_acc: 0.8260\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3758 - acc: 0.8305 - val_loss: 0.3866 - val_acc: 0.8250\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3728 - acc: 0.8311 - val_loss: 0.3783 - val_acc: 0.8296\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3719 - acc: 0.8330 - val_loss: 0.3675 - val_acc: 0.8342\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3727 - acc: 0.8318 - val_loss: 0.3779 - val_acc: 0.8303\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3711 - acc: 0.8328 - val_loss: 0.3760 - val_acc: 0.8276\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3707 - acc: 0.8340 - val_loss: 0.3702 - val_acc: 0.8342\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3698 - acc: 0.8321 - val_loss: 0.3831 - val_acc: 0.8287\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3674 - acc: 0.8341 - val_loss: 0.3686 - val_acc: 0.8320\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.5409 - acc: 0.7359 - val_loss: 0.4704 - val_acc: 0.7719\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4749 - acc: 0.7740 - val_loss: 0.4387 - val_acc: 0.7946\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4527 - acc: 0.7871 - val_loss: 0.4212 - val_acc: 0.8026\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4386 - acc: 0.7956 - val_loss: 0.4308 - val_acc: 0.8009\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4301 - acc: 0.8016 - val_loss: 0.4311 - val_acc: 0.8020\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4231 - acc: 0.8062 - val_loss: 0.4031 - val_acc: 0.8175\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4195 - acc: 0.8069 - val_loss: 0.3995 - val_acc: 0.8166\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4121 - acc: 0.8117 - val_loss: 0.4217 - val_acc: 0.8016\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4099 - acc: 0.8126 - val_loss: 0.4063 - val_acc: 0.8140\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4068 - acc: 0.8153 - val_loss: 0.3924 - val_acc: 0.8214\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4043 - acc: 0.8163 - val_loss: 0.3911 - val_acc: 0.8215\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3998 - acc: 0.8171 - val_loss: 0.3855 - val_acc: 0.8255\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3973 - acc: 0.8192 - val_loss: 0.3835 - val_acc: 0.8243\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3949 - acc: 0.8214 - val_loss: 0.4023 - val_acc: 0.8141\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3936 - acc: 0.8217 - val_loss: 0.3877 - val_acc: 0.8207\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3938 - acc: 0.8221 - val_loss: 0.4198 - val_acc: 0.8047\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3912 - acc: 0.8227 - val_loss: 0.3935 - val_acc: 0.8218\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3880 - acc: 0.8243 - val_loss: 0.4173 - val_acc: 0.8048\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3884 - acc: 0.8249 - val_loss: 0.3770 - val_acc: 0.8293\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3862 - acc: 0.8265 - val_loss: 0.3774 - val_acc: 0.8296\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3849 - acc: 0.8257 - val_loss: 0.3902 - val_acc: 0.8223\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3853 - acc: 0.8260 - val_loss: 0.3821 - val_acc: 0.8274\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3839 - acc: 0.8260 - val_loss: 0.3745 - val_acc: 0.8306\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3804 - acc: 0.8292 - val_loss: 0.3727 - val_acc: 0.8310\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.3807 - acc: 0.8275 - val_loss: 0.3733 - val_acc: 0.8289\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3806 - acc: 0.8289 - val_loss: 0.3751 - val_acc: 0.8295\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.3775 - acc: 0.8305 - val_loss: 0.3718 - val_acc: 0.8314\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3760 - acc: 0.8308 - val_loss: 0.3692 - val_acc: 0.8325\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.3769 - acc: 0.8305 - val_loss: 0.3739 - val_acc: 0.8295\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3756 - acc: 0.8311 - val_loss: 0.3734 - val_acc: 0.8292\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.5099 - acc: 0.7532 - val_loss: 0.4592 - val_acc: 0.7857\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4544 - acc: 0.7859 - val_loss: 0.4500 - val_acc: 0.7851\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4353 - acc: 0.7967 - val_loss: 0.4245 - val_acc: 0.8059\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4233 - acc: 0.8050 - val_loss: 0.4089 - val_acc: 0.8126\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4168 - acc: 0.8078 - val_loss: 0.4027 - val_acc: 0.8140\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4096 - acc: 0.8124 - val_loss: 0.4026 - val_acc: 0.8144\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4051 - acc: 0.8139 - val_loss: 0.4377 - val_acc: 0.7955\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4012 - acc: 0.8159 - val_loss: 0.3976 - val_acc: 0.8184\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3937 - acc: 0.8204 - val_loss: 0.4132 - val_acc: 0.8113\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3921 - acc: 0.8209 - val_loss: 0.3972 - val_acc: 0.8176\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3890 - acc: 0.8230 - val_loss: 0.3911 - val_acc: 0.8213\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3848 - acc: 0.8256 - val_loss: 0.3873 - val_acc: 0.8243\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3802 - acc: 0.8291 - val_loss: 0.3876 - val_acc: 0.8237\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3780 - acc: 0.8286 - val_loss: 0.3866 - val_acc: 0.8235\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3733 - acc: 0.8319 - val_loss: 0.3843 - val_acc: 0.8264\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3715 - acc: 0.8334 - val_loss: 0.4472 - val_acc: 0.7883\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3697 - acc: 0.8328 - val_loss: 0.3857 - val_acc: 0.8253\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3659 - acc: 0.8361 - val_loss: 0.3944 - val_acc: 0.8209\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3601 - acc: 0.8391 - val_loss: 0.3971 - val_acc: 0.8201\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3597 - acc: 0.8387 - val_loss: 0.4015 - val_acc: 0.8220\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3578 - acc: 0.8407 - val_loss: 0.3906 - val_acc: 0.8224\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3554 - acc: 0.8410 - val_loss: 0.3845 - val_acc: 0.8233\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3513 - acc: 0.8422 - val_loss: 0.3873 - val_acc: 0.8221\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3497 - acc: 0.8445 - val_loss: 0.3903 - val_acc: 0.8228\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3461 - acc: 0.8469 - val_loss: 0.3935 - val_acc: 0.8217\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3437 - acc: 0.8460 - val_loss: 0.3934 - val_acc: 0.8196\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3420 - acc: 0.8463 - val_loss: 0.3869 - val_acc: 0.8231\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3379 - acc: 0.8487 - val_loss: 0.3888 - val_acc: 0.8239\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3357 - acc: 0.8502 - val_loss: 0.3999 - val_acc: 0.8169\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3363 - acc: 0.8505 - val_loss: 0.3844 - val_acc: 0.8263\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.5148 - acc: 0.7515 - val_loss: 0.4563 - val_acc: 0.7830\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4530 - acc: 0.7858 - val_loss: 0.4409 - val_acc: 0.7965\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4325 - acc: 0.7986 - val_loss: 0.4133 - val_acc: 0.8105\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4223 - acc: 0.8053 - val_loss: 0.4316 - val_acc: 0.8001\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4135 - acc: 0.8102 - val_loss: 0.4567 - val_acc: 0.7822\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4090 - acc: 0.8130 - val_loss: 0.4136 - val_acc: 0.8097\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.4029 - acc: 0.8167 - val_loss: 0.4140 - val_acc: 0.8092\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3994 - acc: 0.8182 - val_loss: 0.3999 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3960 - acc: 0.8206 - val_loss: 0.3967 - val_acc: 0.8192\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3895 - acc: 0.8238 - val_loss: 0.3916 - val_acc: 0.8230\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3859 - acc: 0.8251 - val_loss: 0.4133 - val_acc: 0.8122\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3820 - acc: 0.8265 - val_loss: 0.3886 - val_acc: 0.8235\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3798 - acc: 0.8285 - val_loss: 0.3920 - val_acc: 0.8221\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3786 - acc: 0.8299 - val_loss: 0.3867 - val_acc: 0.8234\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3730 - acc: 0.8320 - val_loss: 0.3810 - val_acc: 0.8260\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3700 - acc: 0.8342 - val_loss: 0.3838 - val_acc: 0.8277\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3663 - acc: 0.8362 - val_loss: 0.3853 - val_acc: 0.8248\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3637 - acc: 0.8367 - val_loss: 0.3902 - val_acc: 0.8193\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3595 - acc: 0.8392 - val_loss: 0.3911 - val_acc: 0.8209\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3566 - acc: 0.8404 - val_loss: 0.3968 - val_acc: 0.8257\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3543 - acc: 0.8419 - val_loss: 0.3818 - val_acc: 0.8263\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3527 - acc: 0.8425 - val_loss: 0.4159 - val_acc: 0.8154\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3495 - acc: 0.8448 - val_loss: 0.3831 - val_acc: 0.8283\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3468 - acc: 0.8446 - val_loss: 0.3814 - val_acc: 0.8283\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3446 - acc: 0.8462 - val_loss: 0.3869 - val_acc: 0.8281\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3423 - acc: 0.8476 - val_loss: 0.3923 - val_acc: 0.8208\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3410 - acc: 0.8483 - val_loss: 0.3906 - val_acc: 0.8241\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3397 - acc: 0.8498 - val_loss: 0.3973 - val_acc: 0.8239\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3356 - acc: 0.8518 - val_loss: 0.4019 - val_acc: 0.8177\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 110us/sample - loss: 0.3329 - acc: 0.8536 - val_loss: 0.3916 - val_acc: 0.8238\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.5165 - acc: 0.7522 - val_loss: 0.4674 - val_acc: 0.7785\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4552 - acc: 0.7878 - val_loss: 0.4343 - val_acc: 0.7970\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4345 - acc: 0.7968 - val_loss: 0.4085 - val_acc: 0.8098\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4234 - acc: 0.8065 - val_loss: 0.4262 - val_acc: 0.8054\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4166 - acc: 0.8081 - val_loss: 0.3966 - val_acc: 0.8191\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4094 - acc: 0.8124 - val_loss: 0.3963 - val_acc: 0.8184\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4043 - acc: 0.8153 - val_loss: 0.3891 - val_acc: 0.8246\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3991 - acc: 0.8179 - val_loss: 0.3867 - val_acc: 0.8219\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3951 - acc: 0.8200 - val_loss: 0.3907 - val_acc: 0.8191\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3927 - acc: 0.8214 - val_loss: 0.3812 - val_acc: 0.8270\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3879 - acc: 0.8234 - val_loss: 0.3882 - val_acc: 0.8227\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3857 - acc: 0.8250 - val_loss: 0.3852 - val_acc: 0.8242\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3840 - acc: 0.8251 - val_loss: 0.3975 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3791 - acc: 0.8287 - val_loss: 0.3750 - val_acc: 0.8306\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3772 - acc: 0.8296 - val_loss: 0.3726 - val_acc: 0.8295\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3757 - acc: 0.8300 - val_loss: 0.3766 - val_acc: 0.8317\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3728 - acc: 0.8331 - val_loss: 0.3771 - val_acc: 0.8282\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3702 - acc: 0.8330 - val_loss: 0.3819 - val_acc: 0.8279\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3676 - acc: 0.8351 - val_loss: 0.3807 - val_acc: 0.8274\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3644 - acc: 0.8365 - val_loss: 0.3803 - val_acc: 0.8299\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3644 - acc: 0.8358 - val_loss: 0.3928 - val_acc: 0.8223\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3600 - acc: 0.8386 - val_loss: 0.3720 - val_acc: 0.8319\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3593 - acc: 0.8385 - val_loss: 0.3835 - val_acc: 0.8282\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3571 - acc: 0.8385 - val_loss: 0.3798 - val_acc: 0.8232\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3557 - acc: 0.8422 - val_loss: 0.3734 - val_acc: 0.8316\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3547 - acc: 0.8408 - val_loss: 0.3870 - val_acc: 0.8233\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3517 - acc: 0.8422 - val_loss: 0.3727 - val_acc: 0.8317\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3508 - acc: 0.8436 - val_loss: 0.3752 - val_acc: 0.8318\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3504 - acc: 0.8448 - val_loss: 0.3810 - val_acc: 0.8313\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3471 - acc: 0.8432 - val_loss: 0.3790 - val_acc: 0.8285\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.5445 - acc: 0.7396 - val_loss: 0.4569 - val_acc: 0.7856\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4654 - acc: 0.7815 - val_loss: 0.4425 - val_acc: 0.7931\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4417 - acc: 0.7945 - val_loss: 0.4214 - val_acc: 0.8069\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4276 - acc: 0.8032 - val_loss: 0.4042 - val_acc: 0.8158\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4201 - acc: 0.8072 - val_loss: 0.4008 - val_acc: 0.8170\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4133 - acc: 0.8108 - val_loss: 0.4081 - val_acc: 0.8112\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4085 - acc: 0.8145 - val_loss: 0.4100 - val_acc: 0.8082\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4049 - acc: 0.8145 - val_loss: 0.3873 - val_acc: 0.8209\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4005 - acc: 0.8173 - val_loss: 0.3981 - val_acc: 0.8194\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3965 - acc: 0.8192 - val_loss: 0.3794 - val_acc: 0.8270\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3915 - acc: 0.8218 - val_loss: 0.3889 - val_acc: 0.8219\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3894 - acc: 0.8243 - val_loss: 0.3832 - val_acc: 0.8245\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3862 - acc: 0.8235 - val_loss: 0.3787 - val_acc: 0.8301\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3843 - acc: 0.8254 - val_loss: 0.3796 - val_acc: 0.8295\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3824 - acc: 0.8277 - val_loss: 0.3849 - val_acc: 0.8242\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3798 - acc: 0.8279 - val_loss: 0.3889 - val_acc: 0.8209\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3770 - acc: 0.8307 - val_loss: 0.3972 - val_acc: 0.8184\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3752 - acc: 0.8294 - val_loss: 0.3799 - val_acc: 0.8267\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3725 - acc: 0.8319 - val_loss: 0.3700 - val_acc: 0.8349\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3702 - acc: 0.8340 - val_loss: 0.3787 - val_acc: 0.8276\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3689 - acc: 0.8344 - val_loss: 0.3724 - val_acc: 0.8310\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3669 - acc: 0.8350 - val_loss: 0.3753 - val_acc: 0.8292\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3661 - acc: 0.8352 - val_loss: 0.3744 - val_acc: 0.8311\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3648 - acc: 0.8348 - val_loss: 0.3788 - val_acc: 0.8295\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3618 - acc: 0.8381 - val_loss: 0.3684 - val_acc: 0.8331\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3607 - acc: 0.8395 - val_loss: 0.3736 - val_acc: 0.8295\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3590 - acc: 0.8389 - val_loss: 0.3703 - val_acc: 0.8305\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3588 - acc: 0.8388 - val_loss: 0.3684 - val_acc: 0.8347\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3551 - acc: 0.8405 - val_loss: 0.3830 - val_acc: 0.8246\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3570 - acc: 0.8414 - val_loss: 0.3764 - val_acc: 0.8309\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.5873 - acc: 0.7071 - val_loss: 0.5061 - val_acc: 0.7505\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.5114 - acc: 0.7491 - val_loss: 0.4841 - val_acc: 0.7659\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4898 - acc: 0.7644 - val_loss: 0.4682 - val_acc: 0.7741\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4737 - acc: 0.7756 - val_loss: 0.4531 - val_acc: 0.7885\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4669 - acc: 0.7789 - val_loss: 0.4395 - val_acc: 0.7991\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4621 - acc: 0.7822 - val_loss: 0.4433 - val_acc: 0.7896\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4612 - acc: 0.7836 - val_loss: 0.4465 - val_acc: 0.7933\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4569 - acc: 0.7864 - val_loss: 0.4333 - val_acc: 0.8011\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4548 - acc: 0.7857 - val_loss: 0.4433 - val_acc: 0.7940\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4518 - acc: 0.7881 - val_loss: 0.4275 - val_acc: 0.8019\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 89us/sample - loss: 0.4500 - acc: 0.7913 - val_loss: 0.4419 - val_acc: 0.7913\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4482 - acc: 0.7917 - val_loss: 0.4298 - val_acc: 0.8019\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4497 - acc: 0.7900 - val_loss: 0.4310 - val_acc: 0.8022\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4487 - acc: 0.7907 - val_loss: 0.4265 - val_acc: 0.8035\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4469 - acc: 0.7910 - val_loss: 0.4262 - val_acc: 0.8031\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4461 - acc: 0.7924 - val_loss: 0.4293 - val_acc: 0.7976\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.4437 - acc: 0.7936 - val_loss: 0.4264 - val_acc: 0.8033\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.4450 - acc: 0.7922 - val_loss: 0.4247 - val_acc: 0.8072\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4438 - acc: 0.7931 - val_loss: 0.4213 - val_acc: 0.8066\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4430 - acc: 0.7941 - val_loss: 0.4283 - val_acc: 0.7987\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4425 - acc: 0.7943 - val_loss: 0.4215 - val_acc: 0.8019\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4422 - acc: 0.7944 - val_loss: 0.4221 - val_acc: 0.8058\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4408 - acc: 0.7946 - val_loss: 0.4190 - val_acc: 0.8063\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4408 - acc: 0.7953 - val_loss: 0.4196 - val_acc: 0.8091\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4413 - acc: 0.7940 - val_loss: 0.4182 - val_acc: 0.8066\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4393 - acc: 0.7956 - val_loss: 0.4168 - val_acc: 0.8089\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 99us/sample - loss: 0.4399 - acc: 0.7949 - val_loss: 0.4155 - val_acc: 0.8069\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4374 - acc: 0.7964 - val_loss: 0.4398 - val_acc: 0.7932\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 100us/sample - loss: 0.4379 - acc: 0.7982 - val_loss: 0.4195 - val_acc: 0.8073\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4386 - acc: 0.7964 - val_loss: 0.4189 - val_acc: 0.8070\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.5734 - acc: 0.7154 - val_loss: 0.4989 - val_acc: 0.7588\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.5053 - acc: 0.7524 - val_loss: 0.4805 - val_acc: 0.7693\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4881 - acc: 0.7644 - val_loss: 0.4583 - val_acc: 0.7800\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4750 - acc: 0.7735 - val_loss: 0.4499 - val_acc: 0.7894\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4693 - acc: 0.7777 - val_loss: 0.4397 - val_acc: 0.7951\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4624 - acc: 0.7803 - val_loss: 0.4620 - val_acc: 0.7829\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4582 - acc: 0.7847 - val_loss: 0.4442 - val_acc: 0.7898\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4541 - acc: 0.7877 - val_loss: 0.4338 - val_acc: 0.7987\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4511 - acc: 0.7879 - val_loss: 0.4317 - val_acc: 0.8025\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4498 - acc: 0.7896 - val_loss: 0.4255 - val_acc: 0.8047\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4470 - acc: 0.7913 - val_loss: 0.4332 - val_acc: 0.8023\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4460 - acc: 0.7913 - val_loss: 0.4317 - val_acc: 0.8023\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4447 - acc: 0.7933 - val_loss: 0.4255 - val_acc: 0.8031\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.4436 - acc: 0.7930 - val_loss: 0.4204 - val_acc: 0.8069\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4430 - acc: 0.7925 - val_loss: 0.4235 - val_acc: 0.8018\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4427 - acc: 0.7932 - val_loss: 0.4168 - val_acc: 0.8077\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4410 - acc: 0.7946 - val_loss: 0.4207 - val_acc: 0.8073\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4407 - acc: 0.7940 - val_loss: 0.4183 - val_acc: 0.8052\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4386 - acc: 0.7951 - val_loss: 0.4213 - val_acc: 0.8031\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4392 - acc: 0.7950 - val_loss: 0.4169 - val_acc: 0.8104\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 90us/sample - loss: 0.4385 - acc: 0.7960 - val_loss: 0.4261 - val_acc: 0.8046\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4383 - acc: 0.7968 - val_loss: 0.4210 - val_acc: 0.8031\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4376 - acc: 0.7959 - val_loss: 0.4175 - val_acc: 0.8068\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4371 - acc: 0.7973 - val_loss: 0.4222 - val_acc: 0.8020\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4386 - acc: 0.7959 - val_loss: 0.4152 - val_acc: 0.8073\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4372 - acc: 0.7965 - val_loss: 0.4239 - val_acc: 0.8023\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4365 - acc: 0.7977 - val_loss: 0.4211 - val_acc: 0.8105\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4359 - acc: 0.7960 - val_loss: 0.4137 - val_acc: 0.8090\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4373 - acc: 0.7965 - val_loss: 0.4176 - val_acc: 0.8109\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 91us/sample - loss: 0.4364 - acc: 0.7980 - val_loss: 0.4144 - val_acc: 0.8084\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.5832 - acc: 0.7079 - val_loss: 0.4976 - val_acc: 0.7614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.5109 - acc: 0.7526 - val_loss: 0.4763 - val_acc: 0.7721\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4928 - acc: 0.7634 - val_loss: 0.4664 - val_acc: 0.7786\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4849 - acc: 0.7672 - val_loss: 0.4576 - val_acc: 0.7847\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4775 - acc: 0.7724 - val_loss: 0.4489 - val_acc: 0.7878\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4690 - acc: 0.7779 - val_loss: 0.4440 - val_acc: 0.7925\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4624 - acc: 0.7809 - val_loss: 0.4384 - val_acc: 0.7964\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4614 - acc: 0.7819 - val_loss: 0.4512 - val_acc: 0.7908\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4575 - acc: 0.7851 - val_loss: 0.4330 - val_acc: 0.7991\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4554 - acc: 0.7857 - val_loss: 0.4359 - val_acc: 0.7986\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4542 - acc: 0.7879 - val_loss: 0.4256 - val_acc: 0.8043\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4506 - acc: 0.7894 - val_loss: 0.4331 - val_acc: 0.7976\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4508 - acc: 0.7899 - val_loss: 0.4242 - val_acc: 0.8019\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4497 - acc: 0.7893 - val_loss: 0.4252 - val_acc: 0.8050\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.4474 - acc: 0.7906 - val_loss: 0.4277 - val_acc: 0.8013\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4488 - acc: 0.7902 - val_loss: 0.4285 - val_acc: 0.8026\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4464 - acc: 0.7916 - val_loss: 0.4279 - val_acc: 0.8002\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4461 - acc: 0.7911 - val_loss: 0.4238 - val_acc: 0.8026\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4447 - acc: 0.7916 - val_loss: 0.4205 - val_acc: 0.8076\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4447 - acc: 0.7919 - val_loss: 0.4192 - val_acc: 0.8056\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.4445 - acc: 0.7935 - val_loss: 0.4269 - val_acc: 0.8027\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4436 - acc: 0.7951 - val_loss: 0.4189 - val_acc: 0.8073\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4428 - acc: 0.7945 - val_loss: 0.4187 - val_acc: 0.8073\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4421 - acc: 0.7942 - val_loss: 0.4174 - val_acc: 0.8086\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4416 - acc: 0.7949 - val_loss: 0.4175 - val_acc: 0.8073\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4416 - acc: 0.7934 - val_loss: 0.4572 - val_acc: 0.7799\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4415 - acc: 0.7941 - val_loss: 0.4153 - val_acc: 0.8094\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4406 - acc: 0.7942 - val_loss: 0.4303 - val_acc: 0.7994\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4403 - acc: 0.7951 - val_loss: 0.4288 - val_acc: 0.7999\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4411 - acc: 0.7935 - val_loss: 0.4149 - val_acc: 0.8083\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 183us/sample - loss: 0.5876 - acc: 0.7044 - val_loss: 0.5016 - val_acc: 0.7578\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.5180 - acc: 0.7496 - val_loss: 0.4805 - val_acc: 0.7735\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.5002 - acc: 0.7616 - val_loss: 0.4827 - val_acc: 0.7724\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4886 - acc: 0.7666 - val_loss: 0.4648 - val_acc: 0.7749\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4799 - acc: 0.7719 - val_loss: 0.4496 - val_acc: 0.7928\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4750 - acc: 0.7762 - val_loss: 0.4439 - val_acc: 0.7967\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4682 - acc: 0.7801 - val_loss: 0.4434 - val_acc: 0.7926\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4634 - acc: 0.7817 - val_loss: 0.4293 - val_acc: 0.8011\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4608 - acc: 0.7842 - val_loss: 0.4729 - val_acc: 0.7796\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4574 - acc: 0.7856 - val_loss: 0.4257 - val_acc: 0.8023\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4546 - acc: 0.7863 - val_loss: 0.4249 - val_acc: 0.8026\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4524 - acc: 0.7877 - val_loss: 0.4314 - val_acc: 0.8026\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4519 - acc: 0.7897 - val_loss: 0.4184 - val_acc: 0.8076\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4504 - acc: 0.7911 - val_loss: 0.4305 - val_acc: 0.8007\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4476 - acc: 0.7908 - val_loss: 0.4199 - val_acc: 0.8054\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4466 - acc: 0.7917 - val_loss: 0.4194 - val_acc: 0.8073\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4452 - acc: 0.7927 - val_loss: 0.4227 - val_acc: 0.8052\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4442 - acc: 0.7932 - val_loss: 0.4188 - val_acc: 0.8072\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4448 - acc: 0.7931 - val_loss: 0.4163 - val_acc: 0.8069\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4429 - acc: 0.7943 - val_loss: 0.4227 - val_acc: 0.8073\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4427 - acc: 0.7934 - val_loss: 0.4169 - val_acc: 0.8077\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4406 - acc: 0.7949 - val_loss: 0.4123 - val_acc: 0.8111\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4409 - acc: 0.7961 - val_loss: 0.4263 - val_acc: 0.8054\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4418 - acc: 0.7942 - val_loss: 0.4199 - val_acc: 0.8087\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.4423 - acc: 0.7955 - val_loss: 0.4192 - val_acc: 0.8061\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4400 - acc: 0.7966 - val_loss: 0.4140 - val_acc: 0.8096\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4408 - acc: 0.7968 - val_loss: 0.4125 - val_acc: 0.8084\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4399 - acc: 0.7966 - val_loss: 0.4162 - val_acc: 0.8104\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.4386 - acc: 0.7968 - val_loss: 0.4124 - val_acc: 0.8086\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.4380 - acc: 0.7972 - val_loss: 0.4132 - val_acc: 0.8100\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.5376 - acc: 0.7373 - val_loss: 0.4714 - val_acc: 0.7751\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4752 - acc: 0.7736 - val_loss: 0.4623 - val_acc: 0.7797\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4581 - acc: 0.7840 - val_loss: 0.4353 - val_acc: 0.7970\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4451 - acc: 0.7921 - val_loss: 0.4355 - val_acc: 0.7972\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4365 - acc: 0.7971 - val_loss: 0.4286 - val_acc: 0.8012\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4316 - acc: 0.8013 - val_loss: 0.4157 - val_acc: 0.8078\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4257 - acc: 0.8037 - val_loss: 0.4140 - val_acc: 0.8105\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4233 - acc: 0.8044 - val_loss: 0.4033 - val_acc: 0.8145\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4197 - acc: 0.8063 - val_loss: 0.4042 - val_acc: 0.8183\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4156 - acc: 0.8095 - val_loss: 0.4055 - val_acc: 0.8126\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4150 - acc: 0.8094 - val_loss: 0.4007 - val_acc: 0.8181\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4102 - acc: 0.8122 - val_loss: 0.4217 - val_acc: 0.8098\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4099 - acc: 0.8113 - val_loss: 0.4134 - val_acc: 0.8080\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4091 - acc: 0.8128 - val_loss: 0.4041 - val_acc: 0.8148\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4080 - acc: 0.8144 - val_loss: 0.3929 - val_acc: 0.8240\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4051 - acc: 0.8143 - val_loss: 0.3970 - val_acc: 0.8199\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4040 - acc: 0.8157 - val_loss: 0.3964 - val_acc: 0.8228\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4024 - acc: 0.8160 - val_loss: 0.3897 - val_acc: 0.8245\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4005 - acc: 0.8175 - val_loss: 0.3905 - val_acc: 0.8243\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4022 - acc: 0.8178 - val_loss: 0.3914 - val_acc: 0.8220\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4000 - acc: 0.8182 - val_loss: 0.3965 - val_acc: 0.8219\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4005 - acc: 0.8176 - val_loss: 0.3919 - val_acc: 0.8255\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3973 - acc: 0.8204 - val_loss: 0.3926 - val_acc: 0.8213\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3964 - acc: 0.8198 - val_loss: 0.3902 - val_acc: 0.8233\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3958 - acc: 0.8202 - val_loss: 0.3860 - val_acc: 0.8265\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3967 - acc: 0.8198 - val_loss: 0.3872 - val_acc: 0.8245\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3952 - acc: 0.8212 - val_loss: 0.3891 - val_acc: 0.8242\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3949 - acc: 0.8207 - val_loss: 0.3932 - val_acc: 0.8217\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3944 - acc: 0.8211 - val_loss: 0.4115 - val_acc: 0.8108\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.3935 - acc: 0.8207 - val_loss: 0.3993 - val_acc: 0.8177\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.5520 - acc: 0.7315 - val_loss: 0.4787 - val_acc: 0.7704\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4803 - acc: 0.7701 - val_loss: 0.4625 - val_acc: 0.7810\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4603 - acc: 0.7823 - val_loss: 0.4628 - val_acc: 0.7800\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4485 - acc: 0.7906 - val_loss: 0.4449 - val_acc: 0.7897\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4405 - acc: 0.7952 - val_loss: 0.4245 - val_acc: 0.8030\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4323 - acc: 0.7987 - val_loss: 0.4243 - val_acc: 0.8045\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4305 - acc: 0.8010 - val_loss: 0.4170 - val_acc: 0.8086\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4252 - acc: 0.8036 - val_loss: 0.4181 - val_acc: 0.8085\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4205 - acc: 0.8074 - val_loss: 0.4023 - val_acc: 0.8158\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4174 - acc: 0.8082 - val_loss: 0.4146 - val_acc: 0.8104\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4170 - acc: 0.8085 - val_loss: 0.4032 - val_acc: 0.8146\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4124 - acc: 0.8117 - val_loss: 0.3947 - val_acc: 0.8190\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4104 - acc: 0.8121 - val_loss: 0.3988 - val_acc: 0.8164\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4097 - acc: 0.8134 - val_loss: 0.4087 - val_acc: 0.8131\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4082 - acc: 0.8133 - val_loss: 0.4050 - val_acc: 0.8126\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.4065 - acc: 0.8147 - val_loss: 0.4031 - val_acc: 0.8128\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4059 - acc: 0.8158 - val_loss: 0.4201 - val_acc: 0.8094\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4050 - acc: 0.8159 - val_loss: 0.3927 - val_acc: 0.8189\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4021 - acc: 0.8163 - val_loss: 0.3912 - val_acc: 0.8206\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4024 - acc: 0.8161 - val_loss: 0.3901 - val_acc: 0.8230\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.4016 - acc: 0.8175 - val_loss: 0.3975 - val_acc: 0.8178\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.4005 - acc: 0.8181 - val_loss: 0.3920 - val_acc: 0.8237\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4007 - acc: 0.8178 - val_loss: 0.4048 - val_acc: 0.8157\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3988 - acc: 0.8182 - val_loss: 0.4008 - val_acc: 0.8180\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.3978 - acc: 0.8184 - val_loss: 0.3914 - val_acc: 0.8214\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3974 - acc: 0.8194 - val_loss: 0.3980 - val_acc: 0.8148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 92us/sample - loss: 0.3983 - acc: 0.8194 - val_loss: 0.3855 - val_acc: 0.8232\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 93us/sample - loss: 0.3970 - acc: 0.8190 - val_loss: 0.3868 - val_acc: 0.8251\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 94us/sample - loss: 0.3964 - acc: 0.8204 - val_loss: 0.3889 - val_acc: 0.8243\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 95us/sample - loss: 0.3964 - acc: 0.8214 - val_loss: 0.3867 - val_acc: 0.8236\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.5539 - acc: 0.7324 - val_loss: 0.4758 - val_acc: 0.7753\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.4762 - acc: 0.7731 - val_loss: 0.5029 - val_acc: 0.7511\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4552 - acc: 0.7864 - val_loss: 0.4443 - val_acc: 0.7919\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.4403 - acc: 0.7955 - val_loss: 0.4170 - val_acc: 0.8075\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4294 - acc: 0.8016 - val_loss: 0.4100 - val_acc: 0.8120\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4234 - acc: 0.8045 - val_loss: 0.4121 - val_acc: 0.8127\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4168 - acc: 0.8091 - val_loss: 0.3991 - val_acc: 0.8177\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.4125 - acc: 0.8119 - val_loss: 0.4062 - val_acc: 0.8124\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4082 - acc: 0.8131 - val_loss: 0.4411 - val_acc: 0.7954\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4063 - acc: 0.8138 - val_loss: 0.3907 - val_acc: 0.8230\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4044 - acc: 0.8160 - val_loss: 0.4024 - val_acc: 0.8162\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4009 - acc: 0.8171 - val_loss: 0.4022 - val_acc: 0.8165\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3992 - acc: 0.8183 - val_loss: 0.3908 - val_acc: 0.8230\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3963 - acc: 0.8197 - val_loss: 0.3859 - val_acc: 0.8228\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3935 - acc: 0.8222 - val_loss: 0.3949 - val_acc: 0.8206\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3927 - acc: 0.8233 - val_loss: 0.3832 - val_acc: 0.8281\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3914 - acc: 0.8236 - val_loss: 0.3871 - val_acc: 0.8247\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3899 - acc: 0.8243 - val_loss: 0.3881 - val_acc: 0.8234\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3882 - acc: 0.8246 - val_loss: 0.3956 - val_acc: 0.8191\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3885 - acc: 0.8257 - val_loss: 0.4166 - val_acc: 0.8069\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3851 - acc: 0.8287 - val_loss: 0.3789 - val_acc: 0.8285\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3833 - acc: 0.8274 - val_loss: 0.3839 - val_acc: 0.8268\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3835 - acc: 0.8281 - val_loss: 0.3786 - val_acc: 0.8285\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3828 - acc: 0.8276 - val_loss: 0.3899 - val_acc: 0.8223\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.3825 - acc: 0.8275 - val_loss: 0.3795 - val_acc: 0.8274\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3797 - acc: 0.8289 - val_loss: 0.3996 - val_acc: 0.8163\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3794 - acc: 0.8288 - val_loss: 0.3934 - val_acc: 0.8195\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3784 - acc: 0.8307 - val_loss: 0.4018 - val_acc: 0.8146\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3780 - acc: 0.8296 - val_loss: 0.3772 - val_acc: 0.8285\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3779 - acc: 0.8293 - val_loss: 0.3784 - val_acc: 0.8276\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 192us/sample - loss: 0.5564 - acc: 0.7296 - val_loss: 0.4862 - val_acc: 0.7642\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4864 - acc: 0.7673 - val_loss: 0.4516 - val_acc: 0.7888\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4614 - acc: 0.7815 - val_loss: 0.4314 - val_acc: 0.8007\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4461 - acc: 0.7926 - val_loss: 0.4249 - val_acc: 0.8011\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4344 - acc: 0.7993 - val_loss: 0.4113 - val_acc: 0.8099\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4244 - acc: 0.8047 - val_loss: 0.4041 - val_acc: 0.8148\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4192 - acc: 0.8079 - val_loss: 0.4039 - val_acc: 0.8128\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4142 - acc: 0.8117 - val_loss: 0.4057 - val_acc: 0.8148\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4092 - acc: 0.8120 - val_loss: 0.4045 - val_acc: 0.8123\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4055 - acc: 0.8154 - val_loss: 0.3917 - val_acc: 0.8195\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4030 - acc: 0.8154 - val_loss: 0.3983 - val_acc: 0.8202\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.4012 - acc: 0.8180 - val_loss: 0.3880 - val_acc: 0.8220\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3972 - acc: 0.8204 - val_loss: 0.3913 - val_acc: 0.8223\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3969 - acc: 0.8202 - val_loss: 0.3864 - val_acc: 0.8227\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3939 - acc: 0.8213 - val_loss: 0.4220 - val_acc: 0.8048\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3909 - acc: 0.8239 - val_loss: 0.3848 - val_acc: 0.8235\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3879 - acc: 0.8254 - val_loss: 0.3841 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3890 - acc: 0.8246 - val_loss: 0.3829 - val_acc: 0.8258\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3862 - acc: 0.8256 - val_loss: 0.3805 - val_acc: 0.8263\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3860 - acc: 0.8269 - val_loss: 0.3918 - val_acc: 0.8209\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3822 - acc: 0.8285 - val_loss: 0.3920 - val_acc: 0.8225\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3817 - acc: 0.8290 - val_loss: 0.3881 - val_acc: 0.8241\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3812 - acc: 0.8291 - val_loss: 0.3807 - val_acc: 0.8271\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3799 - acc: 0.8290 - val_loss: 0.3924 - val_acc: 0.8222\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3779 - acc: 0.8309 - val_loss: 0.3798 - val_acc: 0.8281\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3771 - acc: 0.8309 - val_loss: 0.3786 - val_acc: 0.8274\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3779 - acc: 0.8304 - val_loss: 0.3762 - val_acc: 0.8289\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3770 - acc: 0.8314 - val_loss: 0.3883 - val_acc: 0.8217\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 118us/sample - loss: 0.3759 - acc: 0.8322 - val_loss: 0.3849 - val_acc: 0.8241\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3758 - acc: 0.8310 - val_loss: 0.3872 - val_acc: 0.8244\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.5213 - acc: 0.7444 - val_loss: 0.4714 - val_acc: 0.7751\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4692 - acc: 0.7768 - val_loss: 0.4536 - val_acc: 0.7883\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4521 - acc: 0.7883 - val_loss: 0.4445 - val_acc: 0.7926\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.4386 - acc: 0.7967 - val_loss: 0.4175 - val_acc: 0.8080\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4316 - acc: 0.7999 - val_loss: 0.4151 - val_acc: 0.8074\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4248 - acc: 0.8035 - val_loss: 0.4278 - val_acc: 0.8026\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4182 - acc: 0.8076 - val_loss: 0.4134 - val_acc: 0.8106\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4121 - acc: 0.8115 - val_loss: 0.4367 - val_acc: 0.8024\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4083 - acc: 0.8123 - val_loss: 0.3981 - val_acc: 0.8171\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4055 - acc: 0.8156 - val_loss: 0.3998 - val_acc: 0.8162\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4017 - acc: 0.8161 - val_loss: 0.4113 - val_acc: 0.8099\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4011 - acc: 0.8187 - val_loss: 0.4148 - val_acc: 0.8081\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3973 - acc: 0.8186 - val_loss: 0.4119 - val_acc: 0.8108\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3960 - acc: 0.8203 - val_loss: 0.3898 - val_acc: 0.8212\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3935 - acc: 0.8212 - val_loss: 0.3897 - val_acc: 0.8241\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3919 - acc: 0.8215 - val_loss: 0.3879 - val_acc: 0.8233\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.3907 - acc: 0.8231 - val_loss: 0.3837 - val_acc: 0.8241\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3893 - acc: 0.8243 - val_loss: 0.3814 - val_acc: 0.8254\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3884 - acc: 0.8255 - val_loss: 0.3988 - val_acc: 0.8156\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3871 - acc: 0.8243 - val_loss: 0.3811 - val_acc: 0.8249\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3855 - acc: 0.8247 - val_loss: 0.3789 - val_acc: 0.8285\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3858 - acc: 0.8254 - val_loss: 0.4062 - val_acc: 0.8149\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3814 - acc: 0.8274 - val_loss: 0.3775 - val_acc: 0.8285\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3824 - acc: 0.8271 - val_loss: 0.3938 - val_acc: 0.8198\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3806 - acc: 0.8270 - val_loss: 0.3796 - val_acc: 0.8283\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3808 - acc: 0.8277 - val_loss: 0.4272 - val_acc: 0.8025\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 98us/sample - loss: 0.3797 - acc: 0.8286 - val_loss: 0.3753 - val_acc: 0.8304\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3786 - acc: 0.8298 - val_loss: 0.3838 - val_acc: 0.8258\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3782 - acc: 0.8297 - val_loss: 0.3782 - val_acc: 0.8272\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3770 - acc: 0.8295 - val_loss: 0.3780 - val_acc: 0.8288\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.5196 - acc: 0.7484 - val_loss: 0.4742 - val_acc: 0.7717\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4645 - acc: 0.7789 - val_loss: 0.4418 - val_acc: 0.7921\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4467 - acc: 0.7915 - val_loss: 0.4246 - val_acc: 0.8036\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4350 - acc: 0.7976 - val_loss: 0.4281 - val_acc: 0.8010\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4258 - acc: 0.8034 - val_loss: 0.4387 - val_acc: 0.7962\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4214 - acc: 0.8062 - val_loss: 0.4084 - val_acc: 0.8136\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4158 - acc: 0.8098 - val_loss: 0.4293 - val_acc: 0.7995\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4105 - acc: 0.8110 - val_loss: 0.4058 - val_acc: 0.8154\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.4072 - acc: 0.8146 - val_loss: 0.3963 - val_acc: 0.8190\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4033 - acc: 0.8162 - val_loss: 0.3955 - val_acc: 0.8203\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.4026 - acc: 0.8155 - val_loss: 0.3915 - val_acc: 0.8220\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3998 - acc: 0.8177 - val_loss: 0.3973 - val_acc: 0.8209\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3962 - acc: 0.8208 - val_loss: 0.3957 - val_acc: 0.8196\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3955 - acc: 0.8193 - val_loss: 0.3935 - val_acc: 0.8184\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3928 - acc: 0.8212 - val_loss: 0.3924 - val_acc: 0.8202\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3920 - acc: 0.8218 - val_loss: 0.4120 - val_acc: 0.8097\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3903 - acc: 0.8230 - val_loss: 0.3894 - val_acc: 0.8231\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3888 - acc: 0.8243 - val_loss: 0.3840 - val_acc: 0.8256\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3861 - acc: 0.8266 - val_loss: 0.4015 - val_acc: 0.8166\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3865 - acc: 0.8255 - val_loss: 0.4086 - val_acc: 0.8104\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3852 - acc: 0.8263 - val_loss: 0.3937 - val_acc: 0.8210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3825 - acc: 0.8284 - val_loss: 0.3830 - val_acc: 0.8261\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3838 - acc: 0.8270 - val_loss: 0.3974 - val_acc: 0.8177\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3826 - acc: 0.8262 - val_loss: 0.3858 - val_acc: 0.8242\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3800 - acc: 0.8299 - val_loss: 0.3818 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3792 - acc: 0.8286 - val_loss: 0.3851 - val_acc: 0.8254\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3806 - acc: 0.8293 - val_loss: 0.3813 - val_acc: 0.8273\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3792 - acc: 0.8304 - val_loss: 0.3794 - val_acc: 0.8280\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 8s 96us/sample - loss: 0.3770 - acc: 0.8302 - val_loss: 0.3807 - val_acc: 0.8278\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 8s 97us/sample - loss: 0.3765 - acc: 0.8300 - val_loss: 0.3801 - val_acc: 0.8282\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.5170 - acc: 0.7500 - val_loss: 0.4626 - val_acc: 0.7837\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4552 - acc: 0.7864 - val_loss: 0.4349 - val_acc: 0.7991\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4354 - acc: 0.7979 - val_loss: 0.4431 - val_acc: 0.7906\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4228 - acc: 0.8063 - val_loss: 0.4236 - val_acc: 0.8045\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4159 - acc: 0.8088 - val_loss: 0.4098 - val_acc: 0.8109\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4084 - acc: 0.8136 - val_loss: 0.3964 - val_acc: 0.8185\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4014 - acc: 0.8176 - val_loss: 0.4023 - val_acc: 0.8155\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3964 - acc: 0.8211 - val_loss: 0.3913 - val_acc: 0.8212\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.3911 - acc: 0.8232 - val_loss: 0.3954 - val_acc: 0.8211\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3884 - acc: 0.8250 - val_loss: 0.4179 - val_acc: 0.8054\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3829 - acc: 0.8276 - val_loss: 0.3877 - val_acc: 0.8249\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3800 - acc: 0.8289 - val_loss: 0.3941 - val_acc: 0.8207\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3762 - acc: 0.8324 - val_loss: 0.3809 - val_acc: 0.8270\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.3732 - acc: 0.8323 - val_loss: 0.3794 - val_acc: 0.8302\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3695 - acc: 0.8356 - val_loss: 0.3940 - val_acc: 0.8230\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3673 - acc: 0.8362 - val_loss: 0.4037 - val_acc: 0.8198\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3648 - acc: 0.8373 - val_loss: 0.3822 - val_acc: 0.8286\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3624 - acc: 0.8373 - val_loss: 0.3962 - val_acc: 0.8217\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3599 - acc: 0.8396 - val_loss: 0.3838 - val_acc: 0.8265\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3579 - acc: 0.8411 - val_loss: 0.3787 - val_acc: 0.8309\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3541 - acc: 0.8427 - val_loss: 0.3794 - val_acc: 0.8295\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3529 - acc: 0.8436 - val_loss: 0.3818 - val_acc: 0.8282\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3502 - acc: 0.8449 - val_loss: 0.3871 - val_acc: 0.8262\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3482 - acc: 0.8457 - val_loss: 0.3801 - val_acc: 0.8274\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3460 - acc: 0.8472 - val_loss: 0.3827 - val_acc: 0.8278\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 114us/sample - loss: 0.3459 - acc: 0.8469 - val_loss: 0.3789 - val_acc: 0.8292\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3418 - acc: 0.8486 - val_loss: 0.3845 - val_acc: 0.8274\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3408 - acc: 0.8496 - val_loss: 0.3771 - val_acc: 0.8298\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3395 - acc: 0.8500 - val_loss: 0.3784 - val_acc: 0.8317\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3396 - acc: 0.8483 - val_loss: 0.3799 - val_acc: 0.8292\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.5207 - acc: 0.7503 - val_loss: 0.4847 - val_acc: 0.7734\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4534 - acc: 0.7883 - val_loss: 0.4439 - val_acc: 0.7912\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4346 - acc: 0.7995 - val_loss: 0.4135 - val_acc: 0.8090\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4201 - acc: 0.8069 - val_loss: 0.4224 - val_acc: 0.8021\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4133 - acc: 0.8120 - val_loss: 0.3948 - val_acc: 0.8202\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4058 - acc: 0.8158 - val_loss: 0.4127 - val_acc: 0.8101\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3982 - acc: 0.8188 - val_loss: 0.3923 - val_acc: 0.8219\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3932 - acc: 0.8219 - val_loss: 0.3924 - val_acc: 0.8197\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3884 - acc: 0.8249 - val_loss: 0.3909 - val_acc: 0.8249\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3827 - acc: 0.8284 - val_loss: 0.3775 - val_acc: 0.8280\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3788 - acc: 0.8298 - val_loss: 0.4392 - val_acc: 0.7951\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3732 - acc: 0.8327 - val_loss: 0.3792 - val_acc: 0.8299\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3707 - acc: 0.8328 - val_loss: 0.3811 - val_acc: 0.8264\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3668 - acc: 0.8358 - val_loss: 0.3805 - val_acc: 0.8249\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3633 - acc: 0.8377 - val_loss: 0.3914 - val_acc: 0.8182\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3609 - acc: 0.8389 - val_loss: 0.3787 - val_acc: 0.8284\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3583 - acc: 0.8407 - val_loss: 0.3758 - val_acc: 0.8288\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3546 - acc: 0.8421 - val_loss: 0.3828 - val_acc: 0.8246\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3527 - acc: 0.8430 - val_loss: 0.3825 - val_acc: 0.8268\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3487 - acc: 0.8456 - val_loss: 0.3832 - val_acc: 0.8285\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3468 - acc: 0.8466 - val_loss: 0.3840 - val_acc: 0.8270\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3464 - acc: 0.8457 - val_loss: 0.3787 - val_acc: 0.8284\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3430 - acc: 0.8494 - val_loss: 0.3855 - val_acc: 0.8237\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3396 - acc: 0.8507 - val_loss: 0.3887 - val_acc: 0.8264\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3375 - acc: 0.8515 - val_loss: 0.3779 - val_acc: 0.8293\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3380 - acc: 0.8510 - val_loss: 0.3876 - val_acc: 0.8273\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3363 - acc: 0.8526 - val_loss: 0.3787 - val_acc: 0.8262\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3337 - acc: 0.8540 - val_loss: 0.3865 - val_acc: 0.8246\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3306 - acc: 0.8550 - val_loss: 0.3732 - val_acc: 0.8347\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3296 - acc: 0.8547 - val_loss: 0.3757 - val_acc: 0.8299\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.5224 - acc: 0.7487 - val_loss: 0.4610 - val_acc: 0.7813\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4595 - acc: 0.7839 - val_loss: 0.4455 - val_acc: 0.7922\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4416 - acc: 0.7949 - val_loss: 0.4464 - val_acc: 0.7925\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4304 - acc: 0.8001 - val_loss: 0.4124 - val_acc: 0.8119\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4223 - acc: 0.8051 - val_loss: 0.4127 - val_acc: 0.8091\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4154 - acc: 0.8081 - val_loss: 0.4487 - val_acc: 0.7875\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4092 - acc: 0.8144 - val_loss: 0.4331 - val_acc: 0.7994\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4035 - acc: 0.8157 - val_loss: 0.4041 - val_acc: 0.8144\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3999 - acc: 0.8180 - val_loss: 0.4016 - val_acc: 0.8151\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3965 - acc: 0.8204 - val_loss: 0.3864 - val_acc: 0.8253\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3927 - acc: 0.8215 - val_loss: 0.4072 - val_acc: 0.8137\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3906 - acc: 0.8242 - val_loss: 0.3955 - val_acc: 0.8213\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3894 - acc: 0.8232 - val_loss: 0.3839 - val_acc: 0.8280\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 100us/sample - loss: 0.3858 - acc: 0.8248 - val_loss: 0.3861 - val_acc: 0.8240\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3857 - acc: 0.8262 - val_loss: 0.3853 - val_acc: 0.8259\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3840 - acc: 0.8268 - val_loss: 0.3810 - val_acc: 0.8303\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3816 - acc: 0.8278 - val_loss: 0.3868 - val_acc: 0.8245\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3791 - acc: 0.8296 - val_loss: 0.3907 - val_acc: 0.8214\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3774 - acc: 0.8307 - val_loss: 0.3850 - val_acc: 0.8258\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3770 - acc: 0.8301 - val_loss: 0.3876 - val_acc: 0.8237\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3742 - acc: 0.8315 - val_loss: 0.3858 - val_acc: 0.8267\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3736 - acc: 0.8314 - val_loss: 0.3769 - val_acc: 0.8335\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3720 - acc: 0.8326 - val_loss: 0.3840 - val_acc: 0.8271\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 100us/sample - loss: 0.3703 - acc: 0.8324 - val_loss: 0.3786 - val_acc: 0.8310\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 100us/sample - loss: 0.3686 - acc: 0.8336 - val_loss: 0.3872 - val_acc: 0.8260\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3687 - acc: 0.8343 - val_loss: 0.3830 - val_acc: 0.8260\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3670 - acc: 0.8353 - val_loss: 0.3844 - val_acc: 0.8258\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3671 - acc: 0.8357 - val_loss: 0.3775 - val_acc: 0.8282\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.3641 - acc: 0.8364 - val_loss: 0.3795 - val_acc: 0.8274\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3643 - acc: 0.8363 - val_loss: 0.3805 - val_acc: 0.8254\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 183us/sample - loss: 0.5282 - acc: 0.7465 - val_loss: 0.4570 - val_acc: 0.7833\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4606 - acc: 0.7826 - val_loss: 0.4392 - val_acc: 0.7955\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4428 - acc: 0.7947 - val_loss: 0.4216 - val_acc: 0.8069\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4291 - acc: 0.8018 - val_loss: 0.4131 - val_acc: 0.8080\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4203 - acc: 0.8057 - val_loss: 0.4094 - val_acc: 0.8115\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4121 - acc: 0.8107 - val_loss: 0.4110 - val_acc: 0.8135\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4091 - acc: 0.8124 - val_loss: 0.4035 - val_acc: 0.8167\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.4039 - acc: 0.8159 - val_loss: 0.3969 - val_acc: 0.8201\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3992 - acc: 0.8193 - val_loss: 0.3956 - val_acc: 0.8153\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3972 - acc: 0.8187 - val_loss: 0.4213 - val_acc: 0.8088\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3938 - acc: 0.8216 - val_loss: 0.3959 - val_acc: 0.8192\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 104us/sample - loss: 0.3922 - acc: 0.8218 - val_loss: 0.3899 - val_acc: 0.8181\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3881 - acc: 0.8245 - val_loss: 0.3879 - val_acc: 0.8207\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3864 - acc: 0.8240 - val_loss: 0.3895 - val_acc: 0.8245\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3845 - acc: 0.8268 - val_loss: 0.3848 - val_acc: 0.8281\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3840 - acc: 0.8271 - val_loss: 0.3892 - val_acc: 0.8228\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3805 - acc: 0.8295 - val_loss: 0.4015 - val_acc: 0.8163\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.3797 - acc: 0.8297 - val_loss: 0.3901 - val_acc: 0.8231\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.3795 - acc: 0.8289 - val_loss: 0.3799 - val_acc: 0.8291\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3753 - acc: 0.8324 - val_loss: 0.3797 - val_acc: 0.8278\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3742 - acc: 0.8303 - val_loss: 0.3785 - val_acc: 0.8314\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3736 - acc: 0.8326 - val_loss: 0.3856 - val_acc: 0.8266\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3714 - acc: 0.8332 - val_loss: 0.3773 - val_acc: 0.8318\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 111us/sample - loss: 0.3690 - acc: 0.8347 - val_loss: 0.3805 - val_acc: 0.8306\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3681 - acc: 0.8360 - val_loss: 0.3748 - val_acc: 0.8322\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3685 - acc: 0.8342 - val_loss: 0.3887 - val_acc: 0.8254\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 105us/sample - loss: 0.3673 - acc: 0.8344 - val_loss: 0.3764 - val_acc: 0.8281\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.3675 - acc: 0.8354 - val_loss: 0.3767 - val_acc: 0.8306\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 116us/sample - loss: 0.3640 - acc: 0.8371 - val_loss: 0.3750 - val_acc: 0.8313\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3653 - acc: 0.8366 - val_loss: 0.3756 - val_acc: 0.8322\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.5215 - acc: 0.7498 - val_loss: 0.4638 - val_acc: 0.7824\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.4474 - acc: 0.7886 - val_loss: 0.4291 - val_acc: 0.8022\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4266 - acc: 0.8038 - val_loss: 0.4306 - val_acc: 0.7984\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4145 - acc: 0.8090 - val_loss: 0.4018 - val_acc: 0.8172\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4055 - acc: 0.8160 - val_loss: 0.4045 - val_acc: 0.8150\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3973 - acc: 0.8196 - val_loss: 0.3996 - val_acc: 0.8164\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3918 - acc: 0.8226 - val_loss: 0.3926 - val_acc: 0.8209\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3832 - acc: 0.8280 - val_loss: 0.4196 - val_acc: 0.8076\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3799 - acc: 0.8294 - val_loss: 0.3842 - val_acc: 0.8249\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3752 - acc: 0.8321 - val_loss: 0.3921 - val_acc: 0.8209\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3684 - acc: 0.8338 - val_loss: 0.3897 - val_acc: 0.8233\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3648 - acc: 0.8373 - val_loss: 0.3927 - val_acc: 0.8215\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3604 - acc: 0.8386 - val_loss: 0.3875 - val_acc: 0.8236\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3559 - acc: 0.8419 - val_loss: 0.3997 - val_acc: 0.8187\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3505 - acc: 0.8432 - val_loss: 0.3839 - val_acc: 0.8280\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3470 - acc: 0.8468 - val_loss: 0.3872 - val_acc: 0.8242\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3434 - acc: 0.8482 - val_loss: 0.4048 - val_acc: 0.8210\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3385 - acc: 0.8498 - val_loss: 0.3835 - val_acc: 0.8278\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3357 - acc: 0.8504 - val_loss: 0.3854 - val_acc: 0.8299\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3327 - acc: 0.8522 - val_loss: 0.3853 - val_acc: 0.8295\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3295 - acc: 0.8541 - val_loss: 0.3849 - val_acc: 0.8296\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3251 - acc: 0.8553 - val_loss: 0.3877 - val_acc: 0.8288\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3224 - acc: 0.8573 - val_loss: 0.3916 - val_acc: 0.8266\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3206 - acc: 0.8581 - val_loss: 0.3995 - val_acc: 0.8267\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.3182 - acc: 0.8600 - val_loss: 0.3928 - val_acc: 0.8299\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3151 - acc: 0.8611 - val_loss: 0.3854 - val_acc: 0.8288\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3108 - acc: 0.8638 - val_loss: 0.3890 - val_acc: 0.8288\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.3068 - acc: 0.8647 - val_loss: 0.3960 - val_acc: 0.8301\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3062 - acc: 0.8652 - val_loss: 0.3958 - val_acc: 0.8249\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 121us/sample - loss: 0.3036 - acc: 0.8666 - val_loss: 0.4111 - val_acc: 0.8249\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 213us/sample - loss: 0.5168 - acc: 0.7543 - val_loss: 0.4842 - val_acc: 0.7649\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4473 - acc: 0.7922 - val_loss: 0.4659 - val_acc: 0.7839\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4252 - acc: 0.8046 - val_loss: 0.4535 - val_acc: 0.7850\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4130 - acc: 0.8107 - val_loss: 0.4010 - val_acc: 0.8162\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4032 - acc: 0.8174 - val_loss: 0.4363 - val_acc: 0.7990\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3944 - acc: 0.8204 - val_loss: 0.3889 - val_acc: 0.8262\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3882 - acc: 0.8244 - val_loss: 0.4003 - val_acc: 0.8174\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3811 - acc: 0.8282 - val_loss: 0.3853 - val_acc: 0.8253\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3745 - acc: 0.8318 - val_loss: 0.3892 - val_acc: 0.8226\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3700 - acc: 0.8341 - val_loss: 0.3917 - val_acc: 0.8224\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3642 - acc: 0.8380 - val_loss: 0.3837 - val_acc: 0.8246\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3587 - acc: 0.8404 - val_loss: 0.3795 - val_acc: 0.8282\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3539 - acc: 0.8431 - val_loss: 0.3767 - val_acc: 0.8302\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3499 - acc: 0.8456 - val_loss: 0.3755 - val_acc: 0.8289\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3444 - acc: 0.8472 - val_loss: 0.3934 - val_acc: 0.8191\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3379 - acc: 0.8517 - val_loss: 0.3883 - val_acc: 0.8247\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3338 - acc: 0.8532 - val_loss: 0.4209 - val_acc: 0.8145\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3307 - acc: 0.8554 - val_loss: 0.3829 - val_acc: 0.8253\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3252 - acc: 0.8566 - val_loss: 0.3843 - val_acc: 0.8303\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3222 - acc: 0.8598 - val_loss: 0.3834 - val_acc: 0.8288\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3194 - acc: 0.8611 - val_loss: 0.4115 - val_acc: 0.8145\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3155 - acc: 0.8627 - val_loss: 0.4007 - val_acc: 0.8245\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3109 - acc: 0.8656 - val_loss: 0.3959 - val_acc: 0.8291\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3075 - acc: 0.8658 - val_loss: 0.3915 - val_acc: 0.8236\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3028 - acc: 0.8684 - val_loss: 0.4021 - val_acc: 0.8214\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.2990 - acc: 0.8705 - val_loss: 0.3981 - val_acc: 0.8250\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.2965 - acc: 0.8704 - val_loss: 0.3966 - val_acc: 0.8236\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.2934 - acc: 0.8730 - val_loss: 0.3925 - val_acc: 0.8264\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.2927 - acc: 0.8732 - val_loss: 0.3982 - val_acc: 0.8234\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.2881 - acc: 0.8759 - val_loss: 0.4052 - val_acc: 0.8287\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 15s 181us/sample - loss: 0.5696 - acc: 0.7145 - val_loss: 0.5021 - val_acc: 0.7570\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.5093 - acc: 0.7506 - val_loss: 0.4777 - val_acc: 0.7709\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4936 - acc: 0.7602 - val_loss: 0.4664 - val_acc: 0.7792\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4829 - acc: 0.7678 - val_loss: 0.4699 - val_acc: 0.7738\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4737 - acc: 0.7738 - val_loss: 0.4490 - val_acc: 0.7893\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4640 - acc: 0.7805 - val_loss: 0.4342 - val_acc: 0.7961\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4572 - acc: 0.7848 - val_loss: 0.4393 - val_acc: 0.7959\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4534 - acc: 0.7867 - val_loss: 0.4390 - val_acc: 0.7934\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4493 - acc: 0.7901 - val_loss: 0.4278 - val_acc: 0.8048\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4480 - acc: 0.7908 - val_loss: 0.4508 - val_acc: 0.7880\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4448 - acc: 0.7921 - val_loss: 0.4219 - val_acc: 0.8059\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4441 - acc: 0.7920 - val_loss: 0.4258 - val_acc: 0.8015\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4415 - acc: 0.7934 - val_loss: 0.4201 - val_acc: 0.8074\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4404 - acc: 0.7946 - val_loss: 0.4323 - val_acc: 0.7965\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4393 - acc: 0.7941 - val_loss: 0.4218 - val_acc: 0.8065\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4382 - acc: 0.7963 - val_loss: 0.4173 - val_acc: 0.8116\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4390 - acc: 0.7956 - val_loss: 0.4156 - val_acc: 0.8112\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4354 - acc: 0.7970 - val_loss: 0.4321 - val_acc: 0.7950\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4364 - acc: 0.7969 - val_loss: 0.4135 - val_acc: 0.8105\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4356 - acc: 0.7963 - val_loss: 0.4143 - val_acc: 0.8133\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4365 - acc: 0.7975 - val_loss: 0.4129 - val_acc: 0.8112\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4325 - acc: 0.7985 - val_loss: 0.4116 - val_acc: 0.8113\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4328 - acc: 0.7988 - val_loss: 0.4120 - val_acc: 0.8091\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4340 - acc: 0.7978 - val_loss: 0.4113 - val_acc: 0.8112\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4329 - acc: 0.7994 - val_loss: 0.4280 - val_acc: 0.7992\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4315 - acc: 0.7990 - val_loss: 0.4192 - val_acc: 0.8049\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4321 - acc: 0.8004 - val_loss: 0.4108 - val_acc: 0.8114\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4327 - acc: 0.7992 - val_loss: 0.4097 - val_acc: 0.8110\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4311 - acc: 0.8007 - val_loss: 0.4092 - val_acc: 0.8117\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4301 - acc: 0.7997 - val_loss: 0.4102 - val_acc: 0.8115\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.5868 - acc: 0.7100 - val_loss: 0.4966 - val_acc: 0.7624\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.5070 - acc: 0.7532 - val_loss: 0.4756 - val_acc: 0.7727\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4861 - acc: 0.7659 - val_loss: 0.4634 - val_acc: 0.7787\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4758 - acc: 0.7727 - val_loss: 0.4492 - val_acc: 0.7898\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4674 - acc: 0.7791 - val_loss: 0.4491 - val_acc: 0.7869\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4612 - acc: 0.7821 - val_loss: 0.4539 - val_acc: 0.7839\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4573 - acc: 0.7851 - val_loss: 0.4346 - val_acc: 0.7968\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4529 - acc: 0.7875 - val_loss: 0.4359 - val_acc: 0.7982\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4486 - acc: 0.7895 - val_loss: 0.4599 - val_acc: 0.7756\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4459 - acc: 0.7912 - val_loss: 0.4364 - val_acc: 0.7940\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4453 - acc: 0.7918 - val_loss: 0.4414 - val_acc: 0.7910\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4446 - acc: 0.7919 - val_loss: 0.4236 - val_acc: 0.8037\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4438 - acc: 0.7935 - val_loss: 0.4192 - val_acc: 0.8032\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4418 - acc: 0.7947 - val_loss: 0.4216 - val_acc: 0.8037\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4397 - acc: 0.7962 - val_loss: 0.4233 - val_acc: 0.8019\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4394 - acc: 0.7958 - val_loss: 0.4205 - val_acc: 0.8037\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4384 - acc: 0.7958 - val_loss: 0.4208 - val_acc: 0.8061\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4367 - acc: 0.7966 - val_loss: 0.4176 - val_acc: 0.8048\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4375 - acc: 0.7961 - val_loss: 0.4211 - val_acc: 0.8036\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4362 - acc: 0.7979 - val_loss: 0.4226 - val_acc: 0.8025\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4359 - acc: 0.7982 - val_loss: 0.4272 - val_acc: 0.8007\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4358 - acc: 0.7969 - val_loss: 0.4183 - val_acc: 0.8048\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 103us/sample - loss: 0.4355 - acc: 0.7979 - val_loss: 0.4199 - val_acc: 0.8067\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4337 - acc: 0.7999 - val_loss: 0.4138 - val_acc: 0.8092\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4344 - acc: 0.7984 - val_loss: 0.4422 - val_acc: 0.7894\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4335 - acc: 0.7994 - val_loss: 0.4126 - val_acc: 0.8080\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4324 - acc: 0.7998 - val_loss: 0.4124 - val_acc: 0.8070\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4320 - acc: 0.7995 - val_loss: 0.4144 - val_acc: 0.8097\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 101us/sample - loss: 0.4328 - acc: 0.7983 - val_loss: 0.4114 - val_acc: 0.8080\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 102us/sample - loss: 0.4327 - acc: 0.7988 - val_loss: 0.4133 - val_acc: 0.8104\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 203us/sample - loss: 0.5919 - acc: 0.6994 - val_loss: 0.4969 - val_acc: 0.7617\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.5128 - acc: 0.7489 - val_loss: 0.4761 - val_acc: 0.7716\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4960 - acc: 0.7590 - val_loss: 0.4650 - val_acc: 0.7793\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4845 - acc: 0.7677 - val_loss: 0.4545 - val_acc: 0.7868\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4771 - acc: 0.7726 - val_loss: 0.4553 - val_acc: 0.7845\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4712 - acc: 0.7760 - val_loss: 0.4478 - val_acc: 0.7932\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4670 - acc: 0.7780 - val_loss: 0.4410 - val_acc: 0.7949\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4619 - acc: 0.7822 - val_loss: 0.4371 - val_acc: 0.7946\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4602 - acc: 0.7842 - val_loss: 0.4422 - val_acc: 0.7959\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4571 - acc: 0.7845 - val_loss: 0.4342 - val_acc: 0.8002\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4548 - acc: 0.7862 - val_loss: 0.4284 - val_acc: 0.8020\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4536 - acc: 0.7860 - val_loss: 0.4316 - val_acc: 0.7963\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4539 - acc: 0.7869 - val_loss: 0.4266 - val_acc: 0.8054\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4517 - acc: 0.7890 - val_loss: 0.4264 - val_acc: 0.8036\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4477 - acc: 0.7916 - val_loss: 0.4276 - val_acc: 0.8044\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4490 - acc: 0.7906 - val_loss: 0.4211 - val_acc: 0.8059\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4466 - acc: 0.7923 - val_loss: 0.4179 - val_acc: 0.8070\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4460 - acc: 0.7913 - val_loss: 0.4232 - val_acc: 0.8069\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4448 - acc: 0.7928 - val_loss: 0.4277 - val_acc: 0.8050\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4429 - acc: 0.7929 - val_loss: 0.4147 - val_acc: 0.8084\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4420 - acc: 0.7952 - val_loss: 0.4153 - val_acc: 0.8080\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4417 - acc: 0.7954 - val_loss: 0.4185 - val_acc: 0.8077\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4419 - acc: 0.7942 - val_loss: 0.4165 - val_acc: 0.8096\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4404 - acc: 0.7946 - val_loss: 0.4143 - val_acc: 0.8105\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4399 - acc: 0.7945 - val_loss: 0.4197 - val_acc: 0.8094\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4394 - acc: 0.7957 - val_loss: 0.4747 - val_acc: 0.7674\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 120us/sample - loss: 0.4391 - acc: 0.7968 - val_loss: 0.4145 - val_acc: 0.8106\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4397 - acc: 0.7951 - val_loss: 0.4203 - val_acc: 0.8080\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4386 - acc: 0.7952 - val_loss: 0.4115 - val_acc: 0.8134\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 119us/sample - loss: 0.4401 - acc: 0.7954 - val_loss: 0.4147 - val_acc: 0.8102\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 215us/sample - loss: 0.6170 - acc: 0.6869 - val_loss: 0.5032 - val_acc: 0.7544\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.5189 - acc: 0.7478 - val_loss: 0.4818 - val_acc: 0.7689\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4997 - acc: 0.7598 - val_loss: 0.4714 - val_acc: 0.7772\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4882 - acc: 0.7652 - val_loss: 0.4597 - val_acc: 0.7840\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4794 - acc: 0.7725 - val_loss: 0.4484 - val_acc: 0.7890\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4714 - acc: 0.7768 - val_loss: 0.4492 - val_acc: 0.7885\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4640 - acc: 0.7821 - val_loss: 0.4330 - val_acc: 0.7997\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4607 - acc: 0.7836 - val_loss: 0.4406 - val_acc: 0.7965\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4579 - acc: 0.7854 - val_loss: 0.4282 - val_acc: 0.8033\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4547 - acc: 0.7876 - val_loss: 0.4356 - val_acc: 0.8019\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4527 - acc: 0.7897 - val_loss: 0.4267 - val_acc: 0.8055\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4502 - acc: 0.7900 - val_loss: 0.4250 - val_acc: 0.8073\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4504 - acc: 0.7910 - val_loss: 0.4221 - val_acc: 0.8071\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4480 - acc: 0.7908 - val_loss: 0.4179 - val_acc: 0.8094\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4454 - acc: 0.7939 - val_loss: 0.5077 - val_acc: 0.7511\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4457 - acc: 0.7934 - val_loss: 0.4149 - val_acc: 0.8119\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4455 - acc: 0.7937 - val_loss: 0.4170 - val_acc: 0.8104\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4445 - acc: 0.7925 - val_loss: 0.4277 - val_acc: 0.8028\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4429 - acc: 0.7955 - val_loss: 0.4190 - val_acc: 0.8081\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4417 - acc: 0.7942 - val_loss: 0.4314 - val_acc: 0.8007\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4411 - acc: 0.7963 - val_loss: 0.4144 - val_acc: 0.8104\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4419 - acc: 0.7954 - val_loss: 0.4277 - val_acc: 0.8006\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4394 - acc: 0.7966 - val_loss: 0.4197 - val_acc: 0.8072\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4401 - acc: 0.7961 - val_loss: 0.4043 - val_acc: 0.8178\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4395 - acc: 0.7972 - val_loss: 0.4069 - val_acc: 0.8142\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4374 - acc: 0.7991 - val_loss: 0.4093 - val_acc: 0.8153\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4378 - acc: 0.7977 - val_loss: 0.4086 - val_acc: 0.8133\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4371 - acc: 0.7981 - val_loss: 0.4090 - val_acc: 0.8151\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4368 - acc: 0.7981 - val_loss: 0.4059 - val_acc: 0.8165\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4358 - acc: 0.8003 - val_loss: 0.4542 - val_acc: 0.7902\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.5519 - acc: 0.7321 - val_loss: 0.4858 - val_acc: 0.7662\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4822 - acc: 0.7692 - val_loss: 0.4554 - val_acc: 0.7847\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4642 - acc: 0.7785 - val_loss: 0.4606 - val_acc: 0.7825\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4489 - acc: 0.7907 - val_loss: 0.4524 - val_acc: 0.7898\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4400 - acc: 0.7956 - val_loss: 0.4202 - val_acc: 0.8084\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4314 - acc: 0.8006 - val_loss: 0.4124 - val_acc: 0.8111\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4273 - acc: 0.8031 - val_loss: 0.4237 - val_acc: 0.8026\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4230 - acc: 0.8063 - val_loss: 0.4050 - val_acc: 0.8145\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4195 - acc: 0.8087 - val_loss: 0.4044 - val_acc: 0.8148\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4157 - acc: 0.8102 - val_loss: 0.3983 - val_acc: 0.8184\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4151 - acc: 0.8087 - val_loss: 0.3970 - val_acc: 0.8198\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4117 - acc: 0.8100 - val_loss: 0.4059 - val_acc: 0.8145\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4088 - acc: 0.8133 - val_loss: 0.4197 - val_acc: 0.8090\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4080 - acc: 0.8137 - val_loss: 0.3974 - val_acc: 0.8191\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4067 - acc: 0.8151 - val_loss: 0.3980 - val_acc: 0.8194\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4072 - acc: 0.8151 - val_loss: 0.4288 - val_acc: 0.8059\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4054 - acc: 0.8151 - val_loss: 0.4012 - val_acc: 0.8165\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4047 - acc: 0.8158 - val_loss: 0.3921 - val_acc: 0.8217\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4021 - acc: 0.8182 - val_loss: 0.3922 - val_acc: 0.8224\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4014 - acc: 0.8179 - val_loss: 0.4055 - val_acc: 0.8184\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4004 - acc: 0.8185 - val_loss: 0.3855 - val_acc: 0.8242\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3986 - acc: 0.8202 - val_loss: 0.3972 - val_acc: 0.8181\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3985 - acc: 0.8180 - val_loss: 0.4031 - val_acc: 0.8159\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4000 - acc: 0.8185 - val_loss: 0.3862 - val_acc: 0.8241\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3980 - acc: 0.8182 - val_loss: 0.3895 - val_acc: 0.8217\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3971 - acc: 0.8196 - val_loss: 0.3976 - val_acc: 0.8160\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3958 - acc: 0.8206 - val_loss: 0.3880 - val_acc: 0.8227\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3946 - acc: 0.8217 - val_loss: 0.3840 - val_acc: 0.8256\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3951 - acc: 0.8204 - val_loss: 0.3890 - val_acc: 0.8234\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.3944 - acc: 0.8210 - val_loss: 0.3985 - val_acc: 0.8174\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.5342 - acc: 0.7384 - val_loss: 0.4753 - val_acc: 0.7740\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4752 - acc: 0.7728 - val_loss: 0.4490 - val_acc: 0.7887\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4563 - acc: 0.7843 - val_loss: 0.4471 - val_acc: 0.7921\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4453 - acc: 0.7924 - val_loss: 0.4467 - val_acc: 0.7877\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4374 - acc: 0.7964 - val_loss: 0.4171 - val_acc: 0.8067\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4306 - acc: 0.8007 - val_loss: 0.4471 - val_acc: 0.7940\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4252 - acc: 0.8042 - val_loss: 0.4168 - val_acc: 0.8105\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4206 - acc: 0.8059 - val_loss: 0.4072 - val_acc: 0.8127\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4177 - acc: 0.8094 - val_loss: 0.3999 - val_acc: 0.8160\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4157 - acc: 0.8100 - val_loss: 0.4698 - val_acc: 0.7764\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4123 - acc: 0.8114 - val_loss: 0.4047 - val_acc: 0.8147\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4115 - acc: 0.8114 - val_loss: 0.4290 - val_acc: 0.7979\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4099 - acc: 0.8133 - val_loss: 0.4034 - val_acc: 0.8181\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4081 - acc: 0.8134 - val_loss: 0.4031 - val_acc: 0.8174\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4060 - acc: 0.8142 - val_loss: 0.4082 - val_acc: 0.8116\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4050 - acc: 0.8150 - val_loss: 0.3976 - val_acc: 0.8166\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4053 - acc: 0.8157 - val_loss: 0.3928 - val_acc: 0.8189\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4030 - acc: 0.8165 - val_loss: 0.3960 - val_acc: 0.8185\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4023 - acc: 0.8166 - val_loss: 0.4092 - val_acc: 0.8091\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 106us/sample - loss: 0.4027 - acc: 0.8157 - val_loss: 0.3905 - val_acc: 0.8224\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4003 - acc: 0.8170 - val_loss: 0.3910 - val_acc: 0.8210\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4010 - acc: 0.8169 - val_loss: 0.3889 - val_acc: 0.8235\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.4029 - acc: 0.8162 - val_loss: 0.3927 - val_acc: 0.8204\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3989 - acc: 0.8185 - val_loss: 0.4096 - val_acc: 0.8112\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3973 - acc: 0.8207 - val_loss: 0.3878 - val_acc: 0.8234\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3976 - acc: 0.8193 - val_loss: 0.3893 - val_acc: 0.8228\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3962 - acc: 0.8198 - val_loss: 0.3927 - val_acc: 0.8201\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3961 - acc: 0.8194 - val_loss: 0.3869 - val_acc: 0.8253\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3961 - acc: 0.8209 - val_loss: 0.3885 - val_acc: 0.8221\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 107us/sample - loss: 0.3957 - acc: 0.8205 - val_loss: 0.3935 - val_acc: 0.8199\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.5453 - acc: 0.7343 - val_loss: 0.4724 - val_acc: 0.7736\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4757 - acc: 0.7739 - val_loss: 0.4465 - val_acc: 0.7885\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4537 - acc: 0.7872 - val_loss: 0.4286 - val_acc: 0.8015\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4392 - acc: 0.7948 - val_loss: 0.4293 - val_acc: 0.7997\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4293 - acc: 0.8008 - val_loss: 0.4200 - val_acc: 0.8057\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4204 - acc: 0.8064 - val_loss: 0.4117 - val_acc: 0.8075\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4169 - acc: 0.8083 - val_loss: 0.4003 - val_acc: 0.8178\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4124 - acc: 0.8112 - val_loss: 0.4039 - val_acc: 0.8151\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4087 - acc: 0.8141 - val_loss: 0.4102 - val_acc: 0.8116\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4059 - acc: 0.8139 - val_loss: 0.4015 - val_acc: 0.8176\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.4002 - acc: 0.8191 - val_loss: 0.3947 - val_acc: 0.8193\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.4007 - acc: 0.8194 - val_loss: 0.3871 - val_acc: 0.8235\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 122us/sample - loss: 0.3967 - acc: 0.8205 - val_loss: 0.3937 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3959 - acc: 0.8212 - val_loss: 0.4103 - val_acc: 0.8112\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3932 - acc: 0.8210 - val_loss: 0.3875 - val_acc: 0.8230\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3923 - acc: 0.8232 - val_loss: 0.3838 - val_acc: 0.8257\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3881 - acc: 0.8248 - val_loss: 0.3980 - val_acc: 0.8155\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.3892 - acc: 0.8242 - val_loss: 0.3860 - val_acc: 0.8256\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3870 - acc: 0.8262 - val_loss: 0.3879 - val_acc: 0.8232\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3855 - acc: 0.8266 - val_loss: 0.3846 - val_acc: 0.8268\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3842 - acc: 0.8271 - val_loss: 0.3781 - val_acc: 0.8285\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3838 - acc: 0.8265 - val_loss: 0.3809 - val_acc: 0.8260\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3829 - acc: 0.8275 - val_loss: 0.4035 - val_acc: 0.8166\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3812 - acc: 0.8289 - val_loss: 0.3786 - val_acc: 0.8267\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3808 - acc: 0.8286 - val_loss: 0.3961 - val_acc: 0.8184\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3805 - acc: 0.8280 - val_loss: 0.3765 - val_acc: 0.8289\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3779 - acc: 0.8297 - val_loss: 0.3811 - val_acc: 0.8280\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.3792 - acc: 0.8289 - val_loss: 0.3765 - val_acc: 0.8288\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3768 - acc: 0.8317 - val_loss: 0.3761 - val_acc: 0.8303\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 123us/sample - loss: 0.3771 - acc: 0.8313 - val_loss: 0.3803 - val_acc: 0.8284\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 19s 223us/sample - loss: 0.5455 - acc: 0.7322 - val_loss: 0.4714 - val_acc: 0.7763\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4804 - acc: 0.7724 - val_loss: 0.4482 - val_acc: 0.7899\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4596 - acc: 0.7837 - val_loss: 0.4418 - val_acc: 0.7962\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4461 - acc: 0.7929 - val_loss: 0.4185 - val_acc: 0.8062\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4339 - acc: 0.7994 - val_loss: 0.4203 - val_acc: 0.8051\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4270 - acc: 0.8041 - val_loss: 0.4361 - val_acc: 0.7980\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4200 - acc: 0.8084 - val_loss: 0.3994 - val_acc: 0.8167\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4152 - acc: 0.8096 - val_loss: 0.3984 - val_acc: 0.8177\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 135us/sample - loss: 0.4105 - acc: 0.8125 - val_loss: 0.4180 - val_acc: 0.8049\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4064 - acc: 0.8158 - val_loss: 0.4146 - val_acc: 0.8064\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4033 - acc: 0.8176 - val_loss: 0.4205 - val_acc: 0.8074\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4011 - acc: 0.8184 - val_loss: 0.3944 - val_acc: 0.8189\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3977 - acc: 0.8210 - val_loss: 0.3932 - val_acc: 0.8194\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3951 - acc: 0.8214 - val_loss: 0.3851 - val_acc: 0.8249\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3949 - acc: 0.8234 - val_loss: 0.3879 - val_acc: 0.8227\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3917 - acc: 0.8232 - val_loss: 0.3854 - val_acc: 0.8266\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3908 - acc: 0.8236 - val_loss: 0.3856 - val_acc: 0.8246\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3868 - acc: 0.8259 - val_loss: 0.3883 - val_acc: 0.8201\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3882 - acc: 0.8242 - val_loss: 0.4047 - val_acc: 0.8159\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3871 - acc: 0.8254 - val_loss: 0.3794 - val_acc: 0.8284\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3861 - acc: 0.8261 - val_loss: 0.3798 - val_acc: 0.8275\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3841 - acc: 0.8260 - val_loss: 0.3765 - val_acc: 0.8267\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3821 - acc: 0.8292 - val_loss: 0.3884 - val_acc: 0.8227\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3817 - acc: 0.8288 - val_loss: 0.3917 - val_acc: 0.8214\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3816 - acc: 0.8286 - val_loss: 0.3806 - val_acc: 0.8270\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3803 - acc: 0.8301 - val_loss: 0.3778 - val_acc: 0.8286\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3781 - acc: 0.8302 - val_loss: 0.3830 - val_acc: 0.8276\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.3762 - acc: 0.8319 - val_loss: 0.3899 - val_acc: 0.8208\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3775 - acc: 0.8309 - val_loss: 0.3773 - val_acc: 0.8300\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3758 - acc: 0.8308 - val_loss: 0.3757 - val_acc: 0.8279\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.5534 - acc: 0.7352 - val_loss: 0.4856 - val_acc: 0.7692\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4694 - acc: 0.7766 - val_loss: 0.4482 - val_acc: 0.7889\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4514 - acc: 0.7873 - val_loss: 0.4650 - val_acc: 0.7811\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4392 - acc: 0.7966 - val_loss: 0.4426 - val_acc: 0.7959\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4300 - acc: 0.8008 - val_loss: 0.4375 - val_acc: 0.7955\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4229 - acc: 0.8048 - val_loss: 0.4266 - val_acc: 0.7978\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4168 - acc: 0.8087 - val_loss: 0.4073 - val_acc: 0.8130\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4128 - acc: 0.8090 - val_loss: 0.4006 - val_acc: 0.8183\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4070 - acc: 0.8139 - val_loss: 0.3973 - val_acc: 0.8184\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4038 - acc: 0.8166 - val_loss: 0.4052 - val_acc: 0.8138\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4013 - acc: 0.8184 - val_loss: 0.4115 - val_acc: 0.8120\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3968 - acc: 0.8208 - val_loss: 0.3907 - val_acc: 0.8219\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3963 - acc: 0.8187 - val_loss: 0.4036 - val_acc: 0.8152\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3928 - acc: 0.8214 - val_loss: 0.3963 - val_acc: 0.8200\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3905 - acc: 0.8223 - val_loss: 0.4207 - val_acc: 0.8063\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3906 - acc: 0.8235 - val_loss: 0.3863 - val_acc: 0.8230\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3868 - acc: 0.8248 - val_loss: 0.3839 - val_acc: 0.8286\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3861 - acc: 0.8245 - val_loss: 0.3858 - val_acc: 0.8233\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3855 - acc: 0.8269 - val_loss: 0.4010 - val_acc: 0.8156\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3837 - acc: 0.8274 - val_loss: 0.3804 - val_acc: 0.8277\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3817 - acc: 0.8275 - val_loss: 0.3822 - val_acc: 0.8264\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3829 - acc: 0.8274 - val_loss: 0.3802 - val_acc: 0.8285\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3800 - acc: 0.8287 - val_loss: 0.3816 - val_acc: 0.8261\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3788 - acc: 0.8298 - val_loss: 0.3789 - val_acc: 0.8277\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3787 - acc: 0.8279 - val_loss: 0.3813 - val_acc: 0.8272\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3766 - acc: 0.8321 - val_loss: 0.3799 - val_acc: 0.8261\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3755 - acc: 0.8305 - val_loss: 0.3901 - val_acc: 0.8216\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3757 - acc: 0.8305 - val_loss: 0.3837 - val_acc: 0.8256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3746 - acc: 0.8321 - val_loss: 0.3806 - val_acc: 0.8260\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3728 - acc: 0.8322 - val_loss: 0.3824 - val_acc: 0.8267\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.5298 - acc: 0.7446 - val_loss: 0.4628 - val_acc: 0.7827\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4661 - acc: 0.7790 - val_loss: 0.4482 - val_acc: 0.7890\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4470 - acc: 0.7905 - val_loss: 0.4369 - val_acc: 0.7982\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4355 - acc: 0.7972 - val_loss: 0.4310 - val_acc: 0.8020\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4268 - acc: 0.8031 - val_loss: 0.4309 - val_acc: 0.7994\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4212 - acc: 0.8041 - val_loss: 0.4296 - val_acc: 0.8030\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4140 - acc: 0.8109 - val_loss: 0.4083 - val_acc: 0.8082\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.4110 - acc: 0.8122 - val_loss: 0.4096 - val_acc: 0.8087\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4076 - acc: 0.8135 - val_loss: 0.3939 - val_acc: 0.8205\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4044 - acc: 0.8157 - val_loss: 0.4002 - val_acc: 0.8180\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.4014 - acc: 0.8168 - val_loss: 0.4189 - val_acc: 0.8016\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3991 - acc: 0.8180 - val_loss: 0.3873 - val_acc: 0.8235\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3958 - acc: 0.8197 - val_loss: 0.3895 - val_acc: 0.8232\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3942 - acc: 0.8204 - val_loss: 0.3845 - val_acc: 0.8263\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3930 - acc: 0.8213 - val_loss: 0.3902 - val_acc: 0.8202\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3904 - acc: 0.8224 - val_loss: 0.3971 - val_acc: 0.8171\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3895 - acc: 0.8242 - val_loss: 0.4245 - val_acc: 0.8076\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3876 - acc: 0.8244 - val_loss: 0.3818 - val_acc: 0.8279\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3855 - acc: 0.8249 - val_loss: 0.3851 - val_acc: 0.8239\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3849 - acc: 0.8265 - val_loss: 0.3810 - val_acc: 0.8287\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3831 - acc: 0.8265 - val_loss: 0.3963 - val_acc: 0.8192\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3829 - acc: 0.8275 - val_loss: 0.3897 - val_acc: 0.8213\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3815 - acc: 0.8277 - val_loss: 0.3819 - val_acc: 0.8271\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 9s 108us/sample - loss: 0.3800 - acc: 0.8293 - val_loss: 0.3893 - val_acc: 0.8246\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 115us/sample - loss: 0.3800 - acc: 0.8286 - val_loss: 0.3868 - val_acc: 0.8257\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3787 - acc: 0.8290 - val_loss: 0.4077 - val_acc: 0.8141\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3776 - acc: 0.8292 - val_loss: 0.3968 - val_acc: 0.8188\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 117us/sample - loss: 0.3767 - acc: 0.8298 - val_loss: 0.3943 - val_acc: 0.8199\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3761 - acc: 0.8313 - val_loss: 0.3775 - val_acc: 0.8299\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 9s 109us/sample - loss: 0.3754 - acc: 0.8320 - val_loss: 0.3795 - val_acc: 0.8298\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 19s 221us/sample - loss: 0.5359 - acc: 0.7425 - val_loss: 0.4551 - val_acc: 0.7889\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4568 - acc: 0.7853 - val_loss: 0.4295 - val_acc: 0.8011\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4348 - acc: 0.7991 - val_loss: 0.4207 - val_acc: 0.8045\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4231 - acc: 0.8059 - val_loss: 0.4208 - val_acc: 0.8045\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4144 - acc: 0.8117 - val_loss: 0.4020 - val_acc: 0.8146\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4072 - acc: 0.8139 - val_loss: 0.4059 - val_acc: 0.8140\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4016 - acc: 0.8175 - val_loss: 0.4068 - val_acc: 0.8152\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3972 - acc: 0.8200 - val_loss: 0.3953 - val_acc: 0.8203\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3913 - acc: 0.8233 - val_loss: 0.4021 - val_acc: 0.8145\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3866 - acc: 0.8251 - val_loss: 0.3929 - val_acc: 0.8204\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3835 - acc: 0.8271 - val_loss: 0.3825 - val_acc: 0.8275\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3787 - acc: 0.8303 - val_loss: 0.3864 - val_acc: 0.8224\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3773 - acc: 0.8312 - val_loss: 0.3926 - val_acc: 0.8227\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3719 - acc: 0.8336 - val_loss: 0.3831 - val_acc: 0.8273\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3699 - acc: 0.8346 - val_loss: 0.3786 - val_acc: 0.8288\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3660 - acc: 0.8360 - val_loss: 0.3925 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3641 - acc: 0.8373 - val_loss: 0.3790 - val_acc: 0.8291\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3608 - acc: 0.8386 - val_loss: 0.3752 - val_acc: 0.8288\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3594 - acc: 0.8391 - val_loss: 0.3787 - val_acc: 0.8294\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3565 - acc: 0.8409 - val_loss: 0.3870 - val_acc: 0.8253\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3532 - acc: 0.8422 - val_loss: 0.3784 - val_acc: 0.8281\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3515 - acc: 0.8450 - val_loss: 0.3806 - val_acc: 0.8277\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3511 - acc: 0.8443 - val_loss: 0.3783 - val_acc: 0.8292\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3478 - acc: 0.8456 - val_loss: 0.3751 - val_acc: 0.8314\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3467 - acc: 0.8463 - val_loss: 0.3876 - val_acc: 0.8286\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3460 - acc: 0.8466 - val_loss: 0.3794 - val_acc: 0.8271\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3417 - acc: 0.8475 - val_loss: 0.3802 - val_acc: 0.8300\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3416 - acc: 0.8481 - val_loss: 0.3764 - val_acc: 0.8293\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3385 - acc: 0.8492 - val_loss: 0.3799 - val_acc: 0.8288\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3388 - acc: 0.8502 - val_loss: 0.3924 - val_acc: 0.8185\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 20s 234us/sample - loss: 0.5320 - acc: 0.7452 - val_loss: 0.4565 - val_acc: 0.7862\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4574 - acc: 0.7860 - val_loss: 0.4241 - val_acc: 0.8037\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4380 - acc: 0.7975 - val_loss: 0.4261 - val_acc: 0.8028\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4231 - acc: 0.8066 - val_loss: 0.4041 - val_acc: 0.8154\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 136us/sample - loss: 0.4137 - acc: 0.8106 - val_loss: 0.4038 - val_acc: 0.8149\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4056 - acc: 0.8152 - val_loss: 0.4077 - val_acc: 0.8131\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4014 - acc: 0.8174 - val_loss: 0.3866 - val_acc: 0.8217\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3949 - acc: 0.8208 - val_loss: 0.4072 - val_acc: 0.8158\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3893 - acc: 0.8240 - val_loss: 0.3818 - val_acc: 0.8270\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3850 - acc: 0.8265 - val_loss: 0.4403 - val_acc: 0.7996\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3792 - acc: 0.8293 - val_loss: 0.3851 - val_acc: 0.8245\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3753 - acc: 0.8327 - val_loss: 0.3801 - val_acc: 0.8264\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3715 - acc: 0.8352 - val_loss: 0.4045 - val_acc: 0.8156\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3690 - acc: 0.8344 - val_loss: 0.3754 - val_acc: 0.8281\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3661 - acc: 0.8358 - val_loss: 0.3759 - val_acc: 0.8308\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 135us/sample - loss: 0.3614 - acc: 0.8394 - val_loss: 0.3789 - val_acc: 0.8280\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3596 - acc: 0.8404 - val_loss: 0.3899 - val_acc: 0.8239\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3561 - acc: 0.8407 - val_loss: 0.3795 - val_acc: 0.8268\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3542 - acc: 0.8418 - val_loss: 0.3716 - val_acc: 0.8334\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3513 - acc: 0.8441 - val_loss: 0.3964 - val_acc: 0.8219\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3468 - acc: 0.8451 - val_loss: 0.3835 - val_acc: 0.8229\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3441 - acc: 0.8477 - val_loss: 0.3745 - val_acc: 0.8309\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3456 - acc: 0.8471 - val_loss: 0.3752 - val_acc: 0.8338\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3410 - acc: 0.8485 - val_loss: 0.3746 - val_acc: 0.8283\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3411 - acc: 0.8489 - val_loss: 0.3838 - val_acc: 0.8280\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 135us/sample - loss: 0.3365 - acc: 0.8520 - val_loss: 0.3776 - val_acc: 0.8298\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3343 - acc: 0.8527 - val_loss: 0.3767 - val_acc: 0.8292\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3328 - acc: 0.8534 - val_loss: 0.3824 - val_acc: 0.8306\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3307 - acc: 0.8539 - val_loss: 0.3829 - val_acc: 0.8271\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.3288 - acc: 0.8562 - val_loss: 0.3821 - val_acc: 0.8260\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.5156 - acc: 0.7496 - val_loss: 0.4594 - val_acc: 0.7819\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4589 - acc: 0.7833 - val_loss: 0.4509 - val_acc: 0.7870\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4395 - acc: 0.7943 - val_loss: 0.4322 - val_acc: 0.8002\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4300 - acc: 0.8003 - val_loss: 0.4189 - val_acc: 0.8077\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4215 - acc: 0.8061 - val_loss: 0.4102 - val_acc: 0.8138\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4166 - acc: 0.8080 - val_loss: 0.4031 - val_acc: 0.8148\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4113 - acc: 0.8119 - val_loss: 0.4000 - val_acc: 0.8163\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4058 - acc: 0.8146 - val_loss: 0.4184 - val_acc: 0.8071\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.4031 - acc: 0.8162 - val_loss: 0.3934 - val_acc: 0.8223\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3992 - acc: 0.8186 - val_loss: 0.3960 - val_acc: 0.8194\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3959 - acc: 0.8186 - val_loss: 0.3934 - val_acc: 0.8207\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3933 - acc: 0.8218 - val_loss: 0.4106 - val_acc: 0.8102\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3910 - acc: 0.8221 - val_loss: 0.3901 - val_acc: 0.8248\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 9s 112us/sample - loss: 0.3878 - acc: 0.8241 - val_loss: 0.3923 - val_acc: 0.8219\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3858 - acc: 0.8247 - val_loss: 0.3846 - val_acc: 0.8238\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3852 - acc: 0.8263 - val_loss: 0.3874 - val_acc: 0.8237\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3835 - acc: 0.8259 - val_loss: 0.4183 - val_acc: 0.8074\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3807 - acc: 0.8280 - val_loss: 0.3790 - val_acc: 0.8268\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3785 - acc: 0.8290 - val_loss: 0.3865 - val_acc: 0.8261\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3770 - acc: 0.8306 - val_loss: 0.3819 - val_acc: 0.8249\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3752 - acc: 0.8308 - val_loss: 0.3841 - val_acc: 0.8262\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3758 - acc: 0.8307 - val_loss: 0.3780 - val_acc: 0.8310\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3729 - acc: 0.8337 - val_loss: 0.3809 - val_acc: 0.8284\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3714 - acc: 0.8329 - val_loss: 0.3884 - val_acc: 0.8241\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3708 - acc: 0.8341 - val_loss: 0.3885 - val_acc: 0.8220\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3720 - acc: 0.8334 - val_loss: 0.3881 - val_acc: 0.8260\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3695 - acc: 0.8336 - val_loss: 0.3839 - val_acc: 0.8258\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3691 - acc: 0.8338 - val_loss: 0.3738 - val_acc: 0.8321\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3674 - acc: 0.8350 - val_loss: 0.3742 - val_acc: 0.8306\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3649 - acc: 0.8370 - val_loss: 0.3940 - val_acc: 0.8206\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 18s 211us/sample - loss: 0.5238 - acc: 0.7477 - val_loss: 0.4626 - val_acc: 0.7862\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4587 - acc: 0.7827 - val_loss: 0.4387 - val_acc: 0.7936\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4392 - acc: 0.7954 - val_loss: 0.4819 - val_acc: 0.7702\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4278 - acc: 0.8022 - val_loss: 0.4127 - val_acc: 0.8130\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4186 - acc: 0.8082 - val_loss: 0.4037 - val_acc: 0.8122\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4133 - acc: 0.8108 - val_loss: 0.4186 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4078 - acc: 0.8138 - val_loss: 0.4088 - val_acc: 0.8108\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.4040 - acc: 0.8166 - val_loss: 0.3965 - val_acc: 0.8180\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.4028 - acc: 0.8160 - val_loss: 0.3933 - val_acc: 0.8200\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3978 - acc: 0.8193 - val_loss: 0.3916 - val_acc: 0.8209\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3956 - acc: 0.8206 - val_loss: 0.3885 - val_acc: 0.8251\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3923 - acc: 0.8232 - val_loss: 0.3969 - val_acc: 0.8184\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3914 - acc: 0.8224 - val_loss: 0.3854 - val_acc: 0.8243\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3882 - acc: 0.8231 - val_loss: 0.3868 - val_acc: 0.8253\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3859 - acc: 0.8265 - val_loss: 0.4445 - val_acc: 0.7944\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3847 - acc: 0.8262 - val_loss: 0.4011 - val_acc: 0.8181\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3812 - acc: 0.8274 - val_loss: 0.3907 - val_acc: 0.8219\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3800 - acc: 0.8288 - val_loss: 0.3840 - val_acc: 0.8268\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3774 - acc: 0.8296 - val_loss: 0.3820 - val_acc: 0.8291\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3806 - acc: 0.8284 - val_loss: 0.3801 - val_acc: 0.8278\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3758 - acc: 0.8303 - val_loss: 0.3790 - val_acc: 0.8300\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3747 - acc: 0.8312 - val_loss: 0.4140 - val_acc: 0.8112\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3731 - acc: 0.8322 - val_loss: 0.3778 - val_acc: 0.8291\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3704 - acc: 0.8329 - val_loss: 0.3879 - val_acc: 0.8227\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3714 - acc: 0.8325 - val_loss: 0.3870 - val_acc: 0.8286\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3701 - acc: 0.8342 - val_loss: 0.3917 - val_acc: 0.8242\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3661 - acc: 0.8347 - val_loss: 0.3880 - val_acc: 0.8221\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3691 - acc: 0.8345 - val_loss: 0.3808 - val_acc: 0.8281\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 10s 113us/sample - loss: 0.3658 - acc: 0.8354 - val_loss: 0.3925 - val_acc: 0.8230\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 10s 112us/sample - loss: 0.3666 - acc: 0.8358 - val_loss: 0.3764 - val_acc: 0.8311\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 20s 232us/sample - loss: 0.5225 - acc: 0.7479 - val_loss: 0.4594 - val_acc: 0.7836\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4484 - acc: 0.7899 - val_loss: 0.4343 - val_acc: 0.7957\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4272 - acc: 0.8036 - val_loss: 0.4066 - val_acc: 0.8155\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4148 - acc: 0.8099 - val_loss: 0.4467 - val_acc: 0.7907\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4069 - acc: 0.8144 - val_loss: 0.4000 - val_acc: 0.8173\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4001 - acc: 0.8185 - val_loss: 0.4011 - val_acc: 0.8162\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3908 - acc: 0.8232 - val_loss: 0.4234 - val_acc: 0.8042\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3868 - acc: 0.8265 - val_loss: 0.3935 - val_acc: 0.8199\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3788 - acc: 0.8295 - val_loss: 0.3913 - val_acc: 0.8224\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3754 - acc: 0.8327 - val_loss: 0.4057 - val_acc: 0.8177\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3708 - acc: 0.8322 - val_loss: 0.3799 - val_acc: 0.8274\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3658 - acc: 0.8362 - val_loss: 0.3830 - val_acc: 0.8253\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3622 - acc: 0.8389 - val_loss: 0.3878 - val_acc: 0.8256\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3574 - acc: 0.8405 - val_loss: 0.3837 - val_acc: 0.8247\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3521 - acc: 0.8436 - val_loss: 0.3899 - val_acc: 0.8237\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3497 - acc: 0.8455 - val_loss: 0.3808 - val_acc: 0.8281\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3451 - acc: 0.8474 - val_loss: 0.4091 - val_acc: 0.8184\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3409 - acc: 0.8502 - val_loss: 0.3872 - val_acc: 0.8280\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3374 - acc: 0.8508 - val_loss: 0.3769 - val_acc: 0.8304\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3334 - acc: 0.8524 - val_loss: 0.3858 - val_acc: 0.8263\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3306 - acc: 0.8530 - val_loss: 0.3915 - val_acc: 0.8277\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3281 - acc: 0.8560 - val_loss: 0.3818 - val_acc: 0.8306\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3258 - acc: 0.8558 - val_loss: 0.4231 - val_acc: 0.8155\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3219 - acc: 0.8583 - val_loss: 0.3844 - val_acc: 0.8304\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3178 - acc: 0.8585 - val_loss: 0.4023 - val_acc: 0.8232\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3144 - acc: 0.8616 - val_loss: 0.3857 - val_acc: 0.8325\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3120 - acc: 0.8628 - val_loss: 0.3977 - val_acc: 0.8254\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3098 - acc: 0.8628 - val_loss: 0.3831 - val_acc: 0.8296\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3081 - acc: 0.8653 - val_loss: 0.3905 - val_acc: 0.8274\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3047 - acc: 0.8662 - val_loss: 0.4002 - val_acc: 0.8274\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 21s 245us/sample - loss: 0.5059 - acc: 0.7562 - val_loss: 0.4519 - val_acc: 0.7850\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4498 - acc: 0.7898 - val_loss: 0.4297 - val_acc: 0.7997\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4284 - acc: 0.8033 - val_loss: 0.4400 - val_acc: 0.8002\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4156 - acc: 0.8098 - val_loss: 0.4245 - val_acc: 0.8051\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4054 - acc: 0.8155 - val_loss: 0.4309 - val_acc: 0.8010\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3955 - acc: 0.8205 - val_loss: 0.4045 - val_acc: 0.8127\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3916 - acc: 0.8234 - val_loss: 0.4048 - val_acc: 0.8120\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3836 - acc: 0.8279 - val_loss: 0.4156 - val_acc: 0.8064\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3769 - acc: 0.8306 - val_loss: 0.3781 - val_acc: 0.8286\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3716 - acc: 0.8337 - val_loss: 0.3911 - val_acc: 0.8223\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3655 - acc: 0.8362 - val_loss: 0.3843 - val_acc: 0.8249\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3587 - acc: 0.8394 - val_loss: 0.3807 - val_acc: 0.8245\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3543 - acc: 0.8431 - val_loss: 0.3930 - val_acc: 0.8231\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3495 - acc: 0.8437 - val_loss: 0.3811 - val_acc: 0.8256\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3421 - acc: 0.8488 - val_loss: 0.3805 - val_acc: 0.8284\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3393 - acc: 0.8502 - val_loss: 0.3822 - val_acc: 0.8271\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3338 - acc: 0.8531 - val_loss: 0.3830 - val_acc: 0.8270\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3287 - acc: 0.8559 - val_loss: 0.3986 - val_acc: 0.8253\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3254 - acc: 0.8565 - val_loss: 0.3779 - val_acc: 0.8286\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3227 - acc: 0.8580 - val_loss: 0.3829 - val_acc: 0.8278\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3165 - acc: 0.8614 - val_loss: 0.3844 - val_acc: 0.8284\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3139 - acc: 0.8615 - val_loss: 0.3970 - val_acc: 0.8262\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3108 - acc: 0.8639 - val_loss: 0.3869 - val_acc: 0.8293\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3062 - acc: 0.8663 - val_loss: 0.3914 - val_acc: 0.8273\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3034 - acc: 0.8660 - val_loss: 0.4049 - val_acc: 0.8210\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.2986 - acc: 0.8705 - val_loss: 0.4070 - val_acc: 0.8236\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.2951 - acc: 0.8715 - val_loss: 0.3953 - val_acc: 0.8210\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.2932 - acc: 0.8723 - val_loss: 0.4040 - val_acc: 0.8252\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.2913 - acc: 0.8744 - val_loss: 0.4040 - val_acc: 0.8314\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.2855 - acc: 0.8762 - val_loss: 0.4028 - val_acc: 0.8221\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 21s 243us/sample - loss: 0.5607 - acc: 0.7244 - val_loss: 0.4914 - val_acc: 0.7643\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4946 - acc: 0.7621 - val_loss: 0.4592 - val_acc: 0.7802\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4715 - acc: 0.7762 - val_loss: 0.4671 - val_acc: 0.7766\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4615 - acc: 0.7838 - val_loss: 0.4386 - val_acc: 0.7954\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4541 - acc: 0.7867 - val_loss: 0.4292 - val_acc: 0.8008\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4473 - acc: 0.7914 - val_loss: 0.4275 - val_acc: 0.8002\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4420 - acc: 0.7953 - val_loss: 0.4430 - val_acc: 0.7916\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4384 - acc: 0.7961 - val_loss: 0.4195 - val_acc: 0.8065\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4363 - acc: 0.7985 - val_loss: 0.4170 - val_acc: 0.8069\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4332 - acc: 0.7996 - val_loss: 0.4175 - val_acc: 0.8087\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4292 - acc: 0.8012 - val_loss: 0.4252 - val_acc: 0.8033\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4283 - acc: 0.8032 - val_loss: 0.4170 - val_acc: 0.8081\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4252 - acc: 0.8036 - val_loss: 0.4124 - val_acc: 0.8126\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4228 - acc: 0.8060 - val_loss: 0.4066 - val_acc: 0.8130\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4210 - acc: 0.8058 - val_loss: 0.4077 - val_acc: 0.8134\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4181 - acc: 0.8069 - val_loss: 0.4151 - val_acc: 0.8083\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4157 - acc: 0.8085 - val_loss: 0.4134 - val_acc: 0.8079\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4168 - acc: 0.8079 - val_loss: 0.4170 - val_acc: 0.8090\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4142 - acc: 0.8096 - val_loss: 0.4212 - val_acc: 0.8084\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4122 - acc: 0.8108 - val_loss: 0.4037 - val_acc: 0.8166\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4126 - acc: 0.8108 - val_loss: 0.4054 - val_acc: 0.8150\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4104 - acc: 0.8114 - val_loss: 0.4061 - val_acc: 0.8145\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4083 - acc: 0.8129 - val_loss: 0.3989 - val_acc: 0.8177\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4056 - acc: 0.8128 - val_loss: 0.4154 - val_acc: 0.8120\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4075 - acc: 0.8136 - val_loss: 0.4072 - val_acc: 0.8149\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4058 - acc: 0.8149 - val_loss: 0.4004 - val_acc: 0.8170\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4048 - acc: 0.8155 - val_loss: 0.4015 - val_acc: 0.8166\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4049 - acc: 0.8149 - val_loss: 0.4010 - val_acc: 0.8177\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4027 - acc: 0.8164 - val_loss: 0.4052 - val_acc: 0.8142\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4011 - acc: 0.8178 - val_loss: 0.4006 - val_acc: 0.8173\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 21s 245us/sample - loss: 0.5447 - acc: 0.7306 - val_loss: 0.4821 - val_acc: 0.7668\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4892 - acc: 0.7628 - val_loss: 0.4614 - val_acc: 0.7794\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4736 - acc: 0.7740 - val_loss: 0.4442 - val_acc: 0.7933\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4621 - acc: 0.7823 - val_loss: 0.4324 - val_acc: 0.7983\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4494 - acc: 0.7901 - val_loss: 0.4306 - val_acc: 0.8000\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4434 - acc: 0.7934 - val_loss: 0.4263 - val_acc: 0.7993\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4391 - acc: 0.7946 - val_loss: 0.4667 - val_acc: 0.7770\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4354 - acc: 0.7988 - val_loss: 0.4103 - val_acc: 0.8094\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4299 - acc: 0.8008 - val_loss: 0.4139 - val_acc: 0.8113\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4281 - acc: 0.8023 - val_loss: 0.4381 - val_acc: 0.7905\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4237 - acc: 0.8044 - val_loss: 0.4098 - val_acc: 0.8112\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4226 - acc: 0.8046 - val_loss: 0.4093 - val_acc: 0.8125\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4188 - acc: 0.8079 - val_loss: 0.4072 - val_acc: 0.8127\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4157 - acc: 0.8098 - val_loss: 0.4074 - val_acc: 0.8084\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4183 - acc: 0.8072 - val_loss: 0.4051 - val_acc: 0.8148\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4142 - acc: 0.8095 - val_loss: 0.4021 - val_acc: 0.8138\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4131 - acc: 0.8111 - val_loss: 0.4054 - val_acc: 0.8116\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4133 - acc: 0.8097 - val_loss: 0.4091 - val_acc: 0.8134\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4078 - acc: 0.8145 - val_loss: 0.4072 - val_acc: 0.8153\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4085 - acc: 0.8125 - val_loss: 0.4036 - val_acc: 0.8142\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4079 - acc: 0.8145 - val_loss: 0.4005 - val_acc: 0.8176\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4043 - acc: 0.8160 - val_loss: 0.4088 - val_acc: 0.8105\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4034 - acc: 0.8166 - val_loss: 0.4065 - val_acc: 0.8133\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4038 - acc: 0.8161 - val_loss: 0.3977 - val_acc: 0.8198\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4020 - acc: 0.8161 - val_loss: 0.4022 - val_acc: 0.8152\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3999 - acc: 0.8182 - val_loss: 0.4070 - val_acc: 0.8151\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3998 - acc: 0.8169 - val_loss: 0.3965 - val_acc: 0.8187\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3971 - acc: 0.8192 - val_loss: 0.3999 - val_acc: 0.8176\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3974 - acc: 0.8187 - val_loss: 0.3938 - val_acc: 0.8209\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3974 - acc: 0.8179 - val_loss: 0.3925 - val_acc: 0.8220\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 22s 257us/sample - loss: 0.5748 - acc: 0.7132 - val_loss: 0.4948 - val_acc: 0.7614\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.5111 - acc: 0.7521 - val_loss: 0.4926 - val_acc: 0.7652\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4943 - acc: 0.7625 - val_loss: 0.4683 - val_acc: 0.7804\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4857 - acc: 0.7693 - val_loss: 0.4585 - val_acc: 0.7845\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4765 - acc: 0.7736 - val_loss: 0.4482 - val_acc: 0.7885\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4691 - acc: 0.7790 - val_loss: 0.4415 - val_acc: 0.7929\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4625 - acc: 0.7824 - val_loss: 0.4442 - val_acc: 0.7932\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4571 - acc: 0.7860 - val_loss: 0.4368 - val_acc: 0.7952\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4531 - acc: 0.7884 - val_loss: 0.4514 - val_acc: 0.7918\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4507 - acc: 0.7904 - val_loss: 0.4289 - val_acc: 0.7997\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4472 - acc: 0.7922 - val_loss: 0.4201 - val_acc: 0.8062\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4448 - acc: 0.7944 - val_loss: 0.4239 - val_acc: 0.8066\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4436 - acc: 0.7945 - val_loss: 0.4190 - val_acc: 0.8073\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4430 - acc: 0.7941 - val_loss: 0.4212 - val_acc: 0.8040\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4416 - acc: 0.7961 - val_loss: 0.4138 - val_acc: 0.8097\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4407 - acc: 0.7944 - val_loss: 0.4284 - val_acc: 0.7996\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4397 - acc: 0.7952 - val_loss: 0.4220 - val_acc: 0.8058\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4387 - acc: 0.7956 - val_loss: 0.4157 - val_acc: 0.8099\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4379 - acc: 0.7960 - val_loss: 0.4145 - val_acc: 0.8105\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4379 - acc: 0.7959 - val_loss: 0.4114 - val_acc: 0.8110\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4374 - acc: 0.7968 - val_loss: 0.4119 - val_acc: 0.8116\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4364 - acc: 0.7992 - val_loss: 0.4073 - val_acc: 0.8140\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4374 - acc: 0.7973 - val_loss: 0.4321 - val_acc: 0.8019\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4350 - acc: 0.7996 - val_loss: 0.4239 - val_acc: 0.8049\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4352 - acc: 0.7985 - val_loss: 0.4109 - val_acc: 0.8095\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4330 - acc: 0.7994 - val_loss: 0.4058 - val_acc: 0.8129\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4318 - acc: 0.7998 - val_loss: 0.4072 - val_acc: 0.8152\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4324 - acc: 0.8011 - val_loss: 0.4202 - val_acc: 0.8082\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4311 - acc: 0.8007 - val_loss: 0.4068 - val_acc: 0.8152\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4305 - acc: 0.8017 - val_loss: 0.4045 - val_acc: 0.8148\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 272us/sample - loss: 0.6055 - acc: 0.6864 - val_loss: 0.5023 - val_acc: 0.7581\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.5188 - acc: 0.7473 - val_loss: 0.5253 - val_acc: 0.7449\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4998 - acc: 0.7593 - val_loss: 0.4651 - val_acc: 0.7808\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4868 - acc: 0.7674 - val_loss: 0.4600 - val_acc: 0.7855\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4794 - acc: 0.7723 - val_loss: 0.4514 - val_acc: 0.7855\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4745 - acc: 0.7741 - val_loss: 0.4431 - val_acc: 0.7948\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4693 - acc: 0.7776 - val_loss: 0.4390 - val_acc: 0.7958\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4652 - acc: 0.7802 - val_loss: 0.4367 - val_acc: 0.7951\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4614 - acc: 0.7826 - val_loss: 0.4299 - val_acc: 0.8015\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4581 - acc: 0.7827 - val_loss: 0.4381 - val_acc: 0.7966\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4575 - acc: 0.7840 - val_loss: 0.4495 - val_acc: 0.7879\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4545 - acc: 0.7855 - val_loss: 0.4246 - val_acc: 0.8048\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4537 - acc: 0.7882 - val_loss: 0.4424 - val_acc: 0.7914\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4527 - acc: 0.7876 - val_loss: 0.4289 - val_acc: 0.8062\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4520 - acc: 0.7887 - val_loss: 0.4228 - val_acc: 0.8042\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4486 - acc: 0.7911 - val_loss: 0.4222 - val_acc: 0.8074\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4490 - acc: 0.7898 - val_loss: 0.4387 - val_acc: 0.7946\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4462 - acc: 0.7906 - val_loss: 0.4181 - val_acc: 0.8099\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4465 - acc: 0.7915 - val_loss: 0.4387 - val_acc: 0.7964\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4452 - acc: 0.7924 - val_loss: 0.4234 - val_acc: 0.8051\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4446 - acc: 0.7927 - val_loss: 0.4155 - val_acc: 0.8097\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4465 - acc: 0.7901 - val_loss: 0.4301 - val_acc: 0.8025\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4427 - acc: 0.7932 - val_loss: 0.4235 - val_acc: 0.8049\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4440 - acc: 0.7917 - val_loss: 0.4186 - val_acc: 0.8078\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4424 - acc: 0.7937 - val_loss: 0.4160 - val_acc: 0.8110\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4434 - acc: 0.7947 - val_loss: 0.4380 - val_acc: 0.7984\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4396 - acc: 0.7952 - val_loss: 0.4299 - val_acc: 0.8036\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4414 - acc: 0.7935 - val_loss: 0.4199 - val_acc: 0.8104\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4400 - acc: 0.7960 - val_loss: 0.4238 - val_acc: 0.8088\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4399 - acc: 0.7948 - val_loss: 0.4227 - val_acc: 0.8062\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 22s 255us/sample - loss: 0.5364 - acc: 0.7386 - val_loss: 0.4668 - val_acc: 0.7758\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4670 - acc: 0.7786 - val_loss: 0.4450 - val_acc: 0.7895\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4474 - acc: 0.7900 - val_loss: 0.4338 - val_acc: 0.7987\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4335 - acc: 0.7994 - val_loss: 0.4258 - val_acc: 0.8031\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4238 - acc: 0.8060 - val_loss: 0.4450 - val_acc: 0.7908\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4167 - acc: 0.8080 - val_loss: 0.4029 - val_acc: 0.8162\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4108 - acc: 0.8132 - val_loss: 0.4068 - val_acc: 0.8136\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4039 - acc: 0.8167 - val_loss: 0.4060 - val_acc: 0.8131\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3989 - acc: 0.8182 - val_loss: 0.4017 - val_acc: 0.8166\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3965 - acc: 0.8196 - val_loss: 0.4111 - val_acc: 0.8112\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3918 - acc: 0.8220 - val_loss: 0.3876 - val_acc: 0.8225\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3892 - acc: 0.8231 - val_loss: 0.4054 - val_acc: 0.8133\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3846 - acc: 0.8257 - val_loss: 0.4032 - val_acc: 0.8139\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3814 - acc: 0.8273 - val_loss: 0.3897 - val_acc: 0.8221\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3752 - acc: 0.8311 - val_loss: 0.4145 - val_acc: 0.8133\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3746 - acc: 0.8320 - val_loss: 0.3919 - val_acc: 0.8222\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3720 - acc: 0.8323 - val_loss: 0.3910 - val_acc: 0.8220\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3699 - acc: 0.8335 - val_loss: 0.3970 - val_acc: 0.8185\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3668 - acc: 0.8359 - val_loss: 0.3908 - val_acc: 0.8220\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3643 - acc: 0.8364 - val_loss: 0.3920 - val_acc: 0.8199\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3611 - acc: 0.8395 - val_loss: 0.3918 - val_acc: 0.8204\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3579 - acc: 0.8399 - val_loss: 0.3900 - val_acc: 0.8238\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3579 - acc: 0.8386 - val_loss: 0.3911 - val_acc: 0.8212\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3533 - acc: 0.8427 - val_loss: 0.3980 - val_acc: 0.8195\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3511 - acc: 0.8438 - val_loss: 0.4068 - val_acc: 0.8179\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3496 - acc: 0.8435 - val_loss: 0.3964 - val_acc: 0.8228\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3478 - acc: 0.8444 - val_loss: 0.4057 - val_acc: 0.8204\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3453 - acc: 0.8462 - val_loss: 0.3905 - val_acc: 0.8243\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3428 - acc: 0.8463 - val_loss: 0.4018 - val_acc: 0.8224\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3400 - acc: 0.8502 - val_loss: 0.3943 - val_acc: 0.8231\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 22s 258us/sample - loss: 0.5150 - acc: 0.7485 - val_loss: 0.4582 - val_acc: 0.7814\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4617 - acc: 0.7820 - val_loss: 0.4367 - val_acc: 0.7962\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4416 - acc: 0.7937 - val_loss: 0.4294 - val_acc: 0.8015\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4298 - acc: 0.8018 - val_loss: 0.4194 - val_acc: 0.8056\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4191 - acc: 0.8068 - val_loss: 0.4108 - val_acc: 0.8124\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4107 - acc: 0.8105 - val_loss: 0.4206 - val_acc: 0.8051\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4045 - acc: 0.8165 - val_loss: 0.4048 - val_acc: 0.8124\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3999 - acc: 0.8185 - val_loss: 0.3979 - val_acc: 0.8189\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3953 - acc: 0.8196 - val_loss: 0.3974 - val_acc: 0.8193\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3904 - acc: 0.8232 - val_loss: 0.3919 - val_acc: 0.8217\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3861 - acc: 0.8252 - val_loss: 0.3999 - val_acc: 0.8181\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3839 - acc: 0.8265 - val_loss: 0.4002 - val_acc: 0.8177\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3789 - acc: 0.8303 - val_loss: 0.3855 - val_acc: 0.8261\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3741 - acc: 0.8314 - val_loss: 0.4173 - val_acc: 0.8090\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3720 - acc: 0.8334 - val_loss: 0.3964 - val_acc: 0.8195\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3695 - acc: 0.8347 - val_loss: 0.3874 - val_acc: 0.8232\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3681 - acc: 0.8362 - val_loss: 0.3975 - val_acc: 0.8215\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3635 - acc: 0.8380 - val_loss: 0.3844 - val_acc: 0.8243\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3590 - acc: 0.8400 - val_loss: 0.4000 - val_acc: 0.8196\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3588 - acc: 0.8391 - val_loss: 0.3906 - val_acc: 0.8235\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3562 - acc: 0.8410 - val_loss: 0.3923 - val_acc: 0.8238\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3540 - acc: 0.8428 - val_loss: 0.4011 - val_acc: 0.8209\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3494 - acc: 0.8437 - val_loss: 0.3902 - val_acc: 0.8214\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3474 - acc: 0.8455 - val_loss: 0.4035 - val_acc: 0.8200\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3447 - acc: 0.8458 - val_loss: 0.4003 - val_acc: 0.8201\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3447 - acc: 0.8471 - val_loss: 0.3958 - val_acc: 0.8216\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3382 - acc: 0.8500 - val_loss: 0.4002 - val_acc: 0.8224\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3391 - acc: 0.8502 - val_loss: 0.3940 - val_acc: 0.8217\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3361 - acc: 0.8515 - val_loss: 0.4052 - val_acc: 0.8188\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3356 - acc: 0.8511 - val_loss: 0.3927 - val_acc: 0.8243\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 271us/sample - loss: 0.5574 - acc: 0.7307 - val_loss: 0.4794 - val_acc: 0.7732\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4757 - acc: 0.7739 - val_loss: 0.4606 - val_acc: 0.7814\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4517 - acc: 0.7876 - val_loss: 0.4579 - val_acc: 0.7837\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4380 - acc: 0.7969 - val_loss: 0.4196 - val_acc: 0.8077\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4285 - acc: 0.8026 - val_loss: 0.4117 - val_acc: 0.8105\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4231 - acc: 0.8056 - val_loss: 0.4116 - val_acc: 0.8124\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4167 - acc: 0.8087 - val_loss: 0.4001 - val_acc: 0.8191\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4116 - acc: 0.8119 - val_loss: 0.3980 - val_acc: 0.8189\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4094 - acc: 0.8131 - val_loss: 0.3907 - val_acc: 0.8230\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4058 - acc: 0.8141 - val_loss: 0.3968 - val_acc: 0.8210\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4012 - acc: 0.8175 - val_loss: 0.4355 - val_acc: 0.8018\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.4003 - acc: 0.8182 - val_loss: 0.4109 - val_acc: 0.8101\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3949 - acc: 0.8213 - val_loss: 0.3905 - val_acc: 0.8233\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3937 - acc: 0.8220 - val_loss: 0.3944 - val_acc: 0.8214\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3893 - acc: 0.8234 - val_loss: 0.3874 - val_acc: 0.8236\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3899 - acc: 0.8220 - val_loss: 0.4334 - val_acc: 0.8033\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3870 - acc: 0.8242 - val_loss: 0.3848 - val_acc: 0.8249\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3864 - acc: 0.8247 - val_loss: 0.3850 - val_acc: 0.8266\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3849 - acc: 0.8252 - val_loss: 0.3848 - val_acc: 0.8229\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3829 - acc: 0.8266 - val_loss: 0.3930 - val_acc: 0.8209\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3809 - acc: 0.8276 - val_loss: 0.3842 - val_acc: 0.8260\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3798 - acc: 0.8292 - val_loss: 0.3767 - val_acc: 0.8306\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3789 - acc: 0.8294 - val_loss: 0.3795 - val_acc: 0.8275\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3800 - acc: 0.8270 - val_loss: 0.3938 - val_acc: 0.8219\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3776 - acc: 0.8294 - val_loss: 0.3777 - val_acc: 0.8256\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3776 - acc: 0.8286 - val_loss: 0.3779 - val_acc: 0.8294\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3752 - acc: 0.8313 - val_loss: 0.3848 - val_acc: 0.8256\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3741 - acc: 0.8307 - val_loss: 0.3788 - val_acc: 0.8271\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3746 - acc: 0.8320 - val_loss: 0.3805 - val_acc: 0.8269\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.3732 - acc: 0.8303 - val_loss: 0.3788 - val_acc: 0.8271\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 280us/sample - loss: 0.5588 - acc: 0.7286 - val_loss: 0.4777 - val_acc: 0.7724\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4840 - acc: 0.7690 - val_loss: 0.4674 - val_acc: 0.7754\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4634 - acc: 0.7831 - val_loss: 0.4426 - val_acc: 0.7923\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4485 - acc: 0.7906 - val_loss: 0.4303 - val_acc: 0.7974\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4361 - acc: 0.7992 - val_loss: 0.4192 - val_acc: 0.8065\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4292 - acc: 0.8029 - val_loss: 0.4146 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4228 - acc: 0.8060 - val_loss: 0.4242 - val_acc: 0.8017\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4184 - acc: 0.8071 - val_loss: 0.4035 - val_acc: 0.8141\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4126 - acc: 0.8116 - val_loss: 0.3986 - val_acc: 0.8191\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4088 - acc: 0.8141 - val_loss: 0.4208 - val_acc: 0.8048\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4046 - acc: 0.8172 - val_loss: 0.4056 - val_acc: 0.8123\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4030 - acc: 0.8180 - val_loss: 0.3960 - val_acc: 0.8170\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.4005 - acc: 0.8189 - val_loss: 0.3875 - val_acc: 0.8243\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3965 - acc: 0.8196 - val_loss: 0.3906 - val_acc: 0.8263\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3962 - acc: 0.8202 - val_loss: 0.4069 - val_acc: 0.8112\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3939 - acc: 0.8216 - val_loss: 0.4175 - val_acc: 0.8115\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3901 - acc: 0.8244 - val_loss: 0.3796 - val_acc: 0.8296\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3890 - acc: 0.8237 - val_loss: 0.3857 - val_acc: 0.8260\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3889 - acc: 0.8233 - val_loss: 0.3842 - val_acc: 0.8255\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3874 - acc: 0.8262 - val_loss: 0.3830 - val_acc: 0.8271\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3847 - acc: 0.8256 - val_loss: 0.4007 - val_acc: 0.8199\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3837 - acc: 0.8274 - val_loss: 0.3825 - val_acc: 0.8269\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3830 - acc: 0.8271 - val_loss: 0.4084 - val_acc: 0.8138\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3816 - acc: 0.8280 - val_loss: 0.3815 - val_acc: 0.8274\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3800 - acc: 0.8290 - val_loss: 0.3825 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.3786 - acc: 0.8296 - val_loss: 0.3868 - val_acc: 0.8241\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3775 - acc: 0.8293 - val_loss: 0.3987 - val_acc: 0.8212\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3791 - acc: 0.8295 - val_loss: 0.3760 - val_acc: 0.8295\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3796 - acc: 0.8281 - val_loss: 0.3859 - val_acc: 0.8242\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3776 - acc: 0.8304 - val_loss: 0.4096 - val_acc: 0.8130\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 265us/sample - loss: 0.5177 - acc: 0.7494 - val_loss: 0.4529 - val_acc: 0.7872\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4541 - acc: 0.7869 - val_loss: 0.4710 - val_acc: 0.7801\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4338 - acc: 0.7967 - val_loss: 0.4168 - val_acc: 0.8075\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4189 - acc: 0.8075 - val_loss: 0.4065 - val_acc: 0.8141\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4101 - acc: 0.8121 - val_loss: 0.4146 - val_acc: 0.8084\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4034 - acc: 0.8170 - val_loss: 0.3968 - val_acc: 0.8195\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3974 - acc: 0.8201 - val_loss: 0.3928 - val_acc: 0.8210\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3897 - acc: 0.8239 - val_loss: 0.4043 - val_acc: 0.8137\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3852 - acc: 0.8266 - val_loss: 0.4058 - val_acc: 0.8138\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3798 - acc: 0.8292 - val_loss: 0.3904 - val_acc: 0.8220\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3758 - acc: 0.8309 - val_loss: 0.3995 - val_acc: 0.8167\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3704 - acc: 0.8343 - val_loss: 0.3802 - val_acc: 0.8257\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3640 - acc: 0.8365 - val_loss: 0.3942 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3614 - acc: 0.8386 - val_loss: 0.3911 - val_acc: 0.8227\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3573 - acc: 0.8405 - val_loss: 0.3851 - val_acc: 0.8278\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3525 - acc: 0.8428 - val_loss: 0.3912 - val_acc: 0.8228\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3467 - acc: 0.8457 - val_loss: 0.3819 - val_acc: 0.8260\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3446 - acc: 0.8478 - val_loss: 0.3935 - val_acc: 0.8237\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3400 - acc: 0.8483 - val_loss: 0.3945 - val_acc: 0.8209\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3358 - acc: 0.8510 - val_loss: 0.3976 - val_acc: 0.8201\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3321 - acc: 0.8529 - val_loss: 0.3948 - val_acc: 0.8224\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3264 - acc: 0.8564 - val_loss: 0.4058 - val_acc: 0.8215\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3251 - acc: 0.8566 - val_loss: 0.3877 - val_acc: 0.8287\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3188 - acc: 0.8591 - val_loss: 0.3951 - val_acc: 0.8268\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3161 - acc: 0.8593 - val_loss: 0.3948 - val_acc: 0.8217\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3118 - acc: 0.8616 - val_loss: 0.3988 - val_acc: 0.8246\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3091 - acc: 0.8642 - val_loss: 0.4065 - val_acc: 0.8274\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.3031 - acc: 0.8659 - val_loss: 0.4038 - val_acc: 0.8249\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3053 - acc: 0.8646 - val_loss: 0.4016 - val_acc: 0.8257\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3014 - acc: 0.8673 - val_loss: 0.4108 - val_acc: 0.8245\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 268us/sample - loss: 0.5142 - acc: 0.7510 - val_loss: 0.4468 - val_acc: 0.7901\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4519 - acc: 0.7876 - val_loss: 0.4502 - val_acc: 0.7876\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4308 - acc: 0.8010 - val_loss: 0.4191 - val_acc: 0.8036\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4205 - acc: 0.8067 - val_loss: 0.4077 - val_acc: 0.8121\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4103 - acc: 0.8130 - val_loss: 0.4023 - val_acc: 0.8184\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4015 - acc: 0.8163 - val_loss: 0.3950 - val_acc: 0.8225\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3964 - acc: 0.8199 - val_loss: 0.3965 - val_acc: 0.8177\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3914 - acc: 0.8233 - val_loss: 0.3921 - val_acc: 0.8239\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3836 - acc: 0.8271 - val_loss: 0.3976 - val_acc: 0.8185\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3790 - acc: 0.8296 - val_loss: 0.3870 - val_acc: 0.8249\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3728 - acc: 0.8331 - val_loss: 0.3936 - val_acc: 0.8197\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3677 - acc: 0.8362 - val_loss: 0.3875 - val_acc: 0.8252\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3647 - acc: 0.8360 - val_loss: 0.3851 - val_acc: 0.8267\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3593 - acc: 0.8386 - val_loss: 0.3857 - val_acc: 0.8253\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3555 - acc: 0.8412 - val_loss: 0.3806 - val_acc: 0.8286\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3505 - acc: 0.8436 - val_loss: 0.3845 - val_acc: 0.8260\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3476 - acc: 0.8462 - val_loss: 0.3890 - val_acc: 0.8228\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3424 - acc: 0.8472 - val_loss: 0.3908 - val_acc: 0.8270\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3393 - acc: 0.8493 - val_loss: 0.4039 - val_acc: 0.8180\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3365 - acc: 0.8508 - val_loss: 0.3938 - val_acc: 0.8278\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3329 - acc: 0.8532 - val_loss: 0.3917 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3284 - acc: 0.8543 - val_loss: 0.4001 - val_acc: 0.8214\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3247 - acc: 0.8562 - val_loss: 0.3929 - val_acc: 0.8234\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3191 - acc: 0.8590 - val_loss: 0.4097 - val_acc: 0.8156\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3178 - acc: 0.8604 - val_loss: 0.3937 - val_acc: 0.8281\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3136 - acc: 0.8621 - val_loss: 0.3964 - val_acc: 0.8268\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3082 - acc: 0.8642 - val_loss: 0.3953 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3068 - acc: 0.8651 - val_loss: 0.3980 - val_acc: 0.8255\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3030 - acc: 0.8668 - val_loss: 0.3957 - val_acc: 0.8237\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.3002 - acc: 0.8681 - val_loss: 0.4093 - val_acc: 0.8212\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 281us/sample - loss: 0.5315 - acc: 0.7450 - val_loss: 0.4551 - val_acc: 0.7856\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4562 - acc: 0.7859 - val_loss: 0.4283 - val_acc: 0.8016\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4341 - acc: 0.7992 - val_loss: 0.4233 - val_acc: 0.8033\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4223 - acc: 0.8055 - val_loss: 0.4167 - val_acc: 0.8110\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4103 - acc: 0.8130 - val_loss: 0.4042 - val_acc: 0.8169\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4047 - acc: 0.8151 - val_loss: 0.4142 - val_acc: 0.8089\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3973 - acc: 0.8191 - val_loss: 0.4411 - val_acc: 0.7976\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3917 - acc: 0.8225 - val_loss: 0.3897 - val_acc: 0.8198\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3858 - acc: 0.8256 - val_loss: 0.3895 - val_acc: 0.8226\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3822 - acc: 0.8276 - val_loss: 0.4043 - val_acc: 0.8188\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3759 - acc: 0.8321 - val_loss: 0.3819 - val_acc: 0.8252\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3721 - acc: 0.8332 - val_loss: 0.3781 - val_acc: 0.8279\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3682 - acc: 0.8351 - val_loss: 0.3911 - val_acc: 0.8232\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3653 - acc: 0.8373 - val_loss: 0.3759 - val_acc: 0.8299\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3631 - acc: 0.8382 - val_loss: 0.3899 - val_acc: 0.8224\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3594 - acc: 0.8404 - val_loss: 0.3735 - val_acc: 0.8310\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3549 - acc: 0.8420 - val_loss: 0.3851 - val_acc: 0.8245\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3518 - acc: 0.8438 - val_loss: 0.3756 - val_acc: 0.8310\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3490 - acc: 0.8439 - val_loss: 0.3942 - val_acc: 0.8203\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3460 - acc: 0.8445 - val_loss: 0.3741 - val_acc: 0.8328\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3442 - acc: 0.8483 - val_loss: 0.3918 - val_acc: 0.8288\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3422 - acc: 0.8472 - val_loss: 0.3790 - val_acc: 0.8300\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3398 - acc: 0.8503 - val_loss: 0.3771 - val_acc: 0.8294\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3362 - acc: 0.8511 - val_loss: 0.3814 - val_acc: 0.8293\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3345 - acc: 0.8510 - val_loss: 0.3723 - val_acc: 0.8344\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3308 - acc: 0.8542 - val_loss: 0.4104 - val_acc: 0.8206\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3274 - acc: 0.8546 - val_loss: 0.3816 - val_acc: 0.8309\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3272 - acc: 0.8553 - val_loss: 0.3896 - val_acc: 0.8295\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3252 - acc: 0.8564 - val_loss: 0.3859 - val_acc: 0.8288\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3236 - acc: 0.8566 - val_loss: 0.3895 - val_acc: 0.8291\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 293us/sample - loss: 0.5387 - acc: 0.7398 - val_loss: 0.4624 - val_acc: 0.7802\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4626 - acc: 0.7827 - val_loss: 0.4429 - val_acc: 0.7947\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.4400 - acc: 0.7945 - val_loss: 0.4183 - val_acc: 0.8056\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4252 - acc: 0.8042 - val_loss: 0.4152 - val_acc: 0.8076\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4149 - acc: 0.8100 - val_loss: 0.4474 - val_acc: 0.7917\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4085 - acc: 0.8133 - val_loss: 0.3970 - val_acc: 0.8196\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4005 - acc: 0.8166 - val_loss: 0.3948 - val_acc: 0.8202\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3943 - acc: 0.8226 - val_loss: 0.4072 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3885 - acc: 0.8254 - val_loss: 0.3887 - val_acc: 0.8219\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3826 - acc: 0.8270 - val_loss: 0.3886 - val_acc: 0.8234\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3794 - acc: 0.8298 - val_loss: 0.3769 - val_acc: 0.8302\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3751 - acc: 0.8322 - val_loss: 0.3830 - val_acc: 0.8235\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3705 - acc: 0.8345 - val_loss: 0.3794 - val_acc: 0.8273\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3658 - acc: 0.8374 - val_loss: 0.3943 - val_acc: 0.8194\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3639 - acc: 0.8369 - val_loss: 0.3807 - val_acc: 0.8285\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3596 - acc: 0.8391 - val_loss: 0.3781 - val_acc: 0.8293\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3585 - acc: 0.8410 - val_loss: 0.3728 - val_acc: 0.8326\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3545 - acc: 0.8417 - val_loss: 0.3747 - val_acc: 0.8316\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3505 - acc: 0.8441 - val_loss: 0.3875 - val_acc: 0.8270\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3493 - acc: 0.8449 - val_loss: 0.4019 - val_acc: 0.8184\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3452 - acc: 0.8464 - val_loss: 0.3883 - val_acc: 0.8256\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3417 - acc: 0.8500 - val_loss: 0.3888 - val_acc: 0.8243\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3403 - acc: 0.8494 - val_loss: 0.3865 - val_acc: 0.8255\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3393 - acc: 0.8497 - val_loss: 0.3764 - val_acc: 0.8298\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.3363 - acc: 0.8505 - val_loss: 0.3860 - val_acc: 0.8257\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3359 - acc: 0.8509 - val_loss: 0.3727 - val_acc: 0.8325\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3314 - acc: 0.8536 - val_loss: 0.3810 - val_acc: 0.8334\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3322 - acc: 0.8529 - val_loss: 0.3839 - val_acc: 0.8276\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3273 - acc: 0.8557 - val_loss: 0.3826 - val_acc: 0.8289\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3277 - acc: 0.8555 - val_loss: 0.3789 - val_acc: 0.8339\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 277us/sample - loss: 0.5120 - acc: 0.7531 - val_loss: 0.4602 - val_acc: 0.7806\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4480 - acc: 0.7902 - val_loss: 0.4251 - val_acc: 0.8037\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4291 - acc: 0.8021 - val_loss: 0.4255 - val_acc: 0.8026\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4164 - acc: 0.8079 - val_loss: 0.4196 - val_acc: 0.8072\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4092 - acc: 0.8126 - val_loss: 0.4804 - val_acc: 0.7725\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3998 - acc: 0.8180 - val_loss: 0.4024 - val_acc: 0.8164\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3913 - acc: 0.8224 - val_loss: 0.4056 - val_acc: 0.8140\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3859 - acc: 0.8252 - val_loss: 0.3944 - val_acc: 0.8187\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3790 - acc: 0.8302 - val_loss: 0.3888 - val_acc: 0.8232\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3744 - acc: 0.8316 - val_loss: 0.3827 - val_acc: 0.8260\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3685 - acc: 0.8344 - val_loss: 0.3850 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3650 - acc: 0.8357 - val_loss: 0.4051 - val_acc: 0.8104\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3583 - acc: 0.8404 - val_loss: 0.3945 - val_acc: 0.8178\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3521 - acc: 0.8424 - val_loss: 0.3870 - val_acc: 0.8244\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3480 - acc: 0.8454 - val_loss: 0.3859 - val_acc: 0.8253\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.3418 - acc: 0.8481 - val_loss: 0.3895 - val_acc: 0.8263\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3390 - acc: 0.8510 - val_loss: 0.4143 - val_acc: 0.8164\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3343 - acc: 0.8527 - val_loss: 0.3891 - val_acc: 0.8231\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.3284 - acc: 0.8554 - val_loss: 0.3999 - val_acc: 0.8196\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3264 - acc: 0.8567 - val_loss: 0.4003 - val_acc: 0.8252\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3225 - acc: 0.8580 - val_loss: 0.4003 - val_acc: 0.8172\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3163 - acc: 0.8613 - val_loss: 0.4149 - val_acc: 0.8186\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3120 - acc: 0.8629 - val_loss: 0.4078 - val_acc: 0.8209\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3083 - acc: 0.8654 - val_loss: 0.3964 - val_acc: 0.8220\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3018 - acc: 0.8675 - val_loss: 0.4011 - val_acc: 0.8220\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.2988 - acc: 0.8700 - val_loss: 0.4087 - val_acc: 0.8232\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 159us/sample - loss: 0.2964 - acc: 0.8718 - val_loss: 0.4077 - val_acc: 0.8238\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 151us/sample - loss: 0.2917 - acc: 0.8745 - val_loss: 0.4125 - val_acc: 0.8134\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.2896 - acc: 0.8744 - val_loss: 0.4322 - val_acc: 0.8212\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.2828 - acc: 0.8773 - val_loss: 0.4339 - val_acc: 0.8152\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 280us/sample - loss: 0.5119 - acc: 0.7512 - val_loss: 0.4996 - val_acc: 0.7530\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4494 - acc: 0.7891 - val_loss: 0.4597 - val_acc: 0.7782\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4290 - acc: 0.8023 - val_loss: 0.4259 - val_acc: 0.8040\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4185 - acc: 0.8080 - val_loss: 0.4034 - val_acc: 0.8164\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4077 - acc: 0.8134 - val_loss: 0.3991 - val_acc: 0.8158\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3997 - acc: 0.8180 - val_loss: 0.4009 - val_acc: 0.8163\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3937 - acc: 0.8213 - val_loss: 0.4004 - val_acc: 0.8149\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3863 - acc: 0.8249 - val_loss: 0.4076 - val_acc: 0.8120\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3828 - acc: 0.8288 - val_loss: 0.4167 - val_acc: 0.8117\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3752 - acc: 0.8309 - val_loss: 0.4453 - val_acc: 0.7937\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3697 - acc: 0.8349 - val_loss: 0.3797 - val_acc: 0.8275\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3638 - acc: 0.8374 - val_loss: 0.3904 - val_acc: 0.8231\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3607 - acc: 0.8391 - val_loss: 0.3864 - val_acc: 0.8223\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3556 - acc: 0.8418 - val_loss: 0.4017 - val_acc: 0.8164\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3504 - acc: 0.8438 - val_loss: 0.3824 - val_acc: 0.8260\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3456 - acc: 0.8468 - val_loss: 0.4008 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3445 - acc: 0.8457 - val_loss: 0.3869 - val_acc: 0.8225\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3362 - acc: 0.8507 - val_loss: 0.4209 - val_acc: 0.8156\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3316 - acc: 0.8539 - val_loss: 0.3934 - val_acc: 0.8202\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3269 - acc: 0.8560 - val_loss: 0.3940 - val_acc: 0.8246\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3213 - acc: 0.8568 - val_loss: 0.3941 - val_acc: 0.8270\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.3169 - acc: 0.8602 - val_loss: 0.4002 - val_acc: 0.8209\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3150 - acc: 0.8618 - val_loss: 0.3936 - val_acc: 0.8278\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3081 - acc: 0.8651 - val_loss: 0.4199 - val_acc: 0.8187\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3075 - acc: 0.8652 - val_loss: 0.4182 - val_acc: 0.8181\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3006 - acc: 0.8686 - val_loss: 0.4270 - val_acc: 0.8198\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.2989 - acc: 0.8699 - val_loss: 0.4002 - val_acc: 0.8217\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.2936 - acc: 0.8727 - val_loss: 0.4228 - val_acc: 0.8203\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.2919 - acc: 0.8720 - val_loss: 0.4089 - val_acc: 0.8242\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.2860 - acc: 0.8754 - val_loss: 0.4199 - val_acc: 0.8209\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 293us/sample - loss: 0.5105 - acc: 0.7571 - val_loss: 0.4486 - val_acc: 0.7910\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.4471 - acc: 0.7909 - val_loss: 0.4815 - val_acc: 0.7661\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.4265 - acc: 0.8041 - val_loss: 0.4098 - val_acc: 0.8121\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.4137 - acc: 0.8112 - val_loss: 0.4917 - val_acc: 0.7630\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.4050 - acc: 0.8152 - val_loss: 0.3975 - val_acc: 0.8172\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3986 - acc: 0.8202 - val_loss: 0.4034 - val_acc: 0.8135\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3900 - acc: 0.8234 - val_loss: 0.3905 - val_acc: 0.8213\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3838 - acc: 0.8284 - val_loss: 0.3846 - val_acc: 0.8234\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3777 - acc: 0.8313 - val_loss: 0.3908 - val_acc: 0.8184\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3705 - acc: 0.8340 - val_loss: 0.3895 - val_acc: 0.8206\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3662 - acc: 0.8367 - val_loss: 0.3774 - val_acc: 0.8266\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3612 - acc: 0.8403 - val_loss: 0.3951 - val_acc: 0.8212\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3560 - acc: 0.8415 - val_loss: 0.3871 - val_acc: 0.8254\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3506 - acc: 0.8436 - val_loss: 0.3800 - val_acc: 0.8308\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3444 - acc: 0.8477 - val_loss: 0.3827 - val_acc: 0.8279\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3400 - acc: 0.8492 - val_loss: 0.3827 - val_acc: 0.8275\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3370 - acc: 0.8518 - val_loss: 0.3866 - val_acc: 0.8306\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3309 - acc: 0.8547 - val_loss: 0.3846 - val_acc: 0.8237\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3271 - acc: 0.8560 - val_loss: 0.3879 - val_acc: 0.8231\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3223 - acc: 0.8588 - val_loss: 0.3827 - val_acc: 0.8299\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3172 - acc: 0.8609 - val_loss: 0.3827 - val_acc: 0.8256\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3150 - acc: 0.8620 - val_loss: 0.3920 - val_acc: 0.8241\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3098 - acc: 0.8647 - val_loss: 0.3937 - val_acc: 0.8272\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3084 - acc: 0.8647 - val_loss: 0.3929 - val_acc: 0.8268\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.3045 - acc: 0.8658 - val_loss: 0.3916 - val_acc: 0.8298\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.3002 - acc: 0.8687 - val_loss: 0.3936 - val_acc: 0.8302\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.2957 - acc: 0.8705 - val_loss: 0.4165 - val_acc: 0.8215\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.2915 - acc: 0.8740 - val_loss: 0.3932 - val_acc: 0.8274\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.2892 - acc: 0.8743 - val_loss: 0.4096 - val_acc: 0.8260\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 160us/sample - loss: 0.2871 - acc: 0.8742 - val_loss: 0.4139 - val_acc: 0.8271\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 308us/sample - loss: 0.5196 - acc: 0.7514 - val_loss: 0.4604 - val_acc: 0.7844\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4507 - acc: 0.7888 - val_loss: 0.4308 - val_acc: 0.7984\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4305 - acc: 0.8009 - val_loss: 0.4138 - val_acc: 0.8098\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4154 - acc: 0.8085 - val_loss: 0.4041 - val_acc: 0.8166\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4052 - acc: 0.8154 - val_loss: 0.4625 - val_acc: 0.7954\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3976 - acc: 0.8197 - val_loss: 0.4012 - val_acc: 0.8166\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3892 - acc: 0.8239 - val_loss: 0.3898 - val_acc: 0.8213\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3833 - acc: 0.8276 - val_loss: 0.3852 - val_acc: 0.8275\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3767 - acc: 0.8297 - val_loss: 0.4394 - val_acc: 0.8029\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3704 - acc: 0.8342 - val_loss: 0.3778 - val_acc: 0.8303\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3651 - acc: 0.8376 - val_loss: 0.3904 - val_acc: 0.8253\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3612 - acc: 0.8378 - val_loss: 0.3845 - val_acc: 0.8260\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3550 - acc: 0.8416 - val_loss: 0.3756 - val_acc: 0.8324\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3490 - acc: 0.8438 - val_loss: 0.3793 - val_acc: 0.8292\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3448 - acc: 0.8456 - val_loss: 0.4112 - val_acc: 0.8156\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3397 - acc: 0.8482 - val_loss: 0.3795 - val_acc: 0.8301\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3348 - acc: 0.8522 - val_loss: 0.3894 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3319 - acc: 0.8539 - val_loss: 0.3800 - val_acc: 0.8353\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3246 - acc: 0.8567 - val_loss: 0.3830 - val_acc: 0.8302\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3221 - acc: 0.8593 - val_loss: 0.3822 - val_acc: 0.8304\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3184 - acc: 0.8599 - val_loss: 0.3834 - val_acc: 0.8293\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3132 - acc: 0.8621 - val_loss: 0.3839 - val_acc: 0.8309\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3101 - acc: 0.8629 - val_loss: 0.3994 - val_acc: 0.8309\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3061 - acc: 0.8667 - val_loss: 0.4026 - val_acc: 0.8212\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3025 - acc: 0.8686 - val_loss: 0.3943 - val_acc: 0.8295\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3005 - acc: 0.8693 - val_loss: 0.3942 - val_acc: 0.8268\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.2984 - acc: 0.8699 - val_loss: 0.4029 - val_acc: 0.8302\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.2919 - acc: 0.8725 - val_loss: 0.4178 - val_acc: 0.8237\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.2924 - acc: 0.8726 - val_loss: 0.4157 - val_acc: 0.8127\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.2878 - acc: 0.8736 - val_loss: 0.4176 - val_acc: 0.8288\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 22s 260us/sample - loss: 0.5740 - acc: 0.7174 - val_loss: 0.4934 - val_acc: 0.7634\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.5013 - acc: 0.7560 - val_loss: 0.4732 - val_acc: 0.7737\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4862 - acc: 0.7663 - val_loss: 0.4831 - val_acc: 0.7656\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4761 - acc: 0.7730 - val_loss: 0.4654 - val_acc: 0.7797\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4681 - acc: 0.7777 - val_loss: 0.4821 - val_acc: 0.7668\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4597 - acc: 0.7827 - val_loss: 0.4402 - val_acc: 0.7986\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4537 - acc: 0.7875 - val_loss: 0.4416 - val_acc: 0.7912\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4496 - acc: 0.7893 - val_loss: 0.4307 - val_acc: 0.7991\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4454 - acc: 0.7916 - val_loss: 0.4334 - val_acc: 0.7973\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4418 - acc: 0.7952 - val_loss: 0.4237 - val_acc: 0.8032\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4388 - acc: 0.7959 - val_loss: 0.4308 - val_acc: 0.8023\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4370 - acc: 0.7975 - val_loss: 0.4267 - val_acc: 0.8041\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4362 - acc: 0.7976 - val_loss: 0.4150 - val_acc: 0.8094\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4354 - acc: 0.7977 - val_loss: 0.4263 - val_acc: 0.8052\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4355 - acc: 0.7970 - val_loss: 0.4291 - val_acc: 0.7993\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4339 - acc: 0.7988 - val_loss: 0.4157 - val_acc: 0.8098\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4311 - acc: 0.7993 - val_loss: 0.4128 - val_acc: 0.8113\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4327 - acc: 0.7998 - val_loss: 0.4143 - val_acc: 0.8094\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4297 - acc: 0.8019 - val_loss: 0.4173 - val_acc: 0.8096\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4297 - acc: 0.8017 - val_loss: 0.4173 - val_acc: 0.8082\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4323 - acc: 0.7994 - val_loss: 0.4100 - val_acc: 0.8117\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4285 - acc: 0.8012 - val_loss: 0.4135 - val_acc: 0.8115\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4297 - acc: 0.8022 - val_loss: 0.4183 - val_acc: 0.8067\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4275 - acc: 0.8033 - val_loss: 0.4080 - val_acc: 0.8134\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4272 - acc: 0.8024 - val_loss: 0.4195 - val_acc: 0.8090\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4273 - acc: 0.8015 - val_loss: 0.4681 - val_acc: 0.7743\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4272 - acc: 0.8025 - val_loss: 0.4088 - val_acc: 0.8130\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4274 - acc: 0.8023 - val_loss: 0.4119 - val_acc: 0.8145\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4258 - acc: 0.8022 - val_loss: 0.4108 - val_acc: 0.8100\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4263 - acc: 0.8025 - val_loss: 0.4078 - val_acc: 0.8139\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 273us/sample - loss: 0.5870 - acc: 0.7082 - val_loss: 0.5305 - val_acc: 0.7375\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.5057 - acc: 0.7537 - val_loss: 0.4772 - val_acc: 0.7707\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4899 - acc: 0.7640 - val_loss: 0.4666 - val_acc: 0.7783\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4794 - acc: 0.7688 - val_loss: 0.4588 - val_acc: 0.7822\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4680 - acc: 0.7765 - val_loss: 0.4746 - val_acc: 0.7675\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4593 - acc: 0.7832 - val_loss: 0.4390 - val_acc: 0.7944\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4527 - acc: 0.7876 - val_loss: 0.4343 - val_acc: 0.7959\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4478 - acc: 0.7897 - val_loss: 0.4456 - val_acc: 0.7890\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4448 - acc: 0.7909 - val_loss: 0.4228 - val_acc: 0.8038\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4438 - acc: 0.7935 - val_loss: 0.4436 - val_acc: 0.7877\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4391 - acc: 0.7952 - val_loss: 0.4226 - val_acc: 0.8062\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4379 - acc: 0.7961 - val_loss: 0.4167 - val_acc: 0.8108\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4363 - acc: 0.7965 - val_loss: 0.4160 - val_acc: 0.8095\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4360 - acc: 0.7980 - val_loss: 0.4140 - val_acc: 0.8116\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4359 - acc: 0.7976 - val_loss: 0.4132 - val_acc: 0.8122\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4346 - acc: 0.7987 - val_loss: 0.4148 - val_acc: 0.8094\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4345 - acc: 0.7979 - val_loss: 0.4176 - val_acc: 0.8102\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4315 - acc: 0.7998 - val_loss: 0.4157 - val_acc: 0.8088\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4322 - acc: 0.7994 - val_loss: 0.4129 - val_acc: 0.8121\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4315 - acc: 0.7993 - val_loss: 0.4095 - val_acc: 0.8114\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4313 - acc: 0.7998 - val_loss: 0.4148 - val_acc: 0.8101\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4299 - acc: 0.8013 - val_loss: 0.4113 - val_acc: 0.8137\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4280 - acc: 0.8018 - val_loss: 0.4131 - val_acc: 0.8117\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4282 - acc: 0.8030 - val_loss: 0.4295 - val_acc: 0.7991\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4276 - acc: 0.8030 - val_loss: 0.4200 - val_acc: 0.8048\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4264 - acc: 0.8036 - val_loss: 0.4088 - val_acc: 0.8130\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4250 - acc: 0.8047 - val_loss: 0.4148 - val_acc: 0.8067\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4266 - acc: 0.8020 - val_loss: 0.4052 - val_acc: 0.8152\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 125us/sample - loss: 0.4264 - acc: 0.8041 - val_loss: 0.4121 - val_acc: 0.8083\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 124us/sample - loss: 0.4247 - acc: 0.8035 - val_loss: 0.4055 - val_acc: 0.8147\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 284us/sample - loss: 0.5859 - acc: 0.7086 - val_loss: 0.4957 - val_acc: 0.7622\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.5113 - acc: 0.7527 - val_loss: 0.4819 - val_acc: 0.7689\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4942 - acc: 0.7623 - val_loss: 0.4815 - val_acc: 0.7721\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4807 - acc: 0.7704 - val_loss: 0.4582 - val_acc: 0.7818\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4700 - acc: 0.7781 - val_loss: 0.4511 - val_acc: 0.7870\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4613 - acc: 0.7822 - val_loss: 0.4399 - val_acc: 0.7987\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4522 - acc: 0.7896 - val_loss: 0.4316 - val_acc: 0.8011\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4484 - acc: 0.7916 - val_loss: 0.4324 - val_acc: 0.8030\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4454 - acc: 0.7942 - val_loss: 0.4220 - val_acc: 0.8080\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 159us/sample - loss: 0.4439 - acc: 0.7945 - val_loss: 0.4186 - val_acc: 0.8074\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.4396 - acc: 0.7958 - val_loss: 0.4181 - val_acc: 0.8073\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4396 - acc: 0.7962 - val_loss: 0.4173 - val_acc: 0.8099\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4381 - acc: 0.7971 - val_loss: 0.4167 - val_acc: 0.8078\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4362 - acc: 0.7981 - val_loss: 0.4147 - val_acc: 0.8099\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4356 - acc: 0.7985 - val_loss: 0.4182 - val_acc: 0.8086\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.4351 - acc: 0.7990 - val_loss: 0.4116 - val_acc: 0.8133\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4333 - acc: 0.8002 - val_loss: 0.4151 - val_acc: 0.8106\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 150us/sample - loss: 0.4348 - acc: 0.7999 - val_loss: 0.4286 - val_acc: 0.7982\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4309 - acc: 0.8007 - val_loss: 0.4128 - val_acc: 0.8088\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4312 - acc: 0.8007 - val_loss: 0.4222 - val_acc: 0.8041\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4309 - acc: 0.8011 - val_loss: 0.4081 - val_acc: 0.8140\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4296 - acc: 0.8021 - val_loss: 0.4062 - val_acc: 0.8130\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4296 - acc: 0.8030 - val_loss: 0.4099 - val_acc: 0.8114\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4288 - acc: 0.8027 - val_loss: 0.4161 - val_acc: 0.8079\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 146us/sample - loss: 0.4283 - acc: 0.8019 - val_loss: 0.4064 - val_acc: 0.8141\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4287 - acc: 0.8037 - val_loss: 0.4066 - val_acc: 0.8127\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.4270 - acc: 0.8033 - val_loss: 0.4109 - val_acc: 0.8128\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4272 - acc: 0.8032 - val_loss: 0.4084 - val_acc: 0.8106\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.4269 - acc: 0.8026 - val_loss: 0.4058 - val_acc: 0.8137\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 147us/sample - loss: 0.4281 - acc: 0.8025 - val_loss: 0.4062 - val_acc: 0.8124\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 311us/sample - loss: 0.6008 - acc: 0.6945 - val_loss: 0.4991 - val_acc: 0.7607\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.5149 - acc: 0.7508 - val_loss: 0.4880 - val_acc: 0.7656\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.4975 - acc: 0.7626 - val_loss: 0.4688 - val_acc: 0.7781\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4859 - acc: 0.7688 - val_loss: 0.4577 - val_acc: 0.7840\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4764 - acc: 0.7745 - val_loss: 0.4521 - val_acc: 0.7903\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.4682 - acc: 0.7808 - val_loss: 0.4476 - val_acc: 0.7911\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4616 - acc: 0.7846 - val_loss: 0.4365 - val_acc: 0.7959\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4594 - acc: 0.7853 - val_loss: 0.4367 - val_acc: 0.7952\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4561 - acc: 0.7868 - val_loss: 0.4372 - val_acc: 0.7958\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4521 - acc: 0.7881 - val_loss: 0.4301 - val_acc: 0.8034\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4498 - acc: 0.7909 - val_loss: 0.4323 - val_acc: 0.7994\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4460 - acc: 0.7926 - val_loss: 0.4387 - val_acc: 0.7972\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4456 - acc: 0.7933 - val_loss: 0.4213 - val_acc: 0.8038\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4423 - acc: 0.7955 - val_loss: 0.4157 - val_acc: 0.8089\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4430 - acc: 0.7947 - val_loss: 0.4210 - val_acc: 0.8061\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4402 - acc: 0.7962 - val_loss: 0.4151 - val_acc: 0.8087\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4386 - acc: 0.7971 - val_loss: 0.4224 - val_acc: 0.8063\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4374 - acc: 0.7979 - val_loss: 0.4150 - val_acc: 0.8080\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4369 - acc: 0.7984 - val_loss: 0.4222 - val_acc: 0.8032\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4378 - acc: 0.7985 - val_loss: 0.4255 - val_acc: 0.8057\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4339 - acc: 0.7999 - val_loss: 0.4160 - val_acc: 0.8094\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4334 - acc: 0.8000 - val_loss: 0.4191 - val_acc: 0.8056\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4325 - acc: 0.8004 - val_loss: 0.4107 - val_acc: 0.8072\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4325 - acc: 0.8009 - val_loss: 0.4238 - val_acc: 0.8069\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4314 - acc: 0.8023 - val_loss: 0.4252 - val_acc: 0.7979\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4313 - acc: 0.8009 - val_loss: 0.4074 - val_acc: 0.8135\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4307 - acc: 0.8012 - val_loss: 0.4034 - val_acc: 0.8132\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4298 - acc: 0.8022 - val_loss: 0.4181 - val_acc: 0.8067\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.4301 - acc: 0.8010 - val_loss: 0.4206 - val_acc: 0.8027\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4279 - acc: 0.8023 - val_loss: 0.4029 - val_acc: 0.8139\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 267us/sample - loss: 0.5207 - acc: 0.7426 - val_loss: 0.5758 - val_acc: 0.7054\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4712 - acc: 0.7749 - val_loss: 0.4461 - val_acc: 0.7936\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4528 - acc: 0.7876 - val_loss: 0.4466 - val_acc: 0.7861\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4405 - acc: 0.7934 - val_loss: 0.4269 - val_acc: 0.8013\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4332 - acc: 0.7984 - val_loss: 0.4234 - val_acc: 0.8048\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4240 - acc: 0.8042 - val_loss: 0.4228 - val_acc: 0.8091\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4202 - acc: 0.8054 - val_loss: 0.4082 - val_acc: 0.8127\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4169 - acc: 0.8073 - val_loss: 0.4061 - val_acc: 0.8138\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4123 - acc: 0.8112 - val_loss: 0.4003 - val_acc: 0.8183\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4098 - acc: 0.8136 - val_loss: 0.4182 - val_acc: 0.8062\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4070 - acc: 0.8146 - val_loss: 0.4098 - val_acc: 0.8135\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4072 - acc: 0.8140 - val_loss: 0.4210 - val_acc: 0.8039\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4041 - acc: 0.8155 - val_loss: 0.3959 - val_acc: 0.8189\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4029 - acc: 0.8172 - val_loss: 0.3938 - val_acc: 0.8206\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4006 - acc: 0.8179 - val_loss: 0.3993 - val_acc: 0.8181\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3989 - acc: 0.8197 - val_loss: 0.3879 - val_acc: 0.8244\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3967 - acc: 0.8211 - val_loss: 0.4178 - val_acc: 0.8037\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3968 - acc: 0.8204 - val_loss: 0.3907 - val_acc: 0.8221\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3944 - acc: 0.8219 - val_loss: 0.3968 - val_acc: 0.8189\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3934 - acc: 0.8216 - val_loss: 0.3899 - val_acc: 0.8221\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.3938 - acc: 0.8222 - val_loss: 0.3871 - val_acc: 0.8241\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3913 - acc: 0.8230 - val_loss: 0.4028 - val_acc: 0.8129\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3926 - acc: 0.8233 - val_loss: 0.3986 - val_acc: 0.8156\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3920 - acc: 0.8223 - val_loss: 0.3869 - val_acc: 0.8215\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3910 - acc: 0.8246 - val_loss: 0.3975 - val_acc: 0.8181\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3897 - acc: 0.8245 - val_loss: 0.4090 - val_acc: 0.8102\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3898 - acc: 0.8249 - val_loss: 0.3855 - val_acc: 0.8237\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3891 - acc: 0.8251 - val_loss: 0.3981 - val_acc: 0.8153\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3871 - acc: 0.8250 - val_loss: 0.3915 - val_acc: 0.8187\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3863 - acc: 0.8249 - val_loss: 0.4007 - val_acc: 0.8175\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 23s 268us/sample - loss: 0.5538 - acc: 0.7309 - val_loss: 0.4919 - val_acc: 0.7636\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4780 - acc: 0.7707 - val_loss: 0.4521 - val_acc: 0.7862\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4585 - acc: 0.7840 - val_loss: 0.4352 - val_acc: 0.7957\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4463 - acc: 0.7908 - val_loss: 0.4285 - val_acc: 0.8004\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4352 - acc: 0.7986 - val_loss: 0.4849 - val_acc: 0.7666\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4291 - acc: 0.8021 - val_loss: 0.4178 - val_acc: 0.8104\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4234 - acc: 0.8057 - val_loss: 0.4072 - val_acc: 0.8152\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4204 - acc: 0.8067 - val_loss: 0.4048 - val_acc: 0.8156\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4154 - acc: 0.8092 - val_loss: 0.4009 - val_acc: 0.8184\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4135 - acc: 0.8107 - val_loss: 0.4031 - val_acc: 0.8180\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4111 - acc: 0.8124 - val_loss: 0.3983 - val_acc: 0.8192\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4083 - acc: 0.8148 - val_loss: 0.3964 - val_acc: 0.8196\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4084 - acc: 0.8135 - val_loss: 0.3982 - val_acc: 0.8189\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.4054 - acc: 0.8160 - val_loss: 0.3944 - val_acc: 0.8207\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4044 - acc: 0.8171 - val_loss: 0.3975 - val_acc: 0.8213\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.4009 - acc: 0.8177 - val_loss: 0.3994 - val_acc: 0.8175\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3999 - acc: 0.8190 - val_loss: 0.3922 - val_acc: 0.8208\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3995 - acc: 0.8187 - val_loss: 0.3900 - val_acc: 0.8234\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3970 - acc: 0.8195 - val_loss: 0.3953 - val_acc: 0.8180\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3964 - acc: 0.8212 - val_loss: 0.4067 - val_acc: 0.8117\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3964 - acc: 0.8197 - val_loss: 0.4184 - val_acc: 0.8104\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3943 - acc: 0.8209 - val_loss: 0.3986 - val_acc: 0.8187\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3937 - acc: 0.8221 - val_loss: 0.3952 - val_acc: 0.8191\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3937 - acc: 0.8227 - val_loss: 0.4154 - val_acc: 0.8099\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3916 - acc: 0.8227 - val_loss: 0.3877 - val_acc: 0.8257\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3919 - acc: 0.8236 - val_loss: 0.4153 - val_acc: 0.8102\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3911 - acc: 0.8226 - val_loss: 0.3875 - val_acc: 0.8260\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 127us/sample - loss: 0.3889 - acc: 0.8238 - val_loss: 0.3888 - val_acc: 0.8243\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3889 - acc: 0.8248 - val_loss: 0.3977 - val_acc: 0.8182\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 126us/sample - loss: 0.3886 - acc: 0.8246 - val_loss: 0.3905 - val_acc: 0.8227\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 292us/sample - loss: 0.5377 - acc: 0.7390 - val_loss: 0.4836 - val_acc: 0.7686\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4721 - acc: 0.7749 - val_loss: 0.4433 - val_acc: 0.7909\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4511 - acc: 0.7884 - val_loss: 0.4334 - val_acc: 0.7976\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4365 - acc: 0.7971 - val_loss: 0.4548 - val_acc: 0.7857\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4275 - acc: 0.8028 - val_loss: 0.4143 - val_acc: 0.8104\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4218 - acc: 0.8055 - val_loss: 0.4129 - val_acc: 0.8096\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.4160 - acc: 0.8090 - val_loss: 0.4173 - val_acc: 0.8076\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4119 - acc: 0.8123 - val_loss: 0.4252 - val_acc: 0.8046\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4057 - acc: 0.8151 - val_loss: 0.4126 - val_acc: 0.8119\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.4024 - acc: 0.8166 - val_loss: 0.4000 - val_acc: 0.8178\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3996 - acc: 0.8185 - val_loss: 0.3971 - val_acc: 0.8191\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.3952 - acc: 0.8215 - val_loss: 0.3988 - val_acc: 0.8187\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3928 - acc: 0.8217 - val_loss: 0.3966 - val_acc: 0.8199\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3917 - acc: 0.8223 - val_loss: 0.3929 - val_acc: 0.8224\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3889 - acc: 0.8251 - val_loss: 0.4161 - val_acc: 0.8109\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3860 - acc: 0.8262 - val_loss: 0.3923 - val_acc: 0.8218\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3856 - acc: 0.8262 - val_loss: 0.3870 - val_acc: 0.8229\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 149us/sample - loss: 0.3831 - acc: 0.8277 - val_loss: 0.4033 - val_acc: 0.8158\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3823 - acc: 0.8283 - val_loss: 0.3912 - val_acc: 0.8231\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3802 - acc: 0.8286 - val_loss: 0.3989 - val_acc: 0.8178\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3794 - acc: 0.8295 - val_loss: 0.3940 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3759 - acc: 0.8306 - val_loss: 0.3905 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 147us/sample - loss: 0.3745 - acc: 0.8318 - val_loss: 0.3807 - val_acc: 0.8277\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3741 - acc: 0.8323 - val_loss: 0.3961 - val_acc: 0.8199\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3740 - acc: 0.8316 - val_loss: 0.3836 - val_acc: 0.8258\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3717 - acc: 0.8343 - val_loss: 0.3851 - val_acc: 0.8264\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3716 - acc: 0.8338 - val_loss: 0.3856 - val_acc: 0.8254\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3716 - acc: 0.8334 - val_loss: 0.3819 - val_acc: 0.8266\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3707 - acc: 0.8330 - val_loss: 0.3812 - val_acc: 0.8262\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 148us/sample - loss: 0.3694 - acc: 0.8348 - val_loss: 0.3912 - val_acc: 0.8225\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 307us/sample - loss: 0.5322 - acc: 0.7404 - val_loss: 0.4732 - val_acc: 0.7708\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4730 - acc: 0.7749 - val_loss: 0.4480 - val_acc: 0.7887\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4541 - acc: 0.7871 - val_loss: 0.4340 - val_acc: 0.7965\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4406 - acc: 0.7951 - val_loss: 0.4234 - val_acc: 0.8026\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4291 - acc: 0.8022 - val_loss: 0.4285 - val_acc: 0.8041\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4212 - acc: 0.8072 - val_loss: 0.4060 - val_acc: 0.8130\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4164 - acc: 0.8093 - val_loss: 0.4095 - val_acc: 0.8100\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4122 - acc: 0.8118 - val_loss: 0.3990 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4051 - acc: 0.8165 - val_loss: 0.3963 - val_acc: 0.8183\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4020 - acc: 0.8162 - val_loss: 0.3939 - val_acc: 0.8212\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3987 - acc: 0.8194 - val_loss: 0.3901 - val_acc: 0.8228\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3938 - acc: 0.8226 - val_loss: 0.4015 - val_acc: 0.8191\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3924 - acc: 0.8228 - val_loss: 0.3892 - val_acc: 0.8224\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3910 - acc: 0.8230 - val_loss: 0.4011 - val_acc: 0.8163\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3874 - acc: 0.8246 - val_loss: 0.3878 - val_acc: 0.8195\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3849 - acc: 0.8268 - val_loss: 0.3872 - val_acc: 0.8238\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3844 - acc: 0.8265 - val_loss: 0.4110 - val_acc: 0.8108\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3826 - acc: 0.8281 - val_loss: 0.3865 - val_acc: 0.8252\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3800 - acc: 0.8288 - val_loss: 0.3786 - val_acc: 0.8286\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3780 - acc: 0.8297 - val_loss: 0.4080 - val_acc: 0.8121\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3771 - acc: 0.8300 - val_loss: 0.3819 - val_acc: 0.8291\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3758 - acc: 0.8305 - val_loss: 0.3818 - val_acc: 0.8263\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3744 - acc: 0.8322 - val_loss: 0.3756 - val_acc: 0.8295\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3711 - acc: 0.8338 - val_loss: 0.3898 - val_acc: 0.8216\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3715 - acc: 0.8342 - val_loss: 0.3861 - val_acc: 0.8243\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3712 - acc: 0.8345 - val_loss: 0.3960 - val_acc: 0.8203\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3696 - acc: 0.8343 - val_loss: 0.3779 - val_acc: 0.8267\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3693 - acc: 0.8343 - val_loss: 0.3792 - val_acc: 0.8263\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3659 - acc: 0.8365 - val_loss: 0.3812 - val_acc: 0.8293\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.3679 - acc: 0.8358 - val_loss: 0.4038 - val_acc: 0.8184\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 279us/sample - loss: 0.5264 - acc: 0.7430 - val_loss: 0.4705 - val_acc: 0.7804\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4650 - acc: 0.7787 - val_loss: 0.4497 - val_acc: 0.7891\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4467 - acc: 0.7915 - val_loss: 0.4601 - val_acc: 0.7832\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4343 - acc: 0.7980 - val_loss: 0.4450 - val_acc: 0.7896\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4250 - acc: 0.8036 - val_loss: 0.4086 - val_acc: 0.8111\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4186 - acc: 0.8087 - val_loss: 0.4089 - val_acc: 0.8145\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4127 - acc: 0.8109 - val_loss: 0.4643 - val_acc: 0.7793\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4098 - acc: 0.8117 - val_loss: 0.4125 - val_acc: 0.8125\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.4053 - acc: 0.8153 - val_loss: 0.4237 - val_acc: 0.8030\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 128us/sample - loss: 0.4027 - acc: 0.8177 - val_loss: 0.4056 - val_acc: 0.8179\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3981 - acc: 0.8183 - val_loss: 0.4071 - val_acc: 0.8112\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3952 - acc: 0.8212 - val_loss: 0.4049 - val_acc: 0.8159\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3932 - acc: 0.8212 - val_loss: 0.3905 - val_acc: 0.8239\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3890 - acc: 0.8254 - val_loss: 0.3901 - val_acc: 0.8224\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3879 - acc: 0.8252 - val_loss: 0.3958 - val_acc: 0.8216\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3856 - acc: 0.8260 - val_loss: 0.3818 - val_acc: 0.8304\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3851 - acc: 0.8260 - val_loss: 0.3878 - val_acc: 0.8238\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3830 - acc: 0.8266 - val_loss: 0.3922 - val_acc: 0.8229\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3817 - acc: 0.8291 - val_loss: 0.3900 - val_acc: 0.8256\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3796 - acc: 0.8290 - val_loss: 0.3977 - val_acc: 0.8170\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3790 - acc: 0.8305 - val_loss: 0.3849 - val_acc: 0.8270\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3765 - acc: 0.8311 - val_loss: 0.3935 - val_acc: 0.8177\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3774 - acc: 0.8294 - val_loss: 0.3804 - val_acc: 0.8278\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3744 - acc: 0.8312 - val_loss: 0.3813 - val_acc: 0.8291\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3728 - acc: 0.8330 - val_loss: 0.3915 - val_acc: 0.8218\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3719 - acc: 0.8336 - val_loss: 0.3910 - val_acc: 0.8213\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3717 - acc: 0.8344 - val_loss: 0.3826 - val_acc: 0.8278\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3702 - acc: 0.8339 - val_loss: 0.3790 - val_acc: 0.8278\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3693 - acc: 0.8342 - val_loss: 0.3840 - val_acc: 0.8266\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 129us/sample - loss: 0.3667 - acc: 0.8351 - val_loss: 0.3787 - val_acc: 0.8281\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 279us/sample - loss: 0.5247 - acc: 0.7449 - val_loss: 0.5270 - val_acc: 0.7448\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4666 - acc: 0.7778 - val_loss: 0.4519 - val_acc: 0.7838\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4473 - acc: 0.7898 - val_loss: 0.4319 - val_acc: 0.7983\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4349 - acc: 0.7974 - val_loss: 0.4316 - val_acc: 0.8024\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4251 - acc: 0.8037 - val_loss: 0.4150 - val_acc: 0.8067\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4186 - acc: 0.8078 - val_loss: 0.4186 - val_acc: 0.8069\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4140 - acc: 0.8107 - val_loss: 0.4029 - val_acc: 0.8159\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.4089 - acc: 0.8141 - val_loss: 0.4051 - val_acc: 0.8108\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4048 - acc: 0.8160 - val_loss: 0.3972 - val_acc: 0.8202\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4027 - acc: 0.8177 - val_loss: 0.3996 - val_acc: 0.8174\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.4001 - acc: 0.8185 - val_loss: 0.3947 - val_acc: 0.8213\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3965 - acc: 0.8206 - val_loss: 0.3966 - val_acc: 0.8213\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3929 - acc: 0.8225 - val_loss: 0.3953 - val_acc: 0.8229\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3918 - acc: 0.8239 - val_loss: 0.3895 - val_acc: 0.8242\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3904 - acc: 0.8252 - val_loss: 0.4196 - val_acc: 0.8053\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3865 - acc: 0.8261 - val_loss: 0.3865 - val_acc: 0.8235\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3857 - acc: 0.8258 - val_loss: 0.3969 - val_acc: 0.8178\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3845 - acc: 0.8270 - val_loss: 0.3887 - val_acc: 0.8245\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3818 - acc: 0.8284 - val_loss: 0.3984 - val_acc: 0.8181\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3809 - acc: 0.8280 - val_loss: 0.3818 - val_acc: 0.8242\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 131us/sample - loss: 0.3778 - acc: 0.8310 - val_loss: 0.3813 - val_acc: 0.8275\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3761 - acc: 0.8305 - val_loss: 0.3841 - val_acc: 0.8260\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3759 - acc: 0.8320 - val_loss: 0.3832 - val_acc: 0.8247\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3751 - acc: 0.8320 - val_loss: 0.3830 - val_acc: 0.8267\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3722 - acc: 0.8341 - val_loss: 0.3811 - val_acc: 0.8271\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3713 - acc: 0.8341 - val_loss: 0.3783 - val_acc: 0.8283\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3707 - acc: 0.8327 - val_loss: 0.3825 - val_acc: 0.8252\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3704 - acc: 0.8345 - val_loss: 0.3795 - val_acc: 0.8283\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3674 - acc: 0.8368 - val_loss: 0.3810 - val_acc: 0.8278\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 130us/sample - loss: 0.3667 - acc: 0.8365 - val_loss: 0.3946 - val_acc: 0.8209\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 303us/sample - loss: 0.5420 - acc: 0.7416 - val_loss: 0.4654 - val_acc: 0.7789\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4622 - acc: 0.7808 - val_loss: 0.4649 - val_acc: 0.7760\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4390 - acc: 0.7950 - val_loss: 0.4211 - val_acc: 0.8077\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4240 - acc: 0.8047 - val_loss: 0.4147 - val_acc: 0.8100\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4152 - acc: 0.8109 - val_loss: 0.4095 - val_acc: 0.8143\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.4072 - acc: 0.8140 - val_loss: 0.3953 - val_acc: 0.8207\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.4022 - acc: 0.8181 - val_loss: 0.4009 - val_acc: 0.8188\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3972 - acc: 0.8207 - val_loss: 0.4262 - val_acc: 0.8019\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3903 - acc: 0.8239 - val_loss: 0.3903 - val_acc: 0.8209\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3852 - acc: 0.8267 - val_loss: 0.3901 - val_acc: 0.8235\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.3825 - acc: 0.8288 - val_loss: 0.3816 - val_acc: 0.8278\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3745 - acc: 0.8330 - val_loss: 0.3825 - val_acc: 0.8255\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3724 - acc: 0.8330 - val_loss: 0.4176 - val_acc: 0.8059\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3701 - acc: 0.8347 - val_loss: 0.3892 - val_acc: 0.8246\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3646 - acc: 0.8377 - val_loss: 0.3827 - val_acc: 0.8270\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3614 - acc: 0.8386 - val_loss: 0.3809 - val_acc: 0.8274\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3561 - acc: 0.8405 - val_loss: 0.3882 - val_acc: 0.8274\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.3536 - acc: 0.8426 - val_loss: 0.3957 - val_acc: 0.8185\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3504 - acc: 0.8448 - val_loss: 0.3796 - val_acc: 0.8272\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3473 - acc: 0.8459 - val_loss: 0.3824 - val_acc: 0.8264\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3436 - acc: 0.8480 - val_loss: 0.4005 - val_acc: 0.8171\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3407 - acc: 0.8489 - val_loss: 0.3905 - val_acc: 0.8266\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3386 - acc: 0.8499 - val_loss: 0.3835 - val_acc: 0.8248\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3339 - acc: 0.8520 - val_loss: 0.3910 - val_acc: 0.8259\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3331 - acc: 0.8535 - val_loss: 0.3953 - val_acc: 0.8223\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 152us/sample - loss: 0.3318 - acc: 0.8527 - val_loss: 0.3913 - val_acc: 0.8254\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3284 - acc: 0.8543 - val_loss: 0.3849 - val_acc: 0.8256\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3271 - acc: 0.8550 - val_loss: 0.3951 - val_acc: 0.8275\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 153us/sample - loss: 0.3232 - acc: 0.8562 - val_loss: 0.3926 - val_acc: 0.8227\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 154us/sample - loss: 0.3196 - acc: 0.8587 - val_loss: 0.4031 - val_acc: 0.8234\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 317us/sample - loss: 0.5291 - acc: 0.7448 - val_loss: 0.5067 - val_acc: 0.7503\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4602 - acc: 0.7839 - val_loss: 0.4728 - val_acc: 0.7776\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.4386 - acc: 0.7965 - val_loss: 0.4448 - val_acc: 0.7917\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4233 - acc: 0.8048 - val_loss: 0.4103 - val_acc: 0.8130\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4133 - acc: 0.8122 - val_loss: 0.4333 - val_acc: 0.8017\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4036 - acc: 0.8161 - val_loss: 0.4044 - val_acc: 0.8130\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3983 - acc: 0.8196 - val_loss: 0.3944 - val_acc: 0.8213\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3907 - acc: 0.8227 - val_loss: 0.3905 - val_acc: 0.8249\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3844 - acc: 0.8282 - val_loss: 0.3860 - val_acc: 0.8251\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3807 - acc: 0.8293 - val_loss: 0.3846 - val_acc: 0.8265\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3758 - acc: 0.8318 - val_loss: 0.3777 - val_acc: 0.8273\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3698 - acc: 0.8349 - val_loss: 0.3906 - val_acc: 0.8224\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3664 - acc: 0.8357 - val_loss: 0.4211 - val_acc: 0.8017\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3627 - acc: 0.8386 - val_loss: 0.4216 - val_acc: 0.8065\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3576 - acc: 0.8411 - val_loss: 0.3827 - val_acc: 0.8243\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3515 - acc: 0.8434 - val_loss: 0.3895 - val_acc: 0.8254\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3509 - acc: 0.8441 - val_loss: 0.3828 - val_acc: 0.8300\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3437 - acc: 0.8474 - val_loss: 0.4066 - val_acc: 0.8140\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3438 - acc: 0.8473 - val_loss: 0.3826 - val_acc: 0.8284\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3390 - acc: 0.8509 - val_loss: 0.3796 - val_acc: 0.8279\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3342 - acc: 0.8518 - val_loss: 0.3988 - val_acc: 0.8198\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3334 - acc: 0.8529 - val_loss: 0.3847 - val_acc: 0.8249\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3301 - acc: 0.8550 - val_loss: 0.3951 - val_acc: 0.8256\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3247 - acc: 0.8570 - val_loss: 0.3859 - val_acc: 0.8293\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3222 - acc: 0.8577 - val_loss: 0.3917 - val_acc: 0.8236\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3217 - acc: 0.8584 - val_loss: 0.3865 - val_acc: 0.8292\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3172 - acc: 0.8607 - val_loss: 0.4015 - val_acc: 0.8223\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3143 - acc: 0.8622 - val_loss: 0.3927 - val_acc: 0.8238\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3123 - acc: 0.8638 - val_loss: 0.3977 - val_acc: 0.8267\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3105 - acc: 0.8646 - val_loss: 0.3921 - val_acc: 0.8271\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 287us/sample - loss: 0.5245 - acc: 0.7454 - val_loss: 0.4726 - val_acc: 0.7775\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4683 - acc: 0.7780 - val_loss: 0.4504 - val_acc: 0.7876\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4463 - acc: 0.7908 - val_loss: 0.4633 - val_acc: 0.7763\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4327 - acc: 0.8004 - val_loss: 0.4254 - val_acc: 0.8026\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4216 - acc: 0.8059 - val_loss: 0.4368 - val_acc: 0.7971\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4154 - acc: 0.8101 - val_loss: 0.4087 - val_acc: 0.8127\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4094 - acc: 0.8133 - val_loss: 0.4181 - val_acc: 0.8091\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4063 - acc: 0.8147 - val_loss: 0.4399 - val_acc: 0.7950\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4027 - acc: 0.8172 - val_loss: 0.4150 - val_acc: 0.8070\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3994 - acc: 0.8189 - val_loss: 0.3961 - val_acc: 0.8206\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3952 - acc: 0.8215 - val_loss: 0.4228 - val_acc: 0.8073\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3928 - acc: 0.8214 - val_loss: 0.3959 - val_acc: 0.8234\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3890 - acc: 0.8233 - val_loss: 0.3851 - val_acc: 0.8253\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3863 - acc: 0.8258 - val_loss: 0.3904 - val_acc: 0.8239\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3844 - acc: 0.8271 - val_loss: 0.3998 - val_acc: 0.8173\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3801 - acc: 0.8285 - val_loss: 0.3848 - val_acc: 0.8254\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3794 - acc: 0.8289 - val_loss: 0.3909 - val_acc: 0.8231\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3789 - acc: 0.8296 - val_loss: 0.3909 - val_acc: 0.8191\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3767 - acc: 0.8305 - val_loss: 0.3816 - val_acc: 0.8269\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3740 - acc: 0.8316 - val_loss: 0.3801 - val_acc: 0.8285\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3738 - acc: 0.8309 - val_loss: 0.3861 - val_acc: 0.8233\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3702 - acc: 0.8340 - val_loss: 0.3806 - val_acc: 0.8292\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3697 - acc: 0.8341 - val_loss: 0.3796 - val_acc: 0.8297\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3693 - acc: 0.8339 - val_loss: 0.3791 - val_acc: 0.8292\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3667 - acc: 0.8358 - val_loss: 0.3820 - val_acc: 0.8273\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3672 - acc: 0.8354 - val_loss: 0.3904 - val_acc: 0.8209\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3652 - acc: 0.8369 - val_loss: 0.3813 - val_acc: 0.8278\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3636 - acc: 0.8375 - val_loss: 0.4080 - val_acc: 0.8195\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3608 - acc: 0.8383 - val_loss: 0.3818 - val_acc: 0.8282\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3605 - acc: 0.8390 - val_loss: 0.3785 - val_acc: 0.8302\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 24s 287us/sample - loss: 0.5221 - acc: 0.7474 - val_loss: 0.5283 - val_acc: 0.7422\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4605 - acc: 0.7831 - val_loss: 0.4593 - val_acc: 0.7868\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4416 - acc: 0.7939 - val_loss: 0.4370 - val_acc: 0.8001\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4273 - acc: 0.8007 - val_loss: 0.4145 - val_acc: 0.8095\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4203 - acc: 0.8055 - val_loss: 0.4101 - val_acc: 0.8129\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4128 - acc: 0.8106 - val_loss: 0.4030 - val_acc: 0.8183\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4075 - acc: 0.8140 - val_loss: 0.3977 - val_acc: 0.8178\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.4029 - acc: 0.8167 - val_loss: 0.4003 - val_acc: 0.8178\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.3996 - acc: 0.8176 - val_loss: 0.3994 - val_acc: 0.8153\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3951 - acc: 0.8209 - val_loss: 0.4088 - val_acc: 0.8120\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3910 - acc: 0.8226 - val_loss: 0.3884 - val_acc: 0.8230\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3891 - acc: 0.8252 - val_loss: 0.3968 - val_acc: 0.8179\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3852 - acc: 0.8259 - val_loss: 0.3888 - val_acc: 0.8219\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3827 - acc: 0.8278 - val_loss: 0.3861 - val_acc: 0.8252\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3806 - acc: 0.8290 - val_loss: 0.3828 - val_acc: 0.8256\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3774 - acc: 0.8305 - val_loss: 0.4057 - val_acc: 0.8149\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3753 - acc: 0.8317 - val_loss: 0.3880 - val_acc: 0.8244\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3737 - acc: 0.8318 - val_loss: 0.3840 - val_acc: 0.8263\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3725 - acc: 0.8330 - val_loss: 0.3943 - val_acc: 0.8227\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3712 - acc: 0.8338 - val_loss: 0.3828 - val_acc: 0.8276\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3691 - acc: 0.8342 - val_loss: 0.3836 - val_acc: 0.8239\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3665 - acc: 0.8365 - val_loss: 0.3820 - val_acc: 0.8277\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3644 - acc: 0.8356 - val_loss: 0.4312 - val_acc: 0.7964\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3625 - acc: 0.8378 - val_loss: 0.3867 - val_acc: 0.8249\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3608 - acc: 0.8381 - val_loss: 0.3829 - val_acc: 0.8273\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3614 - acc: 0.8383 - val_loss: 0.3811 - val_acc: 0.8270\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 132us/sample - loss: 0.3605 - acc: 0.8398 - val_loss: 0.3818 - val_acc: 0.8293\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3571 - acc: 0.8404 - val_loss: 0.3840 - val_acc: 0.8267\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3577 - acc: 0.8410 - val_loss: 0.3983 - val_acc: 0.8215\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.3555 - acc: 0.8420 - val_loss: 0.3805 - val_acc: 0.8316\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 315us/sample - loss: 0.5134 - acc: 0.7536 - val_loss: 0.4457 - val_acc: 0.7899\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4477 - acc: 0.7907 - val_loss: 0.4623 - val_acc: 0.7867\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4290 - acc: 0.8021 - val_loss: 0.4143 - val_acc: 0.8126\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.4126 - acc: 0.8106 - val_loss: 0.3991 - val_acc: 0.8169\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4026 - acc: 0.8185 - val_loss: 0.4071 - val_acc: 0.8099\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3945 - acc: 0.8214 - val_loss: 0.3977 - val_acc: 0.8186\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3884 - acc: 0.8260 - val_loss: 0.4108 - val_acc: 0.8133\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3801 - acc: 0.8289 - val_loss: 0.3831 - val_acc: 0.8261\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3752 - acc: 0.8320 - val_loss: 0.3944 - val_acc: 0.8234\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3693 - acc: 0.8343 - val_loss: 0.3790 - val_acc: 0.8270\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3615 - acc: 0.8379 - val_loss: 0.3855 - val_acc: 0.8245\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.3566 - acc: 0.8411 - val_loss: 0.3912 - val_acc: 0.8227\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3527 - acc: 0.8433 - val_loss: 0.3911 - val_acc: 0.8206\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3462 - acc: 0.8469 - val_loss: 0.3961 - val_acc: 0.8199\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 155us/sample - loss: 0.3404 - acc: 0.8490 - val_loss: 0.3921 - val_acc: 0.8243\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3362 - acc: 0.8510 - val_loss: 0.3923 - val_acc: 0.8260\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3328 - acc: 0.8532 - val_loss: 0.3811 - val_acc: 0.8310\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3257 - acc: 0.8545 - val_loss: 0.3847 - val_acc: 0.8267\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3204 - acc: 0.8591 - val_loss: 0.4133 - val_acc: 0.8167\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3157 - acc: 0.8616 - val_loss: 0.3988 - val_acc: 0.8212\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3109 - acc: 0.8640 - val_loss: 0.4225 - val_acc: 0.8216\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.3062 - acc: 0.8663 - val_loss: 0.3919 - val_acc: 0.8278\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2992 - acc: 0.8681 - val_loss: 0.4033 - val_acc: 0.8241\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2972 - acc: 0.8704 - val_loss: 0.3930 - val_acc: 0.8274\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.2933 - acc: 0.8714 - val_loss: 0.3965 - val_acc: 0.8276\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2881 - acc: 0.8746 - val_loss: 0.4125 - val_acc: 0.8260\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2839 - acc: 0.8763 - val_loss: 0.4134 - val_acc: 0.8229\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2822 - acc: 0.8772 - val_loss: 0.4223 - val_acc: 0.8220\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2798 - acc: 0.8790 - val_loss: 0.4166 - val_acc: 0.8184\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 156us/sample - loss: 0.2781 - acc: 0.8792 - val_loss: 0.4122 - val_acc: 0.8249\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 28s 331us/sample - loss: 0.5186 - acc: 0.7526 - val_loss: 0.4805 - val_acc: 0.7690\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 166us/sample - loss: 0.4500 - acc: 0.7912 - val_loss: 0.4238 - val_acc: 0.8020\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.4278 - acc: 0.8033 - val_loss: 0.4674 - val_acc: 0.7857\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4145 - acc: 0.8118 - val_loss: 0.4152 - val_acc: 0.8105\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4040 - acc: 0.8158 - val_loss: 0.4139 - val_acc: 0.8099\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3961 - acc: 0.8205 - val_loss: 0.4155 - val_acc: 0.8110\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3875 - acc: 0.8236 - val_loss: 0.3877 - val_acc: 0.8235\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3802 - acc: 0.8302 - val_loss: 0.3904 - val_acc: 0.8229\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3737 - acc: 0.8323 - val_loss: 0.3809 - val_acc: 0.8285\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3651 - acc: 0.8359 - val_loss: 0.3796 - val_acc: 0.8292\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3600 - acc: 0.8397 - val_loss: 0.3948 - val_acc: 0.8221\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3536 - acc: 0.8430 - val_loss: 0.3915 - val_acc: 0.8248\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3468 - acc: 0.8445 - val_loss: 0.3885 - val_acc: 0.8267\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3409 - acc: 0.8488 - val_loss: 0.3982 - val_acc: 0.8173\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3330 - acc: 0.8529 - val_loss: 0.3796 - val_acc: 0.8322\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3270 - acc: 0.8564 - val_loss: 0.3800 - val_acc: 0.8286\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3219 - acc: 0.8579 - val_loss: 0.3984 - val_acc: 0.8255\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3153 - acc: 0.8617 - val_loss: 0.4213 - val_acc: 0.8187\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3095 - acc: 0.8655 - val_loss: 0.3999 - val_acc: 0.8257\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.3037 - acc: 0.8676 - val_loss: 0.4047 - val_acc: 0.8231\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2997 - acc: 0.8704 - val_loss: 0.4229 - val_acc: 0.8180\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2954 - acc: 0.8712 - val_loss: 0.4079 - val_acc: 0.8230\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2890 - acc: 0.8745 - val_loss: 0.4056 - val_acc: 0.8129\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2847 - acc: 0.8769 - val_loss: 0.4281 - val_acc: 0.8123\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2798 - acc: 0.8803 - val_loss: 0.4217 - val_acc: 0.8222\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2761 - acc: 0.8813 - val_loss: 0.4265 - val_acc: 0.8229\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2695 - acc: 0.8845 - val_loss: 0.4253 - val_acc: 0.8159\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2677 - acc: 0.8846 - val_loss: 0.4255 - val_acc: 0.8212\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2622 - acc: 0.8880 - val_loss: 0.4213 - val_acc: 0.8203\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.2582 - acc: 0.8893 - val_loss: 0.4333 - val_acc: 0.8242\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 295us/sample - loss: 0.5864 - acc: 0.7103 - val_loss: 0.5018 - val_acc: 0.7584\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.5075 - acc: 0.7504 - val_loss: 0.4840 - val_acc: 0.7700\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4894 - acc: 0.7623 - val_loss: 0.4865 - val_acc: 0.7677\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4781 - acc: 0.7700 - val_loss: 0.4526 - val_acc: 0.7876\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4676 - acc: 0.7781 - val_loss: 0.4628 - val_acc: 0.7825\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4595 - acc: 0.7831 - val_loss: 0.4363 - val_acc: 0.7972\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4530 - acc: 0.7883 - val_loss: 0.4331 - val_acc: 0.8001\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4495 - acc: 0.7892 - val_loss: 0.4358 - val_acc: 0.7952\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4463 - acc: 0.7918 - val_loss: 0.4539 - val_acc: 0.7913\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4446 - acc: 0.7925 - val_loss: 0.4420 - val_acc: 0.7932\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4411 - acc: 0.7956 - val_loss: 0.4277 - val_acc: 0.8001\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4401 - acc: 0.7955 - val_loss: 0.4448 - val_acc: 0.7899\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4398 - acc: 0.7944 - val_loss: 0.4244 - val_acc: 0.8017\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4378 - acc: 0.7973 - val_loss: 0.4176 - val_acc: 0.8066\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4357 - acc: 0.7982 - val_loss: 0.4191 - val_acc: 0.8067\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4333 - acc: 0.7995 - val_loss: 0.4131 - val_acc: 0.8094\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4329 - acc: 0.8009 - val_loss: 0.4144 - val_acc: 0.8098\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4335 - acc: 0.7996 - val_loss: 0.4123 - val_acc: 0.8090\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4318 - acc: 0.7987 - val_loss: 0.4174 - val_acc: 0.8041\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4299 - acc: 0.8010 - val_loss: 0.4147 - val_acc: 0.8082\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4301 - acc: 0.8013 - val_loss: 0.4103 - val_acc: 0.8104\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4287 - acc: 0.8014 - val_loss: 0.4179 - val_acc: 0.8091\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4280 - acc: 0.8018 - val_loss: 0.4152 - val_acc: 0.8063\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4295 - acc: 0.8023 - val_loss: 0.4077 - val_acc: 0.8134\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4287 - acc: 0.8010 - val_loss: 0.4160 - val_acc: 0.8112\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4253 - acc: 0.8051 - val_loss: 0.4254 - val_acc: 0.8027\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4283 - acc: 0.8019 - val_loss: 0.4061 - val_acc: 0.8124\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4261 - acc: 0.8031 - val_loss: 0.4146 - val_acc: 0.8082\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4269 - acc: 0.8018 - val_loss: 0.4048 - val_acc: 0.8159\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 133us/sample - loss: 0.4246 - acc: 0.8045 - val_loss: 0.4043 - val_acc: 0.8145\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 25s 296us/sample - loss: 0.5689 - acc: 0.7174 - val_loss: 0.4947 - val_acc: 0.7613\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.5010 - acc: 0.7547 - val_loss: 0.4726 - val_acc: 0.7760\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4847 - acc: 0.7659 - val_loss: 0.4783 - val_acc: 0.7689\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4742 - acc: 0.7728 - val_loss: 0.4584 - val_acc: 0.7818\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4663 - acc: 0.7786 - val_loss: 0.4543 - val_acc: 0.7882\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4600 - acc: 0.7828 - val_loss: 0.4374 - val_acc: 0.7970\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4553 - acc: 0.7854 - val_loss: 0.4395 - val_acc: 0.7972\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4526 - acc: 0.7881 - val_loss: 0.4369 - val_acc: 0.7935\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4484 - acc: 0.7905 - val_loss: 0.4627 - val_acc: 0.7775\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4468 - acc: 0.7898 - val_loss: 0.4328 - val_acc: 0.8002\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4454 - acc: 0.7926 - val_loss: 0.4306 - val_acc: 0.7975\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4414 - acc: 0.7956 - val_loss: 0.4205 - val_acc: 0.8073\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4423 - acc: 0.7950 - val_loss: 0.4277 - val_acc: 0.8036\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4411 - acc: 0.7956 - val_loss: 0.4221 - val_acc: 0.8061\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4407 - acc: 0.7965 - val_loss: 0.4225 - val_acc: 0.8033\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4375 - acc: 0.7974 - val_loss: 0.4214 - val_acc: 0.8044\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4359 - acc: 0.7986 - val_loss: 0.4128 - val_acc: 0.8103\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4372 - acc: 0.7967 - val_loss: 0.4167 - val_acc: 0.8078\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4354 - acc: 0.7988 - val_loss: 0.4140 - val_acc: 0.8109\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4347 - acc: 0.7987 - val_loss: 0.4220 - val_acc: 0.8061\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4334 - acc: 0.7994 - val_loss: 0.4177 - val_acc: 0.8087\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4324 - acc: 0.8000 - val_loss: 0.4101 - val_acc: 0.8109\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4329 - acc: 0.8003 - val_loss: 0.4140 - val_acc: 0.8108\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4326 - acc: 0.7991 - val_loss: 0.4157 - val_acc: 0.8091\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4320 - acc: 0.7997 - val_loss: 0.4199 - val_acc: 0.8043\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4316 - acc: 0.8000 - val_loss: 0.4158 - val_acc: 0.8098\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 135us/sample - loss: 0.4316 - acc: 0.8017 - val_loss: 0.4117 - val_acc: 0.8120\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 11s 135us/sample - loss: 0.4299 - acc: 0.8020 - val_loss: 0.4108 - val_acc: 0.8112\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4304 - acc: 0.8010 - val_loss: 0.4141 - val_acc: 0.8120\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 11s 134us/sample - loss: 0.4300 - acc: 0.8017 - val_loss: 0.4117 - val_acc: 0.8112\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 317us/sample - loss: 0.5872 - acc: 0.7006 - val_loss: 0.4997 - val_acc: 0.7555\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.5080 - acc: 0.7539 - val_loss: 0.4807 - val_acc: 0.7709\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4879 - acc: 0.7650 - val_loss: 0.4600 - val_acc: 0.7847\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4749 - acc: 0.7726 - val_loss: 0.4494 - val_acc: 0.7875\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4672 - acc: 0.7786 - val_loss: 0.4450 - val_acc: 0.7897\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4618 - acc: 0.7828 - val_loss: 0.4529 - val_acc: 0.7854\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4553 - acc: 0.7859 - val_loss: 0.4356 - val_acc: 0.8014\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4509 - acc: 0.7893 - val_loss: 0.4290 - val_acc: 0.8022\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4492 - acc: 0.7902 - val_loss: 0.4244 - val_acc: 0.8037\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4463 - acc: 0.7922 - val_loss: 0.4250 - val_acc: 0.8065\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4435 - acc: 0.7929 - val_loss: 0.4336 - val_acc: 0.7968\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4415 - acc: 0.7952 - val_loss: 0.4189 - val_acc: 0.8064\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4389 - acc: 0.7973 - val_loss: 0.4190 - val_acc: 0.8051\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4380 - acc: 0.7972 - val_loss: 0.4158 - val_acc: 0.8082\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4364 - acc: 0.7993 - val_loss: 0.4204 - val_acc: 0.8056\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4352 - acc: 0.7998 - val_loss: 0.4125 - val_acc: 0.8112\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4345 - acc: 0.7991 - val_loss: 0.4239 - val_acc: 0.8065\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4329 - acc: 0.7990 - val_loss: 0.4188 - val_acc: 0.8086\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4328 - acc: 0.8015 - val_loss: 0.4072 - val_acc: 0.8114\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4312 - acc: 0.8016 - val_loss: 0.4093 - val_acc: 0.8112\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4299 - acc: 0.8022 - val_loss: 0.4219 - val_acc: 0.8079\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4285 - acc: 0.8037 - val_loss: 0.4079 - val_acc: 0.8137\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4276 - acc: 0.8020 - val_loss: 0.4115 - val_acc: 0.8088\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4284 - acc: 0.8034 - val_loss: 0.4047 - val_acc: 0.8152\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4273 - acc: 0.8028 - val_loss: 0.4042 - val_acc: 0.8137\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4271 - acc: 0.8045 - val_loss: 0.4085 - val_acc: 0.8122\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4271 - acc: 0.8032 - val_loss: 0.4036 - val_acc: 0.8146\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4256 - acc: 0.8041 - val_loss: 0.4102 - val_acc: 0.8125\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 13s 158us/sample - loss: 0.4258 - acc: 0.8038 - val_loss: 0.4029 - val_acc: 0.8160\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 13s 157us/sample - loss: 0.4265 - acc: 0.8041 - val_loss: 0.4009 - val_acc: 0.8170\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 28s 332us/sample - loss: 0.5954 - acc: 0.6993 - val_loss: 0.5016 - val_acc: 0.7541\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.5162 - acc: 0.7477 - val_loss: 0.4798 - val_acc: 0.7708- loss:\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4961 - acc: 0.7623 - val_loss: 0.4692 - val_acc: 0.7768\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4816 - acc: 0.7705 - val_loss: 0.4529 - val_acc: 0.7863\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4719 - acc: 0.7756 - val_loss: 0.4554 - val_acc: 0.7837\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4671 - acc: 0.7796 - val_loss: 0.4402 - val_acc: 0.7966\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4597 - acc: 0.7834 - val_loss: 0.4343 - val_acc: 0.7957\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4538 - acc: 0.7862 - val_loss: 0.4385 - val_acc: 0.7954\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4513 - acc: 0.7890 - val_loss: 0.4218 - val_acc: 0.8062\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 167us/sample - loss: 0.4472 - acc: 0.7922 - val_loss: 0.4242 - val_acc: 0.8044\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4442 - acc: 0.7924 - val_loss: 0.4233 - val_acc: 0.8029\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4428 - acc: 0.7943 - val_loss: 0.4155 - val_acc: 0.8098\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.4387 - acc: 0.7970 - val_loss: 0.4169 - val_acc: 0.8092\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4388 - acc: 0.7966 - val_loss: 0.4179 - val_acc: 0.8115\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4376 - acc: 0.7962 - val_loss: 0.4132 - val_acc: 0.8114\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4366 - acc: 0.7979 - val_loss: 0.4169 - val_acc: 0.8081\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4347 - acc: 0.7985 - val_loss: 0.4316 - val_acc: 0.7991\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4345 - acc: 0.8001 - val_loss: 0.4122 - val_acc: 0.8085\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4331 - acc: 0.7992 - val_loss: 0.4151 - val_acc: 0.8100\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4334 - acc: 0.7993 - val_loss: 0.4177 - val_acc: 0.8063\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4320 - acc: 0.8002 - val_loss: 0.4135 - val_acc: 0.8092\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4311 - acc: 0.8008 - val_loss: 0.4104 - val_acc: 0.8120\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4319 - acc: 0.8003 - val_loss: 0.4088 - val_acc: 0.8115\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4313 - acc: 0.8015 - val_loss: 0.4189 - val_acc: 0.8083\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4288 - acc: 0.8031 - val_loss: 0.4110 - val_acc: 0.8120\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4282 - acc: 0.8025 - val_loss: 0.4058 - val_acc: 0.8136\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.4288 - acc: 0.8021 - val_loss: 0.4129 - val_acc: 0.8087\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4261 - acc: 0.8035 - val_loss: 0.4121 - val_acc: 0.8092\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4282 - acc: 0.8034 - val_loss: 0.4045 - val_acc: 0.8150\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 165us/sample - loss: 0.4282 - acc: 0.8022 - val_loss: 0.4073 - val_acc: 0.8151\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 302us/sample - loss: 0.5454 - acc: 0.7325 - val_loss: 0.4816 - val_acc: 0.7675\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4806 - acc: 0.7694 - val_loss: 0.4791 - val_acc: 0.7693\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4623 - acc: 0.7808 - val_loss: 0.4430 - val_acc: 0.7940\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4470 - acc: 0.7904 - val_loss: 0.4422 - val_acc: 0.7936\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4360 - acc: 0.7978 - val_loss: 0.4418 - val_acc: 0.7919\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4311 - acc: 0.8017 - val_loss: 0.4149 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4243 - acc: 0.8042 - val_loss: 0.4111 - val_acc: 0.8106\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4179 - acc: 0.8083 - val_loss: 0.4043 - val_acc: 0.8142\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4153 - acc: 0.8102 - val_loss: 0.4143 - val_acc: 0.8104\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4140 - acc: 0.8118 - val_loss: 0.4210 - val_acc: 0.8057\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4108 - acc: 0.8129 - val_loss: 0.4060 - val_acc: 0.8131\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4075 - acc: 0.8143 - val_loss: 0.4038 - val_acc: 0.8141\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4047 - acc: 0.8158 - val_loss: 0.3957 - val_acc: 0.8163\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4049 - acc: 0.8155 - val_loss: 0.4363 - val_acc: 0.8037\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.4020 - acc: 0.8175 - val_loss: 0.3940 - val_acc: 0.8194\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3997 - acc: 0.8193 - val_loss: 0.3967 - val_acc: 0.8183\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3984 - acc: 0.8197 - val_loss: 0.3887 - val_acc: 0.8228\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3979 - acc: 0.8190 - val_loss: 0.4134 - val_acc: 0.8077\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3962 - acc: 0.8209 - val_loss: 0.3913 - val_acc: 0.8224\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3956 - acc: 0.8221 - val_loss: 0.3968 - val_acc: 0.8195\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3949 - acc: 0.8212 - val_loss: 0.4015 - val_acc: 0.8159\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3920 - acc: 0.8212 - val_loss: 0.3875 - val_acc: 0.8237\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3917 - acc: 0.8224 - val_loss: 0.3870 - val_acc: 0.8231\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3919 - acc: 0.8234 - val_loss: 0.3909 - val_acc: 0.8209\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3902 - acc: 0.8241 - val_loss: 0.3832 - val_acc: 0.8249\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3901 - acc: 0.8238 - val_loss: 0.3963 - val_acc: 0.8176\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3901 - acc: 0.8242 - val_loss: 0.3828 - val_acc: 0.8249\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3890 - acc: 0.8242 - val_loss: 0.3954 - val_acc: 0.8177\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3880 - acc: 0.8258 - val_loss: 0.3851 - val_acc: 0.8235\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3868 - acc: 0.8254 - val_loss: 0.3850 - val_acc: 0.8262\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 307us/sample - loss: 0.5392 - acc: 0.7363 - val_loss: 0.4835 - val_acc: 0.7711\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4764 - acc: 0.7716 - val_loss: 0.4628 - val_acc: 0.7817\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4576 - acc: 0.7843 - val_loss: 0.4555 - val_acc: 0.7814\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4453 - acc: 0.7925 - val_loss: 0.4351 - val_acc: 0.7998\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4372 - acc: 0.7977 - val_loss: 0.4189 - val_acc: 0.8066\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4309 - acc: 0.8014 - val_loss: 0.4140 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4277 - acc: 0.8028 - val_loss: 0.4158 - val_acc: 0.8083\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4226 - acc: 0.8050 - val_loss: 0.4082 - val_acc: 0.8108\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4183 - acc: 0.8079 - val_loss: 0.4108 - val_acc: 0.8102\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4146 - acc: 0.8102 - val_loss: 0.4460 - val_acc: 0.7929\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4119 - acc: 0.8120 - val_loss: 0.4242 - val_acc: 0.8062\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4081 - acc: 0.8138 - val_loss: 0.4126 - val_acc: 0.8099\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4073 - acc: 0.8146 - val_loss: 0.4004 - val_acc: 0.8189\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.4042 - acc: 0.8161 - val_loss: 0.3967 - val_acc: 0.8173\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4027 - acc: 0.8173 - val_loss: 0.3931 - val_acc: 0.8209\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4018 - acc: 0.8171 - val_loss: 0.4069 - val_acc: 0.8115\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.4000 - acc: 0.8171 - val_loss: 0.3905 - val_acc: 0.8231\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3987 - acc: 0.8200 - val_loss: 0.4024 - val_acc: 0.8153\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3984 - acc: 0.8193 - val_loss: 0.3878 - val_acc: 0.8237\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3974 - acc: 0.8185 - val_loss: 0.3999 - val_acc: 0.8169\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3968 - acc: 0.8202 - val_loss: 0.3881 - val_acc: 0.8227\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3954 - acc: 0.8206 - val_loss: 0.3899 - val_acc: 0.8201\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3927 - acc: 0.8223 - val_loss: 0.3847 - val_acc: 0.8253\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3922 - acc: 0.8238 - val_loss: 0.4021 - val_acc: 0.8163\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3900 - acc: 0.8235 - val_loss: 0.4197 - val_acc: 0.8032\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3901 - acc: 0.8237 - val_loss: 0.3926 - val_acc: 0.8218\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3893 - acc: 0.8238 - val_loss: 0.3880 - val_acc: 0.8238\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3894 - acc: 0.8234 - val_loss: 0.3867 - val_acc: 0.8229\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 138us/sample - loss: 0.3897 - acc: 0.8231 - val_loss: 0.3864 - val_acc: 0.8256\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 137us/sample - loss: 0.3874 - acc: 0.8251 - val_loss: 0.3866 - val_acc: 0.8220\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 28s 329us/sample - loss: 0.5497 - acc: 0.7325 - val_loss: 0.4725 - val_acc: 0.7737\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4735 - acc: 0.7740 - val_loss: 0.4407 - val_acc: 0.7942\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4521 - acc: 0.7870 - val_loss: 0.5382 - val_acc: 0.7281\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4367 - acc: 0.7970 - val_loss: 0.4246 - val_acc: 0.8044\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4289 - acc: 0.8024 - val_loss: 0.4195 - val_acc: 0.8051\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4214 - acc: 0.8067 - val_loss: 0.4117 - val_acc: 0.8116\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4164 - acc: 0.8099 - val_loss: 0.4109 - val_acc: 0.8098\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4127 - acc: 0.8126 - val_loss: 0.4030 - val_acc: 0.8153\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.4073 - acc: 0.8157 - val_loss: 0.4157 - val_acc: 0.8122\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4037 - acc: 0.8173 - val_loss: 0.3947 - val_acc: 0.8175\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4005 - acc: 0.8184 - val_loss: 0.4045 - val_acc: 0.8159\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3967 - acc: 0.8208 - val_loss: 0.3961 - val_acc: 0.8192\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3940 - acc: 0.8224 - val_loss: 0.3903 - val_acc: 0.8222\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3926 - acc: 0.8228 - val_loss: 0.3877 - val_acc: 0.8243\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3889 - acc: 0.8242 - val_loss: 0.3857 - val_acc: 0.8246\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3881 - acc: 0.8250 - val_loss: 0.4041 - val_acc: 0.8140\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3864 - acc: 0.8249 - val_loss: 0.3991 - val_acc: 0.8199\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3835 - acc: 0.8271 - val_loss: 0.3932 - val_acc: 0.8178\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3812 - acc: 0.8279 - val_loss: 0.3846 - val_acc: 0.8274\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3796 - acc: 0.8289 - val_loss: 0.4038 - val_acc: 0.8169\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3790 - acc: 0.8298 - val_loss: 0.3832 - val_acc: 0.8271\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3785 - acc: 0.8309 - val_loss: 0.3865 - val_acc: 0.8242\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3757 - acc: 0.8313 - val_loss: 0.3853 - val_acc: 0.8256\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3755 - acc: 0.8308 - val_loss: 0.3877 - val_acc: 0.8249\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3740 - acc: 0.8330 - val_loss: 0.3856 - val_acc: 0.8249\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 161us/sample - loss: 0.3707 - acc: 0.8352 - val_loss: 0.3870 - val_acc: 0.8243\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3720 - acc: 0.8326 - val_loss: 0.3758 - val_acc: 0.8291\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3699 - acc: 0.8345 - val_loss: 0.3828 - val_acc: 0.8267\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3691 - acc: 0.8334 - val_loss: 0.3851 - val_acc: 0.8275\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3675 - acc: 0.8364 - val_loss: 0.3794 - val_acc: 0.8280\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 29s 346us/sample - loss: 0.5430 - acc: 0.7370 - val_loss: 0.4902 - val_acc: 0.7637\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4766 - acc: 0.7738 - val_loss: 0.4452 - val_acc: 0.7901\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4533 - acc: 0.7893 - val_loss: 0.4252 - val_acc: 0.8036\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4387 - acc: 0.7960 - val_loss: 0.4190 - val_acc: 0.8049\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4285 - acc: 0.8012 - val_loss: 0.4371 - val_acc: 0.7898- acc: 0.801\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4200 - acc: 0.8074 - val_loss: 0.4138 - val_acc: 0.8108\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4158 - acc: 0.8091 - val_loss: 0.4074 - val_acc: 0.8126\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4097 - acc: 0.8128 - val_loss: 0.4012 - val_acc: 0.8163\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.4051 - acc: 0.8167 - val_loss: 0.4005 - val_acc: 0.8159\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4037 - acc: 0.8176 - val_loss: 0.4029 - val_acc: 0.8167\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3994 - acc: 0.8186 - val_loss: 0.3952 - val_acc: 0.8197\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3978 - acc: 0.8204 - val_loss: 0.3947 - val_acc: 0.8202\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3945 - acc: 0.8208 - val_loss: 0.3854 - val_acc: 0.8249\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3898 - acc: 0.8237 - val_loss: 0.3981 - val_acc: 0.8159\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3888 - acc: 0.8250 - val_loss: 0.4165 - val_acc: 0.8067\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3860 - acc: 0.8268 - val_loss: 0.3893 - val_acc: 0.8223\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3852 - acc: 0.8255 - val_loss: 0.4082 - val_acc: 0.8105\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3819 - acc: 0.8272 - val_loss: 0.3866 - val_acc: 0.8228\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3812 - acc: 0.8286 - val_loss: 0.3836 - val_acc: 0.8260\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3801 - acc: 0.8281 - val_loss: 0.4098 - val_acc: 0.8105\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3769 - acc: 0.8308 - val_loss: 0.3843 - val_acc: 0.8243\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3771 - acc: 0.8309 - val_loss: 0.3868 - val_acc: 0.8248\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3748 - acc: 0.8313 - val_loss: 0.3823 - val_acc: 0.8238\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3742 - acc: 0.8321 - val_loss: 0.3830 - val_acc: 0.8248\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3714 - acc: 0.8334 - val_loss: 0.3784 - val_acc: 0.8267\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3694 - acc: 0.8340 - val_loss: 0.3796 - val_acc: 0.8255\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3698 - acc: 0.8349 - val_loss: 0.3807 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3693 - acc: 0.8343 - val_loss: 0.4074 - val_acc: 0.8162\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3676 - acc: 0.8350 - val_loss: 0.3803 - val_acc: 0.8261\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 168us/sample - loss: 0.3663 - acc: 0.8369 - val_loss: 0.3872 - val_acc: 0.8232\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 26s 309us/sample - loss: 0.5240 - acc: 0.7446 - val_loss: 0.4648 - val_acc: 0.7801\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4626 - acc: 0.7797 - val_loss: 0.4503 - val_acc: 0.7860\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4450 - acc: 0.7922 - val_loss: 0.4335 - val_acc: 0.7973\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4328 - acc: 0.7996 - val_loss: 0.4222 - val_acc: 0.8036\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4242 - acc: 0.8032 - val_loss: 0.4197 - val_acc: 0.8056\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4154 - acc: 0.8084 - val_loss: 0.4253 - val_acc: 0.8064\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4126 - acc: 0.8112 - val_loss: 0.4056 - val_acc: 0.8132\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4084 - acc: 0.8143 - val_loss: 0.3983 - val_acc: 0.8173\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4015 - acc: 0.8170 - val_loss: 0.3972 - val_acc: 0.8179\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.4011 - acc: 0.8180 - val_loss: 0.3943 - val_acc: 0.8172\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3962 - acc: 0.8195 - val_loss: 0.3914 - val_acc: 0.8199\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3948 - acc: 0.8223 - val_loss: 0.3923 - val_acc: 0.8202\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3919 - acc: 0.8221 - val_loss: 0.3906 - val_acc: 0.8210\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3904 - acc: 0.8241 - val_loss: 0.3981 - val_acc: 0.8177\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3876 - acc: 0.8240 - val_loss: 0.3850 - val_acc: 0.8248\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3863 - acc: 0.8243 - val_loss: 0.3889 - val_acc: 0.8231\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3832 - acc: 0.8269 - val_loss: 0.3850 - val_acc: 0.8246\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3825 - acc: 0.8267 - val_loss: 0.3910 - val_acc: 0.8207\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3794 - acc: 0.8302 - val_loss: 0.3986 - val_acc: 0.8177\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3766 - acc: 0.8323 - val_loss: 0.4012 - val_acc: 0.8169\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3782 - acc: 0.8290 - val_loss: 0.3890 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3762 - acc: 0.8312 - val_loss: 0.3931 - val_acc: 0.8214\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3746 - acc: 0.8321 - val_loss: 0.3846 - val_acc: 0.8257\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3743 - acc: 0.8321 - val_loss: 0.3915 - val_acc: 0.8213\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3730 - acc: 0.8328 - val_loss: 0.3848 - val_acc: 0.8249\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3737 - acc: 0.8327 - val_loss: 0.4140 - val_acc: 0.8120\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3713 - acc: 0.8334 - val_loss: 0.3841 - val_acc: 0.8248\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3716 - acc: 0.8325 - val_loss: 0.3842 - val_acc: 0.8224\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 139us/sample - loss: 0.3704 - acc: 0.8344 - val_loss: 0.3848 - val_acc: 0.8259\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 140us/sample - loss: 0.3676 - acc: 0.8345 - val_loss: 0.3952 - val_acc: 0.8177\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 312us/sample - loss: 0.5332 - acc: 0.7433 - val_loss: 0.4704 - val_acc: 0.7753\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4654 - acc: 0.7790 - val_loss: 0.4442 - val_acc: 0.7909\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4489 - acc: 0.7904 - val_loss: 0.4399 - val_acc: 0.7930\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4357 - acc: 0.7981 - val_loss: 0.4355 - val_acc: 0.7982\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4266 - acc: 0.8031 - val_loss: 0.4303 - val_acc: 0.8003\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4205 - acc: 0.8064 - val_loss: 0.4117 - val_acc: 0.8080\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4127 - acc: 0.8116 - val_loss: 0.4228 - val_acc: 0.8046\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4083 - acc: 0.8127 - val_loss: 0.3976 - val_acc: 0.8177\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.4045 - acc: 0.8156 - val_loss: 0.4139 - val_acc: 0.8105\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.4015 - acc: 0.8170 - val_loss: 0.4120 - val_acc: 0.8100\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3977 - acc: 0.8201 - val_loss: 0.3907 - val_acc: 0.8209\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3955 - acc: 0.8204 - val_loss: 0.3962 - val_acc: 0.8173\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3929 - acc: 0.8226 - val_loss: 0.3880 - val_acc: 0.8238\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3913 - acc: 0.8221 - val_loss: 0.3949 - val_acc: 0.8198\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3899 - acc: 0.8237 - val_loss: 0.3868 - val_acc: 0.8235\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3865 - acc: 0.8250 - val_loss: 0.3901 - val_acc: 0.8206\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3845 - acc: 0.8264 - val_loss: 0.3873 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3815 - acc: 0.8271 - val_loss: 0.3878 - val_acc: 0.8259\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3822 - acc: 0.8282 - val_loss: 0.3901 - val_acc: 0.8220\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3809 - acc: 0.8278 - val_loss: 0.3863 - val_acc: 0.8242\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3771 - acc: 0.8306 - val_loss: 0.3827 - val_acc: 0.8260\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3773 - acc: 0.8303 - val_loss: 0.4015 - val_acc: 0.8171\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3768 - acc: 0.8314 - val_loss: 0.4122 - val_acc: 0.8122\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 143us/sample - loss: 0.3751 - acc: 0.8314 - val_loss: 0.4014 - val_acc: 0.8182\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3733 - acc: 0.8325 - val_loss: 0.3832 - val_acc: 0.8255\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3732 - acc: 0.8331 - val_loss: 0.3861 - val_acc: 0.8252\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3714 - acc: 0.8340 - val_loss: 0.3865 - val_acc: 0.8257\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3708 - acc: 0.8339 - val_loss: 0.3819 - val_acc: 0.8270\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 142us/sample - loss: 0.3679 - acc: 0.8347 - val_loss: 0.3963 - val_acc: 0.8191\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 141us/sample - loss: 0.3684 - acc: 0.8348 - val_loss: 0.3812 - val_acc: 0.8285\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 29s 338us/sample - loss: 0.5241 - acc: 0.7475 - val_loss: 0.4574 - val_acc: 0.7849\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4571 - acc: 0.7840 - val_loss: 0.4671 - val_acc: 0.7865\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4364 - acc: 0.7986 - val_loss: 0.4238 - val_acc: 0.8040\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4226 - acc: 0.8067 - val_loss: 0.4239 - val_acc: 0.8040\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.4113 - acc: 0.8122 - val_loss: 0.4107 - val_acc: 0.8104\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.4044 - acc: 0.8151 - val_loss: 0.4293 - val_acc: 0.8000\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3969 - acc: 0.8210 - val_loss: 0.4052 - val_acc: 0.8126\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3925 - acc: 0.8222 - val_loss: 0.4067 - val_acc: 0.8147\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3870 - acc: 0.8252 - val_loss: 0.4099 - val_acc: 0.8138\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3811 - acc: 0.8281 - val_loss: 0.3905 - val_acc: 0.8242\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3787 - acc: 0.8310 - val_loss: 0.3826 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3727 - acc: 0.8335 - val_loss: 0.3890 - val_acc: 0.8238\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3670 - acc: 0.8366 - val_loss: 0.3882 - val_acc: 0.8267\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3653 - acc: 0.8378 - val_loss: 0.3846 - val_acc: 0.8262\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3600 - acc: 0.8410 - val_loss: 0.3838 - val_acc: 0.8283\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3569 - acc: 0.8412 - val_loss: 0.3899 - val_acc: 0.8237\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3530 - acc: 0.8438 - val_loss: 0.3798 - val_acc: 0.8270\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 164us/sample - loss: 0.3501 - acc: 0.8448 - val_loss: 0.3834 - val_acc: 0.8291\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3483 - acc: 0.8448 - val_loss: 0.3835 - val_acc: 0.8263\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3437 - acc: 0.8489 - val_loss: 0.3805 - val_acc: 0.8301\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3387 - acc: 0.8510 - val_loss: 0.4059 - val_acc: 0.8193\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3370 - acc: 0.8511 - val_loss: 0.3875 - val_acc: 0.8264\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3342 - acc: 0.8531 - val_loss: 0.3858 - val_acc: 0.8269\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3300 - acc: 0.8541 - val_loss: 0.4068 - val_acc: 0.8172\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3314 - acc: 0.8538 - val_loss: 0.3944 - val_acc: 0.8235 - acc: 0.\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3243 - acc: 0.8580 - val_loss: 0.4092 - val_acc: 0.8191\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3228 - acc: 0.8581 - val_loss: 0.3983 - val_acc: 0.8239\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3229 - acc: 0.8580 - val_loss: 0.3912 - val_acc: 0.8243\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 14s 162us/sample - loss: 0.3184 - acc: 0.8603 - val_loss: 0.3872 - val_acc: 0.8281\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 14s 163us/sample - loss: 0.3179 - acc: 0.8604 - val_loss: 0.3970 - val_acc: 0.8281\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 30s 354us/sample - loss: 0.5157 - acc: 0.7485 - val_loss: 0.5509 - val_acc: 0.7215\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.4503 - acc: 0.7894 - val_loss: 0.4271 - val_acc: 0.8026\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.4312 - acc: 0.8012 - val_loss: 0.4161 - val_acc: 0.8086\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.4194 - acc: 0.8065 - val_loss: 0.4135 - val_acc: 0.8065\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4103 - acc: 0.8129 - val_loss: 0.4035 - val_acc: 0.8191\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.4016 - acc: 0.8180 - val_loss: 0.4099 - val_acc: 0.8149\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3944 - acc: 0.8212 - val_loss: 0.3869 - val_acc: 0.8251\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3898 - acc: 0.8237 - val_loss: 0.4072 - val_acc: 0.8175\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3824 - acc: 0.8283 - val_loss: 0.3837 - val_acc: 0.8249\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3774 - acc: 0.8303 - val_loss: 0.4099 - val_acc: 0.8159\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3724 - acc: 0.8329 - val_loss: 0.3821 - val_acc: 0.8255\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3685 - acc: 0.8352 - val_loss: 0.3889 - val_acc: 0.8249\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3632 - acc: 0.8368 - val_loss: 0.3835 - val_acc: 0.8257\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3606 - acc: 0.8386 - val_loss: 0.3843 - val_acc: 0.8263\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3565 - acc: 0.8414 - val_loss: 0.4140 - val_acc: 0.8183\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3517 - acc: 0.8434 - val_loss: 0.3896 - val_acc: 0.8274\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3486 - acc: 0.8456 - val_loss: 0.3844 - val_acc: 0.8285\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3442 - acc: 0.8473 - val_loss: 0.3793 - val_acc: 0.8277\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3400 - acc: 0.8493 - val_loss: 0.4217 - val_acc: 0.8163\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3385 - acc: 0.8498 - val_loss: 0.3903 - val_acc: 0.8277\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3363 - acc: 0.8505 - val_loss: 0.3922 - val_acc: 0.8264\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3305 - acc: 0.8553 - val_loss: 0.3860 - val_acc: 0.8284\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3288 - acc: 0.8530 - val_loss: 0.3970 - val_acc: 0.8232\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3253 - acc: 0.8557 - val_loss: 0.3890 - val_acc: 0.8235\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3226 - acc: 0.8569 - val_loss: 0.3970 - val_acc: 0.8283\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3213 - acc: 0.8570 - val_loss: 0.3871 - val_acc: 0.8244\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3170 - acc: 0.8606 - val_loss: 0.4174 - val_acc: 0.8127\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3165 - acc: 0.8614 - val_loss: 0.3887 - val_acc: 0.8276\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 173us/sample - loss: 0.3127 - acc: 0.8626 - val_loss: 0.4019 - val_acc: 0.8256\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 172us/sample - loss: 0.3102 - acc: 0.8647 - val_loss: 0.3924 - val_acc: 0.8285\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 321us/sample - loss: 0.5197 - acc: 0.7486 - val_loss: 0.4702 - val_acc: 0.7777\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4660 - acc: 0.7796 - val_loss: 0.4534 - val_acc: 0.7867\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4480 - acc: 0.7911 - val_loss: 0.4581 - val_acc: 0.7822\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4334 - acc: 0.7987 - val_loss: 0.4140 - val_acc: 0.8105\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4242 - acc: 0.8050 - val_loss: 0.4252 - val_acc: 0.7998\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4165 - acc: 0.8105 - val_loss: 0.4312 - val_acc: 0.8016\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4106 - acc: 0.8121 - val_loss: 0.4040 - val_acc: 0.8156\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4067 - acc: 0.8149 - val_loss: 0.3943 - val_acc: 0.8223\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4030 - acc: 0.8168 - val_loss: 0.4300 - val_acc: 0.7984\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3984 - acc: 0.8190 - val_loss: 0.4466 - val_acc: 0.7937\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3946 - acc: 0.8213 - val_loss: 0.4006 - val_acc: 0.8178\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3928 - acc: 0.8228 - val_loss: 0.4262 - val_acc: 0.8030\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3897 - acc: 0.8245 - val_loss: 0.4039 - val_acc: 0.8130\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3863 - acc: 0.8258 - val_loss: 0.3882 - val_acc: 0.8247\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3835 - acc: 0.8285 - val_loss: 0.3883 - val_acc: 0.8241\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3822 - acc: 0.8275 - val_loss: 0.3850 - val_acc: 0.8262\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3796 - acc: 0.8285 - val_loss: 0.4015 - val_acc: 0.8134\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3767 - acc: 0.8309 - val_loss: 0.3847 - val_acc: 0.8275\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3742 - acc: 0.8321 - val_loss: 0.3830 - val_acc: 0.8270\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3733 - acc: 0.8322 - val_loss: 0.3830 - val_acc: 0.8286\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3709 - acc: 0.8321 - val_loss: 0.3920 - val_acc: 0.8221\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3702 - acc: 0.8343 - val_loss: 0.3908 - val_acc: 0.8224\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3669 - acc: 0.8354 - val_loss: 0.3936 - val_acc: 0.8228\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3648 - acc: 0.8375 - val_loss: 0.3862 - val_acc: 0.8262\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3634 - acc: 0.8377 - val_loss: 0.3794 - val_acc: 0.8312\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3622 - acc: 0.8385 - val_loss: 0.3791 - val_acc: 0.8282\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3609 - acc: 0.8400 - val_loss: 0.3857 - val_acc: 0.8255\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3614 - acc: 0.8382 - val_loss: 0.3800 - val_acc: 0.8279\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3579 - acc: 0.8400 - val_loss: 0.3811 - val_acc: 0.8283\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3556 - acc: 0.8419 - val_loss: 0.3837 - val_acc: 0.8263\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 27s 320us/sample - loss: 0.5207 - acc: 0.7491 - val_loss: 0.4571 - val_acc: 0.7814\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4606 - acc: 0.7816 - val_loss: 0.4409 - val_acc: 0.7954\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4408 - acc: 0.7944 - val_loss: 0.4703 - val_acc: 0.7757\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4291 - acc: 0.8001 - val_loss: 0.4157 - val_acc: 0.8082\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4201 - acc: 0.8069 - val_loss: 0.4118 - val_acc: 0.8110\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4125 - acc: 0.8107 - val_loss: 0.4076 - val_acc: 0.8119\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4080 - acc: 0.8137 - val_loss: 0.4024 - val_acc: 0.8141\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.4041 - acc: 0.8160 - val_loss: 0.4178 - val_acc: 0.8066\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.4007 - acc: 0.8182 - val_loss: 0.4102 - val_acc: 0.8111\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3960 - acc: 0.8207 - val_loss: 0.3932 - val_acc: 0.8227\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3920 - acc: 0.8228 - val_loss: 0.3924 - val_acc: 0.8224\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3892 - acc: 0.8234 - val_loss: 0.4024 - val_acc: 0.8143\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3874 - acc: 0.8256 - val_loss: 0.3965 - val_acc: 0.8191\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3852 - acc: 0.8265 - val_loss: 0.3891 - val_acc: 0.8230\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3846 - acc: 0.8275 - val_loss: 0.3862 - val_acc: 0.8255\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3805 - acc: 0.8303 - val_loss: 0.3929 - val_acc: 0.8235\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3794 - acc: 0.8289 - val_loss: 0.3942 - val_acc: 0.8228\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3768 - acc: 0.8309 - val_loss: 0.3869 - val_acc: 0.8262\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3736 - acc: 0.8315 - val_loss: 0.3834 - val_acc: 0.8259\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3735 - acc: 0.8322 - val_loss: 0.3864 - val_acc: 0.8224\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3722 - acc: 0.8336 - val_loss: 0.3915 - val_acc: 0.8227\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3701 - acc: 0.8342 - val_loss: 0.3970 - val_acc: 0.8199\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3678 - acc: 0.8349 - val_loss: 0.3874 - val_acc: 0.8231\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3676 - acc: 0.8355 - val_loss: 0.3882 - val_acc: 0.8242\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3663 - acc: 0.8357 - val_loss: 0.3869 - val_acc: 0.8245\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3631 - acc: 0.8369 - val_loss: 0.3873 - val_acc: 0.8248\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3603 - acc: 0.8390 - val_loss: 0.4020 - val_acc: 0.8165\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 12s 144us/sample - loss: 0.3607 - acc: 0.8387 - val_loss: 0.3841 - val_acc: 0.8255\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3595 - acc: 0.8397 - val_loss: 0.3844 - val_acc: 0.8249\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 12s 145us/sample - loss: 0.3556 - acc: 0.8417 - val_loss: 0.3830 - val_acc: 0.8281\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 30s 351us/sample - loss: 0.5240 - acc: 0.7495 - val_loss: 0.4526 - val_acc: 0.7832\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4505 - acc: 0.7891 - val_loss: 0.5959 - val_acc: 0.7148\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.4291 - acc: 0.8024 - val_loss: 0.4399 - val_acc: 0.7986\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.4171 - acc: 0.8104 - val_loss: 0.4183 - val_acc: 0.8097\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.4066 - acc: 0.8138 - val_loss: 0.4730 - val_acc: 0.7873\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3974 - acc: 0.8193 - val_loss: 0.3960 - val_acc: 0.8206\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3888 - acc: 0.8233 - val_loss: 0.3998 - val_acc: 0.8176\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3835 - acc: 0.8285 - val_loss: 0.3877 - val_acc: 0.8239\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3764 - acc: 0.8319 - val_loss: 0.3992 - val_acc: 0.8178\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3709 - acc: 0.8343 - val_loss: 0.4046 - val_acc: 0.8147\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3653 - acc: 0.8370 - val_loss: 0.3821 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3585 - acc: 0.8403 - val_loss: 0.3828 - val_acc: 0.8284\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3538 - acc: 0.8425 - val_loss: 0.3818 - val_acc: 0.8281\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3473 - acc: 0.8467 - val_loss: 0.3850 - val_acc: 0.8255\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3422 - acc: 0.8488 - val_loss: 0.3832 - val_acc: 0.8274\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3380 - acc: 0.8490 - val_loss: 0.3921 - val_acc: 0.8275\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3317 - acc: 0.8535 - val_loss: 0.3924 - val_acc: 0.8261\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 14s 169us/sample - loss: 0.3268 - acc: 0.8553 - val_loss: 0.3862 - val_acc: 0.8282\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.3213 - acc: 0.8590 - val_loss: 0.3912 - val_acc: 0.8281\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.3174 - acc: 0.8606 - val_loss: 0.3957 - val_acc: 0.8199\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.3111 - acc: 0.8630 - val_loss: 0.3957 - val_acc: 0.8256\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.3064 - acc: 0.8664 - val_loss: 0.4034 - val_acc: 0.8205\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.3036 - acc: 0.8669 - val_loss: 0.4080 - val_acc: 0.8258\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2984 - acc: 0.8701 - val_loss: 0.4098 - val_acc: 0.8207\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2955 - acc: 0.8716 - val_loss: 0.3982 - val_acc: 0.8273\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2905 - acc: 0.8738 - val_loss: 0.4089 - val_acc: 0.8217\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2864 - acc: 0.8749 - val_loss: 0.4070 - val_acc: 0.8248\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 14s 170us/sample - loss: 0.2851 - acc: 0.8760 - val_loss: 0.4141 - val_acc: 0.8219\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2812 - acc: 0.8780 - val_loss: 0.4134 - val_acc: 0.8229\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 171us/sample - loss: 0.2769 - acc: 0.8804 - val_loss: 0.4172 - val_acc: 0.8206\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 31s 367us/sample - loss: 0.5172 - acc: 0.7528 - val_loss: 0.4626 - val_acc: 0.7761\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4512 - acc: 0.7894 - val_loss: 0.4344 - val_acc: 0.7998\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4312 - acc: 0.8021 - val_loss: 0.4137 - val_acc: 0.8105\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4164 - acc: 0.8099 - val_loss: 0.4168 - val_acc: 0.8091\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4090 - acc: 0.8148 - val_loss: 0.4161 - val_acc: 0.8072\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3998 - acc: 0.8185 - val_loss: 0.4143 - val_acc: 0.8081\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3916 - acc: 0.8229 - val_loss: 0.3925 - val_acc: 0.8208\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3832 - acc: 0.8286 - val_loss: 0.3922 - val_acc: 0.8254\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3756 - acc: 0.8317 - val_loss: 0.3916 - val_acc: 0.8220\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3702 - acc: 0.8343 - val_loss: 0.3843 - val_acc: 0.8246\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3630 - acc: 0.8383 - val_loss: 0.3826 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3566 - acc: 0.8406 - val_loss: 0.3880 - val_acc: 0.8250\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3507 - acc: 0.8441 - val_loss: 0.3853 - val_acc: 0.8280\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3447 - acc: 0.8472 - val_loss: 0.3942 - val_acc: 0.8214\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3391 - acc: 0.8505 - val_loss: 0.3815 - val_acc: 0.8259\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3328 - acc: 0.8535 - val_loss: 0.3829 - val_acc: 0.8298\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3255 - acc: 0.8569 - val_loss: 0.3912 - val_acc: 0.8288\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3200 - acc: 0.8612 - val_loss: 0.3920 - val_acc: 0.8266\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3136 - acc: 0.8632 - val_loss: 0.3984 - val_acc: 0.8245\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3090 - acc: 0.8647 - val_loss: 0.4317 - val_acc: 0.8142\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3052 - acc: 0.8663 - val_loss: 0.3949 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2981 - acc: 0.8716 - val_loss: 0.3984 - val_acc: 0.8245\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2935 - acc: 0.8735 - val_loss: 0.4113 - val_acc: 0.8256\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.2891 - acc: 0.8743 - val_loss: 0.4110 - val_acc: 0.8203\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2849 - acc: 0.8768 - val_loss: 0.4141 - val_acc: 0.8220\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2781 - acc: 0.8806 - val_loss: 0.4316 - val_acc: 0.8213\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.2762 - acc: 0.8812 - val_loss: 0.4253 - val_acc: 0.8220\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 180us/sample - loss: 0.2691 - acc: 0.8850 - val_loss: 0.4354 - val_acc: 0.8165\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.2658 - acc: 0.8858 - val_loss: 0.4264 - val_acc: 0.8263\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.2646 - acc: 0.8869 - val_loss: 0.4293 - val_acc: 0.8232\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 31s 366us/sample - loss: 0.5511 - acc: 0.7296 - val_loss: 0.4826 - val_acc: 0.7667\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4912 - acc: 0.7620 - val_loss: 0.4646 - val_acc: 0.7783\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4733 - acc: 0.7743 - val_loss: 0.4495 - val_acc: 0.7876\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4621 - acc: 0.7817 - val_loss: 0.4658 - val_acc: 0.7752\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4538 - acc: 0.7858 - val_loss: 0.4341 - val_acc: 0.7991\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4459 - acc: 0.7893 - val_loss: 0.4299 - val_acc: 0.8008\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4405 - acc: 0.7948 - val_loss: 0.4188 - val_acc: 0.8045\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4361 - acc: 0.7976 - val_loss: 0.4285 - val_acc: 0.7997\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4315 - acc: 0.8002 - val_loss: 0.4291 - val_acc: 0.8001\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4276 - acc: 0.8022 - val_loss: 0.4157 - val_acc: 0.8095\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4252 - acc: 0.8034 - val_loss: 0.4179 - val_acc: 0.8071\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4231 - acc: 0.8043 - val_loss: 0.4102 - val_acc: 0.8130\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4215 - acc: 0.8052 - val_loss: 0.4137 - val_acc: 0.8112\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4188 - acc: 0.8073 - val_loss: 0.4257 - val_acc: 0.8041\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4148 - acc: 0.8104 - val_loss: 0.4012 - val_acc: 0.8163\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4154 - acc: 0.8097 - val_loss: 0.4179 - val_acc: 0.8073\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4124 - acc: 0.8109 - val_loss: 0.4035 - val_acc: 0.8158\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4102 - acc: 0.8109 - val_loss: 0.4068 - val_acc: 0.8122\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4090 - acc: 0.8139 - val_loss: 0.4101 - val_acc: 0.8105 loss: 0.4091 - acc: \n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4079 - acc: 0.8139 - val_loss: 0.4171 - val_acc: 0.8109\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4065 - acc: 0.8146 - val_loss: 0.4002 - val_acc: 0.8178\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4065 - acc: 0.8134 - val_loss: 0.3991 - val_acc: 0.8181\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4058 - acc: 0.8149 - val_loss: 0.4020 - val_acc: 0.8159\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4032 - acc: 0.8153 - val_loss: 0.3990 - val_acc: 0.8152- ETA: 0s - loss: 0.402\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4014 - acc: 0.8162 - val_loss: 0.4003 - val_acc: 0.8151\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4009 - acc: 0.8166 - val_loss: 0.3974 - val_acc: 0.8167\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.3998 - acc: 0.8189 - val_loss: 0.4006 - val_acc: 0.8177\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.3993 - acc: 0.8178 - val_loss: 0.3981 - val_acc: 0.8185\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.3968 - acc: 0.8182 - val_loss: 0.3949 - val_acc: 0.8194\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.3980 - acc: 0.8193 - val_loss: 0.4024 - val_acc: 0.8148\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 31s 365us/sample - loss: 0.5541 - acc: 0.7239 - val_loss: 0.4851 - val_acc: 0.7662\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4893 - acc: 0.7632 - val_loss: 0.4635 - val_acc: 0.7817\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4697 - acc: 0.7760 - val_loss: 0.4571 - val_acc: 0.7839\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4582 - acc: 0.7833 - val_loss: 0.4427 - val_acc: 0.7901\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 180us/sample - loss: 0.4489 - acc: 0.7910 - val_loss: 0.4326 - val_acc: 0.7971\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4437 - acc: 0.7937 - val_loss: 0.4272 - val_acc: 0.8019\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4393 - acc: 0.7960 - val_loss: 0.4238 - val_acc: 0.8038\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4324 - acc: 0.8000 - val_loss: 0.4238 - val_acc: 0.8037\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4304 - acc: 0.8011 - val_loss: 0.4247 - val_acc: 0.8041\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4260 - acc: 0.8033 - val_loss: 0.4136 - val_acc: 0.8104\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4240 - acc: 0.8042 - val_loss: 0.4139 - val_acc: 0.8083\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4203 - acc: 0.8070 - val_loss: 0.4092 - val_acc: 0.8123\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4196 - acc: 0.8068 - val_loss: 0.4127 - val_acc: 0.8091\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4164 - acc: 0.8091 - val_loss: 0.4049 - val_acc: 0.8145\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4142 - acc: 0.8099 - val_loss: 0.4052 - val_acc: 0.8123\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4128 - acc: 0.8099 - val_loss: 0.4072 - val_acc: 0.8121\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.4094 - acc: 0.8126 - val_loss: 0.4048 - val_acc: 0.8141\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 176us/sample - loss: 0.4100 - acc: 0.8127 - val_loss: 0.4174 - val_acc: 0.8066\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4077 - acc: 0.8135 - val_loss: 0.4172 - val_acc: 0.8065\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4062 - acc: 0.8146 - val_loss: 0.3977 - val_acc: 0.8194\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4045 - acc: 0.8149 - val_loss: 0.4003 - val_acc: 0.8152\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4010 - acc: 0.8179 - val_loss: 0.4012 - val_acc: 0.8144\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4024 - acc: 0.8166 - val_loss: 0.4020 - val_acc: 0.8145\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.4014 - acc: 0.8167 - val_loss: 0.4060 - val_acc: 0.8141\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 175us/sample - loss: 0.3990 - acc: 0.8182 - val_loss: 0.4005 - val_acc: 0.8150\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3999 - acc: 0.8183 - val_loss: 0.3982 - val_acc: 0.8174\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3965 - acc: 0.8196 - val_loss: 0.3969 - val_acc: 0.8184\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3948 - acc: 0.8212 - val_loss: 0.4072 - val_acc: 0.8119\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3957 - acc: 0.8191 - val_loss: 0.3996 - val_acc: 0.8162\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 174us/sample - loss: 0.3948 - acc: 0.8200 - val_loss: 0.3978 - val_acc: 0.8173\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 33s 383us/sample - loss: 0.5675 - acc: 0.7193 - val_loss: 0.4868 - val_acc: 0.7691\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.5001 - acc: 0.7584 - val_loss: 0.4666 - val_acc: 0.7786\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4845 - acc: 0.7679 - val_loss: 0.4625 - val_acc: 0.7801\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4713 - acc: 0.7757 - val_loss: 0.4401 - val_acc: 0.7953\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4612 - acc: 0.7844 - val_loss: 0.4434 - val_acc: 0.7916\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4563 - acc: 0.7863 - val_loss: 0.4292 - val_acc: 0.8005\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4522 - acc: 0.7884 - val_loss: 0.4266 - val_acc: 0.8042\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4457 - acc: 0.7929 - val_loss: 0.4278 - val_acc: 0.8037\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4455 - acc: 0.7928 - val_loss: 0.4249 - val_acc: 0.8059\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4423 - acc: 0.7949 - val_loss: 0.4195 - val_acc: 0.8076\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4404 - acc: 0.7960 - val_loss: 0.4191 - val_acc: 0.8074\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4365 - acc: 0.7977 - val_loss: 0.4154 - val_acc: 0.8091\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4360 - acc: 0.7977 - val_loss: 0.4183 - val_acc: 0.8076\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4321 - acc: 0.7995 - val_loss: 0.4291 - val_acc: 0.8005\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4330 - acc: 0.7998 - val_loss: 0.4195 - val_acc: 0.8058\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4324 - acc: 0.8007 - val_loss: 0.4089 - val_acc: 0.8117\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4323 - acc: 0.7990 - val_loss: 0.4118 - val_acc: 0.8114\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4302 - acc: 0.8020 - val_loss: 0.4135 - val_acc: 0.8094\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4299 - acc: 0.8005 - val_loss: 0.4247 - val_acc: 0.8002\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4274 - acc: 0.8039 - val_loss: 0.4100 - val_acc: 0.8134\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4282 - acc: 0.8010 - val_loss: 0.4163 - val_acc: 0.8048\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4266 - acc: 0.8031 - val_loss: 0.4030 - val_acc: 0.8165\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4249 - acc: 0.8046 - val_loss: 0.4147 - val_acc: 0.8101\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4248 - acc: 0.8045 - val_loss: 0.4050 - val_acc: 0.8137\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4257 - acc: 0.8032 - val_loss: 0.4044 - val_acc: 0.8156\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4256 - acc: 0.8039 - val_loss: 0.4058 - val_acc: 0.8143\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4227 - acc: 0.8057 - val_loss: 0.4066 - val_acc: 0.8145\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4232 - acc: 0.8058 - val_loss: 0.4058 - val_acc: 0.8124\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4236 - acc: 0.8061 - val_loss: 0.4022 - val_acc: 0.8155\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4227 - acc: 0.8045 - val_loss: 0.4100 - val_acc: 0.8135\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 34s 395us/sample - loss: 0.5843 - acc: 0.7071 - val_loss: 0.5004 - val_acc: 0.7574\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 192us/sample - loss: 0.5158 - acc: 0.7496 - val_loss: 0.4902 - val_acc: 0.7591\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4969 - acc: 0.7596 - val_loss: 0.4678 - val_acc: 0.7775\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4855 - acc: 0.7680 - val_loss: 0.4683 - val_acc: 0.7777\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4759 - acc: 0.7741 - val_loss: 0.4521 - val_acc: 0.7890\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4682 - acc: 0.7786 - val_loss: 0.4601 - val_acc: 0.7821\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4614 - acc: 0.7820 - val_loss: 0.4364 - val_acc: 0.7987\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4547 - acc: 0.7878 - val_loss: 0.4653 - val_acc: 0.7754\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4515 - acc: 0.7890 - val_loss: 0.4304 - val_acc: 0.8015\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4455 - acc: 0.7918 - val_loss: 0.4433 - val_acc: 0.7842\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4445 - acc: 0.7926 - val_loss: 0.4357 - val_acc: 0.7969\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4423 - acc: 0.7941 - val_loss: 0.4262 - val_acc: 0.8022\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4396 - acc: 0.7961 - val_loss: 0.4230 - val_acc: 0.8060\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4379 - acc: 0.7977 - val_loss: 0.4204 - val_acc: 0.8062\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4376 - acc: 0.7971 - val_loss: 0.4197 - val_acc: 0.8082\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4360 - acc: 0.7980 - val_loss: 0.4217 - val_acc: 0.8055\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4355 - acc: 0.7986 - val_loss: 0.4148 - val_acc: 0.8088\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4347 - acc: 0.8001 - val_loss: 0.4193 - val_acc: 0.8097\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4341 - acc: 0.7978 - val_loss: 0.4326 - val_acc: 0.8015\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4325 - acc: 0.7992 - val_loss: 0.4113 - val_acc: 0.8118\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4321 - acc: 0.7996 - val_loss: 0.4209 - val_acc: 0.8044\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4306 - acc: 0.8012 - val_loss: 0.4181 - val_acc: 0.8064\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4294 - acc: 0.8006 - val_loss: 0.4136 - val_acc: 0.8077\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4292 - acc: 0.8014 - val_loss: 0.4139 - val_acc: 0.8084\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4289 - acc: 0.8012 - val_loss: 0.4317 - val_acc: 0.7985\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4287 - acc: 0.8026 - val_loss: 0.4103 - val_acc: 0.8145\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4295 - acc: 0.8009 - val_loss: 0.4115 - val_acc: 0.8130\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4265 - acc: 0.8041 - val_loss: 0.4081 - val_acc: 0.8127\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4283 - acc: 0.8039 - val_loss: 0.4095 - val_acc: 0.8137\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.4285 - acc: 0.8034 - val_loss: 0.4192 - val_acc: 0.8101\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 32s 379us/sample - loss: 0.5335 - acc: 0.7400 - val_loss: 0.4811 - val_acc: 0.7714\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4680 - acc: 0.7777 - val_loss: 0.4613 - val_acc: 0.7839\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4456 - acc: 0.7919 - val_loss: 0.4315 - val_acc: 0.7968\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4313 - acc: 0.7998 - val_loss: 0.4122 - val_acc: 0.8094\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4221 - acc: 0.8063 - val_loss: 0.4086 - val_acc: 0.8125\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4133 - acc: 0.8103 - val_loss: 0.4168 - val_acc: 0.8094\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4076 - acc: 0.8136 - val_loss: 0.4104 - val_acc: 0.8097\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4032 - acc: 0.8158 - val_loss: 0.4003 - val_acc: 0.8178\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3989 - acc: 0.8184 - val_loss: 0.4154 - val_acc: 0.8090\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3931 - acc: 0.8224 - val_loss: 0.4400 - val_acc: 0.7929\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 180us/sample - loss: 0.3880 - acc: 0.8244 - val_loss: 0.4039 - val_acc: 0.8164\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3838 - acc: 0.8271 - val_loss: 0.3926 - val_acc: 0.8207\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3804 - acc: 0.8282 - val_loss: 0.3930 - val_acc: 0.8212\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3761 - acc: 0.8317 - val_loss: 0.3897 - val_acc: 0.8219\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3734 - acc: 0.8323 - val_loss: 0.3953 - val_acc: 0.8195\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3699 - acc: 0.8335 - val_loss: 0.3934 - val_acc: 0.8198\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3657 - acc: 0.8348 - val_loss: 0.3992 - val_acc: 0.8147\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3614 - acc: 0.8375 - val_loss: 0.3899 - val_acc: 0.8239\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3621 - acc: 0.8384 - val_loss: 0.3901 - val_acc: 0.8204\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3568 - acc: 0.8407 - val_loss: 0.3922 - val_acc: 0.8199 - ETA: 2 - ETA: 0s - loss: 0.3566\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3546 - acc: 0.8425 - val_loss: 0.3925 - val_acc: 0.8224\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3507 - acc: 0.8445 - val_loss: 0.3859 - val_acc: 0.8253\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 177us/sample - loss: 0.3497 - acc: 0.8442 - val_loss: 0.4023 - val_acc: 0.8144\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3475 - acc: 0.8454 - val_loss: 0.3929 - val_acc: 0.8199 loss: 0.3456 - acc: 0.846 - ETA: 6s - loss: 0.34 - ETA: 2s - loss: 0.3479 - \n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3439 - acc: 0.8464 - val_loss: 0.3931 - val_acc: 0.8228\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3402 - acc: 0.8478 - val_loss: 0.3891 - val_acc: 0.8231\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3390 - acc: 0.8498 - val_loss: 0.3939 - val_acc: 0.8230\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3352 - acc: 0.8510 - val_loss: 0.3973 - val_acc: 0.8231\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3340 - acc: 0.8517 - val_loss: 0.3979 - val_acc: 0.8180\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3322 - acc: 0.8537 - val_loss: 0.4016 - val_acc: 0.8192\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 32s 379us/sample - loss: 0.5432 - acc: 0.7361 - val_loss: 0.4637 - val_acc: 0.7793\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4696 - acc: 0.7771 - val_loss: 0.4717 - val_acc: 0.7763\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4480 - acc: 0.7902 - val_loss: 0.4431 - val_acc: 0.7930s - loss\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4337 - acc: 0.7982 - val_loss: 0.4292 - val_acc: 0.8006\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4239 - acc: 0.8057 - val_loss: 0.4137 - val_acc: 0.8091\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4155 - acc: 0.8094 - val_loss: 0.4094 - val_acc: 0.8134\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.4102 - acc: 0.8128 - val_loss: 0.4005 - val_acc: 0.8166\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.4028 - acc: 0.8164 - val_loss: 0.4078 - val_acc: 0.8156\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3985 - acc: 0.8193 - val_loss: 0.4020 - val_acc: 0.8166\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3928 - acc: 0.8219 - val_loss: 0.4184 - val_acc: 0.8054\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3907 - acc: 0.8241 - val_loss: 0.4032 - val_acc: 0.8152\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3851 - acc: 0.8259 - val_loss: 0.3935 - val_acc: 0.8221\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3818 - acc: 0.8283 - val_loss: 0.3978 - val_acc: 0.8183\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3799 - acc: 0.8293 - val_loss: 0.3973 - val_acc: 0.8167\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3735 - acc: 0.8318 - val_loss: 0.3955 - val_acc: 0.8192\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3710 - acc: 0.8339 - val_loss: 0.4036 - val_acc: 0.8162\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3678 - acc: 0.8349 - val_loss: 0.3923 - val_acc: 0.8198\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3652 - acc: 0.8364 - val_loss: 0.3904 - val_acc: 0.8233\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3623 - acc: 0.8380 - val_loss: 0.3939 - val_acc: 0.8198\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 15s 180us/sample - loss: 0.3588 - acc: 0.8403 - val_loss: 0.3911 - val_acc: 0.8223\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3577 - acc: 0.8395 - val_loss: 0.3993 - val_acc: 0.8181\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3533 - acc: 0.8431 - val_loss: 0.3940 - val_acc: 0.8199\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3517 - acc: 0.8434 - val_loss: 0.3900 - val_acc: 0.8217\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3465 - acc: 0.8453 - val_loss: 0.3942 - val_acc: 0.8207\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3441 - acc: 0.8458 - val_loss: 0.4357 - val_acc: 0.8041\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3428 - acc: 0.8467 - val_loss: 0.4059 - val_acc: 0.8156\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3404 - acc: 0.8468 - val_loss: 0.3993 - val_acc: 0.8231\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3367 - acc: 0.8504 - val_loss: 0.3918 - val_acc: 0.8241\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 15s 178us/sample - loss: 0.3352 - acc: 0.8518 - val_loss: 0.3936 - val_acc: 0.8220\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 15s 179us/sample - loss: 0.3349 - acc: 0.8495 - val_loss: 0.3996 - val_acc: 0.8185\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 33s 393us/sample - loss: 0.5413 - acc: 0.7365 - val_loss: 0.4653 - val_acc: 0.7782\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4684 - acc: 0.7792 - val_loss: 0.4459 - val_acc: 0.7906s: 0.4687 - acc: \n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4490 - acc: 0.7900 - val_loss: 0.4468 - val_acc: 0.7947\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4353 - acc: 0.7995 - val_loss: 0.4171 - val_acc: 0.8072\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.4263 - acc: 0.8049 - val_loss: 0.4093 - val_acc: 0.8144\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4194 - acc: 0.8072 - val_loss: 0.4205 - val_acc: 0.8035\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4135 - acc: 0.8116 - val_loss: 0.4210 - val_acc: 0.8040\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4095 - acc: 0.8123 - val_loss: 0.4076 - val_acc: 0.8134\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4052 - acc: 0.8158 - val_loss: 0.3937 - val_acc: 0.8202TA: 1s - loss:\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4016 - acc: 0.8179 - val_loss: 0.4258 - val_acc: 0.8041\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3977 - acc: 0.8186 - val_loss: 0.3961 - val_acc: 0.8180\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3944 - acc: 0.8214 - val_loss: 0.3918 - val_acc: 0.8219\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3906 - acc: 0.8229 - val_loss: 0.3933 - val_acc: 0.8219\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3902 - acc: 0.8228 - val_loss: 0.3891 - val_acc: 0.8233\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3882 - acc: 0.8238 - val_loss: 0.3878 - val_acc: 0.8217\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3850 - acc: 0.8254 - val_loss: 0.3930 - val_acc: 0.8217\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3828 - acc: 0.8277 - val_loss: 0.3976 - val_acc: 0.8166\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3791 - acc: 0.8298 - val_loss: 0.3833 - val_acc: 0.8260\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3793 - acc: 0.8293 - val_loss: 0.3815 - val_acc: 0.8269\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3757 - acc: 0.8310 - val_loss: 0.3870 - val_acc: 0.8261\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3750 - acc: 0.8329 - val_loss: 0.3871 - val_acc: 0.8242\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3732 - acc: 0.8331 - val_loss: 0.3990 - val_acc: 0.8194\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3723 - acc: 0.8324 - val_loss: 0.3967 - val_acc: 0.8181\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3695 - acc: 0.8345 - val_loss: 0.3860 - val_acc: 0.8253\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3682 - acc: 0.8361 - val_loss: 0.3893 - val_acc: 0.8233\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3679 - acc: 0.8345 - val_loss: 0.3821 - val_acc: 0.8263\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3660 - acc: 0.8364 - val_loss: 0.3808 - val_acc: 0.8266\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3661 - acc: 0.8358 - val_loss: 0.3866 - val_acc: 0.8271\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3633 - acc: 0.8379 - val_loss: 0.3887 - val_acc: 0.8222\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3640 - acc: 0.8378 - val_loss: 0.3846 - val_acc: 0.8263\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 35s 410us/sample - loss: 0.5605 - acc: 0.7272 - val_loss: 0.4761 - val_acc: 0.7682\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4803 - acc: 0.7718 - val_loss: 0.4536 - val_acc: 0.7850\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.4601 - acc: 0.7827 - val_loss: 0.4532 - val_acc: 0.7859\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.4471 - acc: 0.7914 - val_loss: 0.4649 - val_acc: 0.7826\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.4367 - acc: 0.7982 - val_loss: 0.4234 - val_acc: 0.8087\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4271 - acc: 0.8025 - val_loss: 0.4229 - val_acc: 0.8026\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4203 - acc: 0.8063 - val_loss: 0.4048 - val_acc: 0.8160\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4151 - acc: 0.8095 - val_loss: 0.4180 - val_acc: 0.8066\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4099 - acc: 0.8132 - val_loss: 0.4385 - val_acc: 0.7946\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4057 - acc: 0.8156 - val_loss: 0.3962 - val_acc: 0.8168\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.4025 - acc: 0.8176 - val_loss: 0.3955 - val_acc: 0.8163\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3990 - acc: 0.8184 - val_loss: 0.3925 - val_acc: 0.8190\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3946 - acc: 0.8212 - val_loss: 0.3838 - val_acc: 0.8249\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3938 - acc: 0.8227 - val_loss: 0.3869 - val_acc: 0.8220\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3895 - acc: 0.8241 - val_loss: 0.4036 - val_acc: 0.8151\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3856 - acc: 0.8266 - val_loss: 0.3919 - val_acc: 0.8224\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3850 - acc: 0.8250 - val_loss: 0.3870 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3833 - acc: 0.8260 - val_loss: 0.3841 - val_acc: 0.8251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3813 - acc: 0.8286 - val_loss: 0.4035 - val_acc: 0.8142\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3797 - acc: 0.8285 - val_loss: 0.3898 - val_acc: 0.8209\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3769 - acc: 0.8301 - val_loss: 0.3826 - val_acc: 0.8250\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3746 - acc: 0.8324 - val_loss: 0.3909 - val_acc: 0.8209\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3755 - acc: 0.8314 - val_loss: 0.3890 - val_acc: 0.8236\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3720 - acc: 0.8337 - val_loss: 0.3821 - val_acc: 0.8255\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3727 - acc: 0.8329 - val_loss: 0.3891 - val_acc: 0.8216\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3716 - acc: 0.8335 - val_loss: 0.3755 - val_acc: 0.8292\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3706 - acc: 0.8342 - val_loss: 0.3804 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3688 - acc: 0.8348 - val_loss: 0.3820 - val_acc: 0.8269\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3672 - acc: 0.8358 - val_loss: 0.3813 - val_acc: 0.8256\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3676 - acc: 0.8350 - val_loss: 0.3753 - val_acc: 0.8266\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 33s 391us/sample - loss: 0.5157 - acc: 0.7499 - val_loss: 0.4730 - val_acc: 0.7750\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4540 - acc: 0.7864 - val_loss: 0.4281 - val_acc: 0.8000\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4318 - acc: 0.7999 - val_loss: 0.4250 - val_acc: 0.8040\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4208 - acc: 0.8069 - val_loss: 0.4314 - val_acc: 0.7966\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4106 - acc: 0.8127 - val_loss: 0.4063 - val_acc: 0.8127\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.4030 - acc: 0.8156 - val_loss: 0.3982 - val_acc: 0.8163\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3949 - acc: 0.8213 - val_loss: 0.4072 - val_acc: 0.8170\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3908 - acc: 0.8238 - val_loss: 0.3942 - val_acc: 0.8196\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3848 - acc: 0.8269 - val_loss: 0.4010 - val_acc: 0.8140\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3788 - acc: 0.8308 - val_loss: 0.3931 - val_acc: 0.8218\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3728 - acc: 0.8346 - val_loss: 0.3904 - val_acc: 0.8231\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3680 - acc: 0.8355 - val_loss: 0.3872 - val_acc: 0.8260\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3637 - acc: 0.8368 - val_loss: 0.4559 - val_acc: 0.7916\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3599 - acc: 0.8395 - val_loss: 0.3859 - val_acc: 0.8260\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3550 - acc: 0.8414 - val_loss: 0.4044 - val_acc: 0.8125\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3522 - acc: 0.8446 - val_loss: 0.3947 - val_acc: 0.8229\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3457 - acc: 0.8463 - val_loss: 0.3947 - val_acc: 0.8217\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3444 - acc: 0.8476 - val_loss: 0.4130 - val_acc: 0.8184\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3383 - acc: 0.8501 - val_loss: 0.4231 - val_acc: 0.8073\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3361 - acc: 0.8499 - val_loss: 0.3959 - val_acc: 0.8228\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3288 - acc: 0.8550 - val_loss: 0.4083 - val_acc: 0.8178\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3259 - acc: 0.8562 - val_loss: 0.3945 - val_acc: 0.8242\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3216 - acc: 0.8575 - val_loss: 0.3988 - val_acc: 0.8263\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3178 - acc: 0.8603 - val_loss: 0.3956 - val_acc: 0.8233\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3156 - acc: 0.8602 - val_loss: 0.4189 - val_acc: 0.8177\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3097 - acc: 0.8648 - val_loss: 0.3937 - val_acc: 0.8263\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3091 - acc: 0.8633 - val_loss: 0.4175 - val_acc: 0.8187\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3037 - acc: 0.8670 - val_loss: 0.4191 - val_acc: 0.8148\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.2991 - acc: 0.8693 - val_loss: 0.4108 - val_acc: 0.8235\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.2964 - acc: 0.8696 - val_loss: 0.4168 - val_acc: 0.8190\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 34s 398us/sample - loss: 0.5150 - acc: 0.7497 - val_loss: 0.4818 - val_acc: 0.7673\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4544 - acc: 0.7861 - val_loss: 0.4368 - val_acc: 0.7964\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.4333 - acc: 0.7991 - val_loss: 0.4192 - val_acc: 0.8076\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4217 - acc: 0.8062 - val_loss: 0.4045 - val_acc: 0.8159\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4095 - acc: 0.8121 - val_loss: 0.4096 - val_acc: 0.8122\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.4009 - acc: 0.8179 - val_loss: 0.4103 - val_acc: 0.8107\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3951 - acc: 0.8205 - val_loss: 0.4174 - val_acc: 0.8072\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3893 - acc: 0.8247 - val_loss: 0.4255 - val_acc: 0.8033\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3843 - acc: 0.8264 - val_loss: 0.4133 - val_acc: 0.8112\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3793 - acc: 0.8297 - val_loss: 0.3876 - val_acc: 0.8257\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3737 - acc: 0.8319 - val_loss: 0.3872 - val_acc: 0.8226\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3695 - acc: 0.8342 - val_loss: 0.3837 - val_acc: 0.8260\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3646 - acc: 0.8371 - val_loss: 0.3977 - val_acc: 0.8192\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3586 - acc: 0.8404 - val_loss: 0.3921 - val_acc: 0.8239 0.840\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3553 - acc: 0.8415 - val_loss: 0.3894 - val_acc: 0.8250\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3495 - acc: 0.8442 - val_loss: 0.3886 - val_acc: 0.8252\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3458 - acc: 0.8467 - val_loss: 0.3902 - val_acc: 0.8230\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3398 - acc: 0.8497 - val_loss: 0.3929 - val_acc: 0.8219\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3381 - acc: 0.8508 - val_loss: 0.3901 - val_acc: 0.8281\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3323 - acc: 0.8537 - val_loss: 0.3998 - val_acc: 0.8214\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3279 - acc: 0.8548 - val_loss: 0.3957 - val_acc: 0.8217\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3248 - acc: 0.8557 - val_loss: 0.4049 - val_acc: 0.8220\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3179 - acc: 0.8603 - val_loss: 0.4069 - val_acc: 0.8189\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3152 - acc: 0.8618 - val_loss: 0.3997 - val_acc: 0.8216\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3129 - acc: 0.8630 - val_loss: 0.3982 - val_acc: 0.8246\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.3061 - acc: 0.8670 - val_loss: 0.4137 - val_acc: 0.8193\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3038 - acc: 0.8671 - val_loss: 0.4031 - val_acc: 0.8270\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3015 - acc: 0.8678 - val_loss: 0.4158 - val_acc: 0.8166\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.2956 - acc: 0.8706 - val_loss: 0.4107 - val_acc: 0.8174\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 184us/sample - loss: 0.2910 - acc: 0.8729 - val_loss: 0.4130 - val_acc: 0.8185\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 35s 409us/sample - loss: 0.5217 - acc: 0.7482 - val_loss: 0.4611 - val_acc: 0.7829\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4574 - acc: 0.7853 - val_loss: 0.4271 - val_acc: 0.8009\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.4362 - acc: 0.7988 - val_loss: 0.4581 - val_acc: 0.7844\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.4230 - acc: 0.8053 - val_loss: 0.4060 - val_acc: 0.8140\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4114 - acc: 0.8136 - val_loss: 0.4062 - val_acc: 0.8135\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4049 - acc: 0.8162 - val_loss: 0.3950 - val_acc: 0.8205\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3971 - acc: 0.8204 - val_loss: 0.4004 - val_acc: 0.8158\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3913 - acc: 0.8238 - val_loss: 0.4017 - val_acc: 0.8159\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3851 - acc: 0.8258 - val_loss: 0.4501 - val_acc: 0.7929\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3791 - acc: 0.8303 - val_loss: 0.3927 - val_acc: 0.8207\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3746 - acc: 0.8331 - val_loss: 0.3917 - val_acc: 0.8199\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3709 - acc: 0.8347 - val_loss: 0.3809 - val_acc: 0.8275\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3644 - acc: 0.8373 - val_loss: 0.3966 - val_acc: 0.8235\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.3584 - acc: 0.8398 - val_loss: 0.3863 - val_acc: 0.8267\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3566 - acc: 0.8411 - val_loss: 0.3890 - val_acc: 0.8245\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3511 - acc: 0.8438 - val_loss: 0.4202 - val_acc: 0.8066\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3481 - acc: 0.8452 - val_loss: 0.3859 - val_acc: 0.8267\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3442 - acc: 0.8475 - val_loss: 0.3874 - val_acc: 0.8277\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3396 - acc: 0.8507 - val_loss: 0.3929 - val_acc: 0.8244\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3371 - acc: 0.8506 - val_loss: 0.3898 - val_acc: 0.8271\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3326 - acc: 0.8541 - val_loss: 0.3905 - val_acc: 0.8236\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3309 - acc: 0.8542 - val_loss: 0.3847 - val_acc: 0.8271\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3270 - acc: 0.8568 - val_loss: 0.4007 - val_acc: 0.8205\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3248 - acc: 0.8574 - val_loss: 0.4028 - val_acc: 0.8192\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3214 - acc: 0.8593 - val_loss: 0.3946 - val_acc: 0.8230\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3165 - acc: 0.8623 - val_loss: 0.3915 - val_acc: 0.8262\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3157 - acc: 0.8629 - val_loss: 0.3991 - val_acc: 0.8254\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3126 - acc: 0.8630 - val_loss: 0.4040 - val_acc: 0.8195\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3092 - acc: 0.8644 - val_loss: 0.4009 - val_acc: 0.8258\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3085 - acc: 0.8659 - val_loss: 0.3962 - val_acc: 0.8257\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 36s 427us/sample - loss: 0.5261 - acc: 0.7454 - val_loss: 0.4783 - val_acc: 0.7732\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.4622 - acc: 0.7818 - val_loss: 0.4473 - val_acc: 0.7911\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.4405 - acc: 0.7955 - val_loss: 0.4162 - val_acc: 0.8087\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.4274 - acc: 0.8029 - val_loss: 0.4113 - val_acc: 0.8115\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.4159 - acc: 0.8090 - val_loss: 0.4074 - val_acc: 0.8134\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.4069 - acc: 0.8152 - val_loss: 0.4049 - val_acc: 0.8172\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.3998 - acc: 0.8197 - val_loss: 0.3941 - val_acc: 0.8230\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3931 - acc: 0.8234 - val_loss: 0.4207 - val_acc: 0.8043\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3869 - acc: 0.8256 - val_loss: 0.3982 - val_acc: 0.8184\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.3806 - acc: 0.8294 - val_loss: 0.3877 - val_acc: 0.8263\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3770 - acc: 0.8320 - val_loss: 0.4226 - val_acc: 0.8098\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 17s 206us/sample - loss: 0.3705 - acc: 0.8333 - val_loss: 0.3815 - val_acc: 0.8266\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3671 - acc: 0.8360 - val_loss: 0.3855 - val_acc: 0.8211\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3627 - acc: 0.8380 - val_loss: 0.3775 - val_acc: 0.8274\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.3582 - acc: 0.8400 - val_loss: 0.3891 - val_acc: 0.8232\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 17s 206us/sample - loss: 0.3547 - acc: 0.8419 - val_loss: 0.3854 - val_acc: 0.8228\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3518 - acc: 0.8422 - val_loss: 0.3863 - val_acc: 0.8271\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3486 - acc: 0.8453 - val_loss: 0.3819 - val_acc: 0.8292\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3437 - acc: 0.8482 - val_loss: 0.3827 - val_acc: 0.8304\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3390 - acc: 0.8501 - val_loss: 0.3990 - val_acc: 0.8210\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3359 - acc: 0.8512 - val_loss: 0.3893 - val_acc: 0.8263\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3333 - acc: 0.8531 - val_loss: 0.3957 - val_acc: 0.8216\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3300 - acc: 0.8559 - val_loss: 0.3984 - val_acc: 0.8266\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3263 - acc: 0.8563 - val_loss: 0.3841 - val_acc: 0.8292\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3226 - acc: 0.8579 - val_loss: 0.3931 - val_acc: 0.8195\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3217 - acc: 0.8602 - val_loss: 0.3865 - val_acc: 0.8291\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3193 - acc: 0.8605 - val_loss: 0.3820 - val_acc: 0.8285\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3166 - acc: 0.8613 - val_loss: 0.3891 - val_acc: 0.8244\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3152 - acc: 0.8628 - val_loss: 0.3913 - val_acc: 0.8278\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.3098 - acc: 0.8659 - val_loss: 0.3886 - val_acc: 0.8270\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 35s 407us/sample - loss: 0.5218 - acc: 0.7505 - val_loss: 0.4615 - val_acc: 0.7790\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4552 - acc: 0.7863 - val_loss: 0.4855 - val_acc: 0.7637\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4335 - acc: 0.7985 - val_loss: 0.4230 - val_acc: 0.8027\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4202 - acc: 0.8070 - val_loss: 0.4097 - val_acc: 0.8098\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4097 - acc: 0.8123 - val_loss: 0.4079 - val_acc: 0.8115\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.4010 - acc: 0.8169 - val_loss: 0.3941 - val_acc: 0.8207\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3948 - acc: 0.8214 - val_loss: 0.3977 - val_acc: 0.8188\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 190us/sample - loss: 0.3875 - acc: 0.8261 - val_loss: 0.4167 - val_acc: 0.8084\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3824 - acc: 0.8276 - val_loss: 0.3876 - val_acc: 0.8243s - loss: 0. - ETA: 0s - loss: 0.3820 \n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3741 - acc: 0.8335 - val_loss: 0.3931 - val_acc: 0.8210\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3712 - acc: 0.8350 - val_loss: 0.3848 - val_acc: 0.8242\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3663 - acc: 0.8372 - val_loss: 0.3919 - val_acc: 0.8249\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3593 - acc: 0.8398 - val_loss: 0.4051 - val_acc: 0.8191\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3542 - acc: 0.8427 - val_loss: 0.3902 - val_acc: 0.8214 loss: 0. - ETA: 2s - loss: 0.3543 - acc: 0. - ETA: 2s - loss: 0.3545 -  - ETA: 1\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3486 - acc: 0.8458 - val_loss: 0.3864 - val_acc: 0.8252\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3429 - acc: 0.8491 - val_loss: 0.3948 - val_acc: 0.8215\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3392 - acc: 0.8510 - val_loss: 0.4000 - val_acc: 0.8227\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3306 - acc: 0.8547 - val_loss: 0.3870 - val_acc: 0.8264\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3282 - acc: 0.8561 - val_loss: 0.4035 - val_acc: 0.8202\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3233 - acc: 0.8596 - val_loss: 0.4031 - val_acc: 0.8191\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3178 - acc: 0.8610 - val_loss: 0.3980 - val_acc: 0.8238\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3136 - acc: 0.8629 - val_loss: 0.3927 - val_acc: 0.8237\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3071 - acc: 0.8661 - val_loss: 0.3998 - val_acc: 0.8229\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3028 - acc: 0.8681 - val_loss: 0.4070 - val_acc: 0.8215\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2961 - acc: 0.8710 - val_loss: 0.4103 - val_acc: 0.8227\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2940 - acc: 0.8713 - val_loss: 0.4243 - val_acc: 0.8180\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2896 - acc: 0.8756 - val_loss: 0.4196 - val_acc: 0.8137 ETA: 1s - lo\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2840 - acc: 0.8773 - val_loss: 0.4256 - val_acc: 0.8181\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2812 - acc: 0.8785 - val_loss: 0.4221 - val_acc: 0.8158\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.2741 - acc: 0.8822 - val_loss: 0.4316 - val_acc: 0.8198\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 35s 410us/sample - loss: 0.5200 - acc: 0.7504 - val_loss: 0.4571 - val_acc: 0.7817\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4523 - acc: 0.7878 - val_loss: 0.4334 - val_acc: 0.7988\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4337 - acc: 0.7992 - val_loss: 0.4266 - val_acc: 0.8016\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4204 - acc: 0.8073 - val_loss: 0.4340 - val_acc: 0.7991\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4096 - acc: 0.8139 - val_loss: 0.4678 - val_acc: 0.7788\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.4010 - acc: 0.8176 - val_loss: 0.4079 - val_acc: 0.8133ETA: 2s - loss: 0.4021 - acc: 0 - E\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3948 - acc: 0.8215 - val_loss: 0.5255 - val_acc: 0.7539\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3890 - acc: 0.8254 - val_loss: 0.4204 - val_acc: 0.8041\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3820 - acc: 0.8287 - val_loss: 0.4153 - val_acc: 0.8109\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3778 - acc: 0.8314 - val_loss: 0.3977 - val_acc: 0.8198\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3722 - acc: 0.8334 - val_loss: 0.3950 - val_acc: 0.8206\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3674 - acc: 0.8352 - val_loss: 0.3951 - val_acc: 0.8184\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3615 - acc: 0.8380 - val_loss: 0.4103 - val_acc: 0.8140\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3575 - acc: 0.8418 - val_loss: 0.4233 - val_acc: 0.8038\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3499 - acc: 0.8451 - val_loss: 0.3888 - val_acc: 0.8266\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3462 - acc: 0.8472 - val_loss: 0.3874 - val_acc: 0.8288\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3406 - acc: 0.8486 - val_loss: 0.3874 - val_acc: 0.8263\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3347 - acc: 0.8524 - val_loss: 0.4160 - val_acc: 0.8125\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3296 - acc: 0.8550 - val_loss: 0.3950 - val_acc: 0.8216\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3257 - acc: 0.8552 - val_loss: 0.3928 - val_acc: 0.8232.32 - ETA: 5s - loss:  - E\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3206 - acc: 0.8593 - val_loss: 0.3912 - val_acc: 0.8266\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3171 - acc: 0.8608 - val_loss: 0.3991 - val_acc: 0.8214\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3109 - acc: 0.8628 - val_loss: 0.3989 - val_acc: 0.823531\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3064 - acc: 0.8679 - val_loss: 0.4106 - val_acc: 0.8229\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3034 - acc: 0.8666 - val_loss: 0.4014 - val_acc: 0.8203\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.2988 - acc: 0.8708 - val_loss: 0.4203 - val_acc: 0.8180\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.2912 - acc: 0.8723 - val_loss: 0.4122 - val_acc: 0.8215\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.2904 - acc: 0.8728 - val_loss: 0.4054 - val_acc: 0.8218\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.2856 - acc: 0.8750 - val_loss: 0.4146 - val_acc: 0.8194\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.2800 - acc: 0.8788 - val_loss: 0.4192 - val_acc: 0.8158\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 36s 426us/sample - loss: 0.5259 - acc: 0.7485 - val_loss: 0.4545 - val_acc: 0.7865\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.4513 - acc: 0.7882 - val_loss: 0.4293 - val_acc: 0.8016\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.4294 - acc: 0.8026 - val_loss: 0.4282 - val_acc: 0.8012\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.4157 - acc: 0.8099 - val_loss: 0.4199 - val_acc: 0.8045\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.4060 - acc: 0.8144 - val_loss: 0.4082 - val_acc: 0.8149\n",
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3991 - acc: 0.8195 - val_loss: 0.3957 - val_acc: 0.8183\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3894 - acc: 0.8248 - val_loss: 0.3947 - val_acc: 0.8213\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.3832 - acc: 0.8282 - val_loss: 0.4306 - val_acc: 0.8023\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3760 - acc: 0.8309 - val_loss: 0.3927 - val_acc: 0.8233\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.3682 - acc: 0.8352 - val_loss: 0.3852 - val_acc: 0.8260\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3637 - acc: 0.8375 - val_loss: 0.4017 - val_acc: 0.8149\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3575 - acc: 0.8411 - val_loss: 0.3846 - val_acc: 0.8246\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3496 - acc: 0.8444 - val_loss: 0.3974 - val_acc: 0.8195\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3445 - acc: 0.8466 - val_loss: 0.3942 - val_acc: 0.8235\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3390 - acc: 0.8505 - val_loss: 0.3845 - val_acc: 0.8263\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3317 - acc: 0.8541 - val_loss: 0.4065 - val_acc: 0.8163\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3249 - acc: 0.8564 - val_loss: 0.3896 - val_acc: 0.8273\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3199 - acc: 0.8589 - val_loss: 0.4187 - val_acc: 0.8128\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3142 - acc: 0.8627 - val_loss: 0.3995 - val_acc: 0.8289\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3088 - acc: 0.8649 - val_loss: 0.4126 - val_acc: 0.8120\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3020 - acc: 0.8685 - val_loss: 0.4163 - val_acc: 0.8165\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2985 - acc: 0.8701 - val_loss: 0.4029 - val_acc: 0.8242 \n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2931 - acc: 0.8721 - val_loss: 0.4139 - val_acc: 0.8220\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2883 - acc: 0.8753 - val_loss: 0.4012 - val_acc: 0.8266\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2834 - acc: 0.8770 - val_loss: 0.4355 - val_acc: 0.8158\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2784 - acc: 0.8789 - val_loss: 0.4125 - val_acc: 0.8219\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2745 - acc: 0.8817 - val_loss: 0.4177 - val_acc: 0.8239\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2707 - acc: 0.8834 - val_loss: 0.4155 - val_acc: 0.8223\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2642 - acc: 0.8864 - val_loss: 0.4462 - val_acc: 0.8184\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.2598 - acc: 0.8889 - val_loss: 0.4281 - val_acc: 0.8256\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      "85000/85000 [==============================] - 38s 445us/sample - loss: 0.5327 - acc: 0.7450 - val_loss: 0.4748 - val_acc: 0.7742\n",
      "Epoch 2/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.4565 - acc: 0.7863 - val_loss: 0.4530 - val_acc: 0.7908\n",
      "Epoch 3/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.4341 - acc: 0.7993 - val_loss: 0.4269 - val_acc: 0.7997\n",
      "Epoch 4/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.4202 - acc: 0.8076 - val_loss: 0.4998 - val_acc: 0.7653\n",
      "Epoch 5/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.4109 - acc: 0.8127 - val_loss: 0.4083 - val_acc: 0.8117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3993 - acc: 0.8195 - val_loss: 0.4099 - val_acc: 0.8093\n",
      "Epoch 7/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3920 - acc: 0.8225 - val_loss: 0.4101 - val_acc: 0.8130\n",
      "Epoch 8/30\n",
      "85000/85000 [==============================] - 18s 207us/sample - loss: 0.3851 - acc: 0.8272 - val_loss: 0.4086 - val_acc: 0.8133\n",
      "Epoch 9/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3778 - acc: 0.8309 - val_loss: 0.4004 - val_acc: 0.8152\n",
      "Epoch 10/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3728 - acc: 0.8317 - val_loss: 0.3942 - val_acc: 0.8205\n",
      "Epoch 11/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3639 - acc: 0.8370 - val_loss: 0.3962 - val_acc: 0.8235\n",
      "Epoch 12/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3595 - acc: 0.8399 - val_loss: 0.3830 - val_acc: 0.8270\n",
      "Epoch 13/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3508 - acc: 0.8440 - val_loss: 0.3879 - val_acc: 0.8220\n",
      "Epoch 14/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3450 - acc: 0.8481 - val_loss: 0.3906 - val_acc: 0.8233\n",
      "Epoch 15/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3382 - acc: 0.8501 - val_loss: 0.4004 - val_acc: 0.8210\n",
      "Epoch 16/30\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.3345 - acc: 0.8518 - val_loss: 0.3843 - val_acc: 0.8293\n",
      "Epoch 17/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3286 - acc: 0.8559 - val_loss: 0.3908 - val_acc: 0.8242\n",
      "Epoch 18/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3197 - acc: 0.8604 - val_loss: 0.3953 - val_acc: 0.8241\n",
      "Epoch 19/30\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.3161 - acc: 0.8620 - val_loss: 0.3923 - val_acc: 0.8253\n",
      "Epoch 20/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3080 - acc: 0.8667 - val_loss: 0.4124 - val_acc: 0.8270\n",
      "Epoch 21/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.3048 - acc: 0.8672 - val_loss: 0.3997 - val_acc: 0.8260\n",
      "Epoch 22/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2970 - acc: 0.8715 - val_loss: 0.4030 - val_acc: 0.8183\n",
      "Epoch 23/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2937 - acc: 0.8721 - val_loss: 0.4109 - val_acc: 0.8244\n",
      "Epoch 24/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2881 - acc: 0.8761 - val_loss: 0.4132 - val_acc: 0.8209\n",
      "Epoch 25/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2827 - acc: 0.8787 - val_loss: 0.4231 - val_acc: 0.8260\n",
      "Epoch 26/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2814 - acc: 0.8783 - val_loss: 0.4176 - val_acc: 0.8233\n",
      "Epoch 27/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2750 - acc: 0.8820 - val_loss: 0.4246 - val_acc: 0.8217\n",
      "Epoch 28/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2699 - acc: 0.8835 - val_loss: 0.4471 - val_acc: 0.8172\n",
      "Epoch 29/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2657 - acc: 0.8854 - val_loss: 0.4482 - val_acc: 0.8166\n",
      "Epoch 30/30\n",
      "85000/85000 [==============================] - 18s 208us/sample - loss: 0.2609 - acc: 0.8877 - val_loss: 0.4319 - val_acc: 0.8205\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [0,1,2]\n",
    "layer_sizes = [15,50,100,150]\n",
    "conv_layers = [0,1,2,3]\n",
    "kernal_size = [(2,2),(3,3),(4,4)]\n",
    "for filter_size in kernal_size:\n",
    "    for dense_layer in dense_layers:\n",
    "        for layer_size in layer_sizes:\n",
    "            for conv_layer in conv_layers:\n",
    "\n",
    "                NAME =\"PMT-MuEl-{}-filter_size-{}-conv-{}-nodes-{}-dense\".format(filter_size,conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "                tensorboard = TensorBoard(log_dir = 'logs\\PMTsmall\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(layer_size,filter_size,strides=1, input_shape= XL.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                for l in range(conv_layer-1):                   \n",
    "                    model.add(Conv2D(layer_size,filter_size,padding='same',activation=\"relu\"))              \n",
    "                    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))            \n",
    "                #model.add(GlobalAveragePooling2D())\n",
    "                model.add(Flatten())\n",
    "                for l in range(dense_layer-1):\n",
    "                    model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))\n",
    "                model.add(Dense(32,activation=\"relu\"))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(2))\n",
    "                model.add(Activation('softmax'))\n",
    "                #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "                model.compile(loss=\"binary_crossentropy\",\n",
    "                             optimizer=\"adam\",\n",
    "                              metrics=['accuracy']\n",
    "                             )   \n",
    "                #filepath=\"LAPPD_Charge_Only_batchnormed_PI_22k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "                #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "                #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "                #model.summary()\n",
    "                history=model.fit(XTraining,YTraining,\n",
    "              validation_data=(XVal,Yval)\n",
    "              ,batch_size=100,\n",
    "                shuffle=True,\n",
    "                class_weight='balanced',\n",
    "                callbacks=[\n",
    "                            #monitor,\n",
    "                            #checkpoint,\n",
    "                            tensorboard \n",
    "                ],\n",
    "              epochs= 30)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bestes Modell 24-PMT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_171\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_294 (Conv2D)          (None, 3, 8, 150)         1350      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_294 (MaxPoolin (None, 2, 4, 150)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_512 (Bat (None, 2, 4, 150)         600       \n",
      "_________________________________________________________________\n",
      "dropout_512 (Dropout)        (None, 2, 4, 150)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_295 (Conv2D)          (None, 2, 4, 150)         90150     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_295 (MaxPoolin (None, 1, 2, 150)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_513 (Bat (None, 1, 2, 150)         600       \n",
      "_________________________________________________________________\n",
      "dropout_513 (Dropout)        (None, 1, 2, 150)         0         \n",
      "_________________________________________________________________\n",
      "flatten_170 (Flatten)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 32)                9632      \n",
      "_________________________________________________________________\n",
      "batch_normalization_514 (Bat (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_514 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 102,526\n",
      "Trainable params: 101,862\n",
      "Non-trainable params: 664\n",
      "_________________________________________________________________\n",
      "Train on 85000 samples, validate on 20000 samples\n",
      "Epoch 1/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.7506\n",
      "Epoch 00001: val_acc improved from -inf to 0.78000, saving model to PMT_24_PID_120k-improvement-val-acc_0.78.model\n",
      "85000/85000 [==============================] - 127s 1ms/sample - loss: 0.5135 - acc: 0.7508 - val_loss: 0.4625 - val_acc: 0.7800\n",
      "Epoch 2/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.7856-\n",
      "Epoch 00002: val_acc improved from 0.78000 to 0.80055, saving model to PMT_24_PID_120k-improvement-val-acc_0.80.model\n",
      "85000/85000 [==============================] - 18s 215us/sample - loss: 0.4552 - acc: 0.7856 - val_loss: 0.4279 - val_acc: 0.8005\n",
      "Epoch 3/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.7975- ETA: 4s - - - ETA: 0s - loss: 0.4362 - acc: - ETA: 0s - loss: 0.4363 - acc: 0.7975\n",
      "Epoch 00003: val_acc did not improve from 0.80055\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.4364 - acc: 0.7974 - val_loss: 0.4444 - val_acc: 0.7936\n",
      "Epoch 4/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8027\n",
      "Epoch 00004: val_acc improved from 0.80055 to 0.81240, saving model to PMT_24_PID_120k-improvement-val-acc_0.81.model\n",
      "85000/85000 [==============================] - 17s 203us/sample - loss: 0.4265 - acc: 0.8027 - val_loss: 0.4092 - val_acc: 0.8124\n",
      "Epoch 5/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.8085\n",
      "Epoch 00005: val_acc improved from 0.81240 to 0.81405, saving model to PMT_24_PID_120k-improvement-val-acc_0.81.model\n",
      "85000/85000 [==============================] - 18s 209us/sample - loss: 0.4176 - acc: 0.8084 - val_loss: 0.4048 - val_acc: 0.8141\n",
      "Epoch 6/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8100\n",
      "Epoch 00006: val_acc did not improve from 0.81405\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4138 - acc: 0.8100 - val_loss: 0.4069 - val_acc: 0.8133\n",
      "Epoch 7/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8133\n",
      "Epoch 00007: val_acc improved from 0.81405 to 0.82055, saving model to PMT_24_PID_120k-improvement-val-acc_0.82.model\n",
      "85000/85000 [==============================] - 17s 205us/sample - loss: 0.4098 - acc: 0.8133 - val_loss: 0.3958 - val_acc: 0.8206\n",
      "Epoch 8/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8157- ETA: 5s - loss: 0 - ETA: 2s - - ETA: 0s - loss: 0.4048 - a\n",
      "Epoch 00008: val_acc did not improve from 0.82055\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.4044 - acc: 0.8157 - val_loss: 0.4008 - val_acc: 0.8174\n",
      "Epoch 9/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8185\n",
      "Epoch 00009: val_acc improved from 0.82055 to 0.82810, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 17s 206us/sample - loss: 0.4014 - acc: 0.8185 - val_loss: 0.3860 - val_acc: 0.8281\n",
      "Epoch 10/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8197\n",
      "Epoch 00010: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.3984 - acc: 0.8197 - val_loss: 0.3885 - val_acc: 0.8239\n",
      "Epoch 11/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8203- ETA: 1s - los\n",
      "Epoch 00011: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 204us/sample - loss: 0.3948 - acc: 0.8202 - val_loss: 0.3882 - val_acc: 0.8241\n",
      "Epoch 12/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8202\n",
      "Epoch 00012: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3954 - acc: 0.8203 - val_loss: 0.3837 - val_acc: 0.8271\n",
      "Epoch 13/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8245\n",
      "Epoch 00013: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3894 - acc: 0.8243 - val_loss: 0.3820 - val_acc: 0.8260\n",
      "Epoch 14/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8247- ET - ETA: 1s - loss: 0.3899 - acc:  - ETA: 1s - l\n",
      "Epoch 00014: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3894 - acc: 0.8246 - val_loss: 0.3817 - val_acc: 0.8266\n",
      "Epoch 15/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8253\n",
      "Epoch 00015: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.3872 - acc: 0.8253 - val_loss: 0.3832 - val_acc: 0.8250\n",
      "Epoch 16/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8264\n",
      "Epoch 00016: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 202us/sample - loss: 0.3853 - acc: 0.8263 - val_loss: 0.3786 - val_acc: 0.8279\n",
      "Epoch 17/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8274\n",
      "Epoch 00017: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 192us/sample - loss: 0.3827 - acc: 0.8273 - val_loss: 0.3925 - val_acc: 0.8178\n",
      "Epoch 18/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3822 - acc: 0.8282\n",
      "Epoch 00018: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 202us/sample - loss: 0.3825 - acc: 0.8281 - val_loss: 0.3876 - val_acc: 0.8245\n",
      "Epoch 19/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8287\n",
      "Epoch 00019: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.3799 - acc: 0.8287 - val_loss: 0.3869 - val_acc: 0.8266\n",
      "Epoch 20/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8296\n",
      "Epoch 00020: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3783 - acc: 0.8296 - val_loss: 0.3865 - val_acc: 0.8255\n",
      "Epoch 21/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8317\n",
      "Epoch 00021: val_acc did not improve from 0.82810\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3757 - acc: 0.8318 - val_loss: 0.3858 - val_acc: 0.8250\n",
      "Epoch 22/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8326\n",
      "Epoch 00022: val_acc improved from 0.82810 to 0.82895, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3743 - acc: 0.8325 - val_loss: 0.3773 - val_acc: 0.8289\n",
      "Epoch 23/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3740 - acc: 0.8318\n",
      "Epoch 00023: val_acc improved from 0.82895 to 0.83020, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.3740 - acc: 0.8318 - val_loss: 0.3758 - val_acc: 0.8302\n",
      "Epoch 24/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8325\n",
      "Epoch 00024: val_acc improved from 0.83020 to 0.83195, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3731 - acc: 0.8325 - val_loss: 0.3710 - val_acc: 0.8320\n",
      "Epoch 25/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8336\n",
      "Epoch 00025: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3707 - acc: 0.8336 - val_loss: 0.3790 - val_acc: 0.8270\n",
      "Epoch 26/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8344\n",
      "Epoch 00026: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 16s 187us/sample - loss: 0.3689 - acc: 0.8344 - val_loss: 0.3791 - val_acc: 0.8266\n",
      "Epoch 27/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8345\n",
      "Epoch 00027: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3681 - acc: 0.8345 - val_loss: 0.3749 - val_acc: 0.8292\n",
      "Epoch 28/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8360\n",
      "Epoch 00028: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 16s 190us/sample - loss: 0.3674 - acc: 0.8361 - val_loss: 0.3731 - val_acc: 0.8316\n",
      "Epoch 29/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8360\n",
      "Epoch 00029: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3672 - acc: 0.8360 - val_loss: 0.3849 - val_acc: 0.8251\n",
      "Epoch 30/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8370- ETA: 0s - loss: 0.3644 \n",
      "Epoch 00030: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3647 - acc: 0.8370 - val_loss: 0.3706 - val_acc: 0.8306\n",
      "Epoch 31/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8372\n",
      "Epoch 00031: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3622 - acc: 0.8372 - val_loss: 0.3804 - val_acc: 0.8275\n",
      "Epoch 32/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8385\n",
      "Epoch 00032: val_acc did not improve from 0.83195\n",
      "85000/85000 [==============================] - 18s 211us/sample - loss: 0.3630 - acc: 0.8385 - val_loss: 0.3877 - val_acc: 0.8263\n",
      "Epoch 33/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8364\n",
      "Epoch 00033: val_acc improved from 0.83195 to 0.83340, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 28s 329us/sample - loss: 0.3643 - acc: 0.8364 - val_loss: 0.3673 - val_acc: 0.8334\n",
      "Epoch 34/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.8388- ETA: 1s - loss: 0.360\n",
      "Epoch 00034: val_acc improved from 0.83340 to 0.83415, saving model to PMT_24_PID_120k-improvement-val-acc_0.83.model\n",
      "85000/85000 [==============================] - 30s 353us/sample - loss: 0.3604 - acc: 0.8389 - val_loss: 0.3685 - val_acc: 0.8342\n",
      "Epoch 35/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8374- ETA: 1s - loss: \n",
      "Epoch 00035: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 21s 252us/sample - loss: 0.3621 - acc: 0.8375 - val_loss: 0.3753 - val_acc: 0.8315\n",
      "Epoch 36/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8378\n",
      "Epoch 00036: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.3612 - acc: 0.8378 - val_loss: 0.3702 - val_acc: 0.8305\n",
      "Epoch 37/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8392\n",
      "Epoch 00037: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3601 - acc: 0.8392 - val_loss: 0.3819 - val_acc: 0.8249\n",
      "Epoch 38/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8406\n",
      "Epoch 00038: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3582 - acc: 0.8406 - val_loss: 0.3689 - val_acc: 0.8309\n",
      "Epoch 39/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8392\n",
      "Epoch 00039: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 190us/sample - loss: 0.3583 - acc: 0.8392 - val_loss: 0.3746 - val_acc: 0.8302\n",
      "Epoch 40/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8424\n",
      "Epoch 00040: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3564 - acc: 0.8424 - val_loss: 0.3721 - val_acc: 0.8320\n",
      "Epoch 41/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8412\n",
      "Epoch 00041: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3565 - acc: 0.8411 - val_loss: 0.3711 - val_acc: 0.8314\n",
      "Epoch 42/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8415\n",
      "Epoch 00042: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 203us/sample - loss: 0.3556 - acc: 0.8416 - val_loss: 0.3720 - val_acc: 0.8291\n",
      "Epoch 43/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8418\n",
      "Epoch 00043: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 200us/sample - loss: 0.3551 - acc: 0.8418 - val_loss: 0.3705 - val_acc: 0.8310\n",
      "Epoch 44/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8424- ETA: 0s - loss: 0.3539 -\n",
      "Epoch 00044: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 22s 259us/sample - loss: 0.3541 - acc: 0.8425 - val_loss: 0.3715 - val_acc: 0.8325\n",
      "Epoch 45/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8430- ETA: 0s - loss: 0.3512 - acc:  - ETA: 0s - loss: 0.3510 - acc: 0.84 - ETA: 0s - loss: 0.3512 - acc:\n",
      "Epoch 00045: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3515 - acc: 0.8430 - val_loss: 0.3693 - val_acc: 0.8331\n",
      "Epoch 46/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8436- ETA: 0s - loss: 0.3516\n",
      "Epoch 00046: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 194us/sample - loss: 0.3521 - acc: 0.8436 - val_loss: 0.3730 - val_acc: 0.8317\n",
      "Epoch 47/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8441\n",
      "Epoch 00047: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3510 - acc: 0.8440 - val_loss: 0.3702 - val_acc: 0.8321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8443\n",
      "Epoch 00048: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 16s 185us/sample - loss: 0.3522 - acc: 0.8443 - val_loss: 0.3820 - val_acc: 0.8255\n",
      "Epoch 49/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8431- ETA: 0s - loss: 0.3516 - acc: 0.8\n",
      "Epoch 00049: val_acc did not improve from 0.83415\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3513 - acc: 0.8430 - val_loss: 0.3698 - val_acc: 0.8325\n",
      "Epoch 50/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8442\n",
      "Epoch 00050: val_acc improved from 0.83415 to 0.83505, saving model to PMT_24_PID_120k-improvement-val-acc_0.84.model\n",
      "85000/85000 [==============================] - 16s 191us/sample - loss: 0.3507 - acc: 0.8441 - val_loss: 0.3676 - val_acc: 0.8350\n",
      "Epoch 51/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8455\n",
      "Epoch 00051: val_acc did not improve from 0.83505\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3495 - acc: 0.8454 - val_loss: 0.3771 - val_acc: 0.8270\n",
      "Epoch 52/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8445\n",
      "Epoch 00052: val_acc did not improve from 0.83505\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3493 - acc: 0.8444 - val_loss: 0.3675 - val_acc: 0.8339\n",
      "Epoch 53/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8456- ETA: 0s - loss: 0.3487 -\n",
      "Epoch 00053: val_acc did not improve from 0.83505\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3487 - acc: 0.8455 - val_loss: 0.3704 - val_acc: 0.8317\n",
      "Epoch 54/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8452\n",
      "Epoch 00054: val_acc improved from 0.83505 to 0.83615, saving model to PMT_24_PID_120k-improvement-val-acc_0.84.model\n",
      "85000/85000 [==============================] - 16s 194us/sample - loss: 0.3493 - acc: 0.8452 - val_loss: 0.3675 - val_acc: 0.8361\n",
      "Epoch 55/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8460\n",
      "Epoch 00055: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 193us/sample - loss: 0.3485 - acc: 0.8460 - val_loss: 0.3707 - val_acc: 0.8336\n",
      "Epoch 56/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8467- ETA: 0s - loss:\n",
      "Epoch 00056: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3470 - acc: 0.8467 - val_loss: 0.3681 - val_acc: 0.8340\n",
      "Epoch 57/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8458\n",
      "Epoch 00057: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3485 - acc: 0.8459 - val_loss: 0.3730 - val_acc: 0.8277\n",
      "Epoch 58/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8472\n",
      "Epoch 00058: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 195us/sample - loss: 0.3468 - acc: 0.8471 - val_loss: 0.3690 - val_acc: 0.8320\n",
      "Epoch 59/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.8458\n",
      "Epoch 00059: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3461 - acc: 0.8459 - val_loss: 0.3731 - val_acc: 0.8303\n",
      "Epoch 60/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8459- ETA: 4\n",
      "Epoch 00060: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3448 - acc: 0.8458 - val_loss: 0.3758 - val_acc: 0.8289\n",
      "Epoch 61/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8487\n",
      "Epoch 00061: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3441 - acc: 0.8486 - val_loss: 0.3788 - val_acc: 0.8312\n",
      "Epoch 62/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8467- ETA\n",
      "Epoch 00062: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 22s 253us/sample - loss: 0.3443 - acc: 0.8466 - val_loss: 0.3725 - val_acc: 0.8344\n",
      "Epoch 63/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8454\n",
      "Epoch 00063: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 19s 218us/sample - loss: 0.3465 - acc: 0.8454 - val_loss: 0.3743 - val_acc: 0.8296\n",
      "Epoch 64/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8459\n",
      "Epoch 00064: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 201us/sample - loss: 0.3449 - acc: 0.8459 - val_loss: 0.3831 - val_acc: 0.8268\n",
      "Epoch 65/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8477\n",
      "Epoch 00065: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 196us/sample - loss: 0.3421 - acc: 0.8476 - val_loss: 0.3684 - val_acc: 0.8330\n",
      "Epoch 66/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8479- ETA: 6s -\n",
      "Epoch 00066: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3431 - acc: 0.8479 - val_loss: 0.3728 - val_acc: 0.8332\n",
      "Epoch 67/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8484\n",
      "Epoch 00067: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3420 - acc: 0.8484 - val_loss: 0.3723 - val_acc: 0.8329\n",
      "Epoch 68/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8479- ETA: 2 - ETA: 0s - loss: 0.3405 - acc\n",
      "Epoch 00068: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3409 - acc: 0.8479 - val_loss: 0.3763 - val_acc: 0.8318\n",
      "Epoch 69/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8468- ETA: 0s - loss: 0.3435 - acc:\n",
      "Epoch 00069: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3436 - acc: 0.8467 - val_loss: 0.3845 - val_acc: 0.8258\n",
      "Epoch 70/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.8489\n",
      "Epoch 00070: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 186us/sample - loss: 0.3405 - acc: 0.8488 - val_loss: 0.3739 - val_acc: 0.8324\n",
      "Epoch 71/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8491- ETA: 3s - ETA: 1s - loss: 0.3406 - acc:  - ETA: 1s - loss: 0.34\n",
      "Epoch 00071: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3410 - acc: 0.8491 - val_loss: 0.3724 - val_acc: 0.8317\n",
      "Epoch 72/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8487\n",
      "Epoch 00072: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3406 - acc: 0.8488 - val_loss: 0.3695 - val_acc: 0.8321\n",
      "Epoch 73/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.8486\n",
      "Epoch 00073: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 198us/sample - loss: 0.3396 - acc: 0.8488 - val_loss: 0.3690 - val_acc: 0.8332\n",
      "Epoch 74/80\n",
      "84800/85000 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8495\n",
      "Epoch 00074: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3381 - acc: 0.8495 - val_loss: 0.3753 - val_acc: 0.8273\n",
      "Epoch 75/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8494\n",
      "Epoch 00075: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3391 - acc: 0.8495 - val_loss: 0.3722 - val_acc: 0.8352\n",
      "Epoch 76/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8496\n",
      "Epoch 00076: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3402 - acc: 0.8495 - val_loss: 0.3753 - val_acc: 0.8285\n",
      "Epoch 77/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8503\n",
      "Epoch 00077: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 189us/sample - loss: 0.3383 - acc: 0.8503 - val_loss: 0.3726 - val_acc: 0.8321\n",
      "Epoch 78/80\n",
      "84900/85000 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8502\n",
      "Epoch 00078: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 197us/sample - loss: 0.3385 - acc: 0.8501 - val_loss: 0.3700 - val_acc: 0.8335\n",
      "Epoch 79/80\n",
      "84700/85000 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8508\n",
      "Epoch 00079: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 16s 188us/sample - loss: 0.3372 - acc: 0.8506 - val_loss: 0.3792 - val_acc: 0.8289\n",
      "Epoch 80/80\n",
      "84600/85000 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8505\n",
      "Epoch 00080: val_acc did not improve from 0.83615\n",
      "85000/85000 [==============================] - 17s 199us/sample - loss: 0.3385 - acc: 0.8503 - val_loss: 0.3706 - val_acc: 0.8344\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ende des Versuchs: \n"
     ]
    }
   ],
   "source": [
    "dense_layers = [1]\n",
    "layer_sizes = [150]\n",
    "conv_layers = [2]\n",
    "kernal_size = [(2,2)]\n",
    "for filter_size in kernal_size:\n",
    "    for dense_layer in dense_layers:\n",
    "        for layer_size in layer_sizes:\n",
    "            for conv_layer in conv_layers:\n",
    "\n",
    "                NAME =\"PMT-MuEl-{}-filter_size-{}-conv-{}-nodes-{}-dense\".format(filter_size,conv_layer, layer_size, dense_layer) #,int(time.time())\n",
    "                tensorboard = TensorBoard(log_dir = 'logs\\PMTsmall\\{}'.format(NAME))\n",
    "\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(layer_size,filter_size,strides=1, input_shape= XL.shape[1:],activation=\"relu\", padding='same'))                                               \n",
    "                model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                for l in range(conv_layer-1):                   \n",
    "                    model.add(Conv2D(layer_size,filter_size,padding='same',activation=\"relu\"))              \n",
    "                    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))            \n",
    "                #model.add(GlobalAveragePooling2D())\n",
    "                model.add(Flatten())\n",
    "                for l in range(dense_layer-1):\n",
    "                    model.add(Dense(512-l*20 ,activation=\"relu\" ))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(Dropout(0.2))\n",
    "                model.add(Dense(32,activation=\"relu\"))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(2))\n",
    "                model.add(Activation('softmax'))\n",
    "                #adam = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True, epsilon = 0.001)\n",
    "                model.compile(loss=\"binary_crossentropy\",\n",
    "                             optimizer=\"adam\",\n",
    "                              metrics=['accuracy']\n",
    "                             )   \n",
    "                filepath=\"PMT_24_PID_120k-improvement-val-acc_{val_acc:.2f}.model\"  \n",
    "                checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "                #monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=False)\n",
    "                model.summary()\n",
    "                history=model.fit(XTraining,YTraining,\n",
    "              validation_data=(XVal,Yval)\n",
    "              ,batch_size=100,\n",
    "                shuffle=True,\n",
    "                class_weight='balanced',\n",
    "                callbacks=[\n",
    "                            #monitor,\n",
    "                            checkpoint,\n",
    "                            #tensorboard \n",
    "                ],\n",
    "              epochs= 80)\n",
    "                print(history.history.keys())\n",
    "                # summarize history for accuracy\n",
    "                plt.plot(history.history['acc'])\n",
    "                plt.plot(history.history['val_acc'])\n",
    "                plt.title('model accuracy')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'test'], loc='upper left')\n",
    "                plt.show()\n",
    "                # summarize history for loss\n",
    "                plt.plot(history.history['loss'])\n",
    "                plt.plot(history.history['val_loss'])\n",
    "                plt.title('model loss')\n",
    "                plt.ylabel('loss')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'test'], loc='upper left')\n",
    "                plt.show()\n",
    "\n",
    "                print(\"Ende des Versuchs: \")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.3565329247734937\n",
      "Test accuracy:  0.83852047\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "\n",
    "cd \"Documents\\Python\\CNN_Masterarbeit\"\n",
    "\n",
    "tensorboard --logdir=logs/ --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/Overfitting_Studie --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/Modell_Studie --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=logs/MuonElectron --host localhost --port 8088\n",
    "\n",
    "tensorboard --logdir=BeamlikePI/logs/Time --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15005, 10, 16, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/X_Test_LAPPD(1x1)_120k.pickle\",\"wb\")\n",
    "pickle.dump(XTest,pickle_out,protocol=4)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"C:/Users/Deep Thought/Documents/Python/CNN_Masterarbeit/BeamlikePI/pickle/Y_Test_LAPPD(1x1)_120k.pickle\",\"wb\")\n",
    "pickle.dump(YTest,pickle_out,protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0521 13:34:22.750493  8792 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0521 13:34:22.766009  8792 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0521 13:34:22.781656  8792 deprecation.py:506] From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"PMT_24_PID_120k-improvement-val-acc_0.84.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAPPD\n",
    "model = tf.keras.models.load_model(\"LAPPD(1x1)_PID_120k-improvement-val-acc_0.83.model\")\n",
    "\n",
    "#Combined\n",
    "#model = tf.keras.models.load_model(\"PMTOnly_Combined_PI_120k-60epoch_-improvement-val-acc_0.93.model\")\n",
    "#model = tf.keras.models.load_model(\"PMTOnly_Combined_PI_22k-80epoch-improvement-val-acc_0.92.model\")\n",
    "#Time\n",
    "#model = tf.keras.models.load_model(\"PMT_Time_Only_batchnormed_PI_22k-improvement-val-acc_0.81.model\")\n",
    "#Charge\n",
    "#model = tf.keras.models.load_model(\"PMT_Charge_Only_batchnormed_PI_22k-improvement-val-acc_0.93.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120005"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTestC = X[:,:,:,0].reshape(120005,10,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120005, 10, 16, 1) (120005, 2)\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(XTestC.shape,Y.shape)\n",
    "for sample in Y[:20]:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cbf7094208>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(XTest[7,:,:,0], cmap='binary', interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.36467623370204594\n",
      "Test accuracy:  0.8353882\n"
     ]
    }
   ],
   "source": [
    "### LAPPD\n",
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.35680383114329917\n",
      "Test accuracy:  0.90992105\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTest, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.23812034977390828\n",
      "Test accuracy:  0.90469563\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestC, Y, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.19774321766031458\n",
      "Test accuracy:  0.92719644\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestC, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.4832646434304745\n",
      "Test accuracy:  0.8005923\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XTestT, YTest, verbose=False) \n",
    "model.metrics_names\n",
    "print('Test score: ', score[0])    #Loss on test\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5818 1629]\n",
      " [ 841 6717]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#prediction = model.predict(XTestC)\n",
    "#print(prediction.shape,YTest.shape)\n",
    "rounded_labels =np.argmax(YTest, axis=1)\n",
    "y_prob = np.array(model.predict(XTest, batch_size=128, verbose=0))\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "cm = confusion_matrix(rounded_labels, y_classes)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    " \n",
    "    print(cm)\n",
    " \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    " \n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAPPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7812542  0.2187458 ]\n",
      " [0.11127282 0.88872718]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "cm = cm.reshape((2,2))\n",
    " \n",
    "class_names = [r\"$e^{-}$\", \"muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n CNN-model with 84% accuracy \\n Pure LAPPD')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL PMTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90041109 0.09958891]\n",
      " [0.03416399 0.96583601]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "cm = cm.reshape((2,2))\n",
    " \n",
    "class_names = [r\"$e^{-}$\", \"muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n CNN-model with 93% accuracy \\n Pure PMT')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 PMTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2=[[0.79498861,0.20501139],\n",
    " [0.11230443,0.88769557]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79498861 0.20501139]\n",
      " [0.11230443 0.88769557]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape into 2 x 2 matrix\n",
    "#cm = cm.reshape((2,2))\n",
    "cm2=np.array(cm2)\n",
    "class_names = [r\"$e^{-}$\", \"$muon\"]\n",
    " \n",
    "    \n",
    "# Plot normalized confusion matrix\n",
    "f=plt.figure()\n",
    "plot_confusion_matrix(cm2, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix \\n CNN-model with 84% accuracy \\n 24 PMTs')\n",
    "#f.savefig(\"Confusion-CNN-85-Prozent-MultiChannel-2-conv-130-nodes-2-dense.pdf\",format =\"pdf\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import pydot_ng as pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, 'my_first_model_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_12 (5, 5, 2, 130)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 130 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summarize filter shapes\n",
    "pyplot.subplots_adjust(wspace = 0.05 ,hspace = 0.05)\n",
    "for layer in model.layers:\n",
    "    #check for conv. layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    #get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)\n",
    "    f_min, f_max= filters.min(),filters.max()\n",
    "    filters = (filters-f_min)/(f_max-f_min)\n",
    "    # plot first few filters\n",
    "    \n",
    "\n",
    "    n_filters, ix = 130, 1\n",
    "\n",
    "\n",
    "    for i in range(n_filters):\n",
    "        #get the filter\n",
    "        f = filters[:, :, :, i]\n",
    "        # plot each channel separately\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        ax = pyplot.subplot(n_filters, 4, ix)\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:,:, 1], cmap='viridis')\n",
    "        ix += 1\n",
    "    # show the figure\n",
    "    #pyplot.savefig(\"PMT Model 85% layer0 ALL ConvFilters-Time.jpg\",format =\"jpg\", bbox_inches='tight')\n",
    "    pyplot.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6.4, 30*4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv2d_12 (?, 10, 16, 130)\n",
      "3 conv2d_13 (?, 5, 8, 130)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    print(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c9023abe10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x10368 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[9,:,:,0], cmap='viridis', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6.4, 10*4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14_input (InputLayer) [(None, 10, 16, 2)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 10, 16, 130)       6630      \n",
      "=================================================================\n",
      "Total params: 6,630\n",
      "Trainable params: 6,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 10, 16, 130)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 52, not 53",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-f9c6a5616c25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# specify subplot and turn of axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     raise ValueError(\n\u001b[1;32m---> 59\u001b[1;33m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[0;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[0;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[1;31mValueError\u001b[0m: num must be 1 <= num <= 52, not 53"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAApDCAYAAACWTfnhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzde5Re10Hf/XNmRtJYY8uWZMW2fJGvqhNfosQXXUJLW+VFJC2strShhFJaeKEprJcGuqAJUJK0JSlhBVK6AjQphFACpCiFcmkiXoustIkl+ZIojpMYOb7IF1mKZMuWPLJmNDPn/eNtSqF+zn6eeZ75ze3z+Xfv2WdHM8/XZ032OVM3TVMBMLeG5nsDAMuB2AIEiC1AgNgCBIgtQIDYAgSM9DJ5Zb2qGa3GOo5fcvNLxTWOPXhe+4S6Lq4xec1o6/jKR8v7WE6m13X+nlVVVY28NF1co3npbMexs9V4NdlMlL9xS0TpczBxTeFnvKqqVY+1/4xe8KqZ4hqnv+xeaaE5XZ080TTNhpcb6ym2o9VYtbXe2XH8h//rV4pr/Pz1r2wdr1esLK7x6Lvb17jmO75QXGM5OfXN21rHL/rS88U1Zh54qOPYgWZvz3tazEqfg0ffvaW4xrVvPtg6/o0fK98wfPrWctTJuqvZfbjTmP80AgSILUCA2AIEiC1AgNgCBPR0GqHkF974N7uY9UjraHNusrjCxl8vn1jgz6z57QOt44/+6/bTClVVVdd8ueVHZarXHS1uE1eOVQ//aOd/sxvevL/vazhpsPS4swUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAgIE+1DB18fnFOfWh/q+z6hP39r/IclL4c/WTl54rLlHfvLnz2EOf7nlLi9mqJ8erG/5Z/w8usLy4swUIEFuAALEFCBBbgACxBQgQW4AAsQUIGOg52/puf0J8MbrkU+Ufg6auW8YGuZvFb8+R9j9TXlVVtWtj+c+d82fqFe1/MKCbPzow39zZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUDAQB9qYHG68KPlF2E/8892dBw799TwILez6HlgYfBO/63XtI4/f335Z/CK99w9qO3MijtbgACxBQgQW4AAsQUIEFuAALEFCBBbgICBnrN9+ANbi3Nu+MEDg7zksnfyu7cX55y4faZ1/Ib/p/w9ueJjj3Qce+LkRPHroR/n/077z+j5oX30w50tQIDYAgSILUCA2AIEiC1AgNgCBIgtQIDYAgQM9KEGDywM3vData3jaz+yr7jG2o+0j3/1/dvKG2k6D51936ry18My584WIEBsAQLEFiBAbAECxBYgQGwBAsQWIGCg52yb7a8uzvnjj7cf+ty1cUvf+zj0H+4oztn8T+7t+zoJ0ydPto4/+t7yy8Ov/bH2s7jXv3V/T3v6i55txvv6+sWmXrmiGtl4ZcfxqcNP9n2Nx3+6/H29+ifKZ6xZONzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUDAQB9qqPd9oTin+NDC0HBxjf/8xGdax990RXGJoq/+xmtax6//B5/v/yLduPOW1uEbPnSsuMR0YbzZUX4Y5aL3PtVxbPh7yt+zpaSZPDeQBxfaDOKBhWe/r/xgxPoPeTAixZ0tQIDYAgSILUCA2AIEiC1AgNgCBIgtQMBAz9l2o/6Ty1vHh759srjGm65oPz/4wj/YVlzjwt9of2H2IM7RfvXn2/dx8efr4hoX/Xr7OcjSGdpu1HeXz0fff1/n/y1nzowOYBcMmjO0C4s7W4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIqJum6X5yXR+vqurw3G2HRWpT0zQb5nsTKT4HtOj4WegptgDMjl8jAASILUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIEjPQyeWW9qhmtxjqOT27sPPa/1jgy3sslX9b0+vbrTI2V39E7emSydbyZmmodr1euLF6jOneufXyk/M8/s3pF+z5eOFPexxw7W41Xk81EPd/7SCl9Drqx+db279uhB1aXFyn9iwdeVV2fN1qc07x0du43skCcrk6e6PTy8J5iO1qNVVvrnR3HD//T7cU1Nv3Uvl4u+bJOfkv7dY5vmy6u8cp3Pt46Pn3sa63jI5dfVbzGzNH2NYYuXl9c48XXXN46PvoH9xTXmGsHmr3zvYWo0uegG3v2HGwd37VxS3GNuvAf69INwyAM3fiq4pyZg1+e830sFHc1uzv+BQ+/RgAIEFuAALEFCBBbgACxBQjo6TRCyQWPl+fUt93UOt7c/6XiGms/0n6iYe1Hyvson1doN/X4E32uUFUzTz1dnHPeseOt44M43TOy6crinKnDTw7gSnxdN6cNShKnDUqW00mDfrmzBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4CAgT7UsO5Xy69P/NMP39Y6vvkfD2o3S8Nz39n+7zU8WX6sYc1v7m+fMD1TXGN47dqOY/ULw8Wvh6Vg5Nqr2yc80nnInS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AwEDP2XbjsV2/0jq+q+r/pcpLydpfaz+7PHLpJcU1Sq+YnuriJeZtmqbfV7HD4jD16OOz/lp3tgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQEH+oYddGDy0M1HmjkcsMbXlVx7H6oc9G9gDz7cW/t7V9wn/e3XHInS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AQPycLYM19djh4py2M7JVVVUzB79cXKNtTtOcLX49zLuh4dbh8b99e3GJ83/nwOwvP+uvBKBrYgsQILYAAWILECC2AAFiCxAgtgABYgsQ4KGGZaCbhxZgqRu+blPr+NjHZ//AQjfc2QIEiC1AgNgCBIgtQIDYAgSILUCA2AIExM/Zjlx2aev41DNHQztZHOpVq1rHm4mJ0E5IOvZDO4pzLvmFuwM7WTqmH350Xq/vzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAuIPNXhooTcL5qGFoeHOY9O5bSwXg3hgYeqv31acM/In9/d9nYVgaPXq4pyZM2cCO+nMnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AQPycLYvUjMO0i81SOUPbjdQZ2qGbb2yf8MWWrx3sVgB4OWILECC2AAFiCxAgtgABYgsQILYAAWILEFA3TdP95Lo+XlXV4bnbDovUpqZpNsz3JlJ8DmjR8bPQU2wBmB2/RgAIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBjpZfLKelUzWo3N1V5YpM5W49VkM1HP9z5SfA6Wpnp0VXFOc3aidfx0dfJEp5eH9xTb0Wqs2lrv7OVLWAYONHvnewtRPgdL0/B1m4tzpr98qHX8rmZ3x7/g4dcIAAFiCxAgtgABYgsQILYAAT2dRmBpevxjtxbnfOSOD3cc+55vHR/kdgh66sd3tI5f8e67Qzvpz2O/Xf4ZvubvP9A6XjppUFVVNbxmTfuEFzoPubMFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgIC6aZquJ6+p1zVeLbewPPt924tzNtzbctK6qqr66eN97WHfc7urF859bdm8z9bnYGmqV6wszmnOTbaO39Xsvr9pmttfbsydLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCAl4cvcus/tK84Z6YwPv5tW4tr1C3Hsaf3rip+/XJy6IN3FOe88v3tZ5+7eZE1f+aZH2l/CXpVVdVlP9f+IvTSGdqqqqrD7ypc56d2dxxyZwsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAR5qWOCe+efth6gve1/7Qe1ujH38QHHO0K03dhwbnpjuew9Lyebvv7c4p/Qvdt6nLymu8dI3HutyR0tf6YGFqqqqobGx1vETb7q1uMamd7Rf5+G26xdXB6BvYgsQILYAAWILECC2AAFiCxAgtgABC+6c7RPvLL8E+Kp39n+2dLEYxDnaQZh54KGOY01zNriThW9m75XFOUM7n2wd7+YM7Z4jB1vHd23cUlxjOZkZH28dX/fh8ov4++HOFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECenqooV4xUo1c3PmlxlNH+3+Z8SAeWBi5fGNxzrE3bmodX/+huT3gPDBDw8Upe566v3X8ja9/U3GN49vWdxyb/q/7i1+/nJQeWOjG8/9we3HOrvKPOQuIO1uAALEFCBBbgACxBQgQW4AAsQUIEFuAgJ7O2TbnpgZylrbN0K03Fuc89a72/0aceXFVcY3rv2txnKMd2vKq1vE//KPfKK7xTX/ve1rH6y+3v4S6qqpq3Zc7jw037S9l5v/0/YcebR3/4ObQRohxZwsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAT091JDw8NtGi3M2/3j7Ifoz14wNajvzbuZgy9MEVVW98fLXFteoq/JDC2R9cPO1870FwtzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgTEz9k+8Y4dreMXXnCiuMa5V5zfOr7qv93b055mY+jVryzOmfnCV/q+zqk3b2sdf/aWurjGNW/v/0Xpj71ne8exyX+/v+/16V29qv0l+c3ERGgndMOdLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIE1E3TdD+5ro9XVXV47rbDIrWpaZoN872JFJ8DWnT8LPQUWwBmx68RAALEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECRnqZvLJe1YxWY3O1l4GZXlfe49T57e/xXfXEmb73Ua9o/+dtzk31fY2unH9e+/iLL/W1/NlqvJpsJuq+FllESp+DiatWF9cYxM8XC8/p6uSJTi8P7ym2o9VYtbXeOZhdzaEX3ritOOfYX55pHd/8lnv63sfIhktbx6eeOdr3NbrRbNnSOl5/9mBf6x9o9vb19YtN6XNw6MfvLK6x+Z/2//PFwnNXs7vjX/DwawSAALEFCBBbgACxBQgQW4CAnk4jLBbjG8v/Ddn8lv1zvo/UaYOSfk8b0BsnDXg57mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWICD+UMOZv721dXz17x7o+xobf/buvtcAGCR3tgABYgsQILYAAWILECC2AAFiCxAgtgAB8XO2gzhHy2DtOVJ+ufiuje1/Dh1o584WIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLiDzVcsm9N6/ix7adCO5l/9W03Fec093+pdXz8k9cW1xh7w2Ot4x5YgO6MXLOpfcKjnYfc2QIEiC1AgNgCBIgtQIDYAgSILUCA2AIExM/Zls7Rzuy9srjG0M4nB7WdeVU6Q9uNsW9uOdjHvDj3TbcX56z44/sCO2HQph47POuvdWcLECC2AAFiCxAgtgABYgsQILYAAWILECC2AAHxhxpKlsoDCyxfqQcWhi64oHV85vTpyD6Wk6HVq9snjLd87WC3AsDLEVuAALEFCBBbgACxBQgQW4AAsQUIWHDnbEcuvaQ4Z+roscBOYO7sOXKwdXzXxi3FNUrnaA998I7iGpu//97iHP7MzJkzs/5ad7YAAWILECC2AAFiCxAgtgABYgsQILYAAWILELDgHmpIPbAwvHZt6/j0yZORfSwEQzffWJwz8+BDgZ0sH908tFBy/Pf/Uuv45m/t/4GFk9+9vXV87Uf29X2NQXjuH7fvs6qqat2H2/dar1hZXGNozfntE060fG1xdQD6JrYAAWILECC2AAFiCxAgtgABYgsQsODO2aYsp3O0Jc7QLk4bvvVP5/waiXO0Z7/lzuKc0T+4p3W8dIa2G825yeKc6Wefm/X67mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIKBumqb7yXV9vKqqw3O3HRapTU3TbJjvTaT4HNCi42ehp9gCMDt+jQAQILYAAWILECC2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAWILEDDSy+SV9apmtBqbq70MzMU3TxTnnHhwVWAnc69esaI4pzl3rrBG+cegOTfVcexsNV5NNhN1cZElYrF8Dsg7XZ080enl4T3FdrQaq7bWOwezqzn0vb/7WHHOr2y+JrCTuTdyycbinKmnj7SvcfEl5TWOHus4dqDZW/z6pWSxfA7Iu6vZ3fEvePg1AkCA2AIEiC1AgNgCBIgtQEBPpxEWi25OGnz1/dtax69/6/5Bbaej4VdtLs6Z/vKh1vHSSYNutJ00YG7Ut93UOt7c/6XQTkhxZwsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAQvuoYYn3rmjOOeqd97d93USDy2UlB5Y6Ea9qvxe3mai/H5fsjy0sPy4swUIEFuAALEFCBBbgACxBQgQW4AAsQUIiJ+zfeId7edoB3GGdjnp5gzts//39tbx9b9SPnM8tHp1x7H6jP9mQ4lPCUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIExB9quOpd7Q8tnP72bcU1LvjY/L/4e6F45p+XX7Z+2fv6f1BkZny841jTzPS9/lKy58jB4pxdG7cEdsJC4s4WIEBsAQLEFiBAbAECxBYgQGwBAsQWICB+zvaR32w/X7jm/JPFNS78g84vsq6qqpo5c6anPS1m3Zyhnf5rr20df3LnquIaV//kvq73tNwtqTO0d97SOjz8Yvnl9dNfPjSo3Sxq7mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIGCgDzVM7bytOOe6N9/f93Xq9evaJyyjhxq6Mfypz7WOX/2p0EZYfO75YuvwdBdLjH/b1tbxsY8f6GFDi5c7W4AAsQUIEFuAALEFCBBbgACxBQgQW4CAgZ6zHdnb/xnaPUcOFucsqZczs+isvHGouuIj53ccf2rbi8HdLHyJc7SnvmNbcc6a39o/5/to484WIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQIG+lDDIHhg4c+rV6xsHW/OTYZ2wtdNPjTjwYUFZr4fWOiGO1uAALEFCBBbgACxBQgQW4AAsQUIEFuAgAV3zpY/r3SOdvji9cU1pk88O6jtwJ8zcvnG4pypp48EdrLwubMFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgIC6aZruJ9f18aqqDs/ddlikNjVNs2G+N5Hic0CLjp+FnmILwOz4NQJAgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIgtQMBIL5NX1qua0Wqs4/jElZ3Hvm7VyZn2CS++VFxjYtPq9mscPlNcg8E5W41Xk81EPd/7SBlePdasuGhdx/FVz00V12gmJtrH17T/jFdVVV121YnW8aMPltcoqVetbB1vRobLi4yXP9MJ9Yr23DXnyt+3ktPVyROdXh7eU2xHq7Fqa72z4/jDP7a1uMb1H2v/Ias/e7C4xqGfuKN1fPM/ube4BoNzoNk731uIWnHRuurq7/uRjuNXf+xYcY3pQ4+0jk++rv1nvKqq6ic/8Kut4++97pbiGiXDm65tHZ9ef355kf0P9L2PQRi5+JLW8amj5e9byV3N7o5/wcOvEQACxBYgQGwBAsQWIEBsAQJ6+lPma+p1TdtphK4u+CeXt443f/3pvtYn70CztzrVPLdsjn4N4nMwCF99/7bW8evfuj+0E77urmb3/U3T3P5yY+5sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiCgp1csDsInb/yj1vFd1ZbQTmBxu+wz3T+QRNnT/2JHcc7lP3P3rNd3ZwsQILYAAWILECC2AAFiCxAgtgABYgsQED9nu2ujc7QLzfAlryjOmT72tcBOFofNt56p9uw52HE89TM+tvtA5DrLRT9naLvhzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAuIPNbDweGChN4ceWO3hHHrmzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgwDnbBe5rP7ijdfwVv1R+gfRz331n6/i6D+/raU+wGJ38oxtax9f+jYeLa0z8jTvaJ/zh7o5D7mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIMBDDQvcKz5wd99rDOKhheEbru04Vh/+H32vv5QMX3Rhcc708y8EdrKEbLu1fXz/A8UlunlooWTVH9076691ZwsQILYAAWILECC2AAFiCxAgtgABYgsQED9nO/ONr2kdH/r050M7oRfTDz/acaxpJoM7Wfi6OUO7/rNrW8effd3Jvvfx8EdeW5xzw3d/ru/rRHRxjrbkqgNjreOPnl5fXGPk9U/M+vrubAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgIP5QQ+KhhYm/cUdxTuklwDN7r2wdH9r5ZE97WuyOv2V7x7Gp3fuDO5l/E1eMVY/8yLaO4ze880vFNQbx0ELJonlgoQuPvafzz19VVdU1by+/IP+JrePtE3be2MVOPNQAsKCJLUCA2AIEiC1AgNgCBIgtQIDYAgTUTdN0PXlNva7ZWu+cw+2wGB1o9lanmufq+d5HSuJzMHL5xuKcqaePzOkeUh77rVcX51zzHV8I7KRseMOG1vE9X/ul+5umuf3lxtzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUBATw811HV9vKqqw3O3HRapTU3TtJ/2XkJ8DmjR8bPQU2wBmB2/RgAIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBjpZfLKelUzWo31dcFmzerW8frUmb7Wp3f1SPnHoJma6jh2thqvJpuJepB7WsiGx8aaFWvXdRxf+fR439eoh8r3QVMXndc6Pvxc//tYSkr/ps3MTN/XOF2dPNHp5eE9xXa0Gqu21jv72szk6+5oHV/5yXv7Wp/eDa8r/5GF6ePHO44daPYOcjsL3oq166orfuiHO45f87Z9fV9j6PwLinOef8NNreNrfnN/3/tYSkr/pjOnT/d9jbua3R3/godfIwAEiC1AgNgCBIgtQIDYAgT0dBphEJ78h+dax6/7ZGYfE29oPxWx6hPL51RE20kD/k9DE1W15pHO4yOXXlJcY+rosfZrXLimuMb4pe33SqPf3P4zXlVdnP4ZGm4fn5kuXmOhKJ02GLnyiuIaU08+Nevru7MFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgID4Qw1f/au/1jq+q9oS2cdyemiBwRo5MV6t/1Dn1ygeftuO4hqX/9v2hxqmnnq6uMYbvqt9zt4PbC+usb40IfDQwmO/fWtxzjV//4E538fU08/M6frubAECxBYgQGwBAsQWIEBsAQLEFiBAbAEC4uds975UeBnxIjG8dm1xzvTJk4GdZDz7vZ3PbE79nj+Z/b978Id+sThn17/t/zz5wde0j6+v+v+T6gkDOUNb1+Upt9/cOv7C9WPFNdb81ux/1t3ZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUBA/KGG9153S/qSc2LitdcW54zsvT+wk4z1v9L5gPxIMx7cycK3a2PmBfiLxXPf0/4S85/+8f9YXON919/UOl56YKGqqurEq89vHV91qimuMfyqze0TvtR5yJ0tQIDYAgSILUCA2AIEiC1AgNgCBIgtQED8nG3CxB9fXZyz6pse7+saS+kMLcylDf/9aOv4+361/QxtN5p7v1ics/7evi9T1ddsmvXXurMFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgIAl+VDDyHvWdTHr8bneBnR05Ed3tI5v/Nm7Qzvpz5m/s7U459id7fd017ztsUFtZ849f8dl7RMe7TzkzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgYMGdsx264ILinJnTp1vHhz/1uUFth/9p+OL1Hcfqk8PBnSwNi+UcbckFny2fkV35/JWBnfTv+e/aXpyz7oHnZ72+O1uAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCIg/1DByzabW8anHDvd/jcs3FudMPX2k7+ssBM32Vxfn1Pu+0Pd1pk8823kPzXTf6y8lT729/cXgVVVVV7xnaTzUMH3sa8U5I13MWQgu+k/7inNm+ljfnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AQPyc7SDO0T7ys+0v+d387of6vkbJ6W/fVpxzwcf2z/k+PrH7w8U5b7z8tXO+D/7MIM7QNjvK56eH7v1K+xrnJstr3Hxj6/jMg3P/WRresKE458XXXdM6ft7v3VO+zubrWsdP3lbex9r7CmeGD3UecmcLECC2AAFiCxAgtgABYgsQILYAAWILECC2AAF10zTdT67r41VV9f9UAkvNpqZpyifClwifA1p0/Cz0FFsAZsevEQACxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAkZ6mbyyXtWMVmNztRcWqbPVeDXZTNTzvY8UnwM6OV2dPNHp5eE9xXa0Gqu21jsHsyuWjAPN3vneQpTPAZ3c1ezu+Bc8/BoBIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQJ6esUisHA8+33bW8fXf2hfaCd0w50tQIDYAgSILUCA2AIEiC1AgNgCBIgtQIBztsvAyKWXtI5PHT0W2gmD5Bzt4uLOFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECPNSwyO05crA4Z9fGLYGdLB+bbz1T7dnT+d899e89cuUVreNTTz4V2Ue/Zr7xNcU5Q5/+fGAnc8udLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCAc7YLXPO69jObH3/x0dBO+LpDj66v/q83/aOO40NV+ezzIJTO0da331xco7nvwUFtZ9ZWfP6R4pzpwD7mmjtbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQjwUMMCV3+2/YD8BzdfG9oJ/8uLL1VDn8k8uNCPhfDAQjemT52KXKceac9dMzU1p9d3ZwsQILYAAWILECC2AAFiCxAgtgABYpLE3BcAACAASURBVAsQ4JwtLECH/9X24pxNP7UvsJOF4am372gdv+I9dxfXmOtztCXubAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgwEMNMA+O/nD7If1NP1U+pL8Q1K+5qTin+fyX+r5ONw8tLHTubAECxBYgQGwBAsQWIEBsAQLEFiBAbAECejpnW48MV8Nr13ccnz7xbN8bgsXu2A+1n6Gtqqq69OcX/7nRqhrMGdqUkUsvaR2fOnpsTq/vzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAuqmabqfXNfHq6o6PHfbYZHa1DTNhvneRIrPAS06fhZ6ii0As+PXCAABYgsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQILYAASO9TF5Zr2pGq7G+LrjxlvHW8SNf7G998s5W49VkM1HP9z5SBvE5YGk6XZ080enl4T3FdrQaq7bWO/vazLt+//7W8Xdce1tf65N3oNk731uIGsTngKXprmZ3x7/g4dcIAAFiCxAgtgABYgsQILYAAT2dRhi5cbha/6trO44/+7qTxTWcNgCWI3e2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxDQ00MNUw9Nd/XgQj/Ovb780MOKu9pf0wiw0LizBQgQW4AAsQUIEFuAALEFCBBbgACxBQjo6ZxtgjO0wFLkzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQIW3MvDYTkYXr+udXz62edCO1kaTn/7tuKcCz62P7CTztzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUBATw81bL71TLVnz8GO47s2bul7Q7AcLJSHFg6/a0fr+KZ33B3aSX9SDyw89u7t7RPevrvjkDtbgACxBQgQW4AAsQUIEFuAALEFCBBbgICeztkeemC1s7QQ8Pi/KZznrKrq6p/c1/d1Fss52oXimh9v/zd/pGXMnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBPT0UAOQMYgHFk58f/nBiIs/2P91FouRa69uHZ969PE5vb47W4AAsQUIEFuAALEFCBBbgACxBQgQW4CAZXvOdujWG1vHZx54KLST/kz/tdcW5wx/6nOBnTBIe44cLM4pvch/oZyhPff621rHV9x1f2Qfc32OtsSdLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIE1E3TdD+5ro9XVXV47rbDIrWpaZoN872JFJ8DWnT8LPQUWwBmx68RAALEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECRnqZvLJe1YxWY3O1Fxaps9V4NdlM1PO9j5SVK8aa0dGLOk948aXcZohq1qxuHX/x1NMnOr08vKfYjlZj1dZ6Zy9fwjJwoNk731uIGh29qLrjNT/QcXzoMweDuyFpcsftreP//ZNv6/gXPPwaASBAbAECxBYgQGwBAsQWIKCn0wiDMLzhZU9F/C/Tx4+HdgKzc+U1x6tf+I1f7Dj+1qt3BHczx+rCib6myexjgVi5575Zf607W4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIiD/U4KGFwTr15m3FOWt+c39gJ8vHk188f2k9uNBmmT20MJfc2QIEiC1AgNgCBIgtQIDYAgSILUCA2AIExM/ZnvxH21vH1/7avtBOlgZnaBeetZ9dV5xz8nXPBXbCQuLOFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAEC4g81eGiBxW7iqtXVobff2XF88+vuCe6GpDN/Z2v7hI/v7jjkzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgIH7Olt6MbLqydXzq8JOhnfB1q544U23+AWdpl6PV/+XArL/WnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBHioYYHz0MLCU583Wg1tvrHj+MwDD0X2ceyHdrSOX/ILd0f2sRBs3H9Bcc6RbacDO+nMnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AQE/nbDffeqbas+dgx/FdG7f0vaGFor7tptbx5v4vhXbCQtO8dDZ2lrbNUjlHO3zTXyrOmf7Sn7aOz/cZ2m64swUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAgJ4eajj0wOol9eBCGw8tMFsnv3t7cc7aj+yb832UXi5eVf0/GHHog3cU52z+/ntbx0sPLCwV7mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAno6Z7tY1CtWFuc05yZbx8990+2t46euWlG8xvr/OPdnKcmbuGp1degn7uw4vvktme/7s9/Xfp63mzO0X/25ba3jj/z9X24d37WxeImBqEfaU9VMTWU20gd3tgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQUDdN0/3kuj5eVdXhudsOi9Smpmk2zPcmUnwOaNHxs9BTbAGYHb9GAAgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIGOll8sp6VTNajc3VXljA6qHO/11+aebFarI5Wwe3M698DhaeZs3q4pz61Jn28ZUry9eZnGwdP12dPNHp5eE9xXa0Gqu21jt7+RKWiKHVneOy/8wfBncy/3wOFp5z228vzlnxx/e1jo9cflVxjanHn2gdv6vZ3fEvePg1AkCA2AIEiC1AgNgCBIgtQEBPpxE233qm2rPnYMfxXRu39L0hBm9483Wt49OHHimuMTM+3nGsaWZ63hMMUumkQTdKJw2qqqpGrri8fcKTnYfc2QIEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AQE8PNRx6YLUHFxahbh5aKBl+1eaOY/Ujn+l7fZavl/7WncU55/3ePe0ThobLF5qZ7nJHnU099fSsv9adLUCA2AIEiC1AgNgCBIgtQIDYAgSILUBAT+dsWb6mv3yo41jTTAR3wlJTPEPbjS7O0D7/D7e3jl/06/uKa9S339w+4d7dHYfc2QIEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgIcaqB7+wNbinBt+8EBgJzB3unlooaS578FZf607W4AAsQUIEFuAALEFCBBbgACxBQgQW4AA52xxhnYeDN9wbev49MOPhnayNAxdcEFxzszp063jk7tuL66xcs99Xe/pL3JnCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgABPT3UMHHlWPXwj3V+0fQNPxQ6HD803D4+M933JYbXrm0dnz55su9rsHx5aGGwSg8sdOPY7SuLc67cM/v13dkCBIgtQIDYAgSILUCA2AIEiC1AgNgCBPR0znb06ET1yvcc7jg+1fd2ujSIc7Tr17WOTz/7XN/XWEqaHa/uPHjw7txGYI5c+dNz+3PszhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAnp6qKE5N1VNPXN0rvZSVVVV1SvKL/Btzk32fR0PLfSmvvsLnQebl3IbWQQm3nBHcc6qT9zbOn78LduLa7z0irp1fNO77ymu0UzFHkXqqKvP/HThQaYuHnTac+Rg6/iujVuKa/TDnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AQE/nbBMGcYaW3sx842uKc4Y+/fnAThaH6fVj1clv7XwOdu2v7ev7GqduaIpzrvvn7dcpr1BVM3+5/Xs/9D/m/vt+6tteW5xzwW/v7/s6c32OtsSdLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIE1E3TzdHn/zm5ro9XVXV47rbDIrWpaZoN872JFJ8DWnT8LPQUWwBmx68RAALEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECRnqZvHL4vOa8FRd2HG8mJvveEH9evXJl63gzWf43r1cV1hgZLm9k/KWOQ2er8WqymajLiywNK+tVzWg1NqfXmLiyvP4ta4+3jj/8p2uLa8ysav/eN8Pt39Zz5xcvUY0ebf8Zbc6dK65RnzfavsZLZ8sbCThdnTzR6eXhPcX2vBUXVjuu+K6O41OPPt7bzigaufyq1vGpx58or3HF1a3j5y7p/B/Qr6v3faHj2IFmb/Hrl5LRaqzaWu/sb5G6PWIP/9idxSXu+bv/oXX8jX/124prvHTtutbxiQvbY3x0R/ES1Svf2/4zOvX0keIaQ9ff2Do+8+BD5Y0E3NXs7vgXPPwaASBAbAECxBYgQGwBAsQWIKCnP2W+pl7X9P3/wtKTp/9F+//du+k/P11cY+qxjv8HadeO/nDnfXz1oz9XvXT0yWVz9MvngE7uanbf3zTN7S835s4WIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQJ6esUieZf/zN2t41OhfVz68533cbgZD+1iYWg2r6ymfqnzqy9HXl9+7SULz/D69tdNVlVVTT/73KzXd2cLECC2AAFiCxAgtgABYgsQILYAAWILENDTOdvpdWPVC2/c1nH8wo/u73tD5A2NjRXnzIwvr7O0bepDk87SLkH9nKHthjtbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQjo6aGG4efGPbiwBHlggYVu4g13tI6v+sS9kX2Mf9vW9gm7d3cccmcLECC2AAFiCxAgtgABYgsQILYAAWILENDTOdt6aKgaOm91x/Fmerq4RjMx0cslX9aeIwdbx3dt3NL3NYCFI3WOtmTs4wdm/bXubAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgoKeHGpqZmWrmzJm52kvXPLQwWF/7gR3FORsOtrxg/ODdA9wNi8nI1VcV50w9/kRgJwufO1uAALEFCBBbgACxBQgQW4AAsQUIEFuAgN5eHr5yRTVy6RUdx6eefKrvDZH3il/s85xs89JgNsKi4wxt99zZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUBAby8PnzzX+uDCMz9Sfgn1ZT/X/4umX/hv17eOX/jGr/Z9jUMfvq11fPM/vr/vawzCzDeUX6Q+9JmDfV+n7d9j4p37+l4fljp3tgABYgsQILYAAWILECC2AAFiCxAgtgABPZ2zndowVh3/u9s7jg/iDG03BnGOtqR0jnZ4/briGtPPPjeo7XQ0iDO03Wj79zjZnInsARYzd7YAAWILECC2AAFiCxAgtgABYgsQILYAAWILEFA3TdP95Lo+XlXV4bnbDovUpqZpNsz3JlJ8DmjR8bPQU2wBmB2/RgAIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBjpZfLKelUzWo3N1V5YpM5W49VkM1HP9z5SBvE5eMXNZ1vHv/bgaF/rV1VVnbuuvMbws8Ot40PPj/e9j0FoLlzdOn7VVV8rrvHkF89vHa9HVxXXmLyoPZlnjz51otPLw3uK7Wg1Vm2td/byJSwDB5q9872FqEF8Dn7w9w61jn/ghs19rV9VVfXUz95UnLPuo+0BWv1fDrQvUHfx39gB/IGCs994Z+v4L//79xfXeOvVO1rHh6+9objG4b/V/gdJHnr3j3T8Cx5+jQAQILYAAWILECC2AAFiCxDQ058yX1Ova5xGWHzqFStbx5tzk32tf6DZW51qnls2R7/G1l/Z3PzNb+04vua39gd3s/gNv7J8CmD6Kw+3T9h2a3GNx/9m+3G9q39yX3GNkrua3fc3TXP7y425swUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAgJ5escjCM3TzjcU5M19uPxA+cu3VxTWmHn28yx0tfcPPjbc+uDD8qvLrEae/3P6KxeWk+MBCN/Y/UJxy9QCeNXn832xvn/ATuzsOubMFCBBbgACxBQgQW4AAsQUIEFuAALEFCFi252yHN7T/SeLp48dDO+nPQz/c/qeoq6qqvumW9peHH/6n7S9VrqqqGl6/ruNY/fxw8euXk2f+6sXFOa9wzjbu0C+3/zn0V73nmeIapReMf7VlzJ0tQIDYAgSILUCA2AIEiC1AgNgCBIgtQIDYAgQsyYcahtesKc5ZLA8tlGz+3vuKc57Y8qrW8ebgl4prTLd9fdM2uvy8eFVTnPOKAVznLz9wtnX8f9w6OoCrLB2b33JP6/hjP7GjuMaVP/3krK/vzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgYEmes50+dWq+tzAwe44cbB3/5m/9B8U1Zu57cFDboQvXvq39BdNVVf6+7tq4pbhG6RztyBWXF9eYeurp4pzlYvKmM3O6vjtbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQhYcA81zHxD+TD30GfaD4QvJeXD7f0/sDBy7dXFOVOPPt73dfgz3Ty00C8PLPTmujeXu/Lwr7+2fcJ37e445M4WIEBsAQLEFiBAbAECxBYgQGwBAsQWICB+zvboW3e0jl/6/ruLa4xcdmnr+NQzR3va03LnDO3Cc+iX7yzO2fyWewI7mXtHfrS9CVVVVRt/ttyFhIvuXtU6frhlzJ0tQIDYAgSILUCA2AIEiC1AgNgCBIgtQIDYAgTEH2ooPbRQj5S31JydGNR2loXSv2kzNRXayfJQ33FLcU5z7xdbxwfxwMJieSn8IB5YGL54fXHON/zJU63jn771vOIaG355X9d7+ovc2QIEiC1AgNgCBIgtQIDYAgSILUCA2AIExM/Zlky8/jXFOSs/eW9gJ3MvdQ7SOdoBq+uqXrGy43DpDG1VVdXwJa9oHZ8+9rWet/V/rPFE+7nSpWT6xLPFOaVztG3f069rzk12vae/yJ0tQIDYAgSILUCA2AIEiC1AgNgCBIgtQIDYAgTUTdN0P7muj1dVdXjutsMitalpmg3zvYkUnwNadPws9BRbAGbHrxEAAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQJGepm8sl7VjFZjc7UXFqmz1Xg12UzU872PFJ8DOjldnTzR6eXhPcV2tBqrttY7B7MrlowDzd753kKUzwGd3NXs7vgXPPwaASBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECenrFIjAYw2vWtI5PnzoV2gkp7mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAgZ6zrZ0drCqnB+EqvI5WIhGLru0OGfqmaOzXt+dLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIEDPShBge1YXE5/e3bWscv+Nj+0E7mXz8PLHTDnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AQE/nbDffeqbas+dgx/FdG7f0vSGgqkauvbo4Z+rRx/u+znI6Rzvf3NkCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIgtQEBPDzUcemC1BxcG6Oy33FmcM/oH9wR2wkIziAcWGLzhtWvbJzzXecidLUCA2AIEiC1AgNgCBIgtQIDYAgSILUBAT+dsJ689rzr8M7d0HN/0pi/2vaHlxBlaOtlzpPNL+r/Omfe86ZMnZ/217mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIKCnhxpWPvqSBxcgYBAPLBz50R3FORt/9u6+r0N33NkCBIgtQIDYAgSILUCA2AIEiC1AgNgCBPR0zrZaPVrVN93ccbi578F+97NgnPgn21vHL/4P+0I7YaGph4eq4fPXdByfPnWq72s8+m/bf/6qqqqufVv7z+BSOkPbvK793HH92fLL1uebO1uAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCKibpul+cl0fr6rq8Nxth0VqU9M0G+Z7Eyk+B7To+FnoKbYAzI5fIwAEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIz0MnllvaoZrcY6T1g9Wlxj8/XPtY4femB1cY2pV7Tsoaqqka+NF9dgcM5W49VkM1HP9z5Sip8Dlq3T1ckTnV4e3lNsR6uxamu9s+N4feNNxTU++UcfbR3ftXFLcY1j37GjdfySX7i7uAaDc6DZO99biCp9Dli+7mp2d/wLHn6NABAgtgABYgsQILYAAWILENDTaYTJy8aqJ7+v80mATb/bfqyrqro7bVDitMFgTe66vThn5Z77AjuBpcudLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIE9PRQw6qvvVRd/e8e7Dg+fepU3xsizwMLMPfc2QIEiC1AgNgCBIgtQIDYAgSILUCA2AIE9HTOtpmeaT1L+8RPtf+J8aqqqqv+lRd/A8uPO1uAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCOjpoYYSDyxAVR19a/nhnkvf3/9n5eF/t611/IZ/tr/vazA47mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAgZ6zvbwu8rnCze9w1lclrZBnKHtxlI5R3vr5+rinAdOXt4+YedTA9rN3HFnCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgABPT3UMHndaPXke2/uOL7p73pgAejNA69tinOG159pHf9vRw4W19i1cUvXe5oL7mwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAno6Zzv8/HB1we9fMFd76drj/2Z76/jVP7kvtJO5NzQ62jo+c/ZsaCcwf6affa51PHWG9tzrb2uf8P/u7jjkzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAnp7qOHZ8eqiX5//BwaW0kMLJR5aWHye/Jc7inNGxtvHL/s5L+L/3w1fdGHr+PTzL0T2seKu+2f9te5sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQJ6OmdbcuhXby/O2fw99w3ykktefdtNrePN/V8K7YRuXfmvnZEdtMg52rouz2maWS/vzhYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAuqmh0O6dV0fr6rq8Nxth0VqU9M0G+Z7Eyk+B7To+FnoKbYAzI5fIwAEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIz0MnllvaoZrcY6jl9001Rxjee/1H7Jc5d0Xv/rVhwbbx2fvPa84horH32pOGdRqOvynDl+Z/HZaryabCa62MjSUPocpDRrVreO16fOlBcZK3xWxuf+c9Jc2P6/4/+f1D5cd/MjfrqLf4+CeuXK1vFTk8dOdHp5eE+xHa3Gqq31zo7j37r72eIav/+q9a3jR79zR3GNS3/+7tbxwz9zS3GNTW/6YnHOYlCvaP/mV1VVNecm53QPB5q9c7r+QlP6HKRMfMMdreOrPnFveZFbb20f3/9ADzuanbN/5c7inKGp9poOT0wX1xj+1Oe63lMnI5df1Tr+ycd+ruNf8PBrBIAAsQUIEFuAALEFCBBbgICeTiOUlE4aVFVV3XGw/f81/Ny3f624xo890v7/kL7nuuIS1Z4jB1vHd23cUl5kAZjrkwbMj+ENL3t66M858lfaP77XfKKLCwVOG5SM/uE9872Frk09/sSsv9adLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIEDPShhm58/ps3to6f3l5+MOI91xVeC9eFxfLQAgvP0F8aqc770CUdx5/6T9cW11j/H/e1jk8fP15c45q3l+cwWMNr1rRPeKHzkDtbgACxBQgQW4AAsQUIEFuAALEFCBBbgID4Odupo8dax1f/bvs48+PM397acWzmT/YHdzL/zp1YVR35cOeztOt/rf0MLYvX9KlTs/5ad7YAAWILECC2AAFiCxAgtgABYgsQILYAAWILEBB/qIHBOvyuHcU5m95xd9/XWf27BzqODTXjfa+/mAw/O16tbXlw4cl/Wf6eXPmv+/+eMFj1SDmHzdTUrNd3ZwsQILYAAWILECC2AAFiCxAgtgABYgsQ4JztItfNGdpTb97WOr7mN5fXy7/n2lXvuac4pwnsg970c4a2G+5sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiDAQw3LgIcWsj75xH3FObs2bgnshIXEnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgHO20KMrb3mxev8fdn5p+66NO4K7oVtHfqz9+7LxveUX8ffDnS1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBHioAXr01ZOXVN/yOz/ccXzz2oeKa0yfPNn3Pl74zm2t4xd+dPm8NP7hD2wtzrnhB+f2oYUSd7YAAWILECC2AAFiCxAgtgABYgsQILYAAQvunO3wDdcW53zlbetaxzd/732D2s6cOvrW8kumL31/+9nAS/atKa5xbPuprvdE2aqnxqvrfnRfx/Hp0D5K52hHLr2kuMbU0WOD2s68uuEHD8z3Forc2QIEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AQN00TfeT6/p4VVWH5247LFKbmqbZMN+bSPE5oEXHz0JPsQVgdvwaASBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgYKSXySvrVc1oNdbXBUdfWbeOn/2K9+suNmer8WqymWj/xi4hpc/B5lvPFNc49MDqvvdRD7XfKzUzM31fg96crk6e6PTy8J5iO1qNVVvrnX1t5ob/tKp1/OE7Jvpan7wDzd753kJU6XOwZ8/B4hq7Nm7pex9D51/QOj5z+nTf16A3dzW7O/4FD79GAAgQW4AAsQUIEFuAALEFCOjpNMLM2rFqfOfWjuNjHz9QvuDQdC+XJOHOW4pThh54uONYfXbZnPrqyq3v+4HinMuqu/u+jtMGgzV88frinOkTz856fXe2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxDQ00MNQyfHu3pwoc3ngqkX7QAAIABJREFU3/Ha1vHR6p6+1l9u9hwZwOv87vlicY1zf63z962591PFr19OLntf+YGFU5+4rnV8zRseGdR26FJz5qU5Xd+dLUCA2AIEiC1AgNgCBIgtQIDYAgSILUBAT+dsq7HzqubVr+44XO/7QnGJ0T9sP0f7xO+UX2R91d8rnwtdLrr5k9gjl29sHZ96+khxjeFPfa7zYHOm+PXLyQvfua0458I37A/sZPmY3HV7cc7KPfe1jtebLi+uMXLmbPuExzsPubMFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgIDeHmoYf6mrBxf64YGFwevmoQUG58KPemAhrfTAQjemv/LwAHbSmTtbgACxBQgQW4AAsQUIEFuAALEFCBBbgICeztlOXDFW/X/t3XuQXmdh3/FzdlertRbJV9myfBE2trg4BjnGli3C5OKkCglJC6EkoQkkDRQXGppCQ4GZNDMwAy6hLeGSOCGTSTI0tMSFkISLErvTDFiWfImFjUksX4Vt2ZZkZFvottrd0z8SB2h8zrPvvu/+9qLP59/n2XMeS/t+dWb9PGfve3v7i5Gf9w77C2Hfn68vzjntJ3YGVsJ3GjlvXef45AO75vT+nmwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIKCnQw1jKyaq9d/7jdbxqb6Xw0L1wDVXto5NfMRhlu/kwMLCNNeHFko82QIEiC1AgNgCBIgtQIDYAgSILUCA2AIE9LTPttl5rJr6wd1ztZaqqqpq35vb93M+47TfuWlO17Co1HV5TtP0fZvz3tX+Z76nOdj39Y83v7XrK53jb1n3faGVkOLJFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECejrUkDCIAwv1stHinObYRN/3WRAGcGCBvMVyaGF4/fM6x5tHHiteY/rg3B96mfjRy4pzRr90S/eEGRwQOvYjl3ZP2HJd65AnW4AAsQUIEFuAALEFCBBbgACxBQgQW4CAnvbZTq4er/a+pv3l3qt2TRavsfyL3XvdBrFHdiZ7aIdWrOhex1lrOsen7rm/eI+Sx9+2qTjnjI9s7fs+T3+xe6/kqlfc1/c9+Lbpl19SnDP05dv7vs/913S/aP/8jhe+z1Tz6J7O8cQe2qqqqt2/2v1ZWfsb/X9OZrJnfdlf3jrry3uyBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4CAuunh5dN1Xe+tqmrX3C2HRWpd0zSr53sRKT4HdGj9LPQUWwBmx48RAALEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECRnqZPFovb8aq8blaC4vUkepgNdEcred7HSnLRsebsRUnt47XTx3q+x7rX1y+xs47VnSOHz2n/FkdOto9fsbqJzvHv3nXsuI9Surh4eKcZmqq7/scO737z2PZnoN93+NAtX9f28vDe4rtWDVebayv6ntBLC3bmxvmewlRYytOri75vre1ji//wi1932PLlh3FOZvXbugcv+edG4vXWHl/d+je9ubPdI5/+oVrivcoGT6x/R+uZ0zt39/3fR77V5s6x9d8eGvf97i+ua71N3j4MQJAgNgCBIgtQIDYAgSILUBAT7sRgL/f2jWIHQddPn9orO9rnHFTeTfe4dO6xwex26DkoV96YXHO2g/1v1NgELsN+uHJFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECHGqABegjF7yg72us+tS28py+79K/QRxYWAw82QIEiC1AgNgCBIgtQIDYAgSILUCA2AIE2GdLVV92cXFOc8udgZXA/Jn+/kuKc4b++vZZX9+TLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIEONQwj4ZPOrE4pz7hhM7xXR8/tXiNs9/XPe7AAlTVvb9QfvZc/9ezv74nW4AAsQUIEFuAALEFCBBbgACxBQgQW4AA+2zn0dSTT5UnFeac+ZtnFi8xvOfxzvHJ8ipgQdvz1k3FOad/fGvn+PpfvG1Qy3lWnmwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIMChhgXuqZ+7onN82wevLV7jyndc3Tm+6lO7e1oTc++Krx4rztn2kmWBlSwOpQMLC4EnW4AAsQUIEFuAALEFCBBbgACxBQgQW4AA+2wXuJPuPtg5vnnthuI1VlXbOsfrZaPFazTHJopzmLk9/677ZdfbXrLw943SG0+2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxDgUMMC19xy59zfw4GFuNM/5tBCL+qR7lQ1k5P93+Oyi4tz+vk8erIFCBBbgACxBQgQW4AAsQUIEFuAALEFCLDPFljwBrGPtniPOd7T7skWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLqpmlmPrmu91ZVtWvulsMita5pmtXzvYgUnwM6tH4WeootALPjxwgAAWILECC2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAWILECC2AAEjvUwerZc3Y9X4XK1lxta/+FDn+M47VoRWQlVV1ZHqYDXRHK3nex0pwyvHm5FTT24dX76r+/tzJibWlj9no7sP9n2fknqo+3msmZ6e8zXMxPRJ5c/80JOFv5cVY8VrHD29+89j4sFH9rW9PLyn2I5V49XG+qpevmRObNmyo3N889oNoZVQVVW1vblhvpcQNXLqydWaX3tb6/j6N93S9z12Xb2pOGfdr2/t+z4lQ89Z2Tk+fXAG/7BMTw1oNe0O/+DlxTkn/OnNneP1iy4qXmPn25Z3ju96/Xtaf4OHHyMABIgtQIDYAgSILUCA2AIE9LQbYaGw24D5tGrF4eoVG+5sHb9vAPdI7DSYiekDB+Z7CTNS2mkwE81tdxXnvOCaCzvHW7ciVJ5sASLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBgUR5qgPk0cd9ItevVp3XMeDi2FhYPT7YAAWILECC2AAFiCxAgtgABYgsQILYAAQPdZzv8/AuKc6buvneQt4S4ZuJYNfmQvbTHo6m/vWfWX+vJFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECejrUUI8uq0bWnN06fui5JxWvMXp3L3cEWBo82QIEiC1AgNgCBIgtQIDYAgSILUCA2AIE9LTPtvTS5FEvVB64Dbd3j++4JLMOWOqal20ozqlv3DHr63uyBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4CA3g41nLiiOvL9l7eOj/35zX0viO/m0AJk9HNgYSY82QIEiC1AgNgCBIgtQIDYAgSILUCA2AIE9LTPdujoVDV+7/7W8am+lwOwcA1f9PzuCV9rH/JkCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgABvb08/MjRaupv75mrtTBPRtadU5wzueuhwEqWhi27yy+h3rx2Q9/3qS+9qHO8ue2uvu+xlBz8qY2d4+P/e3vxGlN33T3r+3uyBQgQW4AAsQUIEFuAALEFCBBbgACxBQjoaZ/tqoumqh/+9IHW8eu/Z2XfCyLPHtrBGsQe2pnY9eMndo6fe1v5GkdeeXnn+Nhf3NzLkha00j7avX9WeDF4VVWrf9I+W4AFTWwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAuqmaWY+ua73VlW1a+6WwyK1rmma1fO9iBSfAzq0fhZ6ii0As+PHCAABYgsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQILYAASM9TT5hvBlddUr7+J6DfS9ocvV4cU5Td48v21teR71sWWEhU91rmJ4u3qO4hqHyv3WDuE/1nBO6x791uK/LH6kOVhPN0cLfytIxWi9vxqry9ynHnwPV/n1tLw/vKbajq06pLvzpt7eOn/6xrT0u7Z/a89pNxTnTo93jZ37s5uI1htee2X2PvU90jx86VLxHydAJK4pzBnGf6Us2dK/jKzv6uv725oa+vn6xGavGq431VfO9DBag65vrWn+Dhx8jAASILUCA2AIEiC1AgNgCBPS0G2FooqlWPjQ5V2upqqqqnr6wvNXpgv+wrXN8Jr+cfXLXQ90ThoZncJX+DGKnwUyM3H5P9zpmcI2hsbHWsfrIcbPra0Z2f/ZFxTlrX/X1wEr4ThN/ta5zfPRHWjcS/KPhM07vnvBY+5AnW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUI6PlQw4rd7e8+nclhgpLSgYWqqqrqihd3j2+7o/+FTHe/z3YxefInL+4cX/Wp8p/59JEjrWNNM4i/+aVj6K9Pmu8lLDn3X3Nl5/j577qpeI2ZHFoomXp8z6y/1pMtQIDYAgSILUCA2AIEiC1AgNgCBIgtQEDdyx7JVfUpTdevcH78l8u/hvyMj/b/684ZrAfe372Hsaqq6sKP3N86tnXfp6unJvYcN28QL30OOH5d31x3W9M0L322MU+2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxDQ08vDS2ZyYKFevrxzvDl6dFDL6V7HstHudRyb6BwfPuP04j36edHwM+77je4DBysfLJ8lOP23t3eOP++9txevMdn58vDJ4tcvJfWyZdXIGWtbxycf2R1cDYMyvP55xTlTO++b9fU92QIEiC1AgNgCBIgtQIDYAgSILUCA2AIEDHSf7Uwk9tEOrVxZnDN94EBf9xjEHtqZeP7HHu4cn9z1UN/32PP6y4tzTv3aofbBHcfXC+GbY8cWxF7a4Ret7xyf+vrO0Erm35bdO4pzzvuzf9M5vv7qmwe1nGflyRYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAgZ6qGHyqkvLN7zhtkHe8ln1e2AhZSYbsTe3v6N6YE773ZvKky6/eO4Xwrdd8eLilKltdwQWsjhsXruhOGd9NbeHFko82QIEiC1AgNgCBIgtQIDYAgSILUCA2AIEDHSf7SD20D7w/iuLc857zwz2hS4A9aUXdY4n9tBWVVUdfM3GzvHJ5XXxGif+j23tg83hXpe0pD387k3FOed+7M7O8Wl7aJccT7YAAWILECC2AAFiCxAgtgABYgsQILYAAWILEDDYQw1nrinOmXz0sc7xQRxYaF5WfpFwfWP5xd39evKFKzvHT5z796hXVVVVq/7vvZ3jU/ueyCxkqVgxVtUvaj+wcvYHthYvMT3I9bAoeLIFCBBbgACxBQgQW4AAsQUIEFuAALEFCBjoPtvSHtqZ2P0fyy9ePuuGp7onBPbQzsSJn+x44fYMHXp194u/V3xme/Ea9tEO2KEjVXPbXX1dYue1l3eOr7/65r6uP1NPv+6KzvFVf9z/93DJ0R+/rDhn+edv6Rx/xV1PFq/xxYtOmvGa2oysOaN7wqPtQ55sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiCgbppm5pPrem9VVbvmbjksUuuaplk934tI8TmgQ+tnoafYAjA7fowAECC2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxAw0svk4ZXjzcjqk1rHlz9wuO8FkVcvK38bNMcmW8eOVAerieZoPcg1LWSj9fJmrBrv6xrNqhWd4/XTh/q6PvPjQLV/X9vLw3uK7cjqk6q173tr6/gFP397j0tjIRg57YzinMnHHm8d297cMMjlLHhj1Xi1sb6qr2tMbHpp5/jollv7uj7z4/rmutbf4OHHCAABYgsQILYAAWILECC2AAE97UZY/sBhOw6WoK6dBsyNEx58snN8KrQOcjzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUBAT4cagMGYuvve+V4CYZ5sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQIGus/2kf+0qTjnrP+ydZC3XPKGXvyCzvHpO/4utBKSdn/2RcU5a1/19cBKGBRPtgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQMNBDDTM5sDD98ks6x4e+fPuglrMkOLRwfDp874nzvQQGzJMtQIDYAgSILUCA2AIEiC1AgNgCBIgtQMBA99nOhH20LHb12PJq+ILnt44fWreqeI3lX7ilc/x5v3pTz+tiYfNkCxAgtgABYgsQILYAAWILECC2AAFiCxAgtgAB8UMNsNg1R45WU3fd3Tq+/K7gYjps2b2jOOe8L72xc3z9L93WfYGm6WVJs3bo1Rs7x+s37yle4zmve6pz/LHXth9Uecbq3579YRNPtgABYgsQILYAAWILECC2AAFiCxAgtgABS3Kf7Uz2F25euyGwEpaiqVPHqydfeWXr+El/lHnx957PvaBzfPPa8jXWV7cOaDVza8VntndP+Ez5GlOF8X720M6EJ1uAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCFiShxqW1IGFuu4eD728+eF3b2odO/b72yJrWCiGnzg45wcX9r+h/dDEM07/55nDEwyGJ1uAALEFCBBbgACxBQgQW4AAsQUIEFuAgPg+212fvrhzfN1r7wytZJEo7KN95F3t+1+fcdY1W/textkfaL/GI83Bvq+/mNTLR6uRs5/bOv7kb5WfYZ7zo/d3jp/8h4tjD+3kD11anDPyf24LrKRs5Kzut6lPPrJ7Tu/vyRYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAuqmh5dP13W9t6qqXXO3HBapdU3TrJ7vRaT4HNCh9bPQU2wBmB0/RgAIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBjpZfJovbwZq8b7uuHk6d1fP7LnYPkidWF4uPyf1UxOlu/DjBypDlYTzdHC38rSMYjPAUvTgWr/vraXh/cU27FqvNpYX9XXYh7/2U2d42d8ZGvxGvVI97KHTj2leI2px/cU5zAz25sb5nsJUYP4HLA0Xd9c1/obPPwYASBAbAECxBYgQGwBAsQWIKCn3QiDcNaf3N85PpMNWaVtW099//nFazz6fed1jj//XXd2jk8fOlS8B8yn4YueX5wzddfdgZUsDnuvvrI4Z/W1N836+p5sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiAgfqjh8Vd2HyY49ROP9X2Pk278RnHOc/7k0c7x6abpex0wnxxY6E0/BxZmwpMtQIDYAgSILUCA2AIEiC1AgNgCBIgtQEB8n+2qByY6x/e/ofwC35P/sHs/3OQju3taEwxSPVL+WJVegM93W7ttZef47isOhFYye55sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiCgp0MN9fBQNfycVa3jU08/XbzGsutv6xw/uZcFwQLkwMLglQ4t7Lz28uI11l9986CWMyuebAECxBYgQGwBAsQWIEBsAQLEFiBAbAECetpn20xNz2gvbT+GVna/JLiqqmr6wMJ/UfCgPPFL3S9Tv/V9v128xkt//d92jp/6ie6XsTN4W3bv6BzfvHZDaCWLw8g5Z3eOz/ce2pnwZAsQILYAAWILECC2AAFiCxAgtgABYgsQILYAAb29PHx0WTWy9pzW8cldD/W9oOPpwMLD795UnDNaOEMyk83vp1YOLSw0Di30ZvKhh+f8Hvf+tyuKcy54+7ZZX9+TLUCA2AIEiC1AgNgCBIgtQIDYAgSILUBAby8Pnzg2kL20/L1zP//N4pzpO/4usBLS7v9g90vhz3+nvdFp/eyhnQlPtgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQ0NOhBgbLgYXj10AOLdR193jT9H8PBsaTLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCAfbYwD3b+zmWd4yecerh4jXNe87VBLWdeDa1cWZxTrz2jc3zq7nsHtZw548kWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLqpocXDNd1vbeqql1ztxwWqXVN06ye70Wk+BzQofWz0FNsAZgdP0YACBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgY6WXyaL28GavG52otLFJHqoPVRHO0nu91pBQ/B885oXyRbx0e3IJYMA5U+/e1vTy8p9iOVePVxvqqwayKJWN7c8N8LyGq9DmY/t5LitcY+vLtg1wSC8T1zXWtv8HDjxEAAsQWIEBsAQLEFiBAbAECetqNUHLo1RuLc1Z8Zvsgb0lI87IN7YM7tuYWsggsu/P+4pypwDrozdCGFxXnTO/4+uyvP+uvBGDGxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgoKdDDcfWjFeP/OKm1vGzrrG5famqb9zRPth4N+t3mnryqfleArPQz4GFmfBkCxAgtgABYgsQILYAAWILECC2AAFiCxDQ0z7bZY8dtJd2gdn5ey8tzln/xlsDK4HFbXj16uKcqb17Z319T7YAAWILECC2AAFiCxAgtgABYgsQILYAAWILENDToYaqrqp6pP1LmsnJftdDj87+wvB8LwGWhJkcWBhe/7zuCXe3D3myBQgQW4AAsQUIEFuAALEFCBBbgACxBQjobZ9tYy/tIO29+srinNXX3tQ5vuIz2we1nE7f+M+bWscmfndbZA3QpnnZhuKc+sYdfd9naud9s/5aT7YAAWILECC2AAFiCxAgtgABYgsQILYAAWILENDboYaC+pKLinOa2+8a5C0XtNKfR+nAwqDc89GNneNrbqyL1zj3vVtbxx5tDva8JujFlt3dBxI2r+3/Hk+/7orinKb0Ufnkda1DnmwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAga6z3b40X3FOYN49fjO37msc3z9m28ZwF36F9lTXJf3yF74y5kXjPP37vtQeb/m2L7u55yzrmnf13w82ry2/HLwfi3fP1We88XZt8WTLUCA2AIEiC1AgNgCBIgtQIDYAgSILUCA2AIE9HSooVm1opp4WceBgi/1f5jgo7tuLM755XV936Za+eXTOscPvLx8QKOkHun+420mB3DEo2n6vwY9OXrueHXPu9pfyH7hW7cFVzO3hk89pXN86olvhlYy9/o5sDATnmwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAnraZ1s/faga7XMv7a73Xtk5Pog9tCPnP7c458DLH+zrHlt27yjO+bEf+KnO8amd9/W1hqqqqom/Kv+Bjf7Irr7vw7ct/8bB6sK3zu0L2YdPO7U4Z2rfE53j93+w+7NWVVV1/jtv6r7HAPbRPvCB7nWc9+7uNVRVVe3/he5rnPwH5Ws0L+t+AXl9Y/kz3Q9PtgABYgsQILYAAWILECC2AAFiCxAgtgABYgsQUDc9vHy6ruu9VVXZIc//b13TNKvnexEpPgd0aP0s9BRbAGbHjxEAAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQJGepk8vGq8Wbb6pPaLfbMuXmPoyUO93PJZ1WPLO8ebI0f7vsdSMvqC7n9TJ/5uuniNo+tWtI5NPrG/mjpwsPyXv0SM1subsWp8vpfBAnSg2r+v7eXhPcV22eqTqnUffHPr+OpPnlC8xgmfu7mXWz6r4ede0Dk+dfe9fd9jKTn3j7rD8I2NB4vX2Plrl7WOPfa+j/S8psVsrBqvNtZXzfcyWICub65r/Q0efowAECC2AAFiCxAgtgABYgsQ0NNuhNH7D1fn/ss7W8ef+kL3LoGqqqoTPleYcPnFxWt844dWdo6f89FHi9eYPlj+P/BLRWm3QX3pRcVrrH/TLa1j+5v+t/PBUufJFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECejrUcOyM8erRn9/UOn7mj23te0HVze2HJp5xVuEtjeW3s/KdmtvuKs7Z+XsvbR07+t6bBrkcWJI82QIEiC1AgNgCBIgtQIDYAgSILUCA2AIE9LTPdmTVseqUzbtbx4c+Pla8xvSRI73ckgVi/RtvbR3z8nD6UV9Sfnl9c3v3XvCd115evMb6qwsb9OeYJ1uAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCOjpUEN9z0S1/J892Drupd1L15bdO1rHLt/sUAOzVzqwMBPzfWBhJjzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgT0tM928oLl1d7//vzW8dU/eXffCzr06o3FOSs+s73v+9CbzWs3tI7tbJ4IrmT+TZ0yXj39iitax1f98bbgalgsPNkCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIgtQEBPhxpG7j06kIMLXWZyYOF/PrS1c/xnztk0qOUseA+/p/zfevb7u/+8hk8+uXiNqf37Z7ympW74mweXzMGFPW/t/v45/ePd3zsLxcj5zy3OOXzBaZ3jy/7y1gGt5tl5sgUIEFuAALEFCBBbgACxBQgQW4AAsQUI6Gmf7UJxPO2jLSntoZ2JXVe/sHyfDyyO/Zb0ZrHsoy2ZvP/B4pxlM5gzlzzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUBAT4ca1nzP4eodn7urdfxDr39d8Rr11q/2css5M3n9uZ3jIz/8jdBK+vPQr5UPeJzzvu6N6195y4eK1/iZDzhIwuK287cu7xxf/5ab5/T+nmwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAnraZ/vQ3tOqd1z7ptbx09/7cPmGP9zLHZ9dab/cus83xWs8fHv3f/oZX+oef86P3l+8R8nkVZcW54zccFvneGkPbVVV1be+dH7n+M+cU7xENbRyZetY/a3j69/s9S8+VG3ZsqN1fPPaDcHVtDv8L7o/J1VVVSf8aX97S7u+L57xsa99sXP8LT/9lvKNtt0x0yW1WnV392f60Ks2Fq+x4rPbZ33/4+tTAjBPxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgoG6a8gGAf5xc13urqto1d8tVsL93AAAHw0lEQVRhkVrXNM3q+V5Eis8BHVo/Cz3FFoDZ8WMEgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQgQW4CAkV4mj9bLm7FqvK8bTp+0onN8+PBk8RrN0YnO8cnV5TWO7D1YnNOlHh0tzmlGh7snfOtwX2tIqofa/10+PP2taqI5UgeXM69G67FmrO74Hgu9I/rYmu7v82WPzeB7vPS3dhy97roeLnxeq6pqpqY6xw9U+/e1vTy8p9iOVePVxvqqXr7knzj8A5d3jq+8c0/xGpP3P9g5vvc1Vxavsfram4pzuoycva44Z+KskzvHh76yo681JA2taP9gbzv0F8GVzL+xery6YvkrWsebo0cj63jkFzd1jp91zdbiNeqR7gQ0k+WHn6VieNWJxTlTTz7VOX59c13rb/DwYwSAALEFCBBbgACxBQgQW4CAnn6V+ar6lKbf3QgsPCNrzijOOfSSc1rH/mbrR6sDTz183Gz98jk4ftWXXdw5/lc3//ptTdO89NnGPNkCBIgtQIDYAgSILUCA2AIEiC1AgNgCBIgtQEBPr1hk4dmyu/yaxh//3s2d45OPPV68xvL9T7aO1UcXz3t5oc1TP3dFcc6Jn9w26+t7sgUIEFuAALEFCBBbgACxBQgQW4AAsQUIsM92kdu8dsMMZnXvo93/hvKvfj/5Dzt+9XsPL6CHheqUz99dnPOFwr724TPbxzzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgSILUCAQw10H1j4B82ml7QP7tg6wNXA3Hjs32/qHF/zm+Xv4/IhontbRzzZAgSILUCA2AIEiC1AgNgCBIgtQIDYAgTYZ0v12K907z+sqqpa8+GOPYjN4QGuBubGTPbRlrzzvjs7x68/v33Mky1AgNgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBAz0UMOTr7+yOOekPyq/qJqszgMLPLu6bh9rmtw6iPrg8y4uzLi7dcSTLUCA2AIEiC1AgNgCBIgtQIDYAgSILUDAQPfZDmIP7fQN5xTnDF31UN/3WSpGzlxTnDP56GOBlRxnFsBe2gfe372v/bz32NPei52/99LinFO3Leue8InrWoc82QIEiC1AgNgCBIgtQIDYAgSILUCA2AIEiC1AwEAPNQyCAwu9SR1YmH75Je2Df3N8vXx88rTx6olXtx8omFjV8WLxf3Dmf+3/zyxxaGHkvHWd45MP7JrzNaSsf+Otc3p9T7YAAWILECC2AAFiCxAgtgABYgsQILYAAfF9tt/8190vPD7l973weCEa+vLt7YPN4dxCFoCRfQerUz8x/9+nj759U+f46IHyC85L/x2LZR/t0EteWJwz/dW/DayknSdbgACxBQgQW4AAsQUIEFuAALEFCBBbgACxBQiom6a88fkfJ9f13qqqFscuZ5LWNU2zer4XkeJzQIfWz0JPsQVgdvwYASBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgYKSXyaP18masGp+rtbBIHakOVhPN0Xq+15EyiM9BvWxZ53hz7FjkGgn1SHdmmsnJ0EoKVoyV5xw60jl8oNq/r+3l4T3FdqwarzbWV/XyJRwHtjc3zPcSogbxORhZc1bn+OTDj5Svccba7ms8srunNc2V4VO6f4nH1N69oZV0q19wUXFOc/tdnePXN9e1/gYPP0YACBBbgACxBQgQW4AAsQUI6Gk3AlA2cnb3ToOqmtlug+I1Fshug5KFstugpLTToF+ebAECxBYgQGwBAsQWIEBsAQLEFiBAbAECxBYgwKEGGLBBHFjguz3wqZd0jp/3s1/t+x47P3FZcc76N90y6+t7sgUIEFuAALEFCBBbgACxBQgQW4AAsQUI6Gmf7dQFy6tv/ub61vFTXrmz7wXB8aD0gnF7db9baR/twddsLF5j/LrtneMz2UP74P96cfeE117XOuTJFiBAbAECxBYgQGwBAsQWIEBsAQLEFiBAbAECejrUMHzvUQcXYAAcWhis0oGFQXnuT9/ROX5vx5gnW4AAsQUIEFuAALEFCBBbgACxBQgQW4CAnvbZAhzP7vmDS7snvMHLwwHmldgCBIgtQIDYAgSILUCA2AIEiC1AgNgCBDjUMIcOvWpj5/iKz2ZeeAwMxoW/cFvn+K6OMU+2AAFiCxAgtgABYgsQILYAAWILECC2AAED3Wc7tHJlcc70gQODvOWceexXNnWOn/4TDxWvseKqud9Hu2X3juKczWs3zPk6+Lah8fHinOmDB/u+T+nv3t97b4ZWrOj/Ih1/rZ5sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiCgp0MNzcoV1bErLm0dX3Z994t1F5M1H97aPeHDmXWU2Li+8AziwMJM+LsfrOlDh+b0+p5sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQJ62mdbHzi0pPbSwkI1eVX7fvZnjNzgs/iMPW/tftl/VVXV6R8v7J2fY55sAQLEFiBAbAECxBYgQGwBAsQWIEBsAQLEFiCgbppm5pPrem9VVbvmbjksUuuaplk934tI8TmgQ+tnoafYAjA7fowAECC2AAFiCxAgtgABYgsQILYAAWILECC2AAFiCxDw/wAtI/Gc0nlwsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 460.8x3456 with 52 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"PMTOnly_PI_22k_RANDOM-improvement-val-acc_0.93.model\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "\n",
    "\n",
    "\n",
    "ixs = [0]\n",
    "\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(XTest[9:10])\n",
    "print(feature_maps.shape)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "\n",
    "#for fmap in feature_maps:\n",
    "ix = 1\n",
    "a=130\n",
    "for _ in range(a):\n",
    "    for _ in range(a):\n",
    "        if ix==a+1:\n",
    "            break\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(26, 2, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='viridis')\n",
    "        ix += 1\n",
    "    # show the figure\n",
    "#pyplot.savefig(\"PMT layer0 ALL Conv ElectronEvent9.jpg\",format =\"jpg\", bbox_inches='tight')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
